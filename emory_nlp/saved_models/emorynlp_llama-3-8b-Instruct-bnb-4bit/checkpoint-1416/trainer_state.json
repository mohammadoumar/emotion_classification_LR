{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 3.0,
  "eval_steps": 500,
  "global_step": 1416,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0211864406779661,
      "grad_norm": 2.16382098197937,
      "learning_rate": 3.1690140845070423e-06,
      "loss": 0.7325,
      "step": 10
    },
    {
      "epoch": 0.0423728813559322,
      "grad_norm": 0.8504422903060913,
      "learning_rate": 6.690140845070423e-06,
      "loss": 0.2334,
      "step": 20
    },
    {
      "epoch": 0.0635593220338983,
      "grad_norm": 0.503600537776947,
      "learning_rate": 1.0211267605633803e-05,
      "loss": 0.19,
      "step": 30
    },
    {
      "epoch": 0.0847457627118644,
      "grad_norm": 0.5568838715553284,
      "learning_rate": 1.3732394366197184e-05,
      "loss": 0.2063,
      "step": 40
    },
    {
      "epoch": 0.1059322033898305,
      "grad_norm": 0.8346989154815674,
      "learning_rate": 1.7253521126760565e-05,
      "loss": 0.1957,
      "step": 50
    },
    {
      "epoch": 0.1271186440677966,
      "grad_norm": 0.5736830234527588,
      "learning_rate": 2.0774647887323944e-05,
      "loss": 0.1959,
      "step": 60
    },
    {
      "epoch": 0.1483050847457627,
      "grad_norm": 0.620027482509613,
      "learning_rate": 2.4295774647887327e-05,
      "loss": 0.1797,
      "step": 70
    },
    {
      "epoch": 0.1694915254237288,
      "grad_norm": 0.9517810344696045,
      "learning_rate": 2.7816901408450706e-05,
      "loss": 0.2185,
      "step": 80
    },
    {
      "epoch": 0.1906779661016949,
      "grad_norm": 1.1107066869735718,
      "learning_rate": 3.133802816901409e-05,
      "loss": 0.2159,
      "step": 90
    },
    {
      "epoch": 0.211864406779661,
      "grad_norm": 1.4502266645431519,
      "learning_rate": 3.485915492957747e-05,
      "loss": 0.1985,
      "step": 100
    },
    {
      "epoch": 0.2330508474576271,
      "grad_norm": 1.713942050933838,
      "learning_rate": 3.8380281690140847e-05,
      "loss": 0.1834,
      "step": 110
    },
    {
      "epoch": 0.2542372881355932,
      "grad_norm": 1.7780897617340088,
      "learning_rate": 4.1901408450704226e-05,
      "loss": 0.2087,
      "step": 120
    },
    {
      "epoch": 0.2754237288135593,
      "grad_norm": 1.1273541450500488,
      "learning_rate": 4.542253521126761e-05,
      "loss": 0.1792,
      "step": 130
    },
    {
      "epoch": 0.2966101694915254,
      "grad_norm": 1.2208482027053833,
      "learning_rate": 4.894366197183099e-05,
      "loss": 0.1968,
      "step": 140
    },
    {
      "epoch": 0.3177966101694915,
      "grad_norm": 1.0894114971160889,
      "learning_rate": 4.999627560102124e-05,
      "loss": 0.1975,
      "step": 150
    },
    {
      "epoch": 0.3389830508474576,
      "grad_norm": 0.8633601069450378,
      "learning_rate": 4.9978036318183895e-05,
      "loss": 0.2037,
      "step": 160
    },
    {
      "epoch": 0.3601694915254237,
      "grad_norm": 0.4632180631160736,
      "learning_rate": 4.994460915472571e-05,
      "loss": 0.2008,
      "step": 170
    },
    {
      "epoch": 0.3813559322033898,
      "grad_norm": 0.7815601229667664,
      "learning_rate": 4.989601443601419e-05,
      "loss": 0.1852,
      "step": 180
    },
    {
      "epoch": 0.4025423728813559,
      "grad_norm": 1.205528974533081,
      "learning_rate": 4.983228171004013e-05,
      "loss": 0.1782,
      "step": 190
    },
    {
      "epoch": 0.423728813559322,
      "grad_norm": 1.0448170900344849,
      "learning_rate": 4.975344972945089e-05,
      "loss": 0.1899,
      "step": 200
    },
    {
      "epoch": 0.4449152542372881,
      "grad_norm": 1.1283148527145386,
      "learning_rate": 4.965956642798694e-05,
      "loss": 0.1891,
      "step": 210
    },
    {
      "epoch": 0.4661016949152542,
      "grad_norm": 1.3092749118804932,
      "learning_rate": 4.955068889133576e-05,
      "loss": 0.1869,
      "step": 220
    },
    {
      "epoch": 0.4872881355932203,
      "grad_norm": 1.5063726902008057,
      "learning_rate": 4.94268833224209e-05,
      "loss": 0.2013,
      "step": 230
    },
    {
      "epoch": 0.5084745762711864,
      "grad_norm": 0.7235831618309021,
      "learning_rate": 4.9288225001147366e-05,
      "loss": 0.1852,
      "step": 240
    },
    {
      "epoch": 0.5296610169491526,
      "grad_norm": 0.5387002229690552,
      "learning_rate": 4.913479823862766e-05,
      "loss": 0.193,
      "step": 250
    },
    {
      "epoch": 0.5508474576271186,
      "grad_norm": 1.0723316669464111,
      "learning_rate": 4.8966696325916515e-05,
      "loss": 0.1714,
      "step": 260
    },
    {
      "epoch": 0.5720338983050848,
      "grad_norm": 1.3897992372512817,
      "learning_rate": 4.878402147728531e-05,
      "loss": 0.1871,
      "step": 270
    },
    {
      "epoch": 0.5932203389830508,
      "grad_norm": 1.2169944047927856,
      "learning_rate": 4.8586884768070696e-05,
      "loss": 0.1818,
      "step": 280
    },
    {
      "epoch": 0.614406779661017,
      "grad_norm": 1.1954553127288818,
      "learning_rate": 4.837540606713538e-05,
      "loss": 0.1878,
      "step": 290
    },
    {
      "epoch": 0.635593220338983,
      "grad_norm": 1.5451010465621948,
      "learning_rate": 4.814971396398184e-05,
      "loss": 0.1945,
      "step": 300
    },
    {
      "epoch": 0.6567796610169492,
      "grad_norm": 1.4715031385421753,
      "learning_rate": 4.790994569056364e-05,
      "loss": 0.1956,
      "step": 310
    },
    {
      "epoch": 0.6779661016949152,
      "grad_norm": 1.1248723268508911,
      "learning_rate": 4.7656247037841564e-05,
      "loss": 0.1987,
      "step": 320
    },
    {
      "epoch": 0.6991525423728814,
      "grad_norm": 0.5973359942436218,
      "learning_rate": 4.7388772267135516e-05,
      "loss": 0.1949,
      "step": 330
    },
    {
      "epoch": 0.7203389830508474,
      "grad_norm": 1.4420369863510132,
      "learning_rate": 4.710768401632604e-05,
      "loss": 0.1911,
      "step": 340
    },
    {
      "epoch": 0.7415254237288136,
      "grad_norm": 1.6980040073394775,
      "learning_rate": 4.6813153200962394e-05,
      "loss": 0.1986,
      "step": 350
    },
    {
      "epoch": 0.7627118644067796,
      "grad_norm": 0.6872005462646484,
      "learning_rate": 4.650535891033752e-05,
      "loss": 0.1885,
      "step": 360
    },
    {
      "epoch": 0.7838983050847458,
      "grad_norm": 0.8075728416442871,
      "learning_rate": 4.618448829859284e-05,
      "loss": 0.1872,
      "step": 370
    },
    {
      "epoch": 0.8050847457627118,
      "grad_norm": 1.7134411334991455,
      "learning_rate": 4.5850736470919334e-05,
      "loss": 0.1789,
      "step": 380
    },
    {
      "epoch": 0.826271186440678,
      "grad_norm": 0.6594101786613464,
      "learning_rate": 4.55043063649239e-05,
      "loss": 0.1804,
      "step": 390
    },
    {
      "epoch": 0.847457627118644,
      "grad_norm": 0.8162625432014465,
      "learning_rate": 4.5145408627233296e-05,
      "loss": 0.1999,
      "step": 400
    },
    {
      "epoch": 0.8686440677966102,
      "grad_norm": 1.2154138088226318,
      "learning_rate": 4.4774261485410556e-05,
      "loss": 0.1876,
      "step": 410
    },
    {
      "epoch": 0.8898305084745762,
      "grad_norm": 1.3310710191726685,
      "learning_rate": 4.439109061526186e-05,
      "loss": 0.1932,
      "step": 420
    },
    {
      "epoch": 0.9110169491525424,
      "grad_norm": 1.423527717590332,
      "learning_rate": 4.3996129003614476e-05,
      "loss": 0.1896,
      "step": 430
    },
    {
      "epoch": 0.9322033898305084,
      "grad_norm": 1.9322710037231445,
      "learning_rate": 4.358961680664925e-05,
      "loss": 0.1809,
      "step": 440
    },
    {
      "epoch": 0.9533898305084746,
      "grad_norm": 0.967793881893158,
      "learning_rate": 4.317180120387382e-05,
      "loss": 0.1928,
      "step": 450
    },
    {
      "epoch": 0.9745762711864406,
      "grad_norm": 0.7025838494300842,
      "learning_rate": 4.27429362478252e-05,
      "loss": 0.1644,
      "step": 460
    },
    {
      "epoch": 0.9957627118644068,
      "grad_norm": 0.8750977516174316,
      "learning_rate": 4.2303282709593316e-05,
      "loss": 0.1849,
      "step": 470
    },
    {
      "epoch": 1.0169491525423728,
      "grad_norm": 1.582013487815857,
      "learning_rate": 4.185310792025929e-05,
      "loss": 0.1705,
      "step": 480
    },
    {
      "epoch": 1.0381355932203389,
      "grad_norm": 1.63051438331604,
      "learning_rate": 4.1392685608344924e-05,
      "loss": 0.172,
      "step": 490
    },
    {
      "epoch": 1.0593220338983051,
      "grad_norm": 2.2374136447906494,
      "learning_rate": 4.092229573337223e-05,
      "loss": 0.1716,
      "step": 500
    },
    {
      "epoch": 1.0805084745762712,
      "grad_norm": 0.5464533567428589,
      "learning_rate": 4.044222431563422e-05,
      "loss": 0.1704,
      "step": 510
    },
    {
      "epoch": 1.1016949152542372,
      "grad_norm": 0.8549949526786804,
      "learning_rate": 3.9952763262280405e-05,
      "loss": 0.1619,
      "step": 520
    },
    {
      "epoch": 1.1228813559322033,
      "grad_norm": 0.8032296299934387,
      "learning_rate": 3.945421018982283e-05,
      "loss": 0.1665,
      "step": 530
    },
    {
      "epoch": 1.1440677966101696,
      "grad_norm": 0.7204502820968628,
      "learning_rate": 3.894686824317053e-05,
      "loss": 0.164,
      "step": 540
    },
    {
      "epoch": 1.1652542372881356,
      "grad_norm": 1.0121608972549438,
      "learning_rate": 3.843104591130245e-05,
      "loss": 0.1507,
      "step": 550
    },
    {
      "epoch": 1.1864406779661016,
      "grad_norm": 0.7410672903060913,
      "learning_rate": 3.7907056839690894e-05,
      "loss": 0.1613,
      "step": 560
    },
    {
      "epoch": 1.207627118644068,
      "grad_norm": 0.6087560057640076,
      "learning_rate": 3.7375219639589536e-05,
      "loss": 0.1711,
      "step": 570
    },
    {
      "epoch": 1.228813559322034,
      "grad_norm": 0.9597582817077637,
      "learning_rate": 3.683585769430211e-05,
      "loss": 0.1546,
      "step": 580
    },
    {
      "epoch": 1.25,
      "grad_norm": 0.5835078954696655,
      "learning_rate": 3.6289298962549315e-05,
      "loss": 0.1518,
      "step": 590
    },
    {
      "epoch": 1.271186440677966,
      "grad_norm": 1.5029398202896118,
      "learning_rate": 3.5735875779053704e-05,
      "loss": 0.1514,
      "step": 600
    },
    {
      "epoch": 1.292372881355932,
      "grad_norm": 2.2837607860565186,
      "learning_rate": 3.5175924652463796e-05,
      "loss": 0.1669,
      "step": 610
    },
    {
      "epoch": 1.3135593220338984,
      "grad_norm": 0.6411810517311096,
      "learning_rate": 3.460978606074013e-05,
      "loss": 0.17,
      "step": 620
    },
    {
      "epoch": 1.3347457627118644,
      "grad_norm": 0.5866168737411499,
      "learning_rate": 3.403780424412789e-05,
      "loss": 0.1654,
      "step": 630
    },
    {
      "epoch": 1.3559322033898304,
      "grad_norm": 0.9238085150718689,
      "learning_rate": 3.346032699584176e-05,
      "loss": 0.1562,
      "step": 640
    },
    {
      "epoch": 1.3771186440677967,
      "grad_norm": 0.9361531138420105,
      "learning_rate": 3.2877705450590526e-05,
      "loss": 0.1506,
      "step": 650
    },
    {
      "epoch": 1.3983050847457628,
      "grad_norm": 0.9090725779533386,
      "learning_rate": 3.229029387106974e-05,
      "loss": 0.1723,
      "step": 660
    },
    {
      "epoch": 1.4194915254237288,
      "grad_norm": 1.0473958253860474,
      "learning_rate": 3.1698449432552596e-05,
      "loss": 0.1705,
      "step": 670
    },
    {
      "epoch": 1.4406779661016949,
      "grad_norm": 1.3955954313278198,
      "learning_rate": 3.110253200570961e-05,
      "loss": 0.1745,
      "step": 680
    },
    {
      "epoch": 1.461864406779661,
      "grad_norm": 1.2433849573135376,
      "learning_rate": 3.0502903937789523e-05,
      "loss": 0.1649,
      "step": 690
    },
    {
      "epoch": 1.4830508474576272,
      "grad_norm": 1.466511845588684,
      "learning_rate": 2.989992983229424e-05,
      "loss": 0.1622,
      "step": 700
    },
    {
      "epoch": 1.5042372881355932,
      "grad_norm": 1.2857203483581543,
      "learning_rate": 2.9293976327281908e-05,
      "loss": 0.1558,
      "step": 710
    },
    {
      "epoch": 1.5254237288135593,
      "grad_norm": 1.1856653690338135,
      "learning_rate": 2.8685411872432804e-05,
      "loss": 0.1584,
      "step": 720
    },
    {
      "epoch": 1.5466101694915255,
      "grad_norm": 1.3768813610076904,
      "learning_rate": 2.80746065050138e-05,
      "loss": 0.1647,
      "step": 730
    },
    {
      "epoch": 1.5677966101694916,
      "grad_norm": 0.7709767818450928,
      "learning_rate": 2.746193162487733e-05,
      "loss": 0.1559,
      "step": 740
    },
    {
      "epoch": 1.5889830508474576,
      "grad_norm": 0.7216262817382812,
      "learning_rate": 2.684775976863196e-05,
      "loss": 0.1614,
      "step": 750
    },
    {
      "epoch": 1.6101694915254239,
      "grad_norm": 0.6918421387672424,
      "learning_rate": 2.623246438312174e-05,
      "loss": 0.1464,
      "step": 760
    },
    {
      "epoch": 1.6313559322033897,
      "grad_norm": 0.9564958810806274,
      "learning_rate": 2.5616419598352042e-05,
      "loss": 0.1461,
      "step": 770
    },
    {
      "epoch": 1.652542372881356,
      "grad_norm": 0.6446188688278198,
      "learning_rate": 2.5e-05,
      "loss": 0.1571,
      "step": 780
    },
    {
      "epoch": 1.673728813559322,
      "grad_norm": 0.6817750930786133,
      "learning_rate": 2.4383580401647963e-05,
      "loss": 0.1503,
      "step": 790
    },
    {
      "epoch": 1.694915254237288,
      "grad_norm": 0.8393815159797668,
      "learning_rate": 2.376753561687826e-05,
      "loss": 0.1555,
      "step": 800
    },
    {
      "epoch": 1.7161016949152543,
      "grad_norm": 0.8914211392402649,
      "learning_rate": 2.315224023136804e-05,
      "loss": 0.1754,
      "step": 810
    },
    {
      "epoch": 1.7372881355932204,
      "grad_norm": 0.6611917018890381,
      "learning_rate": 2.253806837512268e-05,
      "loss": 0.1684,
      "step": 820
    },
    {
      "epoch": 1.7584745762711864,
      "grad_norm": 1.2838358879089355,
      "learning_rate": 2.1925393494986203e-05,
      "loss": 0.1585,
      "step": 830
    },
    {
      "epoch": 1.7796610169491527,
      "grad_norm": 0.6464642286300659,
      "learning_rate": 2.13145881275672e-05,
      "loss": 0.1596,
      "step": 840
    },
    {
      "epoch": 1.8008474576271185,
      "grad_norm": 1.3192753791809082,
      "learning_rate": 2.0706023672718098e-05,
      "loss": 0.1562,
      "step": 850
    },
    {
      "epoch": 1.8220338983050848,
      "grad_norm": 0.5857508778572083,
      "learning_rate": 2.0100070167705763e-05,
      "loss": 0.1424,
      "step": 860
    },
    {
      "epoch": 1.8432203389830508,
      "grad_norm": 0.5429173707962036,
      "learning_rate": 1.9497096062210486e-05,
      "loss": 0.1408,
      "step": 870
    },
    {
      "epoch": 1.8644067796610169,
      "grad_norm": 0.4755130708217621,
      "learning_rate": 1.88974679942904e-05,
      "loss": 0.1656,
      "step": 880
    },
    {
      "epoch": 1.8855932203389831,
      "grad_norm": 0.5120792388916016,
      "learning_rate": 1.8301550567447413e-05,
      "loss": 0.1556,
      "step": 890
    },
    {
      "epoch": 1.9067796610169492,
      "grad_norm": 0.6687507033348083,
      "learning_rate": 1.7709706128930267e-05,
      "loss": 0.1555,
      "step": 900
    },
    {
      "epoch": 1.9279661016949152,
      "grad_norm": 0.7331075668334961,
      "learning_rate": 1.7122294549409484e-05,
      "loss": 0.1626,
      "step": 910
    },
    {
      "epoch": 1.9491525423728815,
      "grad_norm": 0.7339685559272766,
      "learning_rate": 1.653967300415824e-05,
      "loss": 0.1421,
      "step": 920
    },
    {
      "epoch": 1.9703389830508473,
      "grad_norm": 0.6010221838951111,
      "learning_rate": 1.596219575587211e-05,
      "loss": 0.1554,
      "step": 930
    },
    {
      "epoch": 1.9915254237288136,
      "grad_norm": 0.6654820442199707,
      "learning_rate": 1.5390213939259862e-05,
      "loss": 0.1585,
      "step": 940
    },
    {
      "epoch": 2.01271186440678,
      "grad_norm": 0.5928749442100525,
      "learning_rate": 1.4824075347536206e-05,
      "loss": 0.1265,
      "step": 950
    },
    {
      "epoch": 2.0338983050847457,
      "grad_norm": 1.8974438905715942,
      "learning_rate": 1.4264124220946295e-05,
      "loss": 0.0913,
      "step": 960
    },
    {
      "epoch": 2.055084745762712,
      "grad_norm": 0.8196704983711243,
      "learning_rate": 1.3710701037450693e-05,
      "loss": 0.0813,
      "step": 970
    },
    {
      "epoch": 2.0762711864406778,
      "grad_norm": 0.959924042224884,
      "learning_rate": 1.3164142305697885e-05,
      "loss": 0.1021,
      "step": 980
    },
    {
      "epoch": 2.097457627118644,
      "grad_norm": 1.0786346197128296,
      "learning_rate": 1.2624780360410466e-05,
      "loss": 0.0897,
      "step": 990
    },
    {
      "epoch": 2.1186440677966103,
      "grad_norm": 1.0830817222595215,
      "learning_rate": 1.209294316030912e-05,
      "loss": 0.0993,
      "step": 1000
    },
    {
      "epoch": 2.139830508474576,
      "grad_norm": 1.4085960388183594,
      "learning_rate": 1.156895408869756e-05,
      "loss": 0.0906,
      "step": 1010
    },
    {
      "epoch": 2.1610169491525424,
      "grad_norm": 0.9150258302688599,
      "learning_rate": 1.1053131756829477e-05,
      "loss": 0.1042,
      "step": 1020
    },
    {
      "epoch": 2.1822033898305087,
      "grad_norm": 1.6733375787734985,
      "learning_rate": 1.0545789810177185e-05,
      "loss": 0.0954,
      "step": 1030
    },
    {
      "epoch": 2.2033898305084745,
      "grad_norm": 0.9662704467773438,
      "learning_rate": 1.0047236737719601e-05,
      "loss": 0.0808,
      "step": 1040
    },
    {
      "epoch": 2.2245762711864407,
      "grad_norm": 1.2049448490142822,
      "learning_rate": 9.557775684365782e-06,
      "loss": 0.0949,
      "step": 1050
    },
    {
      "epoch": 2.2457627118644066,
      "grad_norm": 1.293367624282837,
      "learning_rate": 9.077704266627776e-06,
      "loss": 0.1035,
      "step": 1060
    },
    {
      "epoch": 2.266949152542373,
      "grad_norm": 0.5412729382514954,
      "learning_rate": 8.60731439165508e-06,
      "loss": 0.084,
      "step": 1070
    },
    {
      "epoch": 2.288135593220339,
      "grad_norm": 1.191385269165039,
      "learning_rate": 8.146892079740716e-06,
      "loss": 0.1091,
      "step": 1080
    },
    {
      "epoch": 2.309322033898305,
      "grad_norm": 1.1243044137954712,
      "learning_rate": 7.696717290406686e-06,
      "loss": 0.0845,
      "step": 1090
    },
    {
      "epoch": 2.330508474576271,
      "grad_norm": 1.509069561958313,
      "learning_rate": 7.257063752174803e-06,
      "loss": 0.0715,
      "step": 1100
    },
    {
      "epoch": 2.3516949152542375,
      "grad_norm": 1.070255160331726,
      "learning_rate": 6.828198796126179e-06,
      "loss": 0.0962,
      "step": 1110
    },
    {
      "epoch": 2.3728813559322033,
      "grad_norm": 3.229994297027588,
      "learning_rate": 6.4103831933507495e-06,
      "loss": 0.1005,
      "step": 1120
    },
    {
      "epoch": 2.3940677966101696,
      "grad_norm": 1.2808297872543335,
      "learning_rate": 6.003870996385533e-06,
      "loss": 0.084,
      "step": 1130
    },
    {
      "epoch": 2.415254237288136,
      "grad_norm": 1.3805601596832275,
      "learning_rate": 5.608909384738148e-06,
      "loss": 0.0752,
      "step": 1140
    },
    {
      "epoch": 2.4364406779661016,
      "grad_norm": 0.46184080839157104,
      "learning_rate": 5.2257385145894455e-06,
      "loss": 0.0671,
      "step": 1150
    },
    {
      "epoch": 2.457627118644068,
      "grad_norm": 1.753792643547058,
      "learning_rate": 4.854591372766709e-06,
      "loss": 0.1041,
      "step": 1160
    },
    {
      "epoch": 2.4788135593220337,
      "grad_norm": 2.30066180229187,
      "learning_rate": 4.495693635076101e-06,
      "loss": 0.0842,
      "step": 1170
    },
    {
      "epoch": 2.5,
      "grad_norm": 1.2960937023162842,
      "learning_rate": 4.149263529080672e-06,
      "loss": 0.0894,
      "step": 1180
    },
    {
      "epoch": 2.5211864406779663,
      "grad_norm": 1.0783286094665527,
      "learning_rate": 3.8155117014071655e-06,
      "loss": 0.0952,
      "step": 1190
    },
    {
      "epoch": 2.542372881355932,
      "grad_norm": 0.8394535183906555,
      "learning_rate": 3.4946410896624817e-06,
      "loss": 0.0932,
      "step": 1200
    },
    {
      "epoch": 2.5635593220338984,
      "grad_norm": 1.0285762548446655,
      "learning_rate": 3.18684679903761e-06,
      "loss": 0.0756,
      "step": 1210
    },
    {
      "epoch": 2.584745762711864,
      "grad_norm": 0.9855713844299316,
      "learning_rate": 2.892315983673963e-06,
      "loss": 0.0772,
      "step": 1220
    },
    {
      "epoch": 2.6059322033898304,
      "grad_norm": 1.0728801488876343,
      "learning_rate": 2.6112277328644826e-06,
      "loss": 0.0749,
      "step": 1230
    },
    {
      "epoch": 2.6271186440677967,
      "grad_norm": 0.8160986304283142,
      "learning_rate": 2.343752962158441e-06,
      "loss": 0.0683,
      "step": 1240
    },
    {
      "epoch": 2.648305084745763,
      "grad_norm": 1.6286308765411377,
      "learning_rate": 2.090054309436368e-06,
      "loss": 0.0971,
      "step": 1250
    },
    {
      "epoch": 2.669491525423729,
      "grad_norm": 1.0557900667190552,
      "learning_rate": 1.8502860360181672e-06,
      "loss": 0.0643,
      "step": 1260
    },
    {
      "epoch": 2.690677966101695,
      "grad_norm": 0.8137847781181335,
      "learning_rate": 1.624593932864632e-06,
      "loss": 0.0766,
      "step": 1270
    },
    {
      "epoch": 2.711864406779661,
      "grad_norm": 1.8660392761230469,
      "learning_rate": 1.4131152319293046e-06,
      "loss": 0.0824,
      "step": 1280
    },
    {
      "epoch": 2.733050847457627,
      "grad_norm": 1.5504869222640991,
      "learning_rate": 1.2159785227146975e-06,
      "loss": 0.0754,
      "step": 1290
    },
    {
      "epoch": 2.7542372881355934,
      "grad_norm": 1.3376381397247314,
      "learning_rate": 1.0333036740834856e-06,
      "loss": 0.092,
      "step": 1300
    },
    {
      "epoch": 2.7754237288135593,
      "grad_norm": 1.0906480550765991,
      "learning_rate": 8.652017613723473e-07,
      "loss": 0.0895,
      "step": 1310
    },
    {
      "epoch": 2.7966101694915255,
      "grad_norm": 0.7495090365409851,
      "learning_rate": 7.117749988526379e-07,
      "loss": 0.087,
      "step": 1320
    },
    {
      "epoch": 2.8177966101694913,
      "grad_norm": 2.242699146270752,
      "learning_rate": 5.731166775790969e-07,
      "loss": 0.0862,
      "step": 1330
    },
    {
      "epoch": 2.8389830508474576,
      "grad_norm": 1.35897696018219,
      "learning_rate": 4.4931110866424375e-07,
      "loss": 0.0929,
      "step": 1340
    },
    {
      "epoch": 2.860169491525424,
      "grad_norm": 2.2922046184539795,
      "learning_rate": 3.40433572013063e-07,
      "loss": 0.0914,
      "step": 1350
    },
    {
      "epoch": 2.8813559322033897,
      "grad_norm": 1.6006914377212524,
      "learning_rate": 2.465502705491174e-07,
      "loss": 0.0862,
      "step": 1360
    },
    {
      "epoch": 2.902542372881356,
      "grad_norm": 0.6247040033340454,
      "learning_rate": 1.6771828995987559e-07,
      "loss": 0.0943,
      "step": 1370
    },
    {
      "epoch": 2.923728813559322,
      "grad_norm": 1.343962550163269,
      "learning_rate": 1.0398556398580894e-07,
      "loss": 0.0875,
      "step": 1380
    },
    {
      "epoch": 2.944915254237288,
      "grad_norm": 1.2565511465072632,
      "learning_rate": 5.539084527429461e-08,
      "loss": 0.0786,
      "step": 1390
    },
    {
      "epoch": 2.9661016949152543,
      "grad_norm": 3.2273571491241455,
      "learning_rate": 2.1963681816106198e-08,
      "loss": 0.0809,
      "step": 1400
    },
    {
      "epoch": 2.9872881355932206,
      "grad_norm": 0.7193903923034668,
      "learning_rate": 3.7243989787633105e-09,
      "loss": 0.0807,
      "step": 1410
    }
  ],
  "logging_steps": 10,
  "max_steps": 1416,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 15000,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.96680929081557e+17,
  "train_batch_size": 2,
  "trial_name": null,
  "trial_params": null
}
