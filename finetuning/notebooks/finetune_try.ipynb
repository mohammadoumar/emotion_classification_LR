{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "from sklearn.metrics import classification_report\n",
    "#from utils.post_processing import post_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    assert torch.cuda.is_available() is True\n",
    "    \n",
    "except AssertionError:\n",
    "    \n",
    "    print(\"Please set up a GPU before using LLaMA Factory...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR = Path.cwd()\n",
    "FT_DIR = CURRENT_DIR / \"emotion_analysis_comics\" / \"finetuning\"\n",
    "DATASET_DIR = CURRENT_DIR / \"emotion_analysis_comics\" / \"finetuning\" / \"datasets\"\n",
    "\n",
    "ERC_DIR = FT_DIR.parent\n",
    "LLAMA_FACTORY_DIR = ERC_DIR / \"LLaMA-Factory\"\n",
    "\n",
    "BASE_MODEL = \"unsloth/Llama-3.2-3B-Instruct-bnb-4bit\"\n",
    "LOGGING_DIR = FT_DIR / \"training_logs\"\n",
    "OUTPUT_DIR = FT_DIR / \"saved_models\" / f\"\"\"comics35_pg_nb_{BASE_MODEL.split(\"/\")[1]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = CURRENT_DIR / \"emotion_analysis_comics\" / \"finetuning\" / \"datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_name = f\"\"\"comics35_utterance_pg_train.json\"\"\"\n",
    "test_dataset_name = f\"\"\"comics35_utterance_pg_test.json\"\"\"\n",
    "\n",
    "train_dataset_file = DATASET_DIR / train_dataset_name\n",
    "test_dataset_file = DATASET_DIR / test_dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/datasets/comics35_utterance_pg_train.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(os.path.join(FT_DIR, \"model_args\")):\n",
    "    os.mkdir(os.path.join(FT_DIR, \"model_args\"))\n",
    "\n",
    "train_file = FT_DIR / \"model_args\" / f\"\"\"{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{BASE_MODEL.split(\"/\")[1]}.json\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info_line =  {\n",
    "  \"file_name\": f\"{train_dataset_file}\",\n",
    "  \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "\n",
    "data[\"comics\"] = dataset_info_line\n",
    "\n",
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(\n",
    "    \n",
    "  stage=\"sft\",                           # do supervised fine-tuning\n",
    "  do_train=True,\n",
    "\n",
    "  model_name_or_path=BASE_MODEL,         # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  num_train_epochs=NB_EPOCHS,            # the epochs of training\n",
    "  output_dir=str(OUTPUT_DIR),                 # the path to save LoRA adapters\n",
    "  overwrite_output_dir=True,             # overrides existing output contents\n",
    "\n",
    "  dataset=\"comics\",                      # dataset name\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  #train_on_prompt=True,\n",
    "  val_size=0.1,\n",
    "  max_samples=10000,                       # use 500 examples in each dataset\n",
    "\n",
    "  finetuning_type=\"lora\",                # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  per_device_train_batch_size=2,         # the batch size\n",
    "  gradient_accumulation_steps=4,         # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",            # use cosine learning rate scheduler\n",
    "  loraplus_lr_ratio=16.0,                # use LoRA+ algorithm with lambda=16.0\n",
    "  #temperature=0.5,\n",
    "  \n",
    "  warmup_ratio=0.1,                      # use warmup scheduler    \n",
    "  learning_rate=5e-5,                    # the learning rate\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  \n",
    "  fp16=True,                             # use float16 mixed precision training\n",
    "  quantization_bit=4,                    # use 4-bit QLoRA  \n",
    "  #use_liger_kernel=True,\n",
    "  #quantization_device_map=\"auto\",\n",
    "  \n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  save_steps=5000,                       # save checkpoint every 1000 steps    \n",
    "  logging_dir=str(LOGGING_DIR),\n",
    "  \n",
    "  # use_unsloth=True,\n",
    "  report_to=\"tensorboard\"                       # discards wandb\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(args, open(train_file, \"w\", encoding=\"utf-8\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen([\"llamafactory-cli\", \"train\", train_file], cwd=LLAMA_FACTORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:20:34 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:29541\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1108 10:20:35.876000 139748068037952 torch/distributed/run.py:779] \n",
      "W1108 10:20:35.876000 139748068037952 torch/distributed/run.py:779] *****************************************\n",
      "W1108 10:20:35.876000 139748068037952 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1108 10:20:35.876000 139748068037952 torch/distributed/run.py:779] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:20:53 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "11/08/2024 10:20:53 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "11/08/2024 10:20:53 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:672] 2024-11-08 10:20:53,702 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/config.json\n",
      "[INFO|configuration_utils.py:739] 2024-11-08 10:20:53,703 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:20:53 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "11/08/2024 10:20:53 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "11/08/2024 10:20:53 - INFO - llamafactory.hparams.parser - Process rank: 3, device: cuda:3, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "11/08/2024 10:20:53 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "11/08/2024 10:20:53 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "11/08/2024 10:20:53 - INFO - llamafactory.hparams.parser - Process rank: 4, device: cuda:4, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "11/08/2024 10:20:53 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "11/08/2024 10:20:53 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "11/08/2024 10:20:53 - INFO - llamafactory.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "11/08/2024 10:20:53 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "11/08/2024 10:20:53 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "11/08/2024 10:20:53 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:20:53,977 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:20:53,977 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:20:53,977 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:20:53,977 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:20:53,977 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2478] 2024-11-08 10:20:54,781 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:672] 2024-11-08 10:20:55,389 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/config.json\n",
      "[INFO|configuration_utils.py:739] 2024-11-08 10:20:55,390 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:20:55,520 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:20:55,520 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:20:55,520 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:20:55,520 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:20:55,520 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2478] 2024-11-08 10:20:56,003 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:20:56 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "11/08/2024 10:20:56 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/datasets/comics35_utterance_pg_train.json...\n",
      "11/08/2024 10:20:57 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "11/08/2024 10:20:58 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "11/08/2024 10:20:58 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "11/08/2024 10:20:58 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "11/08/2024 10:21:01 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/datasets/comics35_utterance_pg_train.json...\n",
      "11/08/2024 10:21:01 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/datasets/comics35_utterance_pg_train.json...\n",
      "11/08/2024 10:21:01 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/datasets/comics35_utterance_pg_train.json...\n",
      "11/08/2024 10:21:01 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/datasets/comics35_utterance_pg_train.json...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 718/718 [00:02<00:00, 332.57 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 14711, 5867, 6082, 18825, 33257, 15766, 271, 2675, 527, 459, 11084, 20356, 6492, 6335, 58394, 304, 20303, 2363, 21976, 23692, 13, 4718, 3465, 374, 311, 24564, 22256, 3095, 323, 10765, 872, 14604, 2262, 382, 30521, 512, 12, 1472, 690, 5371, 264, 1160, 315, 22256, 3095, 505, 264, 2199, 304, 264, 20303, 2363, 198, 12, 578, 22256, 685, 1253, 3237, 832, 477, 5361, 21958, 271, 66913, 512, 16, 13, 10852, 3725, 24564, 279, 14604, 2317, 323, 16630, 315, 1855, 22256, 685, 304, 279, 2199, 198, 17, 13, 65647, 8581, 21958, 505, 279, 2768, 6989, 512, 256, 330, 4091, 498, 330, 4338, 70, 592, 498, 330, 69, 686, 498, 330, 83214, 2136, 498, 330, 20370, 9868, 498, 330, 4215, 498, 330, 60668, 702, 18, 13, 1789, 1855, 22256, 685, 304, 264, 20303, 2199, 11, 10765, 682, 21958, 3118, 323, 471, 459, 1358, 315, 20356, 18893, 304, 2015, 382, 93016, 50, 512, 16, 13, 5560, 27785, 279, 9382, 10212, 3485, 198, 17, 13, 9442, 2011, 387, 264, 4823, 449, 3254, 1401, 330, 2964, 62, 6339, 685, 23319, 41356, 702, 18, 13, 5273, 2011, 387, 459, 1358, 1405, 512, 256, 482, 9062, 2449, 374, 459, 1358, 315, 21958, 369, 832, 22256, 685, 198, 256, 482, 7365, 9248, 279, 1988, 22256, 3095, 2015, 198, 256, 482, 29911, 21958, 527, 5535, 824, 22256, 685, 198, 19, 13, 2360, 41941, 11, 1193, 4823, 2612, 271, 99843, 512, 12, 9062, 1358, 2449, 34310, 311, 832, 22256, 685, 198, 12, 3861, 22256, 685, 649, 617, 5361, 21958, 198, 12, 87477, 4839, 43529, 323, 1162, 315, 20356, 9382, 198, 12, 13969, 21958, 304, 18893, 1524, 369, 3254, 21958, 1432, 7184, 24564, 1521, 22256, 3095, 304, 264, 2199, 512, 16, 13, 10245, 650, 3015, 4534, 1753, 99550, 1507, 3247, 50297, 7354, 6483, 50, 3083, 18725, 6570, 6005, 50, 1981, 720, 17, 13, 4696, 8871, 393, 1899, 35457, 18725, 7354, 58376, 2843, 1112, 25832, 1750, 6486, 1981, 3651, 23214, 8871, 7354, 16929, 4716, 61094, 627, 18, 13, 18725, 72297, 4999, 19, 13, 54233, 4999, 20, 13, 11155, 358, 19102, 4276, 6969, 4157, 35, 10245, 29637, 39023, 13398, 10245, 25404, 5257, 36757, 8871, 480, 1308, 12073, 4999, 21, 13, 11947, 984, 386, 33937, 4999, 22, 13, 358, 19102, 7837, 36, 5257, 3501, 48, 313, 4999, 23, 13, 38535, 7233, 24, 13, 1198, 1669, 26336, 30, 4999, 605, 13, 5782, 0, 5782, 0, 128009, 128006, 78191, 128007, 271, 5018, 2964, 62, 6339, 685, 23319, 41356, 794, 79786, 4091, 8073, 4482, 4091, 8073, 4482, 69, 686, 8073, 4482, 69, 686, 8073, 4482, 69, 686, 498, 330, 83214, 2136, 8073, 4482, 83214, 2136, 8073, 4482, 4091, 8073, 4482, 20370, 9868, 8073, 4482, 20370, 9868, 8073, 4482, 69, 686, 498, 330, 20370, 9868, 1365, 14316, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### Emotion Analysis Expert Role\n",
      "\n",
      "You are an advanced emotion analysis expert specializing in comic book dialogue interpretation. Your task is to analyze utterances and identify their emotional content.\n",
      "\n",
      "INPUT:\n",
      "- You will receive a list of utterances from a page in a comic book\n",
      "- The utterance may express one or multiple emotions\n",
      "\n",
      "TASK:\n",
      "1. Carefully analyze the emotional context and tone of each utterance in the page\n",
      "2. Identify applicable emotions from the following classes:\n",
      "   \"anger\", \"disgust\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"neutral\"\n",
      "3. For each utterance in a comic page, identify all emotions present and return an array of emotion arrays in order.\n",
      "\n",
      "RULES:\n",
      "1. Use ONLY the labels listed above\n",
      "2. Output must be a JSON with single key \"page_utterance_emotions\"\n",
      "3. Value must be an array where:\n",
      "   - Each element is an array of emotions for one utterance\n",
      "   - Order matches the input utterances order\n",
      "   - Multiple emotions are allowed per utterance\n",
      "4. No explanations, only JSON output\n",
      "\n",
      "IMPORTANT:\n",
      "- Each array element corresponds to one utterance\n",
      "- One utterance can have multiple emotions\n",
      "- Maintain exact spelling and case of emotion labels\n",
      "- Keep emotions in arrays even for single emotions\n",
      "\n",
      "\n",
      "Now analyze these utterances in a page:\n",
      "1. THIS VILE THING ATTACKED THE SMALL BEASTS OF MY SHORES… \n",
      "2. … IT PUNCHED MY BEAUTIFUL MATILDA… AND NOW IT BEGS FOR LIFE.\n",
      "3. MY MASTER!\n",
      "4. PLEASE!\n",
      "5. BUT I HAVE NOT CHASED THIS MONSTER ALL THIS WAY TO LET IT GROVEL!\n",
      "6. HEAL MEEE!\n",
      "7. I HAVE COME TO CONQ--!\n",
      "8. WHAT--\n",
      "9. --IS THAT?!\n",
      "10. NO! NO!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"page_utterance_emotions\": [[\"anger\"], [\"anger\"], [\"fear\"], [\"fear\"], [\"fear\", \"sadness\"], [\"sadness\"], [\"anger\"], [\"surprise\"], [\"surprise\"], [\"fear\", \"surprise\"]]}<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 2964, 62, 6339, 685, 23319, 41356, 794, 79786, 4091, 8073, 4482, 4091, 8073, 4482, 69, 686, 8073, 4482, 69, 686, 8073, 4482, 69, 686, 498, 330, 83214, 2136, 8073, 4482, 83214, 2136, 8073, 4482, 4091, 8073, 4482, 20370, 9868, 8073, 4482, 20370, 9868, 8073, 4482, 69, 686, 498, 330, 20370, 9868, 1365, 14316, 128009]\n",
      "labels:\n",
      "{\"page_utterance_emotions\": [[\"anger\"], [\"anger\"], [\"fear\"], [\"fear\"], [\"fear\", \"sadness\"], [\"sadness\"], [\"anger\"], [\"surprise\"], [\"surprise\"], [\"fear\", \"surprise\"]]}<|eot_id|>\n",
      "11/08/2024 10:21:03 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "11/08/2024 10:21:03 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:672] 2024-11-08 10:21:03,553 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/config.json\n",
      "[INFO|configuration_utils.py:739] 2024-11-08 10:21:03,554 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[WARNING|quantization_config.py:400] 2024-11-08 10:21:03,706 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3726] 2024-11-08 10:21:04,182 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/model.safetensors\n",
      "[INFO|modeling_utils.py:1622] 2024-11-08 10:21:04,210 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1099] 2024-11-08 10:21:04,214 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"pad_token_id\": 128004\n",
      "}\n",
      "\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:21:04 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "11/08/2024 10:21:04 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "11/08/2024 10:21:04 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "11/08/2024 10:21:04 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "11/08/2024 10:21:04 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "11/08/2024 10:21:04 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "11/08/2024 10:21:04 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "11/08/2024 10:21:04 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:21:16 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "11/08/2024 10:21:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "11/08/2024 10:21:16 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "11/08/2024 10:21:16 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "11/08/2024 10:21:16 - INFO - llamafactory.model.model_utils.misc - Found linear modules: q_proj,up_proj,o_proj,down_proj,v_proj,k_proj,gate_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4568] 2024-11-08 10:21:16,812 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4576] 2024-11-08 10:21:16,813 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1054] 2024-11-08 10:21:16,951 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/generation_config.json\n",
      "[INFO|configuration_utils.py:1099] 2024-11-08 10:21:16,951 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 131072,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:21:17 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "11/08/2024 10:21:17 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "11/08/2024 10:21:17 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "11/08/2024 10:21:17 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "11/08/2024 10:21:17 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,v_proj,q_proj,o_proj,up_proj,gate_proj,k_proj\n",
      "11/08/2024 10:21:17 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "11/08/2024 10:21:17 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "11/08/2024 10:21:17 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "11/08/2024 10:21:17 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "11/08/2024 10:21:17 - INFO - llamafactory.model.model_utils.misc - Found linear modules: down_proj,k_proj,o_proj,gate_proj,up_proj,q_proj,v_proj\n",
      "11/08/2024 10:21:18 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:21:18 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "11/08/2024 10:21:18 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "11/08/2024 10:21:18 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "11/08/2024 10:21:18 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "11/08/2024 10:21:18 - INFO - llamafactory.model.model_utils.misc - Found linear modules: up_proj,k_proj,o_proj,down_proj,q_proj,v_proj,gate_proj\n",
      "11/08/2024 10:21:19 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "11/08/2024 10:21:19 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "11/08/2024 10:21:19 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "11/08/2024 10:21:19 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "11/08/2024 10:21:19 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,o_proj,up_proj,q_proj,gate_proj,v_proj,down_proj\n",
      "11/08/2024 10:21:19 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n",
      "11/08/2024 10:21:19 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "[INFO|trainer.py:667] 2024-11-08 10:21:19,507 >> Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:21:19 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "11/08/2024 10:21:20 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:21:20 - INFO - llamafactory.model.loader - trainable params: 20,971,520 || all params: 8,051,232,768 || trainable%: 0.2605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:21:20 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "11/08/2024 10:21:20 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "11/08/2024 10:21:21 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "11/08/2024 10:21:22 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2243] 2024-11-08 10:21:24,087 >> ***** Running training *****\n",
      "[INFO|trainer.py:2244] 2024-11-08 10:21:24,087 >>   Num examples = 646\n",
      "[INFO|trainer.py:2245] 2024-11-08 10:21:24,087 >>   Num Epochs = 8\n",
      "[INFO|trainer.py:2246] 2024-11-08 10:21:24,088 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2249] 2024-11-08 10:21:24,088 >>   Total train batch size (w. parallel, distributed & accumulation) = 40\n",
      "[INFO|trainer.py:2250] 2024-11-08 10:21:24,088 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2251] 2024-11-08 10:21:24,096 >>   Total optimization steps = 128\n",
      "[INFO|trainer.py:2252] 2024-11-08 10:21:24,115 >>   Number of trainable parameters = 20,971,520\n",
      "  0%|          | 0/128 [00:00<?, ?it/s]/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "  8%|▊         | 10/128 [01:20<15:27,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4856, 'grad_norm': 0.3200698494911194, 'learning_rate': 3.846153846153846e-05, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 20/128 [02:38<14:08,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3252, 'grad_norm': 0.8281487226486206, 'learning_rate': 4.9544292351888966e-05, 'epoch': 1.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 30/128 [03:57<12:49,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2968, 'grad_norm': 0.5490431785583496, 'learning_rate': 4.735215677869128e-05, 'epoch': 1.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███▏      | 40/128 [05:15<11:29,  7.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2698, 'grad_norm': 0.5752435326576233, 'learning_rate': 4.35022639915313e-05, 'epoch': 2.46}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 50/128 [06:34<10:11,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2494, 'grad_norm': 0.4409679174423218, 'learning_rate': 3.828014292634509e-05, 'epoch': 3.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 60/128 [07:52<08:59,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2027, 'grad_norm': 0.5224316120147705, 'learning_rate': 3.207309441292325e-05, 'epoch': 3.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▍    | 70/128 [09:11<07:40,  7.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1767, 'grad_norm': 0.5752484202384949, 'learning_rate': 2.5341466844148775e-05, 'epoch': 4.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▎   | 80/128 [10:30<06:19,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1306, 'grad_norm': 0.7083129286766052, 'learning_rate': 1.8584514241650666e-05, 'epoch': 4.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 90/128 [11:49<05:01,  7.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0789, 'grad_norm': 0.5232424139976501, 'learning_rate': 1.2303368868954848e-05, 'epoch': 5.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 100/128 [13:08<03:40,  7.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0608, 'grad_norm': 0.45405325293540955, 'learning_rate': 6.9638745440261084e-06, 'epoch': 6.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 110/128 [14:27<02:22,  7.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0283, 'grad_norm': 0.5958675146102905, 'learning_rate': 2.962037134383211e-06, 'epoch': 6.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 120/128 [15:46<01:02,  7.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.019, 'grad_norm': 0.31589388847351074, 'learning_rate': 5.946546135113862e-07, 'epoch': 7.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 128/128 [16:49<00:00,  7.87s/it][INFO|trainer.py:3705] 2024-11-08 10:38:14,052 >> Saving model checkpoint to /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_nb_Meta-Llama-3.1-8B-Instruct-bnb-4bit/checkpoint-128\n",
      "[INFO|configuration_utils.py:672] 2024-11-08 10:38:14,649 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/config.json\n",
      "[INFO|configuration_utils.py:739] 2024-11-08 10:38:14,650 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2649] 2024-11-08 10:38:16,124 >> tokenizer config file saved in /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_nb_Meta-Llama-3.1-8B-Instruct-bnb-4bit/checkpoint-128/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2658] 2024-11-08 10:38:16,125 >> Special tokens file saved in /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_nb_Meta-Llama-3.1-8B-Instruct-bnb-4bit/checkpoint-128/special_tokens_map.json\n",
      "[INFO|trainer.py:2505] 2024-11-08 10:38:19,722 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 128/128 [16:55<00:00,  7.93s/it]\n",
      "[INFO|trainer.py:3705] 2024-11-08 10:38:19,732 >> Saving model checkpoint to /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_nb_Meta-Llama-3.1-8B-Instruct-bnb-4bit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1015.6072, 'train_samples_per_second': 5.089, 'train_steps_per_second': 0.126, 'train_loss': 0.18269346526358277, 'epoch': 7.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:672] 2024-11-08 10:38:20,169 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/config.json\n",
      "[INFO|configuration_utils.py:739] 2024-11-08 10:38:20,170 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2649] 2024-11-08 10:38:21,665 >> tokenizer config file saved in /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_nb_Meta-Llama-3.1-8B-Instruct-bnb-4bit/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2658] 2024-11-08 10:38:21,666 >> Special tokens file saved in /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_nb_Meta-Llama-3.1-8B-Instruct-bnb-4bit/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =      7.8769\n",
      "  total_flos               = 116685326GF\n",
      "  train_loss               =      0.1827\n",
      "  train_runtime            =  0:16:55.60\n",
      "  train_samples_per_second =       5.089\n",
      "  train_steps_per_second   =       0.126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modelcard.py:449] 2024-11-08 10:38:22,652 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(\n",
    "  model_name_or_path=BASE_MODEL, # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  adapter_name_or_path=str(OUTPUT_DIR),            # load the saved LoRA adapters\n",
    "  template=\"llama3\",                     # same to the one in training\n",
    "  finetuning_type=\"lora\",                  # same to the one in training\n",
    "  quantization_bit=4,                    # load 4-bit quantized model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:672] 2024-11-08 10:38:27,000 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/config.json\n",
      "[INFO|configuration_utils.py:739] 2024-11-08 10:38:27,002 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:38:27,423 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:38:27,424 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:38:27,425 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:38:27,426 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:38:27,426 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2478] 2024-11-08 10:38:27,722 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:672] 2024-11-08 10:38:28,459 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/config.json\n",
      "[INFO|configuration_utils.py:739] 2024-11-08 10:38:28,460 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:38:28,592 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:38:28,593 >> loading file tokenizer.model from cache at None\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:38:28,594 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:38:28,595 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2214] 2024-11-08 10:38:28,596 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2478] 2024-11-08 10:38:28,868 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:38:28 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:672] 2024-11-08 10:38:29,038 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/config.json\n",
      "[INFO|configuration_utils.py:739] 2024-11-08 10:38:29,039 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 14336,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.45.0\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:38:29 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "11/08/2024 10:38:29 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "11/08/2024 10:38:29 - INFO - llamafactory.model.patcher - Using KV cache for faster generation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|quantization_config.py:400] 2024-11-08 10:38:29,151 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3726] 2024-11-08 10:38:29,363 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/model.safetensors\n",
      "[INFO|modeling_utils.py:1622] 2024-11-08 10:38:29,393 >> Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.\n",
      "[INFO|configuration_utils.py:1099] 2024-11-08 10:38:29,398 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"pad_token_id\": 128004\n",
      "}\n",
      "\n",
      "[INFO|quantizer_bnb_4bit.py:122] 2024-11-08 10:38:29,516 >> target_dtype {target_dtype} is replaced by `CustomDtype.INT4` for 4-bit BnB quantization\n",
      "[INFO|modeling_utils.py:4568] 2024-11-08 10:38:31,655 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4576] 2024-11-08 10:38:31,657 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:1054] 2024-11-08 10:38:31,793 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-8B-Instruct-bnb-4bit/snapshots/5b0dd3039c312969e7950951486714bff26f0822/generation_config.json\n",
      "[INFO|configuration_utils.py:1099] 2024-11-08 10:38:31,794 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"max_length\": 131072,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/08/2024 10:38:31 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "11/08/2024 10:38:32 - INFO - llamafactory.model.adapter - Loaded adapter(s): /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_nb_Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
      "11/08/2024 10:38:32 - INFO - llamafactory.model.loader - all params: 8,051,232,768\n"
     ]
    }
   ],
   "source": [
    "model = ChatModel(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<llamafactory.chat.hf_engine.HuggingfaceEngine at 0x7fcc5e230550>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'huggingface'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.engine_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_dataset_file, \"r+\") as fh:\n",
    "    test_dataset = json.load(fh)\n",
    "\n",
    "test_prompts = []\n",
    "test_grounds = []\n",
    "\n",
    "for sample in test_dataset:\n",
    "    test_prompts.append(sample[\"instruction\"] + sample[\"input\"])\n",
    "    test_grounds.append(sample[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inferences ...:   1%|▏         | 2/156 [00:06<08:03,  3.14s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:328] 2024-11-08 10:38:33,096 >> Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running inferences ...: 100%|██████████| 156/156 [10:30<00:00,  4.04s/it]\n"
     ]
    }
   ],
   "source": [
    "messages = []\n",
    "\n",
    "for prompt in tqdm(test_prompts, desc=\"Running inferences ...\"):\n",
    "    #print(type(prompt))\n",
    "    #messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    message = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    test_predictions.append(model.chat(message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_preds = []\n",
    "\n",
    "for raw_pred in test_predictions:\n",
    "    x = json.loads(raw_pred[0].response_text)[\"page_utterance_emotions\"]\n",
    "    processed_preds.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_grounds = []\n",
    "\n",
    "for ground in test_grounds:\n",
    "    x = json.loads(ground)[\"page_utterance_emotions\"]\n",
    "    processed_grounds.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_grounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 20 19\n",
      "6 15 14\n",
      "69 6 5\n",
      "121 19 18\n"
     ]
    }
   ],
   "source": [
    "bad_idx = []\n",
    "\n",
    "for idx, (i,j) in enumerate(zip(processed_grounds, processed_preds)):\n",
    "    if len(i) != len(j):\n",
    "        print(idx, len(i), len(j))\n",
    "        bad_idx.append(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_idx.sort(reverse=True)\n",
    "\n",
    "# Remove elements from 'grounds' at the specified indices\n",
    "for idx in bad_idx:\n",
    "    \n",
    "    del processed_grounds[idx]\n",
    "    del processed_preds[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(152, 152)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(processed_grounds), len(processed_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = [item for sublist in processed_grounds for item in sublist]\n",
    "predictions = [item for sublist in processed_preds for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1266, 1266)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grounds), len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/sklearn/preprocessing/_label.py:900: UserWarning: unknown class(es) ['concern'] will be ignored\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "y_true_mhot = mlb.fit_transform(grounds)\n",
    "y_pred_mhot = mlb.transform(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1266, 7), (1266, 7))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_mhot.shape, y_pred_mhot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = mlb.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'disgust', 'fear', 'joy', 'neutral', 'sadness',\n",
       "       'surprise'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger      0.557     0.582     0.570       443\n",
      "     disgust      0.196     0.200     0.198        50\n",
      "        fear      0.504     0.465     0.484       284\n",
      "         joy      0.529     0.583     0.555       278\n",
      "     neutral      0.397     0.237     0.297        97\n",
      "     sadness      0.509     0.515     0.512       328\n",
      "    surprise      0.616     0.624     0.620       348\n",
      "\n",
      "   micro avg      0.532     0.531     0.532      1828\n",
      "   macro avg      0.473     0.458     0.462      1828\n",
      "weighted avg      0.529     0.531     0.529      1828\n",
      " samples avg      0.550     0.544     0.523      1828\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_true_mhot, y_pred_mhot, target_names=class_labels, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open({Path(FT_DIR)} / \"classification_report.pickle\", 'wb') as fh:\n",
    "    \n",
    "#      pickle.dump(classification_report(y_true_mhot, y_pred_mhot, target_names=class_labels, digits=3, output_dict=True), fh)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "unsloth/Meta-Llama-3.1-70B-Instruct, 8 Epochs.\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "       anger      0.601     0.569     0.584       445\n",
    "     disgust      0.160     0.174     0.167        46\n",
    "        fear      0.454     0.505     0.478       293\n",
    "         joy      0.534     0.465     0.497       271\n",
    "     neutral      0.574     0.321     0.412       109\n",
    "     sadness      0.535     0.601     0.566       333\n",
    "    surprise      0.580     0.678     0.625       342\n",
    "\n",
    "   micro avg      0.536     0.545     0.541      1839\n",
    "   macro avg      0.491     0.473     0.476      1839\n",
    "weighted avg      0.539     0.545     0.538      1839\n",
    " samples avg      0.555     0.557     0.532      1839"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "unsloth/llama-3-8b-Instruct-bnb-4bit, 8 epochs\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "       anger      0.625     0.528     0.572       417\n",
    "     disgust      0.182     0.154     0.167        39\n",
    "        fear      0.520     0.449     0.482       294\n",
    "         joy      0.540     0.580     0.559       281\n",
    "     neutral      0.518     0.406     0.455       106\n",
    "     sadness      0.499     0.557     0.526       323\n",
    "    surprise      0.579     0.606     0.593       343\n",
    "\n",
    "   micro avg      0.546     0.528     0.537      1803\n",
    "   macro avg      0.495     0.469     0.479      1803\n",
    "weighted avg      0.547     0.528     0.535      1803\n",
    " samples avg      0.561     0.548     0.529      1803"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\n",
    "\n",
    "precision    recall  f1-score   support\n",
    "\n",
    "       anger      0.557     0.582     0.570       443\n",
    "     disgust      0.196     0.200     0.198        50\n",
    "        fear      0.504     0.465     0.484       284\n",
    "         joy      0.529     0.583     0.555       278\n",
    "     neutral      0.397     0.237     0.297        97\n",
    "     sadness      0.509     0.515     0.512       328\n",
    "    surprise      0.616     0.624     0.620       348\n",
    "\n",
    "   micro avg      0.532     0.531     0.532      1828\n",
    "   macro avg      0.473     0.458     0.462      1828\n",
    "weighted avg      0.529     0.531     0.529      1828\n",
    " samples avg      0.550     0.544     0.523      1828"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "unsloth/Llama-3.2-3B-Instruct-bnb-4bit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
