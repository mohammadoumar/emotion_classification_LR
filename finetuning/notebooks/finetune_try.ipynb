{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ast\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import subprocess\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from llamafactory.chat import ChatModel\n",
    "from llamafactory.extras.misc import torch_gc\n",
    "from sklearn.metrics import classification_report\n",
    "#from utils.post_processing import post_process\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    assert torch.cuda.is_available() is True\n",
    "    \n",
    "except AssertionError:\n",
    "    \n",
    "    print(\"Please set up a GPU before using LLaMA Factory...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR = Path.cwd()\n",
    "FT_DIR = CURRENT_DIR / \"emotion_analysis_comics\" / \"finetuning\"\n",
    "DATASET_DIR = CURRENT_DIR / \"emotion_analysis_comics\" / \"finetuning\" / \"datasets\"\n",
    "\n",
    "ERC_DIR = FT_DIR.parent\n",
    "LLAMA_FACTORY_DIR = ERC_DIR / \"LLaMA-Factory\"\n",
    "\n",
    "BASE_MODEL = \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\"\n",
    "LOGGING_DIR = FT_DIR / \"training_logs\"\n",
    "OUTPUT_DIR = FT_DIR / \"saved_models\" / f\"\"\"comics35_pg_{BASE_MODEL.split(\"/\")[1]}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_DIR = CURRENT_DIR / \"emotion_analysis_comics\" / \"finetuning\" / \"datasets\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_name = f\"\"\"comics35_utterance_pg_train.json\"\"\"\n",
    "test_dataset_name = f\"\"\"comics35_utterance_pg_test.json\"\"\"\n",
    "\n",
    "train_dataset_file = DATASET_DIR / train_dataset_name\n",
    "test_dataset_file = DATASET_DIR / test_dataset_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/datasets/comics35_utterance_pg_train.json')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if not os.path.exists(os.path.join(FT_DIR, \"model_args\")):\n",
    "    os.mkdir(os.path.join(FT_DIR, \"model_args\"))\n",
    "\n",
    "train_file = FT_DIR / \"model_args\" / f\"\"\"{train_dataset_name.split(\".\")[0].split(\"train\")[0]}{BASE_MODEL.split(\"/\")[1]}.json\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_info_line =  {\n",
    "  \"file_name\": f\"{train_dataset_file}\",\n",
    "  \"columns\": {\n",
    "    \"prompt\": \"instruction\",\n",
    "    \"query\": \"input\",\n",
    "    \"response\": \"output\"\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"r\") as jsonFile:\n",
    "    data = json.load(jsonFile)\n",
    "\n",
    "data[\"comics\"] = dataset_info_line\n",
    "\n",
    "with open(os.path.join(LLAMA_FACTORY_DIR, \"data/dataset_info.json\"), \"w\") as jsonFile:\n",
    "    json.dump(data, jsonFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_EPOCHS = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict(\n",
    "    \n",
    "  stage=\"sft\",                           # do supervised fine-tuning\n",
    "  do_train=True,\n",
    "\n",
    "  model_name_or_path=BASE_MODEL,         # use bnb-4bit-quantized Llama-3-8B-Instruct model\n",
    "  num_train_epochs=NB_EPOCHS,            # the epochs of training\n",
    "  output_dir=str(OUTPUT_DIR),                 # the path to save LoRA adapters\n",
    "  overwrite_output_dir=True,             # overrides existing output contents\n",
    "\n",
    "  dataset=\"comics\",                      # dataset name\n",
    "  template=\"llama3\",                     # use llama3 prompt template\n",
    "  #train_on_prompt=True,\n",
    "  val_size=0.1,\n",
    "  max_samples=10000,                       # use 500 examples in each dataset\n",
    "\n",
    "  finetuning_type=\"lora\",                # use LoRA adapters to save memory\n",
    "  lora_target=\"all\",                     # attach LoRA adapters to all linear layers\n",
    "  per_device_train_batch_size=2,         # the batch size\n",
    "  gradient_accumulation_steps=4,         # the gradient accumulation steps\n",
    "  lr_scheduler_type=\"cosine\",            # use cosine learning rate scheduler\n",
    "  loraplus_lr_ratio=16.0,                # use LoRA+ algorithm with lambda=16.0\n",
    "  #temperature=0.5,\n",
    "  \n",
    "  warmup_ratio=0.1,                      # use warmup scheduler    \n",
    "  learning_rate=5e-5,                    # the learning rate\n",
    "  max_grad_norm=1.0,                     # clip gradient norm to 1.0\n",
    "  \n",
    "  fp16=True,                             # use float16 mixed precision training\n",
    "  quantization_bit=4,                    # use 4-bit QLoRA  \n",
    "  #use_liger_kernel=True,\n",
    "  #quantization_device_map=\"auto\",\n",
    "  \n",
    "  logging_steps=10,                      # log every 10 steps\n",
    "  save_steps=5000,                       # save checkpoint every 1000 steps    \n",
    "  logging_dir=str(LOGGING_DIR),\n",
    "  \n",
    "  # use_unsloth=True,\n",
    "  report_to=\"tensorboard\"                       # discards wandb\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(args, open(train_file, \"w\", encoding=\"utf-8\"), indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = subprocess.Popen([\"llamafactory-cli\", \"train\", train_file], cwd=LLAMA_FACTORY_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/31/2024 10:56:31 - INFO - llamafactory.cli - Initializing distributed tasks at: 127.0.0.1:26860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1031 10:56:32.611000 140462288172352 torch/distributed/run.py:779] \n",
      "W1031 10:56:32.611000 140462288172352 torch/distributed/run.py:779] *****************************************\n",
      "W1031 10:56:32.611000 140462288172352 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W1031 10:56:32.611000 140462288172352 torch/distributed/run.py:779] *****************************************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/31/2024 10:56:42 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "10/31/2024 10:56:42 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "10/31/2024 10:56:42 - INFO - llamafactory.hparams.parser - Process rank: 0, device: cuda:0, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "10/31/2024 10:56:42 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "10/31/2024 10:56:42 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "10/31/2024 10:56:42 - INFO - llamafactory.hparams.parser - Process rank: 2, device: cuda:2, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n",
      "10/31/2024 10:56:42 - WARNING - llamafactory.hparams.parser - We recommend enable `upcast_layernorm` in quantized training.\n",
      "10/31/2024 10:56:42 - WARNING - llamafactory.hparams.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
      "10/31/2024 10:56:42 - INFO - llamafactory.hparams.parser - Process rank: 1, device: cuda:1, n_gpu: 1, distributed training: True, compute dtype: torch.float16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-10-31 10:56:42,930 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-10-31 10:56:42,931 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-10-31 10:56:43,049 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-10-31 10:56:43,049 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-10-31 10:56:43,049 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-10-31 10:56:43,049 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-10-31 10:56:43,598 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "[INFO|configuration_utils.py:733] 2024-10-31 10:56:44,138 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-10-31 10:56:44,139 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-10-31 10:56:44,265 >> loading file tokenizer.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-10-31 10:56:44,266 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-10-31 10:56:44,266 >> loading file special_tokens_map.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2269] 2024-10-31 10:56:44,266 >> loading file tokenizer_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-10-31 10:56:44,762 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/31/2024 10:56:44 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "10/31/2024 10:56:44 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/datasets/comics35_utterance_pg_train.json...\n",
      "10/31/2024 10:56:45 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "10/31/2024 10:56:45 - INFO - llamafactory.data.template - Replace eos token: <|eot_id|>\n",
      "10/31/2024 10:56:46 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/datasets/comics35_utterance_pg_train.json...\n",
      "10/31/2024 10:56:46 - INFO - llamafactory.data.loader - Loading dataset /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/datasets/comics35_utterance_pg_train.json...\n",
      "training example:\n",
      "input_ids:\n",
      "[128000, 128006, 882, 128007, 271, 14711, 5867, 6082, 18825, 33257, 15766, 271, 2675, 527, 459, 11084, 20356, 6492, 6335, 58394, 304, 20303, 2363, 21976, 23692, 13, 4718, 3465, 374, 311, 24564, 22256, 3095, 323, 10765, 872, 14604, 2262, 382, 30521, 512, 12, 1472, 690, 5371, 264, 1160, 315, 22256, 3095, 505, 264, 2199, 304, 264, 20303, 2363, 198, 12, 578, 22256, 685, 1253, 3237, 832, 477, 5361, 21958, 271, 66913, 512, 16, 13, 10852, 3725, 24564, 279, 14604, 2317, 323, 16630, 315, 1855, 22256, 685, 304, 279, 2199, 198, 17, 13, 65647, 8581, 21958, 505, 279, 2768, 6989, 512, 256, 330, 4091, 498, 330, 4338, 70, 592, 498, 330, 69, 686, 498, 330, 83214, 2136, 498, 330, 20370, 9868, 498, 330, 4215, 498, 330, 60668, 702, 18, 13, 1789, 1855, 22256, 685, 304, 264, 20303, 2199, 11, 10765, 682, 21958, 3118, 323, 471, 459, 1358, 315, 20356, 18893, 304, 2015, 382, 93016, 50, 512, 16, 13, 5560, 27785, 279, 9382, 10212, 3485, 198, 17, 13, 9442, 2011, 387, 264, 4823, 449, 3254, 1401, 330, 2964, 62, 6339, 685, 23319, 41356, 702, 18, 13, 5273, 2011, 387, 459, 1358, 1405, 512, 256, 482, 9062, 2449, 374, 459, 1358, 315, 21958, 369, 832, 22256, 685, 198, 256, 482, 7365, 9248, 279, 1988, 22256, 3095, 2015, 198, 256, 482, 29911, 21958, 527, 5535, 824, 22256, 685, 198, 19, 13, 2360, 41941, 11, 1193, 4823, 2612, 271, 99843, 512, 12, 9062, 1358, 2449, 34310, 311, 832, 22256, 685, 198, 12, 3861, 22256, 685, 649, 617, 5361, 21958, 198, 12, 87477, 4839, 43529, 323, 1162, 315, 20356, 9382, 198, 12, 13969, 21958, 304, 18893, 1524, 369, 3254, 21958, 1432, 7184, 24564, 1521, 22256, 3095, 304, 264, 2199, 512, 16, 13, 10245, 650, 3015, 4534, 1753, 99550, 1507, 3247, 50297, 7354, 6483, 50, 3083, 18725, 6570, 6005, 50, 1981, 720, 17, 13, 4696, 8871, 393, 1899, 35457, 18725, 7354, 58376, 2843, 1112, 25832, 1750, 6486, 1981, 3651, 23214, 8871, 7354, 16929, 4716, 61094, 627, 18, 13, 18725, 72297, 4999, 19, 13, 54233, 4999, 20, 13, 11155, 358, 19102, 4276, 6969, 4157, 35, 10245, 29637, 39023, 13398, 10245, 25404, 5257, 36757, 8871, 480, 1308, 12073, 4999, 21, 13, 11947, 984, 386, 33937, 4999, 22, 13, 358, 19102, 7837, 36, 5257, 3501, 48, 313, 4999, 23, 13, 38535, 7233, 24, 13, 1198, 1669, 26336, 30, 4999, 605, 13, 5782, 0, 5782, 0, 128009, 128006, 78191, 128007, 271, 5018, 2964, 62, 6339, 685, 23319, 41356, 794, 79786, 4091, 8073, 4482, 4091, 8073, 4482, 69, 686, 8073, 4482, 69, 686, 8073, 4482, 69, 686, 498, 330, 83214, 2136, 8073, 4482, 83214, 2136, 8073, 4482, 4091, 8073, 4482, 20370, 9868, 8073, 4482, 20370, 9868, 8073, 4482, 69, 686, 498, 330, 20370, 9868, 1365, 14316, 128009]\n",
      "inputs:\n",
      "<|begin_of_text|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "### Emotion Analysis Expert Role\n",
      "\n",
      "You are an advanced emotion analysis expert specializing in comic book dialogue interpretation. Your task is to analyze utterances and identify their emotional content.\n",
      "\n",
      "INPUT:\n",
      "- You will receive a list of utterances from a page in a comic book\n",
      "- The utterance may express one or multiple emotions\n",
      "\n",
      "TASK:\n",
      "1. Carefully analyze the emotional context and tone of each utterance in the page\n",
      "2. Identify applicable emotions from the following classes:\n",
      "   \"anger\", \"disgust\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"neutral\"\n",
      "3. For each utterance in a comic page, identify all emotions present and return an array of emotion arrays in order.\n",
      "\n",
      "RULES:\n",
      "1. Use ONLY the labels listed above\n",
      "2. Output must be a JSON with single key \"page_utterance_emotions\"\n",
      "3. Value must be an array where:\n",
      "   - Each element is an array of emotions for one utterance\n",
      "   - Order matches the input utterances order\n",
      "   - Multiple emotions are allowed per utterance\n",
      "4. No explanations, only JSON output\n",
      "\n",
      "IMPORTANT:\n",
      "- Each array element corresponds to one utterance\n",
      "- One utterance can have multiple emotions\n",
      "- Maintain exact spelling and case of emotion labels\n",
      "- Keep emotions in arrays even for single emotions\n",
      "\n",
      "\n",
      "Now analyze these utterances in a page:\n",
      "1. THIS VILE THING ATTACKED THE SMALL BEASTS OF MY SHORES… \n",
      "2. … IT PUNCHED MY BEAUTIFUL MATILDA… AND NOW IT BEGS FOR LIFE.\n",
      "3. MY MASTER!\n",
      "4. PLEASE!\n",
      "5. BUT I HAVE NOT CHASED THIS MONSTER ALL THIS WAY TO LET IT GROVEL!\n",
      "6. HEAL MEEE!\n",
      "7. I HAVE COME TO CONQ--!\n",
      "8. WHAT--\n",
      "9. --IS THAT?!\n",
      "10. NO! NO!<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "{\"page_utterance_emotions\": [[\"anger\"], [\"anger\"], [\"fear\"], [\"fear\"], [\"fear\", \"sadness\"], [\"sadness\"], [\"anger\"], [\"surprise\"], [\"surprise\"], [\"fear\", \"surprise\"]]}<|eot_id|>\n",
      "label_ids:\n",
      "[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 5018, 2964, 62, 6339, 685, 23319, 41356, 794, 79786, 4091, 8073, 4482, 4091, 8073, 4482, 69, 686, 8073, 4482, 69, 686, 8073, 4482, 69, 686, 498, 330, 83214, 2136, 8073, 4482, 83214, 2136, 8073, 4482, 4091, 8073, 4482, 20370, 9868, 8073, 4482, 20370, 9868, 8073, 4482, 69, 686, 498, 330, 20370, 9868, 1365, 14316, 128009]\n",
      "labels:\n",
      "{\"page_utterance_emotions\": [[\"anger\"], [\"anger\"], [\"fear\"], [\"fear\"], [\"fear\", \"sadness\"], [\"sadness\"], [\"anger\"], [\"surprise\"], [\"surprise\"], [\"fear\", \"surprise\"]]}<|eot_id|>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-10-31 10:56:46,726 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-10-31 10:56:46,726 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-70B-bnb-4bit\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[WARNING|quantization_config.py:398] 2024-10-31 10:56:46,814 >> Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "[INFO|modeling_utils.py:3678] 2024-10-31 10:56:46,816 >> loading weights file model.safetensors from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1606] 2024-10-31 10:56:46,820 >> Instantiating LlamaForCausalLM model under default dtype torch.float16.\n",
      "[INFO|configuration_utils.py:1038] 2024-10-31 10:56:46,820 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"pad_token_id\": 128004\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/31/2024 10:56:46 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "10/31/2024 10:56:46 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "10/31/2024 10:56:46 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "10/31/2024 10:56:46 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n",
      "10/31/2024 10:56:46 - WARNING - llamafactory.model.model_utils.quantization - `quantization_bit` will not affect on the PTQ-quantized models.\n",
      "10/31/2024 10:56:46 - INFO - llamafactory.model.model_utils.quantization - Loading ?-bit BITSANDBYTES-quantized model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:25<00:00,  4.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/31/2024 10:57:16 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "10/31/2024 10:57:16 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "10/31/2024 10:57:16 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "10/31/2024 10:57:16 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "10/31/2024 10:57:16 - INFO - llamafactory.model.model_utils.misc - Found linear modules: v_proj,q_proj,k_proj,down_proj,gate_proj,o_proj,up_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:29<00:00,  4.96s/it]\n",
      "[INFO|modeling_utils.py:4507] 2024-10-31 10:57:19,408 >> All model checkpoint weights were used when initializing LlamaForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4515] 2024-10-31 10:57:19,408 >> All the weights of LlamaForCausalLM were initialized from the model checkpoint at unsloth/Meta-Llama-3.1-70B-bnb-4bit.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use LlamaForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:993] 2024-10-31 10:57:19,538 >> loading configuration file generation_config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/generation_config.json\n",
      "[INFO|configuration_utils.py:1038] 2024-10-31 10:57:19,538 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"max_length\": 131072,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 6/6 [00:30<00:00,  5.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/31/2024 10:57:20 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465\n",
      "10/31/2024 10:57:20 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "10/31/2024 10:57:20 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "10/31/2024 10:57:20 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "10/31/2024 10:57:20 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "10/31/2024 10:57:20 - INFO - llamafactory.model.model_utils.misc - Found linear modules: k_proj,v_proj,gate_proj,down_proj,up_proj,o_proj,q_proj\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/31/2024 10:57:21 - INFO - llamafactory.model.model_utils.checkpointing - Gradient checkpointing enabled.\n",
      "10/31/2024 10:57:21 - INFO - llamafactory.model.model_utils.attention - Using torch SDPA for faster training and inference.\n",
      "10/31/2024 10:57:21 - INFO - llamafactory.model.adapter - Upcasting trainable params to float32.\n",
      "10/31/2024 10:57:21 - INFO - llamafactory.model.adapter - Fine-tuning method: LoRA\n",
      "10/31/2024 10:57:21 - INFO - llamafactory.model.model_utils.misc - Found linear modules: gate_proj,k_proj,v_proj,q_proj,o_proj,down_proj,up_proj\n",
      "10/31/2024 10:57:21 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n",
      "10/31/2024 10:57:24 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465\n",
      "10/31/2024 10:57:24 - WARNING - llamafactory.train.callbacks - Previous trainer log in this folder will be deleted.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "[INFO|trainer.py:648] 2024-10-31 10:57:24,907 >> Using auto half precision backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/31/2024 10:57:25 - INFO - llamafactory.model.loader - trainable params: 103,546,880 || all params: 70,657,253,376 || trainable%: 0.1465\n",
      "10/31/2024 10:57:25 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/31/2024 10:57:26 - INFO - llamafactory.train.trainer_utils - Using LoRA+ optimizer with loraplus lr ratio 16.00.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2134] 2024-10-31 10:57:31,117 >> ***** Running training *****\n",
      "[INFO|trainer.py:2135] 2024-10-31 10:57:31,118 >>   Num examples = 646\n",
      "[INFO|trainer.py:2136] 2024-10-31 10:57:31,118 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2137] 2024-10-31 10:57:31,118 >>   Instantaneous batch size per device = 2\n",
      "[INFO|trainer.py:2140] 2024-10-31 10:57:31,118 >>   Total train batch size (w. parallel, distributed & accumulation) = 24\n",
      "[INFO|trainer.py:2141] 2024-10-31 10:57:31,118 >>   Gradient Accumulation steps = 4\n",
      "[INFO|trainer.py:2142] 2024-10-31 10:57:31,118 >>   Total optimization steps = 3\n",
      "[INFO|trainer.py:2143] 2024-10-31 10:57:31,148 >>   Number of trainable parameters = 103,546,880\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "100%|██████████| 3/3 [01:49<00:00, 36.57s/it][INFO|trainer.py:3503] 2024-10-31 10:59:20,741 >> Saving model checkpoint to /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-3.1-70B-bnb-4bit/checkpoint-3\n",
      "[INFO|configuration_utils.py:733] 2024-10-31 10:59:21,206 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-10-31 10:59:21,207 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-70B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-10-31 10:59:26,969 >> tokenizer config file saved in /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-3.1-70B-bnb-4bit/checkpoint-3/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-10-31 10:59:26,970 >> Special tokens file saved in /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-3.1-70B-bnb-4bit/checkpoint-3/special_tokens_map.json\n",
      "[INFO|trainer.py:2394] 2024-10-31 10:59:38,393 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "100%|██████████| 3/3 [02:06<00:00, 42.32s/it]\n",
      "[INFO|trainer.py:3503] 2024-10-31 10:59:38,426 >> Saving model checkpoint to /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-3.1-70B-bnb-4bit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 127.2421, 'train_samples_per_second': 0.508, 'train_steps_per_second': 0.024, 'train_loss': 1.1624692281087239, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-10-31 10:59:38,843 >> loading configuration file config.json from cache at /Utilisateurs/umushtaq/.cache/huggingface/hub/models--unsloth--Meta-Llama-3.1-70B-bnb-4bit/snapshots/7edacd248efbb480633dca1bb4fe8b290072b6be/config.json\n",
      "[INFO|configuration_utils.py:800] 2024-10-31 10:59:38,844 >> Model config LlamaConfig {\n",
      "  \"_name_or_path\": \"unsloth/Meta-Llama-3.1-70B\",\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": 128001,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 8192,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 28672,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 64,\n",
      "  \"num_hidden_layers\": 80,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 128004,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"quantization_config\": {\n",
      "    \"_load_in_4bit\": true,\n",
      "    \"_load_in_8bit\": false,\n",
      "    \"bnb_4bit_compute_dtype\": \"bfloat16\",\n",
      "    \"bnb_4bit_quant_storage\": \"uint8\",\n",
      "    \"bnb_4bit_quant_type\": \"nf4\",\n",
      "    \"bnb_4bit_use_double_quant\": true,\n",
      "    \"llm_int8_enable_fp32_cpu_offload\": false,\n",
      "    \"llm_int8_has_fp16_weight\": false,\n",
      "    \"llm_int8_skip_modules\": null,\n",
      "    \"llm_int8_threshold\": 6.0,\n",
      "    \"load_in_4bit\": true,\n",
      "    \"load_in_8bit\": false,\n",
      "    \"quant_method\": \"bitsandbytes\"\n",
      "  },\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 8.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.44.2\",\n",
      "  \"unsloth_version\": \"2024.9\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2684] 2024-10-31 10:59:44,741 >> tokenizer config file saved in /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-3.1-70B-bnb-4bit/tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2693] 2024-10-31 10:59:44,743 >> Special tokens file saved in /Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-3.1-70B-bnb-4bit/special_tokens_map.json\n",
      "[INFO|modelcard.py:449] 2024-10-31 10:59:44,912 >> Dropping the following result as it does not have all the necessary fields:\n",
      "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =     0.1111\n",
      "  total_flos               = 15384030GF\n",
      "  train_loss               =     1.1625\n",
      "  train_runtime            = 0:02:07.24\n",
      "  train_samples_per_second =      0.508\n",
      "  train_steps_per_second   =      0.024\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7815ee66c8b64e1f9cc4a580ef7915f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(BASE_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(OUTPUT_DIR, padding_side=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "terminators = [tokenizer.eos_token_id, tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraModel, LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained(str(OUTPUT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='unsloth/Meta-Llama-3.1-70B-bnb-4bit', revision=None, task_type='CAUSAL_LM', inference_mode=True, r=8, target_modules={'down_proj', 'v_proj', 'q_proj', 'o_proj', 'up_proj', 'gate_proj', 'k_proj'}, lora_alpha=16, lora_dropout=0.0, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lora_model = LoraModel(model, lora_config, \"/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): LoraModel(\n",
       "    (model): LlamaForCausalLM(\n",
       "      (model): LlamaModel(\n",
       "        (embed_tokens): Embedding(128256, 8192, padding_idx=128004)\n",
       "        (layers): ModuleList(\n",
       "          (0-79): 80 x LlamaDecoderLayer(\n",
       "            (self_attn): LlamaSdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=1024, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (rotary_emb): LlamaRotaryEmbedding()\n",
       "            )\n",
       "            (mlp): LlamaMLP(\n",
       "              (gate_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8, out_features=28672, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (up_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=8192, out_features=28672, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8192, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8, out_features=28672, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (down_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=28672, out_features=8192, bias=False)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Identity()\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=28672, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (/Utilisateurs/umushtaq/emotion_analysis_comics/finetuning/saved_models/comics35_pg_Meta-Llama-31-70B-bnb-4bit): Linear(in_features=8, out_features=8192, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): LlamaRMSNorm((8192,), eps=1e-05)\n",
       "            (post_attention_layernorm): LlamaRMSNorm((8192,), eps=1e-05)\n",
       "          )\n",
       "        )\n",
       "        (norm): LlamaRMSNorm((8192,), eps=1e-05)\n",
       "        (rotary_emb): LlamaRotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=8192, out_features=128256, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device_count = torch.cuda.device_count()\n",
    "if device_count > 1:\n",
    "    lora_model = torch.nn.DataParallel(lora_model, device_ids=list(range(device_count)))\n",
    "lora_model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(test_dataset_file, \"r+\") as fh:\n",
    "    test_dataset = json.load(fh)\n",
    "\n",
    "test_prompts = []\n",
    "test_grounds = []\n",
    "\n",
    "for sample in test_dataset:\n",
    "    #print(sample)\n",
    "    #test_prompts.append(sample[\"instruction\"] + sample[\"input\"])\n",
    "    #test_prompts.append(\"user:\" + sample[\"instruction\"] + sample[\"input\"])\n",
    "    test_prompts.append([{\"role\": \"user\", \"content\": sample[\"instruction\"] + sample[\"input\"]}])\n",
    "    test_grounds.append(sample[\"output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "156"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prompts = test_prompts[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user',\n",
       "   'content': '### Emotion Analysis Expert Role\\n\\nYou are an advanced emotion analysis expert specializing in comic book dialogue interpretation. Your task is to analyze utterances and identify their emotional content.\\n\\nINPUT:\\n- You will receive a list of utterances from a page in a comic book\\n- The utterance may express one or multiple emotions\\n\\nTASK:\\n1. Carefully analyze the emotional context and tone of each utterance in the page\\n2. Identify applicable emotions from the following classes:\\n   \"anger\", \"disgust\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"neutral\"\\n3. For each utterance in a comic page, identify all emotions present and return an array of emotion arrays in order.\\n\\nRULES:\\n1. Use ONLY the labels listed above\\n2. Output must be a JSON with single key \"page_utterance_emotions\"\\n3. Value must be an array where:\\n   - Each element is an array of emotions for one utterance\\n   - Order matches the input utterances order\\n   - Multiple emotions are allowed per utterance\\n4. No explanations, only JSON output\\n\\nIMPORTANT:\\n- Each array element corresponds to one utterance\\n- One utterance can have multiple emotions\\n- Maintain exact spelling and case of emotion labels\\n- Keep emotions in arrays even for single emotions\\n\\nNow analyze these utterances in a page:\\n1. HOW\\'S IT GOING?\\n2. HEY.\\n3. CAN I GET YOU ANYTHING?\\n4. JUST A COKE.\\n5. OKAY. COMING UP.\\n6. THANKS.\\n7. HOW IS IT OUT THERE? GETTING HOT?\\n8. IT\\'S ALL RIGHT.\\n9. ONE COKE. ENJOY.\\n10. @ONCE UPON A TIME…\\n11. @… IN A FAR OFF KINGDOM…'}]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "            test_prompts,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[128000, 128006,    882, 128007,    271,  14711,   5867,   6082,  18825,\n",
       "          33257,  15766,    271,   2675,    527,    459,  11084,  20356,   6492,\n",
       "           6335,  58394,    304,  20303,   2363,  21976,  23692,     13,   4718,\n",
       "           3465,    374,    311,  24564,  22256,   3095,    323,  10765,    872,\n",
       "          14604,   2262,    382,  30521,    512,     12,   1472,    690,   5371,\n",
       "            264,   1160,    315,  22256,   3095,    505,    264,   2199,    304,\n",
       "            264,  20303,   2363,    198,     12,    578,  22256,    685,   1253,\n",
       "           3237,    832,    477,   5361,  21958,    271,  66913,    512,     16,\n",
       "             13,  10852,   3725,  24564,    279,  14604,   2317,    323,  16630,\n",
       "            315,   1855,  22256,    685,    304,    279,   2199,    198,     17,\n",
       "             13,  65647,   8581,  21958,    505,    279,   2768,   6989,    512,\n",
       "            256,    330,   4091,    498,    330,   4338,     70,    592,    498,\n",
       "            330,     69,    686,    498,    330,  83214,   2136,    498,    330,\n",
       "          20370,   9868,    498,    330,   4215,    498,    330,  60668,    702,\n",
       "             18,     13,   1789,   1855,  22256,    685,    304,    264,  20303,\n",
       "           2199,     11,  10765,    682,  21958,   3118,    323,    471,    459,\n",
       "           1358,    315,  20356,  18893,    304,   2015,    382,  93016,     50,\n",
       "            512,     16,     13,   5560,  27785,    279,   9382,  10212,   3485,\n",
       "            198,     17,     13,   9442,   2011,    387,    264,   4823,    449,\n",
       "           3254,   1401,    330,   2964,     62,   6339,    685,  23319,  41356,\n",
       "            702,     18,     13,   5273,   2011,    387,    459,   1358,   1405,\n",
       "            512,    256,    482,   9062,   2449,    374,    459,   1358,    315,\n",
       "          21958,    369,    832,  22256,    685,    198,    256,    482,   7365,\n",
       "           9248,    279,   1988,  22256,   3095,   2015,    198,    256,    482,\n",
       "          29911,  21958,    527,   5535,    824,  22256,    685,    198,     19,\n",
       "             13,   2360,  41941,     11,   1193,   4823,   2612,    271,  99843,\n",
       "            512,     12,   9062,   1358,   2449,  34310,    311,    832,  22256,\n",
       "            685,    198,     12,   3861,  22256,    685,    649,    617,   5361,\n",
       "          21958,    198,     12,  87477,   4839,  43529,    323,   1162,    315,\n",
       "          20356,   9382,    198,     12,  13969,  21958,    304,  18893,   1524,\n",
       "            369,   3254,  21958,    271,   7184,  24564,   1521,  22256,   3095,\n",
       "            304,    264,   2199,    512,     16,     13,  24440,  13575,   8871,\n",
       "          12890,   1753,   5380,     17,     13,  11947,     56,    627,     18,\n",
       "             13,  20076,    358,   8049,  15334,   4230,  57764,   5380,     19,\n",
       "             13,  48630,    362,   7432,   3472,    627,     20,     13,  10619,\n",
       "           3097,     13,   7837,   1753,  12250,    627,     21,     13,  81957,\n",
       "             50,    627,     22,     13,  24440,   3507,   8871,  10009,  62207,\n",
       "             30,   8049,  26098,  54473,   5380,     23,     13,   8871,  13575,\n",
       "          13398,  28577,    627,     24,     13,  25002,   7432,   3472,     13,\n",
       "           5301,  27237,     56,    627,    605,     13,    571,    715,   2152,\n",
       "          12250,    715,    362,  23029,  90578,    806,     13,    571,   1981,\n",
       "           2006,    362,  57415,  18076,  74911,  15599,   1981, 128009, 128006,\n",
       "          78191, 128007,    271]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([381])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.input_ids[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n### Emotion Analysis Expert Role\\n\\nYou are an advanced emotion analysis expert specializing in comic book dialogue interpretation. Your task is to analyze utterances and identify their emotional content.\\n\\nINPUT:\\n- You will receive a list of utterances from a page in a comic book\\n- The utterance may express one or multiple emotions\\n\\nTASK:\\n1. Carefully analyze the emotional context and tone of each utterance in the page\\n2. Identify applicable emotions from the following classes:\\n   \"anger\", \"disgust\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"neutral\"\\n3. For each utterance in a comic page, identify all emotions present and return an array of emotion arrays in order.\\n\\nRULES:\\n1. Use ONLY the labels listed above\\n2. Output must be a JSON with single key \"page_utterance_emotions\"\\n3. Value must be an array where:\\n   - Each element is an array of emotions for one utterance\\n   - Order matches the input utterances order\\n   - Multiple emotions are allowed per utterance\\n4. No explanations, only JSON output\\n\\nIMPORTANT:\\n- Each array element corresponds to one utterance\\n- One utterance can have multiple emotions\\n- Maintain exact spelling and case of emotion labels\\n- Keep emotions in arrays even for single emotions\\n\\nNow analyze these utterances in a page:\\n1. HOW\\'S IT GOING?\\n2. HEY.\\n3. CAN I GET YOU ANYTHING?\\n4. JUST A COKE.\\n5. OKAY. COMING UP.\\n6. THANKS.\\n7. HOW IS IT OUT THERE? GETTING HOT?\\n8. IT\\'S ALL RIGHT.\\n9. ONE COKE. ENJOY.\\n10. @ONCE UPON A TIME…\\n11. @… IN A FAR OFF KINGDOM…<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(inputs.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated = generation_model.generate(**inputs, max_new_tokens=32, pad_token_id=inference_tokenizer.eos_token_id, do_sample=True,\n",
    "#      temperature=0.1,\n",
    "#      top_p=0.9,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "\n",
    "def generate_text(prompt, max_new_tokens=512, num_return_sequences=1, do_sample=True, top_k=50, top_p=0.95, num_beams=1):\n",
    "    input_ids = inputs.input_ids.to(\"cuda\")\n",
    "    output_ids = lora_model.module.generate(input_ids, max_new_tokens=32, pad_token_id=tokenizer.eos_token_id, do_sample=True, eos_token_id=terminators,\n",
    "     temperature=0.1,\n",
    "     top_p=0.9,)\n",
    "    #responses.append(tokenizer.decode(output_ids[0], skip_special_tokens=True))\n",
    "    responses.append(output_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prompt = \"The quick brown fox\"\n",
    "#generated_text = generate_text(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    }
   ],
   "source": [
    "for prompt in test_prompts:\n",
    "    \n",
    "    generate_text(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'role': 'user',\n",
       "   'content': '### Emotion Analysis Expert Role\\n\\nYou are an advanced emotion analysis expert specializing in comic book dialogue interpretation. Your task is to analyze utterances and identify their emotional content.\\n\\nINPUT:\\n- You will receive a list of utterances from a page in a comic book\\n- The utterance may express one or multiple emotions\\n\\nTASK:\\n1. Carefully analyze the emotional context and tone of each utterance in the page\\n2. Identify applicable emotions from the following classes:\\n   \"anger\", \"disgust\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"neutral\"\\n3. For each utterance in a comic page, identify all emotions present and return an array of emotion arrays in order.\\n\\nRULES:\\n1. Use ONLY the labels listed above\\n2. Output must be a JSON with single key \"page_utterance_emotions\"\\n3. Value must be an array where:\\n   - Each element is an array of emotions for one utterance\\n   - Order matches the input utterances order\\n   - Multiple emotions are allowed per utterance\\n4. No explanations, only JSON output\\n\\nIMPORTANT:\\n- Each array element corresponds to one utterance\\n- One utterance can have multiple emotions\\n- Maintain exact spelling and case of emotion labels\\n- Keep emotions in arrays even for single emotions\\n\\nNow analyze these utterances in a page:\\n1. HOW\\'S IT GOING?\\n2. HEY.\\n3. CAN I GET YOU ANYTHING?\\n4. JUST A COKE.\\n5. OKAY. COMING UP.\\n6. THANKS.\\n7. HOW IS IT OUT THERE? GETTING HOT?\\n8. IT\\'S ALL RIGHT.\\n9. ONE COKE. ENJOY.\\n10. @ONCE UPON A TIME…\\n11. @… IN A FAR OFF KINGDOM…'}]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[128000, 128006,    882, 128007,    271,  14711,   5867,   6082,  18825,\n",
       "           33257,  15766,    271,   2675,    527,    459,  11084,  20356,   6492,\n",
       "            6335,  58394,    304,  20303,   2363,  21976,  23692,     13,   4718,\n",
       "            3465,    374,    311,  24564,  22256,   3095,    323,  10765,    872,\n",
       "           14604,   2262,    382,  30521,    512,     12,   1472,    690,   5371,\n",
       "             264,   1160,    315,  22256,   3095,    505,    264,   2199,    304,\n",
       "             264,  20303,   2363,    198,     12,    578,  22256,    685,   1253,\n",
       "            3237,    832,    477,   5361,  21958,    271,  66913,    512,     16,\n",
       "              13,  10852,   3725,  24564,    279,  14604,   2317,    323,  16630,\n",
       "             315,   1855,  22256,    685,    304,    279,   2199,    198,     17,\n",
       "              13,  65647,   8581,  21958,    505,    279,   2768,   6989,    512,\n",
       "             256,    330,   4091,    498,    330,   4338,     70,    592,    498,\n",
       "             330,     69,    686,    498,    330,  83214,   2136,    498,    330,\n",
       "           20370,   9868,    498,    330,   4215,    498,    330,  60668,    702,\n",
       "              18,     13,   1789,   1855,  22256,    685,    304,    264,  20303,\n",
       "            2199,     11,  10765,    682,  21958,   3118,    323,    471,    459,\n",
       "            1358,    315,  20356,  18893,    304,   2015,    382,  93016,     50,\n",
       "             512,     16,     13,   5560,  27785,    279,   9382,  10212,   3485,\n",
       "             198,     17,     13,   9442,   2011,    387,    264,   4823,    449,\n",
       "            3254,   1401,    330,   2964,     62,   6339,    685,  23319,  41356,\n",
       "             702,     18,     13,   5273,   2011,    387,    459,   1358,   1405,\n",
       "             512,    256,    482,   9062,   2449,    374,    459,   1358,    315,\n",
       "           21958,    369,    832,  22256,    685,    198,    256,    482,   7365,\n",
       "            9248,    279,   1988,  22256,   3095,   2015,    198,    256,    482,\n",
       "           29911,  21958,    527,   5535,    824,  22256,    685,    198,     19,\n",
       "              13,   2360,  41941,     11,   1193,   4823,   2612,    271,  99843,\n",
       "             512,     12,   9062,   1358,   2449,  34310,    311,    832,  22256,\n",
       "             685,    198,     12,   3861,  22256,    685,    649,    617,   5361,\n",
       "           21958,    198,     12,  87477,   4839,  43529,    323,   1162,    315,\n",
       "           20356,   9382,    198,     12,  13969,  21958,    304,  18893,   1524,\n",
       "             369,   3254,  21958,    271,   7184,  24564,   1521,  22256,   3095,\n",
       "             304,    264,   2199,    512,     16,     13,  24440,  13575,   8871,\n",
       "           12890,   1753,   5380,     17,     13,  11947,     56,    627,     18,\n",
       "              13,  20076,    358,   8049,  15334,   4230,  57764,   5380,     19,\n",
       "              13,  48630,    362,   7432,   3472,    627,     20,     13,  10619,\n",
       "            3097,     13,   7837,   1753,  12250,    627,     21,     13,  81957,\n",
       "              50,    627,     22,     13,  24440,   3507,   8871,  10009,  62207,\n",
       "              30,   8049,  26098,  54473,   5380,     23,     13,   8871,  13575,\n",
       "           13398,  28577,    627,     24,     13,  25002,   7432,   3472,     13,\n",
       "            5301,  27237,     56,    627,    605,     13,    571,    715,   2152,\n",
       "           12250,    715,    362,  23029,  90578,    806,     13,    571,   1981,\n",
       "            2006,    362,  57415,  18076,  74911,  15599,   1981, 128009, 128006,\n",
       "           78191, 128007,    271,  14711,   5867,   6082,  18825,  33257,  15766,\n",
       "             271,   2675,    527,    459,  11084,  20356,   6492,   6335,  58394,\n",
       "             304,  20303,   2363,  21976,  23692,     13,   4718,   3465,    374,\n",
       "             311,  24564,  22256,   3095,    323,  10765,    872,  14604]],\n",
       "        device='cuda:0')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'user\\n\\n### Emotion Analysis Expert Role\\n\\nYou are an advanced emotion analysis expert specializing in comic book dialogue interpretation. Your task is to analyze utterances and identify their emotional content.\\n\\nINPUT:\\n- You will receive a list of utterances from a page in a comic book\\n- The utterance may express one or multiple emotions\\n\\nTASK:\\n1. Carefully analyze the emotional context and tone of each utterance in the page\\n2. Identify applicable emotions from the following classes:\\n   \"anger\", \"disgust\", \"fear\", \"sadness\", \"surprise\", \"joy\", \"neutral\"\\n3. For each utterance in a comic page, identify all emotions present and return an array of emotion arrays in order.\\n\\nRULES:\\n1. Use ONLY the labels listed above\\n2. Output must be a JSON with single key \"page_utterance_emotions\"\\n3. Value must be an array where:\\n   - Each element is an array of emotions for one utterance\\n   - Order matches the input utterances order\\n   - Multiple emotions are allowed per utterance\\n4. No explanations, only JSON output\\n\\nIMPORTANT:\\n- Each array element corresponds to one utterance\\n- One utterance can have multiple emotions\\n- Maintain exact spelling and case of emotion labels\\n- Keep emotions in arrays even for single emotions\\n\\nNow analyze these utterances in a page:\\n1. HOW\\'S IT GOING?\\n2. HEY.\\n3. CAN I GET YOU ANYTHING?\\n4. JUST A COKE.\\n5. OKAY. COMING UP.\\n6. THANKS.\\n7. HOW IS IT OUT THERE? GETTING HOT?\\n8. IT\\'S ALL RIGHT.\\n9. ONE COKE. ENJOY.\\n10. @ONCE UPON A TIME…\\n11. @… IN A FAR OFF KINGDOM…assistant\\n\\n### Emotion Analysis Expert Role\\n\\nYou are an advanced emotion analysis expert specializing in comic book dialogue interpretation. Your task is to analyze utterances and identify their emotional'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenizer.decode(responses[0][0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generated = generation_model.generate(**inputs, max_new_tokens=32, pad_token_id=inference_tokenizer.eos_token_id, do_sample=True,\n",
    "#      temperature=0.1,\n",
    "#      top_p=0.9,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
