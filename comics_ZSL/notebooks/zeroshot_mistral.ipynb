{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfad27ad1be34978be0bd308e6805e7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/141k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12bd71c85d85404d9a427fdd0dd1f61d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/587k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5d4c55e68d9473fb904ba7529860a91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1acc67d086f44ff999cd2b8a30fa2aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/414 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(utterance):\n",
    "\n",
    "    conversation = [\n",
    "        {\"role\": \"system\", \"content\": \"### Task description: You are an expert sentiment analysis assistant that takes an utterance from a comic book and must classify the utterance into appropriate emotion class(s): anger, surprise, fear, disgust, sadness, joy, neutral. You must absolutely not generate any text or explanation other than the following JSON format: {\\\"utterance_emotion\\\": \\\"<predicted emotion classes for the utterance (str)>}\\\"\\n\\n\"},\n",
    "        {\"role\":\"user\", \"content\": f\"# Utterance:\\n{utterance}\\n\\n# Result:\\n\"}\n",
    "    ]\n",
    "\n",
    "    return conversation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Utilisateurs/umushtaq/emotion_analysis_comics/zeroshot/datasets/comics_data_processed.csv\")\n",
    "df = df.drop(columns=[df.columns[0], df.columns[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {\n",
    "    'AN': 'anger',\n",
    "    'DI': 'disgust',\n",
    "    'FE': 'fear',\n",
    "    'SA': 'sadness',\n",
    "    'SU': 'surprise',\n",
    "    'JO': 'joy'\n",
    "}\n",
    "labels = [\"anger\", \"surprise\", \"fear\", \"disgust\", \"sadness\", \"joy\", \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_emotions(row):\n",
    "\n",
    "    emotion_str = row.emotion\n",
    "\n",
    "    if emotion_str == 'Neutral':\n",
    "        return ['neutral']\n",
    "\n",
    "    emotions = emotion_str.split('-')\n",
    "    tags = []\n",
    "\n",
    "    for emotion in emotions:\n",
    "        abbrev = emotion[:2]  # Get the abbreviation\n",
    "        value_part = emotion[2:]  # Get the value part\n",
    "        \n",
    "        if abbrev in emotion_map and value_part.isdigit():\n",
    "            value = int(value_part)\n",
    "            if value > 0:\n",
    "                tags.append(emotion_map[abbrev].lower())\n",
    "        else:\n",
    "            print(f\"Warning: Skipping invalid emotion entry: '{emotion}'\")\n",
    "    return tags  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['emotions_list'] = df.apply(lambda row: extract_emotions(row), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df.utterance.tolist()\n",
    "texts = [make_prompt(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.apply_chat_template(\n",
    "            texts,\n",
    "            #tools=tools,\n",
    "            # pad_token = tokenizer.eos_token,\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            add_generation_prompt=True,\n",
    "            return_dict=True,\n",
    "            return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9a14f4be374c62a965b6abdf7e2b9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/601 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056958bb52034f10bfdb0d3e63704ab8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45194151a87e4ed794bd5a1361c08694",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12684147c5654482b2da1bc84b024067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb7dd8036c74c61967a37d6e2f9eda3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e42e4318a5e44cde92a394ae98dd7671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.55G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9214cc7613b346fbbd5f0339a182bf51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0896e7d50b9a406283e87229d18cc683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_tensor(tensor, batch_size):\n",
    "    return [tensor[i:i+batch_size] for i in range(0, tensor.size(0), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids_batches = batch_tensor(inputs['input_ids'], BATCH_SIZE)\n",
    "attention_mask_batches = batch_tensor(inputs['attention_mask'], BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_outputs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "for i, (input_ids_batch, attention_mask_batch) in enumerate(zip(input_ids_batches, attention_mask_batches)):\n",
    "    \n",
    "    print(f\"Processing batch {i + 1}\")\n",
    "    \n",
    "    # Move tensors to model device\n",
    "    inputs = {\n",
    "        'input_ids': input_ids_batch.to(model.device),\n",
    "        'attention_mask': attention_mask_batch.to(model.device)\n",
    "    }\n",
    "    \n",
    "    # Generate output using model.generate\n",
    "    generated = model.generate(**inputs, max_new_tokens=32)\n",
    "    \n",
    "    # Store the generated output\n",
    "    generated_outputs.append(generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_outputs = []\n",
    "\n",
    "for batch in generated_outputs:\n",
    "\n",
    "    for prediction in batch:\n",
    "\n",
    "        decoded_outputs.append(tokenizer.decode(prediction, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5282"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(decoded_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = []\n",
    "\n",
    "for decoded_ouput in decoded_outputs:\n",
    "    x.append(decoded_ouput.split(\"Result:\\n\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = []\n",
    "\n",
    "for y in x:\n",
    "\n",
    "    match = re.search(r'(\\{.*?\\})', y)\n",
    "\n",
    "    if match:\n",
    "\n",
    "        json_str = match.group(1)  # Extract the JSON object part\n",
    "        try:\n",
    "            # Parse the JSON string\n",
    "            parsed_json = json.loads(json_str)\n",
    "            \n",
    "            # Extract the 'utterance_emotion' value\n",
    "            utterance_emotion = parsed_json.get('utterance_emotion')\n",
    "            z.append(utterance_emotion)\n",
    "            #print(\"Extracted utterance_emotion:\", utterance_emotion)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error parsing JSON: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_l = [[emotion] for emotion in z]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = df.emotions_list.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = [\"anger\", \"surprise\", \"fear\", \"disgust\", \"sadness\", \"joy\", \"neutral\"]\n",
    "\n",
    "def labels_to_binary_matrix(label_list, all_labels):\n",
    "    binary_matrix = np.zeros((len(label_list), len(all_labels)))\n",
    "    for i, labels in enumerate(label_list):\n",
    "        for label in labels:\n",
    "            if label in all_labels:\n",
    "                binary_matrix[i][all_labels.index(label)] = 1\n",
    "    return binary_matrix\n",
    "\n",
    "def opposite(component_type):\n",
    "\n",
    "    if component_type == \"anger\":\n",
    "        return \"surprise\"\n",
    "    elif component_type == \"disgust\":\n",
    "        return \"joy\"\n",
    "    elif component_type == \"fear\":\n",
    "        return \"sadness\"\n",
    "    elif component_type == \"sadness\":\n",
    "        return \"anger\"\n",
    "    elif component_type == \"surprise\":\n",
    "        return \"disgust\"\n",
    "    elif component_type == \"joy\":\n",
    "        return \"fear\"\n",
    "    elif component_type == \"Neutral\":\n",
    "        return \"sadness\"\n",
    "    \n",
    "\n",
    "def harmonize_preds(grounds, preds):\n",
    "\n",
    "    l1, l2 = len(preds), len(grounds)\n",
    "    if l1 < l2:\n",
    "        diff = l2 - l1\n",
    "        preds = preds + [opposite(x) for x in grounds[l1:]]\n",
    "    else:\n",
    "        preds = preds[:l2]\n",
    "        \n",
    "    return preds \n",
    "\n",
    "def post_process_zs(grounds, preds):\n",
    "\n",
    "    for i,(x,y) in enumerate(zip(grounds, preds)):\n",
    "        \n",
    "        if len(x) != len(y):\n",
    "            \n",
    "            preds[i] = harmonize_preds(x, y)\n",
    "\n",
    "    true_matrix = labels_to_binary_matrix(grounds, all_labels)\n",
    "    predicted_matrix = labels_to_binary_matrix(preds, all_labels)\n",
    "\n",
    "    return true_matrix, predicted_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds_matrix, preds_matrix = post_process_zs(grounds, preds_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n       anger      0.562     0.524     0.542      1791\\n    surprise      0.710     0.200     0.312      1590\\n        fear      0.240     0.101     0.142      1373\\n     disgust      0.058     0.180     0.088       311\\n     sadness      0.382     0.183     0.247      1238\\n         joy      0.427     0.228     0.298      1104\\n     neutral      0.116     0.761     0.201       343\\n\\n   micro avg      0.309     0.283     0.295      7750\\n   macro avg      0.357     0.311     0.261      7750\\nweighted avg      0.447     0.283     0.309      7750\\n samples avg      0.309     0.296     0.300      7750\\n'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report(grounds_matrix, preds_matrix, target_names=all_labels, digits=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_file = Path(\"/Utilisateurs/umushtaq/emotion_analysis_comics/zeroshot/results/zs_Mistral-7B-Instruct-v0.3\") / \"results.pickle\"\n",
    "results_file.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with results_file.open('wb') as fh:\n",
    "    results_d = {\"ground_truths\": grounds,\n",
    "                 \"predictions\": preds_l    \n",
    "        \n",
    "    }\n",
    "    pickle.dump(results_d, fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Utilisateurs/umushtaq/.conda/envs/er_nb_env/lib/python3.11/site-packages/sklearn/metrics/_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "classification_file = Path(\"/Utilisateurs/umushtaq/emotion_analysis_comics/zeroshot/results/zs_Mistral-7B-Instruct-v0.3\") / \"classification_report.pickle\"\n",
    "\n",
    "with classification_file.open('wb') as fh:\n",
    "    \n",
    "    pickle.dump(classification_report(grounds_matrix, preds_matrix, target_names=all_labels, output_dict=True), fh)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
