{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.5891934843067144,
  "eval_steps": 755,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007945967421533572,
      "grad_norm": 5.095927715301514,
      "learning_rate": 2e-05,
      "loss": 3.0773,
      "step": 1
    },
    {
      "epoch": 0.0015891934843067143,
      "grad_norm": 5.295345783233643,
      "learning_rate": 4e-05,
      "loss": 3.0638,
      "step": 2
    },
    {
      "epoch": 0.0023837902264600714,
      "grad_norm": 3.9243922233581543,
      "learning_rate": 6e-05,
      "loss": 2.9447,
      "step": 3
    },
    {
      "epoch": 0.0031783869686134287,
      "grad_norm": 2.9748141765594482,
      "learning_rate": 8e-05,
      "loss": 2.7782,
      "step": 4
    },
    {
      "epoch": 0.003972983710766786,
      "grad_norm": 6.790021896362305,
      "learning_rate": 0.0001,
      "loss": 2.4546,
      "step": 5
    },
    {
      "epoch": 0.004767580452920143,
      "grad_norm": 2.016477584838867,
      "learning_rate": 0.00012,
      "loss": 2.2744,
      "step": 6
    },
    {
      "epoch": 0.0055621771950735005,
      "grad_norm": 1.9537750482559204,
      "learning_rate": 0.00014,
      "loss": 2.0014,
      "step": 7
    },
    {
      "epoch": 0.006356773937226857,
      "grad_norm": 1.7805707454681396,
      "learning_rate": 0.00016,
      "loss": 1.7197,
      "step": 8
    },
    {
      "epoch": 0.007151370679380214,
      "grad_norm": 1.8606624603271484,
      "learning_rate": 0.00018,
      "loss": 1.3944,
      "step": 9
    },
    {
      "epoch": 0.007945967421533572,
      "grad_norm": 5.094567775726318,
      "learning_rate": 0.0002,
      "loss": 1.068,
      "step": 10
    },
    {
      "epoch": 0.00874056416368693,
      "grad_norm": 7.3542938232421875,
      "learning_rate": 0.0001999468650371945,
      "loss": 0.8414,
      "step": 11
    },
    {
      "epoch": 0.009535160905840286,
      "grad_norm": 2.5358312129974365,
      "learning_rate": 0.00019989373007438897,
      "loss": 0.575,
      "step": 12
    },
    {
      "epoch": 0.010329757647993643,
      "grad_norm": 1.625028133392334,
      "learning_rate": 0.00019984059511158345,
      "loss": 0.4307,
      "step": 13
    },
    {
      "epoch": 0.011124354390147001,
      "grad_norm": 1.3451693058013916,
      "learning_rate": 0.0001997874601487779,
      "loss": 0.3319,
      "step": 14
    },
    {
      "epoch": 0.011918951132300357,
      "grad_norm": 1.885640025138855,
      "learning_rate": 0.00019973432518597238,
      "loss": 0.3196,
      "step": 15
    },
    {
      "epoch": 0.012713547874453715,
      "grad_norm": 1.2925968170166016,
      "learning_rate": 0.00019968119022316684,
      "loss": 0.2892,
      "step": 16
    },
    {
      "epoch": 0.013508144616607072,
      "grad_norm": 0.6818913221359253,
      "learning_rate": 0.00019962805526036132,
      "loss": 0.2906,
      "step": 17
    },
    {
      "epoch": 0.014302741358760428,
      "grad_norm": 1.793684482574463,
      "learning_rate": 0.0001995749202975558,
      "loss": 0.2529,
      "step": 18
    },
    {
      "epoch": 0.015097338100913786,
      "grad_norm": 1.2905668020248413,
      "learning_rate": 0.00019952178533475028,
      "loss": 0.2545,
      "step": 19
    },
    {
      "epoch": 0.015891934843067144,
      "grad_norm": 3.4983420372009277,
      "learning_rate": 0.00019946865037194476,
      "loss": 0.2479,
      "step": 20
    },
    {
      "epoch": 0.0166865315852205,
      "grad_norm": 1.3703911304473877,
      "learning_rate": 0.00019941551540913924,
      "loss": 0.2466,
      "step": 21
    },
    {
      "epoch": 0.01748112832737386,
      "grad_norm": 1.2794137001037598,
      "learning_rate": 0.00019936238044633372,
      "loss": 0.2425,
      "step": 22
    },
    {
      "epoch": 0.018275725069527213,
      "grad_norm": 4.779952049255371,
      "learning_rate": 0.00019930924548352817,
      "loss": 0.1825,
      "step": 23
    },
    {
      "epoch": 0.01907032181168057,
      "grad_norm": 6.491456985473633,
      "learning_rate": 0.00019925611052072265,
      "loss": 0.2387,
      "step": 24
    },
    {
      "epoch": 0.01986491855383393,
      "grad_norm": 5.676915645599365,
      "learning_rate": 0.0001992029755579171,
      "loss": 0.2204,
      "step": 25
    },
    {
      "epoch": 0.020659515295987287,
      "grad_norm": 0.3911113440990448,
      "learning_rate": 0.00019914984059511158,
      "loss": 0.1705,
      "step": 26
    },
    {
      "epoch": 0.021454112038140644,
      "grad_norm": 0.50523841381073,
      "learning_rate": 0.00019909670563230606,
      "loss": 0.1899,
      "step": 27
    },
    {
      "epoch": 0.022248708780294002,
      "grad_norm": 0.5957397222518921,
      "learning_rate": 0.00019904357066950054,
      "loss": 0.1861,
      "step": 28
    },
    {
      "epoch": 0.023043305522447356,
      "grad_norm": 0.775301992893219,
      "learning_rate": 0.00019899043570669502,
      "loss": 0.1574,
      "step": 29
    },
    {
      "epoch": 0.023837902264600714,
      "grad_norm": 1.0150798559188843,
      "learning_rate": 0.0001989373007438895,
      "loss": 0.1264,
      "step": 30
    },
    {
      "epoch": 0.02463249900675407,
      "grad_norm": 0.7563843131065369,
      "learning_rate": 0.00019888416578108396,
      "loss": 0.1429,
      "step": 31
    },
    {
      "epoch": 0.02542709574890743,
      "grad_norm": 0.4484422504901886,
      "learning_rate": 0.00019883103081827844,
      "loss": 0.1387,
      "step": 32
    },
    {
      "epoch": 0.026221692491060787,
      "grad_norm": 0.37793880701065063,
      "learning_rate": 0.00019877789585547292,
      "loss": 0.0917,
      "step": 33
    },
    {
      "epoch": 0.027016289233214145,
      "grad_norm": 0.6019443273544312,
      "learning_rate": 0.00019872476089266737,
      "loss": 0.123,
      "step": 34
    },
    {
      "epoch": 0.027810885975367503,
      "grad_norm": 0.48734718561172485,
      "learning_rate": 0.00019867162592986185,
      "loss": 0.131,
      "step": 35
    },
    {
      "epoch": 0.028605482717520857,
      "grad_norm": 0.5307793617248535,
      "learning_rate": 0.00019861849096705633,
      "loss": 0.0855,
      "step": 36
    },
    {
      "epoch": 0.029400079459674214,
      "grad_norm": 0.577855110168457,
      "learning_rate": 0.0001985653560042508,
      "loss": 0.082,
      "step": 37
    },
    {
      "epoch": 0.030194676201827572,
      "grad_norm": 0.37780269980430603,
      "learning_rate": 0.0001985122210414453,
      "loss": 0.0856,
      "step": 38
    },
    {
      "epoch": 0.03098927294398093,
      "grad_norm": 0.20342932641506195,
      "learning_rate": 0.00019845908607863975,
      "loss": 0.0824,
      "step": 39
    },
    {
      "epoch": 0.03178386968613429,
      "grad_norm": 0.36485666036605835,
      "learning_rate": 0.00019840595111583423,
      "loss": 0.0866,
      "step": 40
    },
    {
      "epoch": 0.03257846642828764,
      "grad_norm": 0.38292914628982544,
      "learning_rate": 0.0001983528161530287,
      "loss": 0.0813,
      "step": 41
    },
    {
      "epoch": 0.033373063170441,
      "grad_norm": 0.42815956473350525,
      "learning_rate": 0.00019829968119022319,
      "loss": 0.0777,
      "step": 42
    },
    {
      "epoch": 0.03416765991259436,
      "grad_norm": 0.42552876472473145,
      "learning_rate": 0.00019824654622741764,
      "loss": 0.0506,
      "step": 43
    },
    {
      "epoch": 0.03496225665474772,
      "grad_norm": 0.47190940380096436,
      "learning_rate": 0.00019819341126461212,
      "loss": 0.0624,
      "step": 44
    },
    {
      "epoch": 0.03575685339690107,
      "grad_norm": 0.2830035090446472,
      "learning_rate": 0.0001981402763018066,
      "loss": 0.0616,
      "step": 45
    },
    {
      "epoch": 0.03655145013905443,
      "grad_norm": 0.5423195362091064,
      "learning_rate": 0.00019808714133900108,
      "loss": 0.0614,
      "step": 46
    },
    {
      "epoch": 0.03734604688120779,
      "grad_norm": 0.20404629409313202,
      "learning_rate": 0.00019803400637619553,
      "loss": 0.0504,
      "step": 47
    },
    {
      "epoch": 0.03814064362336114,
      "grad_norm": 0.2916088402271271,
      "learning_rate": 0.00019798087141339,
      "loss": 0.0394,
      "step": 48
    },
    {
      "epoch": 0.038935240365514504,
      "grad_norm": 6.089454650878906,
      "learning_rate": 0.0001979277364505845,
      "loss": 0.0444,
      "step": 49
    },
    {
      "epoch": 0.03972983710766786,
      "grad_norm": 0.31506067514419556,
      "learning_rate": 0.00019787460148777897,
      "loss": 0.035,
      "step": 50
    },
    {
      "epoch": 0.04052443384982122,
      "grad_norm": 1.4184181690216064,
      "learning_rate": 0.00019782146652497345,
      "loss": 0.749,
      "step": 51
    },
    {
      "epoch": 0.04131903059197457,
      "grad_norm": 2.142108201980591,
      "learning_rate": 0.0001977683315621679,
      "loss": 0.5594,
      "step": 52
    },
    {
      "epoch": 0.04211362733412793,
      "grad_norm": 0.6221466064453125,
      "learning_rate": 0.0001977151965993624,
      "loss": 0.494,
      "step": 53
    },
    {
      "epoch": 0.04290822407628129,
      "grad_norm": 0.39055222272872925,
      "learning_rate": 0.00019766206163655687,
      "loss": 0.3899,
      "step": 54
    },
    {
      "epoch": 0.04370282081843464,
      "grad_norm": 0.4236755669116974,
      "learning_rate": 0.00019760892667375135,
      "loss": 0.3525,
      "step": 55
    },
    {
      "epoch": 0.044497417560588004,
      "grad_norm": 0.3338366746902466,
      "learning_rate": 0.0001975557917109458,
      "loss": 0.3568,
      "step": 56
    },
    {
      "epoch": 0.04529201430274136,
      "grad_norm": 0.34272903203964233,
      "learning_rate": 0.00019750265674814028,
      "loss": 0.3519,
      "step": 57
    },
    {
      "epoch": 0.04608661104489471,
      "grad_norm": 0.28583958745002747,
      "learning_rate": 0.00019744952178533476,
      "loss": 0.2993,
      "step": 58
    },
    {
      "epoch": 0.046881207787048074,
      "grad_norm": 0.3079226315021515,
      "learning_rate": 0.00019739638682252924,
      "loss": 0.3023,
      "step": 59
    },
    {
      "epoch": 0.04767580452920143,
      "grad_norm": 0.2857295572757721,
      "learning_rate": 0.00019734325185972372,
      "loss": 0.2599,
      "step": 60
    },
    {
      "epoch": 0.04847040127135479,
      "grad_norm": 0.26531514525413513,
      "learning_rate": 0.00019729011689691817,
      "loss": 0.2562,
      "step": 61
    },
    {
      "epoch": 0.04926499801350814,
      "grad_norm": 0.2522497773170471,
      "learning_rate": 0.00019723698193411265,
      "loss": 0.2805,
      "step": 62
    },
    {
      "epoch": 0.050059594755661505,
      "grad_norm": 0.19372712075710297,
      "learning_rate": 0.00019718384697130713,
      "loss": 0.2578,
      "step": 63
    },
    {
      "epoch": 0.05085419149781486,
      "grad_norm": 0.2371576875448227,
      "learning_rate": 0.0001971307120085016,
      "loss": 0.2443,
      "step": 64
    },
    {
      "epoch": 0.05164878823996821,
      "grad_norm": 0.2858528792858124,
      "learning_rate": 0.00019707757704569607,
      "loss": 0.2271,
      "step": 65
    },
    {
      "epoch": 0.052443384982121574,
      "grad_norm": 0.23870187997817993,
      "learning_rate": 0.00019702444208289055,
      "loss": 0.1795,
      "step": 66
    },
    {
      "epoch": 0.05323798172427493,
      "grad_norm": 0.22088764607906342,
      "learning_rate": 0.00019697130712008503,
      "loss": 0.2156,
      "step": 67
    },
    {
      "epoch": 0.05403257846642829,
      "grad_norm": 0.22069445252418518,
      "learning_rate": 0.0001969181721572795,
      "loss": 0.1977,
      "step": 68
    },
    {
      "epoch": 0.054827175208581644,
      "grad_norm": 0.2257574200630188,
      "learning_rate": 0.000196865037194474,
      "loss": 0.1556,
      "step": 69
    },
    {
      "epoch": 0.055621771950735005,
      "grad_norm": 0.24251490831375122,
      "learning_rate": 0.00019681190223166844,
      "loss": 0.1942,
      "step": 70
    },
    {
      "epoch": 0.05641636869288836,
      "grad_norm": 0.22877848148345947,
      "learning_rate": 0.00019675876726886292,
      "loss": 0.2078,
      "step": 71
    },
    {
      "epoch": 0.057210965435041714,
      "grad_norm": 0.2661331593990326,
      "learning_rate": 0.00019670563230605737,
      "loss": 0.1829,
      "step": 72
    },
    {
      "epoch": 0.058005562177195075,
      "grad_norm": 0.18543265759944916,
      "learning_rate": 0.00019665249734325185,
      "loss": 0.1766,
      "step": 73
    },
    {
      "epoch": 0.05880015891934843,
      "grad_norm": 0.19438567757606506,
      "learning_rate": 0.00019659936238044634,
      "loss": 0.1803,
      "step": 74
    },
    {
      "epoch": 0.05959475566150179,
      "grad_norm": 0.20390024781227112,
      "learning_rate": 0.00019654622741764082,
      "loss": 0.1298,
      "step": 75
    },
    {
      "epoch": 0.060389352403655144,
      "grad_norm": 0.18488165736198425,
      "learning_rate": 0.0001964930924548353,
      "loss": 0.1342,
      "step": 76
    },
    {
      "epoch": 0.061183949145808506,
      "grad_norm": 0.29963284730911255,
      "learning_rate": 0.00019643995749202978,
      "loss": 0.1634,
      "step": 77
    },
    {
      "epoch": 0.06197854588796186,
      "grad_norm": 0.1895529329776764,
      "learning_rate": 0.00019638682252922426,
      "loss": 0.1159,
      "step": 78
    },
    {
      "epoch": 0.06277314263011521,
      "grad_norm": 0.1705477386713028,
      "learning_rate": 0.0001963336875664187,
      "loss": 0.126,
      "step": 79
    },
    {
      "epoch": 0.06356773937226858,
      "grad_norm": 0.20234054327011108,
      "learning_rate": 0.0001962805526036132,
      "loss": 0.109,
      "step": 80
    },
    {
      "epoch": 0.06436233611442194,
      "grad_norm": 0.16916969418525696,
      "learning_rate": 0.00019622741764080764,
      "loss": 0.1424,
      "step": 81
    },
    {
      "epoch": 0.06515693285657528,
      "grad_norm": 0.17861494421958923,
      "learning_rate": 0.00019617428267800212,
      "loss": 0.1154,
      "step": 82
    },
    {
      "epoch": 0.06595152959872864,
      "grad_norm": 0.20245608687400818,
      "learning_rate": 0.0001961211477151966,
      "loss": 0.1337,
      "step": 83
    },
    {
      "epoch": 0.066746126340882,
      "grad_norm": 0.15971405804157257,
      "learning_rate": 0.00019606801275239108,
      "loss": 0.127,
      "step": 84
    },
    {
      "epoch": 0.06754072308303535,
      "grad_norm": 0.1884482502937317,
      "learning_rate": 0.00019601487778958556,
      "loss": 0.1215,
      "step": 85
    },
    {
      "epoch": 0.06833531982518871,
      "grad_norm": 1.080165147781372,
      "learning_rate": 0.00019596174282678004,
      "loss": 0.1195,
      "step": 86
    },
    {
      "epoch": 0.06912991656734208,
      "grad_norm": 0.1586466282606125,
      "learning_rate": 0.00019590860786397452,
      "loss": 0.0987,
      "step": 87
    },
    {
      "epoch": 0.06992451330949544,
      "grad_norm": 0.19644121825695038,
      "learning_rate": 0.00019585547290116898,
      "loss": 0.0735,
      "step": 88
    },
    {
      "epoch": 0.07071911005164878,
      "grad_norm": 0.15086694061756134,
      "learning_rate": 0.00019580233793836346,
      "loss": 0.0821,
      "step": 89
    },
    {
      "epoch": 0.07151370679380215,
      "grad_norm": 0.17575614154338837,
      "learning_rate": 0.0001957492029755579,
      "loss": 0.0847,
      "step": 90
    },
    {
      "epoch": 0.0723083035359555,
      "grad_norm": 0.15817910432815552,
      "learning_rate": 0.0001956960680127524,
      "loss": 0.0775,
      "step": 91
    },
    {
      "epoch": 0.07310290027810885,
      "grad_norm": 0.28526571393013,
      "learning_rate": 0.00019564293304994687,
      "loss": 0.0745,
      "step": 92
    },
    {
      "epoch": 0.07389749702026222,
      "grad_norm": 0.1360766887664795,
      "learning_rate": 0.00019558979808714135,
      "loss": 0.0637,
      "step": 93
    },
    {
      "epoch": 0.07469209376241558,
      "grad_norm": 0.13167662918567657,
      "learning_rate": 0.00019553666312433583,
      "loss": 0.0592,
      "step": 94
    },
    {
      "epoch": 0.07548669050456894,
      "grad_norm": 0.14108988642692566,
      "learning_rate": 0.0001954835281615303,
      "loss": 0.0381,
      "step": 95
    },
    {
      "epoch": 0.07628128724672228,
      "grad_norm": 0.09228210151195526,
      "learning_rate": 0.00019543039319872476,
      "loss": 0.0453,
      "step": 96
    },
    {
      "epoch": 0.07707588398887565,
      "grad_norm": 0.11658669263124466,
      "learning_rate": 0.00019537725823591924,
      "loss": 0.0418,
      "step": 97
    },
    {
      "epoch": 0.07787048073102901,
      "grad_norm": 0.13790568709373474,
      "learning_rate": 0.00019532412327311372,
      "loss": 0.0307,
      "step": 98
    },
    {
      "epoch": 0.07866507747318235,
      "grad_norm": 0.12323784083127975,
      "learning_rate": 0.00019527098831030818,
      "loss": 0.0473,
      "step": 99
    },
    {
      "epoch": 0.07945967421533572,
      "grad_norm": 0.10029814392328262,
      "learning_rate": 0.00019521785334750266,
      "loss": 0.033,
      "step": 100
    },
    {
      "epoch": 0.08025427095748908,
      "grad_norm": 2.2295618057250977,
      "learning_rate": 0.00019516471838469714,
      "loss": 0.8273,
      "step": 101
    },
    {
      "epoch": 0.08104886769964244,
      "grad_norm": 0.9544523358345032,
      "learning_rate": 0.00019511158342189162,
      "loss": 0.5899,
      "step": 102
    },
    {
      "epoch": 0.08184346444179579,
      "grad_norm": 0.7177934646606445,
      "learning_rate": 0.0001950584484590861,
      "loss": 0.4572,
      "step": 103
    },
    {
      "epoch": 0.08263806118394915,
      "grad_norm": 0.38069862127304077,
      "learning_rate": 0.00019500531349628058,
      "loss": 0.3659,
      "step": 104
    },
    {
      "epoch": 0.08343265792610251,
      "grad_norm": 0.3473935127258301,
      "learning_rate": 0.00019495217853347503,
      "loss": 0.3858,
      "step": 105
    },
    {
      "epoch": 0.08422725466825585,
      "grad_norm": 0.36638519167900085,
      "learning_rate": 0.0001948990435706695,
      "loss": 0.3261,
      "step": 106
    },
    {
      "epoch": 0.08502185141040922,
      "grad_norm": 0.5606485605239868,
      "learning_rate": 0.000194845908607864,
      "loss": 0.3292,
      "step": 107
    },
    {
      "epoch": 0.08581644815256258,
      "grad_norm": 2.4229931831359863,
      "learning_rate": 0.00019479277364505844,
      "loss": 0.419,
      "step": 108
    },
    {
      "epoch": 0.08661104489471594,
      "grad_norm": 5.446245193481445,
      "learning_rate": 0.00019473963868225293,
      "loss": 0.3847,
      "step": 109
    },
    {
      "epoch": 0.08740564163686929,
      "grad_norm": 0.5252211689949036,
      "learning_rate": 0.0001946865037194474,
      "loss": 0.3849,
      "step": 110
    },
    {
      "epoch": 0.08820023837902265,
      "grad_norm": 0.539202868938446,
      "learning_rate": 0.00019463336875664189,
      "loss": 0.309,
      "step": 111
    },
    {
      "epoch": 0.08899483512117601,
      "grad_norm": 0.27690789103507996,
      "learning_rate": 0.00019458023379383637,
      "loss": 0.2729,
      "step": 112
    },
    {
      "epoch": 0.08978943186332936,
      "grad_norm": 0.27345505356788635,
      "learning_rate": 0.00019452709883103082,
      "loss": 0.2915,
      "step": 113
    },
    {
      "epoch": 0.09058402860548272,
      "grad_norm": 0.30040210485458374,
      "learning_rate": 0.0001944739638682253,
      "loss": 0.2996,
      "step": 114
    },
    {
      "epoch": 0.09137862534763608,
      "grad_norm": 0.31928932666778564,
      "learning_rate": 0.00019442082890541978,
      "loss": 0.2556,
      "step": 115
    },
    {
      "epoch": 0.09217322208978942,
      "grad_norm": 0.2683221995830536,
      "learning_rate": 0.00019436769394261426,
      "loss": 0.2559,
      "step": 116
    },
    {
      "epoch": 0.09296781883194279,
      "grad_norm": 0.27670955657958984,
      "learning_rate": 0.0001943145589798087,
      "loss": 0.2535,
      "step": 117
    },
    {
      "epoch": 0.09376241557409615,
      "grad_norm": 0.2962208390235901,
      "learning_rate": 0.0001942614240170032,
      "loss": 0.2281,
      "step": 118
    },
    {
      "epoch": 0.09455701231624951,
      "grad_norm": 0.24815694987773895,
      "learning_rate": 0.00019420828905419767,
      "loss": 0.2095,
      "step": 119
    },
    {
      "epoch": 0.09535160905840286,
      "grad_norm": 0.3147117793560028,
      "learning_rate": 0.00019415515409139215,
      "loss": 0.2255,
      "step": 120
    },
    {
      "epoch": 0.09614620580055622,
      "grad_norm": 0.30625513195991516,
      "learning_rate": 0.0001941020191285866,
      "loss": 0.1762,
      "step": 121
    },
    {
      "epoch": 0.09694080254270958,
      "grad_norm": 0.38549360632896423,
      "learning_rate": 0.00019404888416578109,
      "loss": 0.182,
      "step": 122
    },
    {
      "epoch": 0.09773539928486293,
      "grad_norm": 0.34338653087615967,
      "learning_rate": 0.00019399574920297557,
      "loss": 0.1489,
      "step": 123
    },
    {
      "epoch": 0.09852999602701629,
      "grad_norm": 0.3558342456817627,
      "learning_rate": 0.00019394261424017005,
      "loss": 0.1786,
      "step": 124
    },
    {
      "epoch": 0.09932459276916965,
      "grad_norm": 0.39403101801872253,
      "learning_rate": 0.00019388947927736453,
      "loss": 0.1454,
      "step": 125
    },
    {
      "epoch": 0.10011918951132301,
      "grad_norm": 0.45242002606391907,
      "learning_rate": 0.00019383634431455898,
      "loss": 0.1497,
      "step": 126
    },
    {
      "epoch": 0.10091378625347636,
      "grad_norm": 0.66505366563797,
      "learning_rate": 0.00019378320935175346,
      "loss": 0.1607,
      "step": 127
    },
    {
      "epoch": 0.10170838299562972,
      "grad_norm": 1.2089029550552368,
      "learning_rate": 0.00019373007438894794,
      "loss": 0.1418,
      "step": 128
    },
    {
      "epoch": 0.10250297973778308,
      "grad_norm": 0.8559134602546692,
      "learning_rate": 0.0001936769394261424,
      "loss": 0.1274,
      "step": 129
    },
    {
      "epoch": 0.10329757647993643,
      "grad_norm": 0.29022493958473206,
      "learning_rate": 0.00019362380446333687,
      "loss": 0.0959,
      "step": 130
    },
    {
      "epoch": 0.10409217322208979,
      "grad_norm": 0.15817007422447205,
      "learning_rate": 0.00019357066950053135,
      "loss": 0.1159,
      "step": 131
    },
    {
      "epoch": 0.10488676996424315,
      "grad_norm": 0.178268700838089,
      "learning_rate": 0.00019351753453772583,
      "loss": 0.1063,
      "step": 132
    },
    {
      "epoch": 0.10568136670639651,
      "grad_norm": 0.19003534317016602,
      "learning_rate": 0.00019346439957492031,
      "loss": 0.136,
      "step": 133
    },
    {
      "epoch": 0.10647596344854986,
      "grad_norm": 0.1463114619255066,
      "learning_rate": 0.0001934112646121148,
      "loss": 0.1085,
      "step": 134
    },
    {
      "epoch": 0.10727056019070322,
      "grad_norm": 0.15220893919467926,
      "learning_rate": 0.00019335812964930927,
      "loss": 0.0957,
      "step": 135
    },
    {
      "epoch": 0.10806515693285658,
      "grad_norm": 0.13502717018127441,
      "learning_rate": 0.00019330499468650373,
      "loss": 0.0958,
      "step": 136
    },
    {
      "epoch": 0.10885975367500993,
      "grad_norm": 0.14319714903831482,
      "learning_rate": 0.00019325185972369818,
      "loss": 0.1062,
      "step": 137
    },
    {
      "epoch": 0.10965435041716329,
      "grad_norm": 0.15498748421669006,
      "learning_rate": 0.00019319872476089266,
      "loss": 0.0713,
      "step": 138
    },
    {
      "epoch": 0.11044894715931665,
      "grad_norm": 0.1616978794336319,
      "learning_rate": 0.00019314558979808714,
      "loss": 0.103,
      "step": 139
    },
    {
      "epoch": 0.11124354390147001,
      "grad_norm": 0.13654562830924988,
      "learning_rate": 0.00019309245483528162,
      "loss": 0.0786,
      "step": 140
    },
    {
      "epoch": 0.11203814064362336,
      "grad_norm": 0.1615074723958969,
      "learning_rate": 0.0001930393198724761,
      "loss": 0.075,
      "step": 141
    },
    {
      "epoch": 0.11283273738577672,
      "grad_norm": 0.10743191093206406,
      "learning_rate": 0.00019298618490967058,
      "loss": 0.0641,
      "step": 142
    },
    {
      "epoch": 0.11362733412793008,
      "grad_norm": 0.14171114563941956,
      "learning_rate": 0.00019293304994686506,
      "loss": 0.0518,
      "step": 143
    },
    {
      "epoch": 0.11442193087008343,
      "grad_norm": 0.11164332926273346,
      "learning_rate": 0.00019287991498405954,
      "loss": 0.048,
      "step": 144
    },
    {
      "epoch": 0.11521652761223679,
      "grad_norm": 0.10373976826667786,
      "learning_rate": 0.000192826780021254,
      "loss": 0.0583,
      "step": 145
    },
    {
      "epoch": 0.11601112435439015,
      "grad_norm": 0.13332654535770416,
      "learning_rate": 0.00019277364505844845,
      "loss": 0.0367,
      "step": 146
    },
    {
      "epoch": 0.11680572109654351,
      "grad_norm": 0.09105151146650314,
      "learning_rate": 0.00019272051009564293,
      "loss": 0.0506,
      "step": 147
    },
    {
      "epoch": 0.11760031783869686,
      "grad_norm": 0.07898186892271042,
      "learning_rate": 0.0001926673751328374,
      "loss": 0.0367,
      "step": 148
    },
    {
      "epoch": 0.11839491458085022,
      "grad_norm": 0.15004244446754456,
      "learning_rate": 0.0001926142401700319,
      "loss": 0.0397,
      "step": 149
    },
    {
      "epoch": 0.11918951132300358,
      "grad_norm": 0.10065368562936783,
      "learning_rate": 0.00019256110520722637,
      "loss": 0.0252,
      "step": 150
    },
    {
      "epoch": 0.11998410806515693,
      "grad_norm": 2.053464651107788,
      "learning_rate": 0.00019250797024442085,
      "loss": 0.911,
      "step": 151
    },
    {
      "epoch": 0.12077870480731029,
      "grad_norm": 1.0877612829208374,
      "learning_rate": 0.00019245483528161533,
      "loss": 0.5193,
      "step": 152
    },
    {
      "epoch": 0.12157330154946365,
      "grad_norm": 0.5766066908836365,
      "learning_rate": 0.0001924017003188098,
      "loss": 0.4419,
      "step": 153
    },
    {
      "epoch": 0.12236789829161701,
      "grad_norm": 0.3879833221435547,
      "learning_rate": 0.00019234856535600426,
      "loss": 0.3987,
      "step": 154
    },
    {
      "epoch": 0.12316249503377036,
      "grad_norm": 0.3370971083641052,
      "learning_rate": 0.00019229543039319872,
      "loss": 0.3516,
      "step": 155
    },
    {
      "epoch": 0.12395709177592372,
      "grad_norm": 0.3980254530906677,
      "learning_rate": 0.0001922422954303932,
      "loss": 0.3753,
      "step": 156
    },
    {
      "epoch": 0.12475168851807708,
      "grad_norm": 0.3528785705566406,
      "learning_rate": 0.00019218916046758768,
      "loss": 0.3268,
      "step": 157
    },
    {
      "epoch": 0.12554628526023043,
      "grad_norm": 0.3944130837917328,
      "learning_rate": 0.00019213602550478216,
      "loss": 0.3272,
      "step": 158
    },
    {
      "epoch": 0.1263408820023838,
      "grad_norm": 0.25333133339881897,
      "learning_rate": 0.00019208289054197664,
      "loss": 0.2303,
      "step": 159
    },
    {
      "epoch": 0.12713547874453715,
      "grad_norm": 0.3143738806247711,
      "learning_rate": 0.00019202975557917112,
      "loss": 0.3348,
      "step": 160
    },
    {
      "epoch": 0.1279300754866905,
      "grad_norm": 0.25687018036842346,
      "learning_rate": 0.0001919766206163656,
      "loss": 0.3177,
      "step": 161
    },
    {
      "epoch": 0.12872467222884387,
      "grad_norm": 0.27663454413414,
      "learning_rate": 0.00019192348565356005,
      "loss": 0.2664,
      "step": 162
    },
    {
      "epoch": 0.12951926897099722,
      "grad_norm": 0.26116472482681274,
      "learning_rate": 0.00019187035069075453,
      "loss": 0.2391,
      "step": 163
    },
    {
      "epoch": 0.13031386571315057,
      "grad_norm": 0.2535942792892456,
      "learning_rate": 0.00019181721572794898,
      "loss": 0.2354,
      "step": 164
    },
    {
      "epoch": 0.13110846245530394,
      "grad_norm": 0.30917888879776,
      "learning_rate": 0.00019176408076514346,
      "loss": 0.2492,
      "step": 165
    },
    {
      "epoch": 0.1319030591974573,
      "grad_norm": 0.22282648086547852,
      "learning_rate": 0.00019171094580233794,
      "loss": 0.1903,
      "step": 166
    },
    {
      "epoch": 0.13269765593961064,
      "grad_norm": 0.21345986425876617,
      "learning_rate": 0.00019165781083953242,
      "loss": 0.2048,
      "step": 167
    },
    {
      "epoch": 0.133492252681764,
      "grad_norm": 0.20483341813087463,
      "learning_rate": 0.0001916046758767269,
      "loss": 0.1784,
      "step": 168
    },
    {
      "epoch": 0.13428684942391736,
      "grad_norm": 0.23842015862464905,
      "learning_rate": 0.00019155154091392138,
      "loss": 0.1894,
      "step": 169
    },
    {
      "epoch": 0.1350814461660707,
      "grad_norm": 0.19351354241371155,
      "learning_rate": 0.00019149840595111584,
      "loss": 0.1733,
      "step": 170
    },
    {
      "epoch": 0.13587604290822408,
      "grad_norm": 0.1911739706993103,
      "learning_rate": 0.00019144527098831032,
      "loss": 0.1664,
      "step": 171
    },
    {
      "epoch": 0.13667063965037743,
      "grad_norm": 0.18692897260189056,
      "learning_rate": 0.0001913921360255048,
      "loss": 0.1592,
      "step": 172
    },
    {
      "epoch": 0.1374652363925308,
      "grad_norm": 0.19956745207309723,
      "learning_rate": 0.00019133900106269925,
      "loss": 0.1689,
      "step": 173
    },
    {
      "epoch": 0.13825983313468415,
      "grad_norm": 0.20633526146411896,
      "learning_rate": 0.00019128586609989373,
      "loss": 0.1447,
      "step": 174
    },
    {
      "epoch": 0.1390544298768375,
      "grad_norm": 0.1701001673936844,
      "learning_rate": 0.0001912327311370882,
      "loss": 0.1432,
      "step": 175
    },
    {
      "epoch": 0.13984902661899087,
      "grad_norm": 0.217815563082695,
      "learning_rate": 0.0001911795961742827,
      "loss": 0.1469,
      "step": 176
    },
    {
      "epoch": 0.14064362336114422,
      "grad_norm": 0.16553503274917603,
      "learning_rate": 0.00019112646121147717,
      "loss": 0.1273,
      "step": 177
    },
    {
      "epoch": 0.14143822010329757,
      "grad_norm": 0.1608160138130188,
      "learning_rate": 0.00019107332624867162,
      "loss": 0.107,
      "step": 178
    },
    {
      "epoch": 0.14223281684545094,
      "grad_norm": 0.15976440906524658,
      "learning_rate": 0.0001910201912858661,
      "loss": 0.1038,
      "step": 179
    },
    {
      "epoch": 0.1430274135876043,
      "grad_norm": 0.14395356178283691,
      "learning_rate": 0.00019096705632306058,
      "loss": 0.1176,
      "step": 180
    },
    {
      "epoch": 0.14382201032975764,
      "grad_norm": 0.1603294312953949,
      "learning_rate": 0.00019091392136025507,
      "loss": 0.1308,
      "step": 181
    },
    {
      "epoch": 0.144616607071911,
      "grad_norm": 0.1494060456752777,
      "learning_rate": 0.00019086078639744952,
      "loss": 0.1047,
      "step": 182
    },
    {
      "epoch": 0.14541120381406436,
      "grad_norm": 0.14104509353637695,
      "learning_rate": 0.000190807651434644,
      "loss": 0.1106,
      "step": 183
    },
    {
      "epoch": 0.1462058005562177,
      "grad_norm": 0.13810361921787262,
      "learning_rate": 0.00019075451647183848,
      "loss": 0.1022,
      "step": 184
    },
    {
      "epoch": 0.14700039729837108,
      "grad_norm": 0.1312577873468399,
      "learning_rate": 0.00019070138150903296,
      "loss": 0.0748,
      "step": 185
    },
    {
      "epoch": 0.14779499404052443,
      "grad_norm": 0.12264079600572586,
      "learning_rate": 0.0001906482465462274,
      "loss": 0.0969,
      "step": 186
    },
    {
      "epoch": 0.14858959078267778,
      "grad_norm": 0.1455862671136856,
      "learning_rate": 0.0001905951115834219,
      "loss": 0.1209,
      "step": 187
    },
    {
      "epoch": 0.14938418752483115,
      "grad_norm": 0.17013278603553772,
      "learning_rate": 0.00019054197662061637,
      "loss": 0.0815,
      "step": 188
    },
    {
      "epoch": 0.1501787842669845,
      "grad_norm": 0.12392491102218628,
      "learning_rate": 0.00019048884165781085,
      "loss": 0.0988,
      "step": 189
    },
    {
      "epoch": 0.15097338100913787,
      "grad_norm": 0.1342221349477768,
      "learning_rate": 0.00019043570669500533,
      "loss": 0.0677,
      "step": 190
    },
    {
      "epoch": 0.15176797775129122,
      "grad_norm": 0.12016434967517853,
      "learning_rate": 0.0001903825717321998,
      "loss": 0.0629,
      "step": 191
    },
    {
      "epoch": 0.15256257449344457,
      "grad_norm": 0.14949846267700195,
      "learning_rate": 0.00019032943676939427,
      "loss": 0.0847,
      "step": 192
    },
    {
      "epoch": 0.15335717123559794,
      "grad_norm": 0.11531606316566467,
      "learning_rate": 0.00019027630180658875,
      "loss": 0.0965,
      "step": 193
    },
    {
      "epoch": 0.1541517679777513,
      "grad_norm": 0.09808109700679779,
      "learning_rate": 0.00019022316684378323,
      "loss": 0.0548,
      "step": 194
    },
    {
      "epoch": 0.15494636471990464,
      "grad_norm": 0.090995192527771,
      "learning_rate": 0.00019017003188097768,
      "loss": 0.0583,
      "step": 195
    },
    {
      "epoch": 0.15574096146205801,
      "grad_norm": 0.10583622008562088,
      "learning_rate": 0.00019011689691817216,
      "loss": 0.0479,
      "step": 196
    },
    {
      "epoch": 0.15653555820421136,
      "grad_norm": 0.09739190340042114,
      "learning_rate": 0.00019006376195536664,
      "loss": 0.0562,
      "step": 197
    },
    {
      "epoch": 0.1573301549463647,
      "grad_norm": 0.0883338451385498,
      "learning_rate": 0.00019001062699256112,
      "loss": 0.0369,
      "step": 198
    },
    {
      "epoch": 0.15812475168851808,
      "grad_norm": 0.0873837023973465,
      "learning_rate": 0.0001899574920297556,
      "loss": 0.0472,
      "step": 199
    },
    {
      "epoch": 0.15891934843067143,
      "grad_norm": 0.10254475474357605,
      "learning_rate": 0.00018990435706695008,
      "loss": 0.045,
      "step": 200
    },
    {
      "epoch": 0.15971394517282478,
      "grad_norm": 1.2343448400497437,
      "learning_rate": 0.00018985122210414453,
      "loss": 0.7572,
      "step": 201
    },
    {
      "epoch": 0.16050854191497815,
      "grad_norm": 0.6580986380577087,
      "learning_rate": 0.000189798087141339,
      "loss": 0.4785,
      "step": 202
    },
    {
      "epoch": 0.1613031386571315,
      "grad_norm": 0.5708999633789062,
      "learning_rate": 0.00018974495217853347,
      "loss": 0.4374,
      "step": 203
    },
    {
      "epoch": 0.16209773539928488,
      "grad_norm": 0.4224390387535095,
      "learning_rate": 0.00018969181721572795,
      "loss": 0.3299,
      "step": 204
    },
    {
      "epoch": 0.16289233214143822,
      "grad_norm": 0.2796633541584015,
      "learning_rate": 0.00018963868225292243,
      "loss": 0.3331,
      "step": 205
    },
    {
      "epoch": 0.16368692888359157,
      "grad_norm": 0.323624849319458,
      "learning_rate": 0.0001895855472901169,
      "loss": 0.2753,
      "step": 206
    },
    {
      "epoch": 0.16448152562574495,
      "grad_norm": 0.31121471524238586,
      "learning_rate": 0.0001895324123273114,
      "loss": 0.2937,
      "step": 207
    },
    {
      "epoch": 0.1652761223678983,
      "grad_norm": 0.320325642824173,
      "learning_rate": 0.00018947927736450587,
      "loss": 0.3051,
      "step": 208
    },
    {
      "epoch": 0.16607071911005164,
      "grad_norm": 0.2650983929634094,
      "learning_rate": 0.00018942614240170035,
      "loss": 0.233,
      "step": 209
    },
    {
      "epoch": 0.16686531585220502,
      "grad_norm": 0.2619933485984802,
      "learning_rate": 0.0001893730074388948,
      "loss": 0.268,
      "step": 210
    },
    {
      "epoch": 0.16765991259435836,
      "grad_norm": 0.2706146538257599,
      "learning_rate": 0.00018931987247608925,
      "loss": 0.2425,
      "step": 211
    },
    {
      "epoch": 0.1684545093365117,
      "grad_norm": 0.26196661591529846,
      "learning_rate": 0.00018926673751328373,
      "loss": 0.2045,
      "step": 212
    },
    {
      "epoch": 0.16924910607866508,
      "grad_norm": 0.2148316353559494,
      "learning_rate": 0.00018921360255047821,
      "loss": 0.2268,
      "step": 213
    },
    {
      "epoch": 0.17004370282081843,
      "grad_norm": 0.24098920822143555,
      "learning_rate": 0.0001891604675876727,
      "loss": 0.2187,
      "step": 214
    },
    {
      "epoch": 0.17083829956297178,
      "grad_norm": 0.21333810687065125,
      "learning_rate": 0.00018910733262486717,
      "loss": 0.2266,
      "step": 215
    },
    {
      "epoch": 0.17163289630512515,
      "grad_norm": 0.20518247783184052,
      "learning_rate": 0.00018905419766206165,
      "loss": 0.2121,
      "step": 216
    },
    {
      "epoch": 0.1724274930472785,
      "grad_norm": 0.20147433876991272,
      "learning_rate": 0.00018900106269925614,
      "loss": 0.178,
      "step": 217
    },
    {
      "epoch": 0.17322208978943188,
      "grad_norm": 0.19207751750946045,
      "learning_rate": 0.00018894792773645062,
      "loss": 0.1983,
      "step": 218
    },
    {
      "epoch": 0.17401668653158522,
      "grad_norm": 0.18413332104682922,
      "learning_rate": 0.00018889479277364507,
      "loss": 0.1786,
      "step": 219
    },
    {
      "epoch": 0.17481128327373857,
      "grad_norm": 0.1892576813697815,
      "learning_rate": 0.00018884165781083952,
      "loss": 0.15,
      "step": 220
    },
    {
      "epoch": 0.17560588001589195,
      "grad_norm": 0.3022839426994324,
      "learning_rate": 0.000188788522848034,
      "loss": 0.1626,
      "step": 221
    },
    {
      "epoch": 0.1764004767580453,
      "grad_norm": 0.1650765836238861,
      "learning_rate": 0.00018873538788522848,
      "loss": 0.1732,
      "step": 222
    },
    {
      "epoch": 0.17719507350019864,
      "grad_norm": 0.1676028072834015,
      "learning_rate": 0.00018868225292242296,
      "loss": 0.1559,
      "step": 223
    },
    {
      "epoch": 0.17798967024235202,
      "grad_norm": 0.1494373381137848,
      "learning_rate": 0.00018862911795961744,
      "loss": 0.1356,
      "step": 224
    },
    {
      "epoch": 0.17878426698450536,
      "grad_norm": 0.1572408378124237,
      "learning_rate": 0.00018857598299681192,
      "loss": 0.1467,
      "step": 225
    },
    {
      "epoch": 0.1795788637266587,
      "grad_norm": 0.18835245072841644,
      "learning_rate": 0.0001885228480340064,
      "loss": 0.1344,
      "step": 226
    },
    {
      "epoch": 0.18037346046881209,
      "grad_norm": 0.19205854833126068,
      "learning_rate": 0.00018846971307120086,
      "loss": 0.1515,
      "step": 227
    },
    {
      "epoch": 0.18116805721096543,
      "grad_norm": 0.1536608785390854,
      "learning_rate": 0.00018841657810839534,
      "loss": 0.1243,
      "step": 228
    },
    {
      "epoch": 0.18196265395311878,
      "grad_norm": 0.16713586449623108,
      "learning_rate": 0.0001883634431455898,
      "loss": 0.1416,
      "step": 229
    },
    {
      "epoch": 0.18275725069527216,
      "grad_norm": 0.13304764032363892,
      "learning_rate": 0.00018831030818278427,
      "loss": 0.1065,
      "step": 230
    },
    {
      "epoch": 0.1835518474374255,
      "grad_norm": 0.14029183983802795,
      "learning_rate": 0.00018825717321997875,
      "loss": 0.105,
      "step": 231
    },
    {
      "epoch": 0.18434644417957885,
      "grad_norm": 0.14151990413665771,
      "learning_rate": 0.00018820403825717323,
      "loss": 0.1163,
      "step": 232
    },
    {
      "epoch": 0.18514104092173223,
      "grad_norm": 0.13942307233810425,
      "learning_rate": 0.0001881509032943677,
      "loss": 0.0947,
      "step": 233
    },
    {
      "epoch": 0.18593563766388557,
      "grad_norm": 0.12254980951547623,
      "learning_rate": 0.0001880977683315622,
      "loss": 0.0821,
      "step": 234
    },
    {
      "epoch": 0.18673023440603895,
      "grad_norm": 0.13873480260372162,
      "learning_rate": 0.00018804463336875667,
      "loss": 0.0944,
      "step": 235
    },
    {
      "epoch": 0.1875248311481923,
      "grad_norm": 0.1375495195388794,
      "learning_rate": 0.00018799149840595112,
      "loss": 0.0952,
      "step": 236
    },
    {
      "epoch": 0.18831942789034564,
      "grad_norm": 0.12424521148204803,
      "learning_rate": 0.0001879383634431456,
      "loss": 0.0786,
      "step": 237
    },
    {
      "epoch": 0.18911402463249902,
      "grad_norm": 0.18482917547225952,
      "learning_rate": 0.00018788522848034006,
      "loss": 0.0885,
      "step": 238
    },
    {
      "epoch": 0.18990862137465236,
      "grad_norm": 0.10911871492862701,
      "learning_rate": 0.00018783209351753454,
      "loss": 0.0759,
      "step": 239
    },
    {
      "epoch": 0.1907032181168057,
      "grad_norm": 0.11924850195646286,
      "learning_rate": 0.00018777895855472902,
      "loss": 0.0847,
      "step": 240
    },
    {
      "epoch": 0.1914978148589591,
      "grad_norm": 0.10834884643554688,
      "learning_rate": 0.0001877258235919235,
      "loss": 0.0754,
      "step": 241
    },
    {
      "epoch": 0.19229241160111243,
      "grad_norm": 0.09732711315155029,
      "learning_rate": 0.00018767268862911798,
      "loss": 0.0827,
      "step": 242
    },
    {
      "epoch": 0.19308700834326578,
      "grad_norm": 0.09349105507135391,
      "learning_rate": 0.00018761955366631246,
      "loss": 0.0579,
      "step": 243
    },
    {
      "epoch": 0.19388160508541916,
      "grad_norm": 0.11625517904758453,
      "learning_rate": 0.0001875664187035069,
      "loss": 0.0691,
      "step": 244
    },
    {
      "epoch": 0.1946762018275725,
      "grad_norm": 0.10142417252063751,
      "learning_rate": 0.0001875132837407014,
      "loss": 0.0634,
      "step": 245
    },
    {
      "epoch": 0.19547079856972585,
      "grad_norm": 0.11022219806909561,
      "learning_rate": 0.00018746014877789587,
      "loss": 0.0497,
      "step": 246
    },
    {
      "epoch": 0.19626539531187923,
      "grad_norm": 0.07341694831848145,
      "learning_rate": 0.00018740701381509035,
      "loss": 0.0478,
      "step": 247
    },
    {
      "epoch": 0.19705999205403257,
      "grad_norm": 0.08137212693691254,
      "learning_rate": 0.0001873538788522848,
      "loss": 0.0432,
      "step": 248
    },
    {
      "epoch": 0.19785458879618595,
      "grad_norm": 0.09138885140419006,
      "learning_rate": 0.00018730074388947928,
      "loss": 0.0424,
      "step": 249
    },
    {
      "epoch": 0.1986491855383393,
      "grad_norm": 0.09371628612279892,
      "learning_rate": 0.00018724760892667376,
      "loss": 0.038,
      "step": 250
    },
    {
      "epoch": 0.19944378228049264,
      "grad_norm": 0.7443809509277344,
      "learning_rate": 0.00018719447396386824,
      "loss": 0.5868,
      "step": 251
    },
    {
      "epoch": 0.20023837902264602,
      "grad_norm": 0.5104140043258667,
      "learning_rate": 0.0001871413390010627,
      "loss": 0.4128,
      "step": 252
    },
    {
      "epoch": 0.20103297576479937,
      "grad_norm": 0.3993620276451111,
      "learning_rate": 0.00018708820403825718,
      "loss": 0.3477,
      "step": 253
    },
    {
      "epoch": 0.2018275725069527,
      "grad_norm": 0.3042195439338684,
      "learning_rate": 0.00018703506907545166,
      "loss": 0.3175,
      "step": 254
    },
    {
      "epoch": 0.2026221692491061,
      "grad_norm": 0.2376960664987564,
      "learning_rate": 0.00018698193411264614,
      "loss": 0.2944,
      "step": 255
    },
    {
      "epoch": 0.20341676599125944,
      "grad_norm": 0.20891733467578888,
      "learning_rate": 0.00018692879914984062,
      "loss": 0.2698,
      "step": 256
    },
    {
      "epoch": 0.20421136273341278,
      "grad_norm": 0.28546926379203796,
      "learning_rate": 0.00018687566418703507,
      "loss": 0.2663,
      "step": 257
    },
    {
      "epoch": 0.20500595947556616,
      "grad_norm": 0.20936451852321625,
      "learning_rate": 0.00018682252922422955,
      "loss": 0.238,
      "step": 258
    },
    {
      "epoch": 0.2058005562177195,
      "grad_norm": 0.22359099984169006,
      "learning_rate": 0.00018676939426142403,
      "loss": 0.2139,
      "step": 259
    },
    {
      "epoch": 0.20659515295987285,
      "grad_norm": 0.18875464797019958,
      "learning_rate": 0.00018671625929861849,
      "loss": 0.2149,
      "step": 260
    },
    {
      "epoch": 0.20738974970202623,
      "grad_norm": 0.19195617735385895,
      "learning_rate": 0.00018666312433581297,
      "loss": 0.1757,
      "step": 261
    },
    {
      "epoch": 0.20818434644417957,
      "grad_norm": 0.18069887161254883,
      "learning_rate": 0.00018660998937300745,
      "loss": 0.196,
      "step": 262
    },
    {
      "epoch": 0.20897894318633295,
      "grad_norm": 0.1520906686782837,
      "learning_rate": 0.00018655685441020193,
      "loss": 0.1388,
      "step": 263
    },
    {
      "epoch": 0.2097735399284863,
      "grad_norm": 0.19778677821159363,
      "learning_rate": 0.0001865037194473964,
      "loss": 0.1947,
      "step": 264
    },
    {
      "epoch": 0.21056813667063964,
      "grad_norm": 0.18842343986034393,
      "learning_rate": 0.00018645058448459089,
      "loss": 0.1839,
      "step": 265
    },
    {
      "epoch": 0.21136273341279302,
      "grad_norm": 0.1762600988149643,
      "learning_rate": 0.00018639744952178534,
      "loss": 0.1881,
      "step": 266
    },
    {
      "epoch": 0.21215733015494637,
      "grad_norm": 0.20037174224853516,
      "learning_rate": 0.00018634431455897982,
      "loss": 0.1493,
      "step": 267
    },
    {
      "epoch": 0.2129519268970997,
      "grad_norm": 0.17650973796844482,
      "learning_rate": 0.00018629117959617427,
      "loss": 0.1744,
      "step": 268
    },
    {
      "epoch": 0.2137465236392531,
      "grad_norm": 0.20521490275859833,
      "learning_rate": 0.00018623804463336875,
      "loss": 0.1454,
      "step": 269
    },
    {
      "epoch": 0.21454112038140644,
      "grad_norm": 0.171891450881958,
      "learning_rate": 0.00018618490967056323,
      "loss": 0.1903,
      "step": 270
    },
    {
      "epoch": 0.21533571712355978,
      "grad_norm": 0.19535954296588898,
      "learning_rate": 0.0001861317747077577,
      "loss": 0.1648,
      "step": 271
    },
    {
      "epoch": 0.21613031386571316,
      "grad_norm": 0.1603385955095291,
      "learning_rate": 0.0001860786397449522,
      "loss": 0.1662,
      "step": 272
    },
    {
      "epoch": 0.2169249106078665,
      "grad_norm": 0.1688283085823059,
      "learning_rate": 0.00018602550478214667,
      "loss": 0.1324,
      "step": 273
    },
    {
      "epoch": 0.21771950735001985,
      "grad_norm": 0.1454406976699829,
      "learning_rate": 0.00018597236981934115,
      "loss": 0.1349,
      "step": 274
    },
    {
      "epoch": 0.21851410409217323,
      "grad_norm": 0.14433448016643524,
      "learning_rate": 0.0001859192348565356,
      "loss": 0.1244,
      "step": 275
    },
    {
      "epoch": 0.21930870083432658,
      "grad_norm": 0.16377180814743042,
      "learning_rate": 0.00018586609989373006,
      "loss": 0.146,
      "step": 276
    },
    {
      "epoch": 0.22010329757647992,
      "grad_norm": 0.1509755551815033,
      "learning_rate": 0.00018581296493092454,
      "loss": 0.1381,
      "step": 277
    },
    {
      "epoch": 0.2208978943186333,
      "grad_norm": 0.13850636780261993,
      "learning_rate": 0.00018575982996811902,
      "loss": 0.1141,
      "step": 278
    },
    {
      "epoch": 0.22169249106078665,
      "grad_norm": 0.14569461345672607,
      "learning_rate": 0.0001857066950053135,
      "loss": 0.1132,
      "step": 279
    },
    {
      "epoch": 0.22248708780294002,
      "grad_norm": 0.15754817426204681,
      "learning_rate": 0.00018565356004250798,
      "loss": 0.1083,
      "step": 280
    },
    {
      "epoch": 0.22328168454509337,
      "grad_norm": 0.12774896621704102,
      "learning_rate": 0.00018560042507970246,
      "loss": 0.1194,
      "step": 281
    },
    {
      "epoch": 0.22407628128724671,
      "grad_norm": 0.1569223254919052,
      "learning_rate": 0.00018554729011689694,
      "loss": 0.0931,
      "step": 282
    },
    {
      "epoch": 0.2248708780294001,
      "grad_norm": 0.13270846009254456,
      "learning_rate": 0.00018549415515409142,
      "loss": 0.0869,
      "step": 283
    },
    {
      "epoch": 0.22566547477155344,
      "grad_norm": 0.12781941890716553,
      "learning_rate": 0.00018544102019128587,
      "loss": 0.1003,
      "step": 284
    },
    {
      "epoch": 0.22646007151370678,
      "grad_norm": 0.13108161091804504,
      "learning_rate": 0.00018538788522848033,
      "loss": 0.0987,
      "step": 285
    },
    {
      "epoch": 0.22725466825586016,
      "grad_norm": 0.12517087161540985,
      "learning_rate": 0.0001853347502656748,
      "loss": 0.0843,
      "step": 286
    },
    {
      "epoch": 0.2280492649980135,
      "grad_norm": 0.14053288102149963,
      "learning_rate": 0.0001852816153028693,
      "loss": 0.0876,
      "step": 287
    },
    {
      "epoch": 0.22884386174016685,
      "grad_norm": 0.11192777752876282,
      "learning_rate": 0.00018522848034006377,
      "loss": 0.0775,
      "step": 288
    },
    {
      "epoch": 0.22963845848232023,
      "grad_norm": 0.1255486011505127,
      "learning_rate": 0.00018517534537725825,
      "loss": 0.0932,
      "step": 289
    },
    {
      "epoch": 0.23043305522447358,
      "grad_norm": 0.10283544659614563,
      "learning_rate": 0.00018512221041445273,
      "loss": 0.0526,
      "step": 290
    },
    {
      "epoch": 0.23122765196662692,
      "grad_norm": 0.11816684156656265,
      "learning_rate": 0.0001850690754516472,
      "loss": 0.0875,
      "step": 291
    },
    {
      "epoch": 0.2320222487087803,
      "grad_norm": 0.11222042888402939,
      "learning_rate": 0.0001850159404888417,
      "loss": 0.0731,
      "step": 292
    },
    {
      "epoch": 0.23281684545093365,
      "grad_norm": 0.0812532976269722,
      "learning_rate": 0.00018496280552603614,
      "loss": 0.0406,
      "step": 293
    },
    {
      "epoch": 0.23361144219308702,
      "grad_norm": 0.1140429675579071,
      "learning_rate": 0.0001849096705632306,
      "loss": 0.0521,
      "step": 294
    },
    {
      "epoch": 0.23440603893524037,
      "grad_norm": 0.1127929836511612,
      "learning_rate": 0.00018485653560042507,
      "loss": 0.0669,
      "step": 295
    },
    {
      "epoch": 0.23520063567739372,
      "grad_norm": 0.08382664620876312,
      "learning_rate": 0.00018480340063761956,
      "loss": 0.0525,
      "step": 296
    },
    {
      "epoch": 0.2359952324195471,
      "grad_norm": 0.08195595443248749,
      "learning_rate": 0.00018475026567481404,
      "loss": 0.0415,
      "step": 297
    },
    {
      "epoch": 0.23678982916170044,
      "grad_norm": 0.07257259637117386,
      "learning_rate": 0.00018469713071200852,
      "loss": 0.0403,
      "step": 298
    },
    {
      "epoch": 0.23758442590385379,
      "grad_norm": 0.0934387817978859,
      "learning_rate": 0.000184643995749203,
      "loss": 0.056,
      "step": 299
    },
    {
      "epoch": 0.23837902264600716,
      "grad_norm": 0.08408962190151215,
      "learning_rate": 0.00018459086078639748,
      "loss": 0.049,
      "step": 300
    },
    {
      "epoch": 0.2391736193881605,
      "grad_norm": 0.4774269759654999,
      "learning_rate": 0.00018453772582359193,
      "loss": 0.524,
      "step": 301
    },
    {
      "epoch": 0.23996821613031386,
      "grad_norm": 0.3650152385234833,
      "learning_rate": 0.0001844845908607864,
      "loss": 0.3981,
      "step": 302
    },
    {
      "epoch": 0.24076281287246723,
      "grad_norm": 0.4363173842430115,
      "learning_rate": 0.0001844314558979809,
      "loss": 0.4276,
      "step": 303
    },
    {
      "epoch": 0.24155740961462058,
      "grad_norm": 0.27391302585601807,
      "learning_rate": 0.00018437832093517534,
      "loss": 0.3553,
      "step": 304
    },
    {
      "epoch": 0.24235200635677392,
      "grad_norm": 0.3125152587890625,
      "learning_rate": 0.00018432518597236982,
      "loss": 0.3139,
      "step": 305
    },
    {
      "epoch": 0.2431466030989273,
      "grad_norm": 0.24081191420555115,
      "learning_rate": 0.0001842720510095643,
      "loss": 0.3128,
      "step": 306
    },
    {
      "epoch": 0.24394119984108065,
      "grad_norm": 0.1966840624809265,
      "learning_rate": 0.00018421891604675878,
      "loss": 0.2515,
      "step": 307
    },
    {
      "epoch": 0.24473579658323402,
      "grad_norm": 0.18584159016609192,
      "learning_rate": 0.00018416578108395326,
      "loss": 0.2428,
      "step": 308
    },
    {
      "epoch": 0.24553039332538737,
      "grad_norm": 0.21181358397006989,
      "learning_rate": 0.00018411264612114772,
      "loss": 0.2622,
      "step": 309
    },
    {
      "epoch": 0.24632499006754072,
      "grad_norm": 0.1748988926410675,
      "learning_rate": 0.0001840595111583422,
      "loss": 0.2182,
      "step": 310
    },
    {
      "epoch": 0.2471195868096941,
      "grad_norm": 0.20695795118808746,
      "learning_rate": 0.00018400637619553668,
      "loss": 0.2075,
      "step": 311
    },
    {
      "epoch": 0.24791418355184744,
      "grad_norm": 0.1929444521665573,
      "learning_rate": 0.00018395324123273116,
      "loss": 0.179,
      "step": 312
    },
    {
      "epoch": 0.2487087802940008,
      "grad_norm": 0.19815370440483093,
      "learning_rate": 0.0001839001062699256,
      "loss": 0.2067,
      "step": 313
    },
    {
      "epoch": 0.24950337703615416,
      "grad_norm": 0.16783186793327332,
      "learning_rate": 0.0001838469713071201,
      "loss": 0.2002,
      "step": 314
    },
    {
      "epoch": 0.25029797377830754,
      "grad_norm": 0.2093949019908905,
      "learning_rate": 0.00018379383634431457,
      "loss": 0.2317,
      "step": 315
    },
    {
      "epoch": 0.25109257052046086,
      "grad_norm": 0.17659251391887665,
      "learning_rate": 0.00018374070138150905,
      "loss": 0.1722,
      "step": 316
    },
    {
      "epoch": 0.25188716726261423,
      "grad_norm": 0.16365720331668854,
      "learning_rate": 0.0001836875664187035,
      "loss": 0.1864,
      "step": 317
    },
    {
      "epoch": 0.2526817640047676,
      "grad_norm": 0.18505944311618805,
      "learning_rate": 0.00018363443145589798,
      "loss": 0.1847,
      "step": 318
    },
    {
      "epoch": 0.2534763607469209,
      "grad_norm": 0.18928128480911255,
      "learning_rate": 0.00018358129649309246,
      "loss": 0.1922,
      "step": 319
    },
    {
      "epoch": 0.2542709574890743,
      "grad_norm": 0.18404938280582428,
      "learning_rate": 0.00018352816153028694,
      "loss": 0.156,
      "step": 320
    },
    {
      "epoch": 0.2550655542312277,
      "grad_norm": 0.1524052619934082,
      "learning_rate": 0.00018347502656748142,
      "loss": 0.173,
      "step": 321
    },
    {
      "epoch": 0.255860150973381,
      "grad_norm": 0.15880431234836578,
      "learning_rate": 0.00018342189160467588,
      "loss": 0.1636,
      "step": 322
    },
    {
      "epoch": 0.25665474771553437,
      "grad_norm": 0.17561404407024384,
      "learning_rate": 0.00018336875664187036,
      "loss": 0.1545,
      "step": 323
    },
    {
      "epoch": 0.25744934445768775,
      "grad_norm": 0.20384939014911652,
      "learning_rate": 0.00018331562167906484,
      "loss": 0.162,
      "step": 324
    },
    {
      "epoch": 0.25824394119984106,
      "grad_norm": 0.17336349189281464,
      "learning_rate": 0.00018326248671625932,
      "loss": 0.1413,
      "step": 325
    },
    {
      "epoch": 0.25903853794199444,
      "grad_norm": 0.16647028923034668,
      "learning_rate": 0.00018320935175345377,
      "loss": 0.1673,
      "step": 326
    },
    {
      "epoch": 0.2598331346841478,
      "grad_norm": 0.17103615403175354,
      "learning_rate": 0.00018315621679064825,
      "loss": 0.1292,
      "step": 327
    },
    {
      "epoch": 0.26062773142630113,
      "grad_norm": 0.13630561530590057,
      "learning_rate": 0.00018310308182784273,
      "loss": 0.1179,
      "step": 328
    },
    {
      "epoch": 0.2614223281684545,
      "grad_norm": 0.1453106552362442,
      "learning_rate": 0.0001830499468650372,
      "loss": 0.1195,
      "step": 329
    },
    {
      "epoch": 0.2622169249106079,
      "grad_norm": 0.1364022195339203,
      "learning_rate": 0.0001829968119022317,
      "loss": 0.1318,
      "step": 330
    },
    {
      "epoch": 0.2630115216527612,
      "grad_norm": 0.12560318410396576,
      "learning_rate": 0.00018294367693942614,
      "loss": 0.1189,
      "step": 331
    },
    {
      "epoch": 0.2638061183949146,
      "grad_norm": 0.15900033712387085,
      "learning_rate": 0.00018289054197662063,
      "loss": 0.1495,
      "step": 332
    },
    {
      "epoch": 0.26460071513706795,
      "grad_norm": 0.12530042231082916,
      "learning_rate": 0.0001828374070138151,
      "loss": 0.1131,
      "step": 333
    },
    {
      "epoch": 0.2653953118792213,
      "grad_norm": 0.12570373713970184,
      "learning_rate": 0.00018278427205100956,
      "loss": 0.0941,
      "step": 334
    },
    {
      "epoch": 0.26618990862137465,
      "grad_norm": 0.12103325873613358,
      "learning_rate": 0.00018273113708820404,
      "loss": 0.1125,
      "step": 335
    },
    {
      "epoch": 0.266984505363528,
      "grad_norm": 0.22631710767745972,
      "learning_rate": 0.00018267800212539852,
      "loss": 0.1005,
      "step": 336
    },
    {
      "epoch": 0.26777910210568134,
      "grad_norm": 0.12893043458461761,
      "learning_rate": 0.000182624867162593,
      "loss": 0.0928,
      "step": 337
    },
    {
      "epoch": 0.2685736988478347,
      "grad_norm": 0.10584737360477448,
      "learning_rate": 0.00018257173219978748,
      "loss": 0.1083,
      "step": 338
    },
    {
      "epoch": 0.2693682955899881,
      "grad_norm": 0.11462851613759995,
      "learning_rate": 0.00018251859723698196,
      "loss": 0.0821,
      "step": 339
    },
    {
      "epoch": 0.2701628923321414,
      "grad_norm": 0.1762094497680664,
      "learning_rate": 0.0001824654622741764,
      "loss": 0.0866,
      "step": 340
    },
    {
      "epoch": 0.2709574890742948,
      "grad_norm": 0.1287495344877243,
      "learning_rate": 0.0001824123273113709,
      "loss": 0.0956,
      "step": 341
    },
    {
      "epoch": 0.27175208581644816,
      "grad_norm": 0.09796353429555893,
      "learning_rate": 0.00018235919234856535,
      "loss": 0.0682,
      "step": 342
    },
    {
      "epoch": 0.2725466825586015,
      "grad_norm": 0.09211049228906631,
      "learning_rate": 0.00018230605738575983,
      "loss": 0.0709,
      "step": 343
    },
    {
      "epoch": 0.27334127930075486,
      "grad_norm": 0.07535607367753983,
      "learning_rate": 0.0001822529224229543,
      "loss": 0.0663,
      "step": 344
    },
    {
      "epoch": 0.27413587604290823,
      "grad_norm": 0.07346019893884659,
      "learning_rate": 0.00018219978746014879,
      "loss": 0.0489,
      "step": 345
    },
    {
      "epoch": 0.2749304727850616,
      "grad_norm": 0.08231117576360703,
      "learning_rate": 0.00018214665249734327,
      "loss": 0.0513,
      "step": 346
    },
    {
      "epoch": 0.2757250695272149,
      "grad_norm": 0.07508116960525513,
      "learning_rate": 0.00018209351753453775,
      "loss": 0.0453,
      "step": 347
    },
    {
      "epoch": 0.2765196662693683,
      "grad_norm": 0.08778450638055801,
      "learning_rate": 0.00018204038257173223,
      "loss": 0.0418,
      "step": 348
    },
    {
      "epoch": 0.2773142630115217,
      "grad_norm": 0.08524610847234726,
      "learning_rate": 0.00018198724760892668,
      "loss": 0.0454,
      "step": 349
    },
    {
      "epoch": 0.278108859753675,
      "grad_norm": 0.08434765785932541,
      "learning_rate": 0.00018193411264612113,
      "loss": 0.0412,
      "step": 350
    },
    {
      "epoch": 0.2789034564958284,
      "grad_norm": 0.5407375693321228,
      "learning_rate": 0.0001818809776833156,
      "loss": 0.6257,
      "step": 351
    },
    {
      "epoch": 0.27969805323798175,
      "grad_norm": 0.4241466522216797,
      "learning_rate": 0.0001818278427205101,
      "loss": 0.4843,
      "step": 352
    },
    {
      "epoch": 0.28049264998013507,
      "grad_norm": 0.4140092432498932,
      "learning_rate": 0.00018177470775770457,
      "loss": 0.3838,
      "step": 353
    },
    {
      "epoch": 0.28128724672228844,
      "grad_norm": 0.3156092166900635,
      "learning_rate": 0.00018172157279489905,
      "loss": 0.3705,
      "step": 354
    },
    {
      "epoch": 0.2820818434644418,
      "grad_norm": 0.26371702551841736,
      "learning_rate": 0.00018166843783209353,
      "loss": 0.3146,
      "step": 355
    },
    {
      "epoch": 0.28287644020659514,
      "grad_norm": 0.2153661996126175,
      "learning_rate": 0.00018161530286928801,
      "loss": 0.3199,
      "step": 356
    },
    {
      "epoch": 0.2836710369487485,
      "grad_norm": 0.20497655868530273,
      "learning_rate": 0.0001815621679064825,
      "loss": 0.3051,
      "step": 357
    },
    {
      "epoch": 0.2844656336909019,
      "grad_norm": 0.23229101300239563,
      "learning_rate": 0.00018150903294367695,
      "loss": 0.2773,
      "step": 358
    },
    {
      "epoch": 0.2852602304330552,
      "grad_norm": 0.19682466983795166,
      "learning_rate": 0.00018145589798087143,
      "loss": 0.2325,
      "step": 359
    },
    {
      "epoch": 0.2860548271752086,
      "grad_norm": 0.21088896691799164,
      "learning_rate": 0.00018140276301806588,
      "loss": 0.2387,
      "step": 360
    },
    {
      "epoch": 0.28684942391736196,
      "grad_norm": 0.21169604361057281,
      "learning_rate": 0.00018134962805526036,
      "loss": 0.2579,
      "step": 361
    },
    {
      "epoch": 0.2876440206595153,
      "grad_norm": 0.1782296746969223,
      "learning_rate": 0.00018129649309245484,
      "loss": 0.2114,
      "step": 362
    },
    {
      "epoch": 0.28843861740166865,
      "grad_norm": 0.1722038835287094,
      "learning_rate": 0.00018124335812964932,
      "loss": 0.2269,
      "step": 363
    },
    {
      "epoch": 0.289233214143822,
      "grad_norm": 0.1924397051334381,
      "learning_rate": 0.0001811902231668438,
      "loss": 0.2185,
      "step": 364
    },
    {
      "epoch": 0.29002781088597535,
      "grad_norm": 0.16414694488048553,
      "learning_rate": 0.00018113708820403828,
      "loss": 0.2058,
      "step": 365
    },
    {
      "epoch": 0.2908224076281287,
      "grad_norm": 0.15329936146736145,
      "learning_rate": 0.00018108395324123273,
      "loss": 0.1657,
      "step": 366
    },
    {
      "epoch": 0.2916170043702821,
      "grad_norm": 0.1886078417301178,
      "learning_rate": 0.00018103081827842721,
      "loss": 0.1923,
      "step": 367
    },
    {
      "epoch": 0.2924116011124354,
      "grad_norm": 0.15114136040210724,
      "learning_rate": 0.0001809776833156217,
      "loss": 0.1712,
      "step": 368
    },
    {
      "epoch": 0.2932061978545888,
      "grad_norm": 0.16258707642555237,
      "learning_rate": 0.00018092454835281615,
      "loss": 0.1614,
      "step": 369
    },
    {
      "epoch": 0.29400079459674217,
      "grad_norm": 0.174740269780159,
      "learning_rate": 0.00018087141339001063,
      "loss": 0.1563,
      "step": 370
    },
    {
      "epoch": 0.2947953913388955,
      "grad_norm": 0.16331592202186584,
      "learning_rate": 0.0001808182784272051,
      "loss": 0.1242,
      "step": 371
    },
    {
      "epoch": 0.29558998808104886,
      "grad_norm": 0.14589998126029968,
      "learning_rate": 0.0001807651434643996,
      "loss": 0.1481,
      "step": 372
    },
    {
      "epoch": 0.29638458482320224,
      "grad_norm": 0.17812074720859528,
      "learning_rate": 0.00018071200850159407,
      "loss": 0.1262,
      "step": 373
    },
    {
      "epoch": 0.29717918156535555,
      "grad_norm": 0.16151568293571472,
      "learning_rate": 0.00018065887353878855,
      "loss": 0.1442,
      "step": 374
    },
    {
      "epoch": 0.29797377830750893,
      "grad_norm": 0.19634833931922913,
      "learning_rate": 0.000180605738575983,
      "loss": 0.1232,
      "step": 375
    },
    {
      "epoch": 0.2987683750496623,
      "grad_norm": 0.1378592848777771,
      "learning_rate": 0.00018055260361317748,
      "loss": 0.1157,
      "step": 376
    },
    {
      "epoch": 0.2995629717918157,
      "grad_norm": 0.1596001535654068,
      "learning_rate": 0.00018049946865037196,
      "loss": 0.1253,
      "step": 377
    },
    {
      "epoch": 0.300357568533969,
      "grad_norm": 0.18994049727916718,
      "learning_rate": 0.00018044633368756642,
      "loss": 0.1413,
      "step": 378
    },
    {
      "epoch": 0.3011521652761224,
      "grad_norm": 0.22402173280715942,
      "learning_rate": 0.0001803931987247609,
      "loss": 0.1374,
      "step": 379
    },
    {
      "epoch": 0.30194676201827575,
      "grad_norm": 0.15065738558769226,
      "learning_rate": 0.00018034006376195538,
      "loss": 0.1121,
      "step": 380
    },
    {
      "epoch": 0.30274135876042907,
      "grad_norm": 0.16096651554107666,
      "learning_rate": 0.00018028692879914986,
      "loss": 0.1007,
      "step": 381
    },
    {
      "epoch": 0.30353595550258244,
      "grad_norm": 0.18562498688697815,
      "learning_rate": 0.00018023379383634434,
      "loss": 0.1261,
      "step": 382
    },
    {
      "epoch": 0.3043305522447358,
      "grad_norm": 0.1083584576845169,
      "learning_rate": 0.0001801806588735388,
      "loss": 0.0867,
      "step": 383
    },
    {
      "epoch": 0.30512514898688914,
      "grad_norm": 0.13345786929130554,
      "learning_rate": 0.00018012752391073327,
      "loss": 0.0986,
      "step": 384
    },
    {
      "epoch": 0.3059197457290425,
      "grad_norm": 0.11899159103631973,
      "learning_rate": 0.00018007438894792775,
      "loss": 0.0867,
      "step": 385
    },
    {
      "epoch": 0.3067143424711959,
      "grad_norm": 0.11915700882673264,
      "learning_rate": 0.00018002125398512223,
      "loss": 0.1014,
      "step": 386
    },
    {
      "epoch": 0.3075089392133492,
      "grad_norm": 0.1470143049955368,
      "learning_rate": 0.00017996811902231668,
      "loss": 0.0981,
      "step": 387
    },
    {
      "epoch": 0.3083035359555026,
      "grad_norm": 0.13227562606334686,
      "learning_rate": 0.00017991498405951116,
      "loss": 0.0923,
      "step": 388
    },
    {
      "epoch": 0.30909813269765596,
      "grad_norm": 0.11816904693841934,
      "learning_rate": 0.00017986184909670564,
      "loss": 0.0731,
      "step": 389
    },
    {
      "epoch": 0.3098927294398093,
      "grad_norm": 0.1253548562526703,
      "learning_rate": 0.00017980871413390012,
      "loss": 0.0913,
      "step": 390
    },
    {
      "epoch": 0.31068732618196265,
      "grad_norm": 0.10315605252981186,
      "learning_rate": 0.00017975557917109458,
      "loss": 0.0752,
      "step": 391
    },
    {
      "epoch": 0.31148192292411603,
      "grad_norm": 0.0959479808807373,
      "learning_rate": 0.00017970244420828906,
      "loss": 0.0664,
      "step": 392
    },
    {
      "epoch": 0.31227651966626935,
      "grad_norm": 0.11472196877002716,
      "learning_rate": 0.00017964930924548354,
      "loss": 0.0755,
      "step": 393
    },
    {
      "epoch": 0.3130711164084227,
      "grad_norm": 0.12279083579778671,
      "learning_rate": 0.00017959617428267802,
      "loss": 0.0802,
      "step": 394
    },
    {
      "epoch": 0.3138657131505761,
      "grad_norm": 0.10654245316982269,
      "learning_rate": 0.0001795430393198725,
      "loss": 0.0575,
      "step": 395
    },
    {
      "epoch": 0.3146603098927294,
      "grad_norm": 0.102593332529068,
      "learning_rate": 0.00017948990435706695,
      "loss": 0.0597,
      "step": 396
    },
    {
      "epoch": 0.3154549066348828,
      "grad_norm": 0.09773418307304382,
      "learning_rate": 0.00017943676939426143,
      "loss": 0.0527,
      "step": 397
    },
    {
      "epoch": 0.31624950337703617,
      "grad_norm": 0.13269393146038055,
      "learning_rate": 0.0001793836344314559,
      "loss": 0.0457,
      "step": 398
    },
    {
      "epoch": 0.3170441001191895,
      "grad_norm": 0.10617385804653168,
      "learning_rate": 0.00017933049946865036,
      "loss": 0.0359,
      "step": 399
    },
    {
      "epoch": 0.31783869686134286,
      "grad_norm": 0.10344933718442917,
      "learning_rate": 0.00017927736450584484,
      "loss": 0.038,
      "step": 400
    },
    {
      "epoch": 0.31863329360349624,
      "grad_norm": 0.815968930721283,
      "learning_rate": 0.00017922422954303932,
      "loss": 0.5297,
      "step": 401
    },
    {
      "epoch": 0.31942789034564956,
      "grad_norm": 0.5795943140983582,
      "learning_rate": 0.0001791710945802338,
      "loss": 0.512,
      "step": 402
    },
    {
      "epoch": 0.32022248708780293,
      "grad_norm": 0.5341584086418152,
      "learning_rate": 0.00017911795961742828,
      "loss": 0.4963,
      "step": 403
    },
    {
      "epoch": 0.3210170838299563,
      "grad_norm": 0.33977898955345154,
      "learning_rate": 0.00017906482465462277,
      "loss": 0.3811,
      "step": 404
    },
    {
      "epoch": 0.3218116805721097,
      "grad_norm": 0.2823364734649658,
      "learning_rate": 0.00017901168969181722,
      "loss": 0.31,
      "step": 405
    },
    {
      "epoch": 0.322606277314263,
      "grad_norm": 0.21516966819763184,
      "learning_rate": 0.0001789585547290117,
      "loss": 0.3021,
      "step": 406
    },
    {
      "epoch": 0.3234008740564164,
      "grad_norm": 0.2240457832813263,
      "learning_rate": 0.00017890541976620615,
      "loss": 0.2808,
      "step": 407
    },
    {
      "epoch": 0.32419547079856975,
      "grad_norm": 0.19748429954051971,
      "learning_rate": 0.00017885228480340063,
      "loss": 0.2692,
      "step": 408
    },
    {
      "epoch": 0.32499006754072307,
      "grad_norm": 0.21727398037910461,
      "learning_rate": 0.0001787991498405951,
      "loss": 0.2696,
      "step": 409
    },
    {
      "epoch": 0.32578466428287645,
      "grad_norm": 0.29306739568710327,
      "learning_rate": 0.0001787460148777896,
      "loss": 0.2531,
      "step": 410
    },
    {
      "epoch": 0.3265792610250298,
      "grad_norm": 0.21117408573627472,
      "learning_rate": 0.00017869287991498407,
      "loss": 0.2479,
      "step": 411
    },
    {
      "epoch": 0.32737385776718314,
      "grad_norm": 0.19446948170661926,
      "learning_rate": 0.00017863974495217855,
      "loss": 0.2156,
      "step": 412
    },
    {
      "epoch": 0.3281684545093365,
      "grad_norm": 0.17669881880283356,
      "learning_rate": 0.00017858660998937303,
      "loss": 0.1922,
      "step": 413
    },
    {
      "epoch": 0.3289630512514899,
      "grad_norm": 0.18860073387622833,
      "learning_rate": 0.00017853347502656749,
      "loss": 0.2055,
      "step": 414
    },
    {
      "epoch": 0.3297576479936432,
      "grad_norm": 0.20367181301116943,
      "learning_rate": 0.00017848034006376197,
      "loss": 0.217,
      "step": 415
    },
    {
      "epoch": 0.3305522447357966,
      "grad_norm": 0.20219364762306213,
      "learning_rate": 0.00017842720510095642,
      "loss": 0.22,
      "step": 416
    },
    {
      "epoch": 0.33134684147794996,
      "grad_norm": 0.20035719871520996,
      "learning_rate": 0.0001783740701381509,
      "loss": 0.1911,
      "step": 417
    },
    {
      "epoch": 0.3321414382201033,
      "grad_norm": 0.22803983092308044,
      "learning_rate": 0.00017832093517534538,
      "loss": 0.2466,
      "step": 418
    },
    {
      "epoch": 0.33293603496225666,
      "grad_norm": 0.18323004245758057,
      "learning_rate": 0.00017826780021253986,
      "loss": 0.1696,
      "step": 419
    },
    {
      "epoch": 0.33373063170441003,
      "grad_norm": 0.19410443305969238,
      "learning_rate": 0.00017821466524973434,
      "loss": 0.1858,
      "step": 420
    },
    {
      "epoch": 0.33452522844656335,
      "grad_norm": 0.18000756204128265,
      "learning_rate": 0.00017816153028692882,
      "loss": 0.1522,
      "step": 421
    },
    {
      "epoch": 0.3353198251887167,
      "grad_norm": 0.19147346913814545,
      "learning_rate": 0.0001781083953241233,
      "loss": 0.1403,
      "step": 422
    },
    {
      "epoch": 0.3361144219308701,
      "grad_norm": 0.16993501782417297,
      "learning_rate": 0.00017805526036131775,
      "loss": 0.1501,
      "step": 423
    },
    {
      "epoch": 0.3369090186730234,
      "grad_norm": 0.32250386476516724,
      "learning_rate": 0.00017800212539851223,
      "loss": 0.152,
      "step": 424
    },
    {
      "epoch": 0.3377036154151768,
      "grad_norm": 0.1447669267654419,
      "learning_rate": 0.0001779489904357067,
      "loss": 0.1295,
      "step": 425
    },
    {
      "epoch": 0.33849821215733017,
      "grad_norm": 0.15887509286403656,
      "learning_rate": 0.00017789585547290117,
      "loss": 0.126,
      "step": 426
    },
    {
      "epoch": 0.3392928088994835,
      "grad_norm": 0.20491543412208557,
      "learning_rate": 0.00017784272051009565,
      "loss": 0.1499,
      "step": 427
    },
    {
      "epoch": 0.34008740564163686,
      "grad_norm": 0.14907881617546082,
      "learning_rate": 0.00017778958554729013,
      "loss": 0.1324,
      "step": 428
    },
    {
      "epoch": 0.34088200238379024,
      "grad_norm": 0.23097571730613708,
      "learning_rate": 0.0001777364505844846,
      "loss": 0.1274,
      "step": 429
    },
    {
      "epoch": 0.34167659912594356,
      "grad_norm": 0.14563998579978943,
      "learning_rate": 0.0001776833156216791,
      "loss": 0.1071,
      "step": 430
    },
    {
      "epoch": 0.34247119586809693,
      "grad_norm": 0.14084996283054352,
      "learning_rate": 0.00017763018065887357,
      "loss": 0.1164,
      "step": 431
    },
    {
      "epoch": 0.3432657926102503,
      "grad_norm": 0.15502925217151642,
      "learning_rate": 0.00017757704569606802,
      "loss": 0.116,
      "step": 432
    },
    {
      "epoch": 0.34406038935240363,
      "grad_norm": 0.15625321865081787,
      "learning_rate": 0.0001775239107332625,
      "loss": 0.0876,
      "step": 433
    },
    {
      "epoch": 0.344854986094557,
      "grad_norm": 0.12899446487426758,
      "learning_rate": 0.00017747077577045695,
      "loss": 0.1049,
      "step": 434
    },
    {
      "epoch": 0.3456495828367104,
      "grad_norm": 0.1274585872888565,
      "learning_rate": 0.00017741764080765143,
      "loss": 0.0988,
      "step": 435
    },
    {
      "epoch": 0.34644417957886375,
      "grad_norm": 0.1087879091501236,
      "learning_rate": 0.00017736450584484591,
      "loss": 0.0789,
      "step": 436
    },
    {
      "epoch": 0.3472387763210171,
      "grad_norm": 0.13481460511684418,
      "learning_rate": 0.0001773113708820404,
      "loss": 0.0827,
      "step": 437
    },
    {
      "epoch": 0.34803337306317045,
      "grad_norm": 0.12952278554439545,
      "learning_rate": 0.00017725823591923487,
      "loss": 0.1103,
      "step": 438
    },
    {
      "epoch": 0.3488279698053238,
      "grad_norm": 0.13442721962928772,
      "learning_rate": 0.00017720510095642935,
      "loss": 0.0872,
      "step": 439
    },
    {
      "epoch": 0.34962256654747714,
      "grad_norm": 0.13618211448192596,
      "learning_rate": 0.0001771519659936238,
      "loss": 0.0737,
      "step": 440
    },
    {
      "epoch": 0.3504171632896305,
      "grad_norm": 0.1181938424706459,
      "learning_rate": 0.0001770988310308183,
      "loss": 0.0886,
      "step": 441
    },
    {
      "epoch": 0.3512117600317839,
      "grad_norm": 0.12918734550476074,
      "learning_rate": 0.00017704569606801277,
      "loss": 0.0795,
      "step": 442
    },
    {
      "epoch": 0.3520063567739372,
      "grad_norm": 0.13566656410694122,
      "learning_rate": 0.00017699256110520722,
      "loss": 0.0614,
      "step": 443
    },
    {
      "epoch": 0.3528009535160906,
      "grad_norm": 0.10747001320123672,
      "learning_rate": 0.0001769394261424017,
      "loss": 0.06,
      "step": 444
    },
    {
      "epoch": 0.35359555025824396,
      "grad_norm": 0.10960114747285843,
      "learning_rate": 0.00017688629117959618,
      "loss": 0.0484,
      "step": 445
    },
    {
      "epoch": 0.3543901470003973,
      "grad_norm": 0.1085306704044342,
      "learning_rate": 0.00017683315621679066,
      "loss": 0.0652,
      "step": 446
    },
    {
      "epoch": 0.35518474374255066,
      "grad_norm": 0.09818721562623978,
      "learning_rate": 0.00017678002125398514,
      "loss": 0.0434,
      "step": 447
    },
    {
      "epoch": 0.35597934048470403,
      "grad_norm": 0.11145469546318054,
      "learning_rate": 0.0001767268862911796,
      "loss": 0.0358,
      "step": 448
    },
    {
      "epoch": 0.35677393722685735,
      "grad_norm": 0.09422564506530762,
      "learning_rate": 0.00017667375132837408,
      "loss": 0.0398,
      "step": 449
    },
    {
      "epoch": 0.3575685339690107,
      "grad_norm": 0.09531432390213013,
      "learning_rate": 0.00017662061636556856,
      "loss": 0.0331,
      "step": 450
    },
    {
      "epoch": 0.3583631307111641,
      "grad_norm": 1.0477309226989746,
      "learning_rate": 0.00017656748140276304,
      "loss": 0.5933,
      "step": 451
    },
    {
      "epoch": 0.3591577274533174,
      "grad_norm": 0.6669835448265076,
      "learning_rate": 0.0001765143464399575,
      "loss": 0.5073,
      "step": 452
    },
    {
      "epoch": 0.3599523241954708,
      "grad_norm": 0.5718496441841125,
      "learning_rate": 0.00017646121147715197,
      "loss": 0.4632,
      "step": 453
    },
    {
      "epoch": 0.36074692093762417,
      "grad_norm": 0.38763612508773804,
      "learning_rate": 0.00017640807651434645,
      "loss": 0.4034,
      "step": 454
    },
    {
      "epoch": 0.3615415176797775,
      "grad_norm": 0.3436497747898102,
      "learning_rate": 0.00017635494155154093,
      "loss": 0.3826,
      "step": 455
    },
    {
      "epoch": 0.36233611442193087,
      "grad_norm": 0.2841578423976898,
      "learning_rate": 0.00017630180658873538,
      "loss": 0.2877,
      "step": 456
    },
    {
      "epoch": 0.36313071116408424,
      "grad_norm": 0.19007021188735962,
      "learning_rate": 0.00017624867162592986,
      "loss": 0.2408,
      "step": 457
    },
    {
      "epoch": 0.36392530790623756,
      "grad_norm": 0.20825901627540588,
      "learning_rate": 0.00017619553666312434,
      "loss": 0.26,
      "step": 458
    },
    {
      "epoch": 0.36471990464839094,
      "grad_norm": 0.21573509275913239,
      "learning_rate": 0.00017614240170031882,
      "loss": 0.2335,
      "step": 459
    },
    {
      "epoch": 0.3655145013905443,
      "grad_norm": 0.30814629793167114,
      "learning_rate": 0.0001760892667375133,
      "loss": 0.2577,
      "step": 460
    },
    {
      "epoch": 0.36630909813269763,
      "grad_norm": 0.21933895349502563,
      "learning_rate": 0.00017603613177470776,
      "loss": 0.2347,
      "step": 461
    },
    {
      "epoch": 0.367103694874851,
      "grad_norm": 0.19775919616222382,
      "learning_rate": 0.00017598299681190224,
      "loss": 0.2031,
      "step": 462
    },
    {
      "epoch": 0.3678982916170044,
      "grad_norm": 0.2925421893596649,
      "learning_rate": 0.00017592986184909672,
      "loss": 0.2292,
      "step": 463
    },
    {
      "epoch": 0.3686928883591577,
      "grad_norm": 0.22522221505641937,
      "learning_rate": 0.0001758767268862912,
      "loss": 0.2199,
      "step": 464
    },
    {
      "epoch": 0.3694874851013111,
      "grad_norm": 0.21991795301437378,
      "learning_rate": 0.00017582359192348565,
      "loss": 0.1943,
      "step": 465
    },
    {
      "epoch": 0.37028208184346445,
      "grad_norm": 0.19432100653648376,
      "learning_rate": 0.00017577045696068013,
      "loss": 0.1902,
      "step": 466
    },
    {
      "epoch": 0.3710766785856178,
      "grad_norm": 0.19416193664073944,
      "learning_rate": 0.0001757173219978746,
      "loss": 0.1873,
      "step": 467
    },
    {
      "epoch": 0.37187127532777114,
      "grad_norm": 0.22416891157627106,
      "learning_rate": 0.0001756641870350691,
      "loss": 0.1635,
      "step": 468
    },
    {
      "epoch": 0.3726658720699245,
      "grad_norm": 0.15995021164417267,
      "learning_rate": 0.00017561105207226357,
      "loss": 0.1601,
      "step": 469
    },
    {
      "epoch": 0.3734604688120779,
      "grad_norm": 0.1844729781150818,
      "learning_rate": 0.00017555791710945802,
      "loss": 0.1675,
      "step": 470
    },
    {
      "epoch": 0.3742550655542312,
      "grad_norm": 0.17936724424362183,
      "learning_rate": 0.0001755047821466525,
      "loss": 0.161,
      "step": 471
    },
    {
      "epoch": 0.3750496622963846,
      "grad_norm": 0.2097606360912323,
      "learning_rate": 0.00017545164718384698,
      "loss": 0.1472,
      "step": 472
    },
    {
      "epoch": 0.37584425903853796,
      "grad_norm": 0.18350501358509064,
      "learning_rate": 0.00017539851222104144,
      "loss": 0.1527,
      "step": 473
    },
    {
      "epoch": 0.3766388557806913,
      "grad_norm": 0.16713862121105194,
      "learning_rate": 0.00017534537725823592,
      "loss": 0.1599,
      "step": 474
    },
    {
      "epoch": 0.37743345252284466,
      "grad_norm": 0.19787098467350006,
      "learning_rate": 0.0001752922422954304,
      "loss": 0.1341,
      "step": 475
    },
    {
      "epoch": 0.37822804926499803,
      "grad_norm": 0.23306138813495636,
      "learning_rate": 0.00017523910733262488,
      "loss": 0.1548,
      "step": 476
    },
    {
      "epoch": 0.37902264600715135,
      "grad_norm": 0.172861248254776,
      "learning_rate": 0.00017518597236981936,
      "loss": 0.1209,
      "step": 477
    },
    {
      "epoch": 0.37981724274930473,
      "grad_norm": 0.18460042774677277,
      "learning_rate": 0.00017513283740701384,
      "loss": 0.1088,
      "step": 478
    },
    {
      "epoch": 0.3806118394914581,
      "grad_norm": 0.14146049320697784,
      "learning_rate": 0.0001750797024442083,
      "loss": 0.1257,
      "step": 479
    },
    {
      "epoch": 0.3814064362336114,
      "grad_norm": 0.16980722546577454,
      "learning_rate": 0.00017502656748140277,
      "loss": 0.1331,
      "step": 480
    },
    {
      "epoch": 0.3822010329757648,
      "grad_norm": 0.14929276704788208,
      "learning_rate": 0.00017497343251859722,
      "loss": 0.1172,
      "step": 481
    },
    {
      "epoch": 0.3829956297179182,
      "grad_norm": 0.14789730310440063,
      "learning_rate": 0.0001749202975557917,
      "loss": 0.1067,
      "step": 482
    },
    {
      "epoch": 0.3837902264600715,
      "grad_norm": 2.624328136444092,
      "learning_rate": 0.00017486716259298619,
      "loss": 0.0885,
      "step": 483
    },
    {
      "epoch": 0.38458482320222487,
      "grad_norm": 16.18034553527832,
      "learning_rate": 0.00017481402763018067,
      "loss": 0.5988,
      "step": 484
    },
    {
      "epoch": 0.38537941994437824,
      "grad_norm": 1.0832313299179077,
      "learning_rate": 0.00017476089266737515,
      "loss": 0.1451,
      "step": 485
    },
    {
      "epoch": 0.38617401668653156,
      "grad_norm": 0.3220563232898712,
      "learning_rate": 0.00017470775770456963,
      "loss": 0.1114,
      "step": 486
    },
    {
      "epoch": 0.38696861342868494,
      "grad_norm": 0.11379662156105042,
      "learning_rate": 0.0001746546227417641,
      "loss": 0.095,
      "step": 487
    },
    {
      "epoch": 0.3877632101708383,
      "grad_norm": 0.1331019550561905,
      "learning_rate": 0.00017460148777895856,
      "loss": 0.0681,
      "step": 488
    },
    {
      "epoch": 0.38855780691299163,
      "grad_norm": 0.12626010179519653,
      "learning_rate": 0.00017454835281615304,
      "loss": 0.083,
      "step": 489
    },
    {
      "epoch": 0.389352403655145,
      "grad_norm": 0.13573628664016724,
      "learning_rate": 0.0001744952178533475,
      "loss": 0.0999,
      "step": 490
    },
    {
      "epoch": 0.3901470003972984,
      "grad_norm": 0.17958004772663116,
      "learning_rate": 0.00017444208289054197,
      "loss": 0.0647,
      "step": 491
    },
    {
      "epoch": 0.3909415971394517,
      "grad_norm": 0.12049554288387299,
      "learning_rate": 0.00017438894792773645,
      "loss": 0.0723,
      "step": 492
    },
    {
      "epoch": 0.3917361938816051,
      "grad_norm": 0.1240827888250351,
      "learning_rate": 0.00017433581296493093,
      "loss": 0.0601,
      "step": 493
    },
    {
      "epoch": 0.39253079062375845,
      "grad_norm": 0.10330583155155182,
      "learning_rate": 0.0001742826780021254,
      "loss": 0.0454,
      "step": 494
    },
    {
      "epoch": 0.3933253873659118,
      "grad_norm": 0.10036259144544601,
      "learning_rate": 0.0001742295430393199,
      "loss": 0.0665,
      "step": 495
    },
    {
      "epoch": 0.39411998410806515,
      "grad_norm": 0.11036942899227142,
      "learning_rate": 0.00017417640807651437,
      "loss": 0.0612,
      "step": 496
    },
    {
      "epoch": 0.3949145808502185,
      "grad_norm": 0.13097025454044342,
      "learning_rate": 0.00017412327311370883,
      "loss": 0.0447,
      "step": 497
    },
    {
      "epoch": 0.3957091775923719,
      "grad_norm": 0.1235135868191719,
      "learning_rate": 0.0001740701381509033,
      "loss": 0.0434,
      "step": 498
    },
    {
      "epoch": 0.3965037743345252,
      "grad_norm": 0.10748361051082611,
      "learning_rate": 0.00017401700318809776,
      "loss": 0.042,
      "step": 499
    },
    {
      "epoch": 0.3972983710766786,
      "grad_norm": 0.08436639606952667,
      "learning_rate": 0.00017396386822529224,
      "loss": 0.0389,
      "step": 500
    },
    {
      "epoch": 0.39809296781883197,
      "grad_norm": 1.3532744646072388,
      "learning_rate": 0.00017391073326248672,
      "loss": 0.6865,
      "step": 501
    },
    {
      "epoch": 0.3988875645609853,
      "grad_norm": 0.8535931706428528,
      "learning_rate": 0.0001738575982996812,
      "loss": 0.4783,
      "step": 502
    },
    {
      "epoch": 0.39968216130313866,
      "grad_norm": 0.4678332805633545,
      "learning_rate": 0.00017380446333687568,
      "loss": 0.4309,
      "step": 503
    },
    {
      "epoch": 0.40047675804529204,
      "grad_norm": 0.3833351135253906,
      "learning_rate": 0.00017375132837407016,
      "loss": 0.4289,
      "step": 504
    },
    {
      "epoch": 0.40127135478744536,
      "grad_norm": 0.2639719545841217,
      "learning_rate": 0.00017369819341126464,
      "loss": 0.3328,
      "step": 505
    },
    {
      "epoch": 0.40206595152959873,
      "grad_norm": 0.24227659404277802,
      "learning_rate": 0.0001736450584484591,
      "loss": 0.2857,
      "step": 506
    },
    {
      "epoch": 0.4028605482717521,
      "grad_norm": 0.2591593563556671,
      "learning_rate": 0.00017359192348565357,
      "loss": 0.3045,
      "step": 507
    },
    {
      "epoch": 0.4036551450139054,
      "grad_norm": 0.24281808733940125,
      "learning_rate": 0.00017353878852284803,
      "loss": 0.3178,
      "step": 508
    },
    {
      "epoch": 0.4044497417560588,
      "grad_norm": 0.27179622650146484,
      "learning_rate": 0.0001734856535600425,
      "loss": 0.2896,
      "step": 509
    },
    {
      "epoch": 0.4052443384982122,
      "grad_norm": 0.2061675488948822,
      "learning_rate": 0.000173432518597237,
      "loss": 0.2501,
      "step": 510
    },
    {
      "epoch": 0.4060389352403655,
      "grad_norm": 0.21467508375644684,
      "learning_rate": 0.00017337938363443147,
      "loss": 0.216,
      "step": 511
    },
    {
      "epoch": 0.40683353198251887,
      "grad_norm": 0.24157379567623138,
      "learning_rate": 0.00017332624867162595,
      "loss": 0.2379,
      "step": 512
    },
    {
      "epoch": 0.40762812872467225,
      "grad_norm": 0.19514483213424683,
      "learning_rate": 0.00017327311370882043,
      "loss": 0.186,
      "step": 513
    },
    {
      "epoch": 0.40842272546682556,
      "grad_norm": 0.19463635981082916,
      "learning_rate": 0.00017321997874601488,
      "loss": 0.2124,
      "step": 514
    },
    {
      "epoch": 0.40921732220897894,
      "grad_norm": 0.20250986516475677,
      "learning_rate": 0.00017316684378320936,
      "loss": 0.2299,
      "step": 515
    },
    {
      "epoch": 0.4100119189511323,
      "grad_norm": 0.2274000644683838,
      "learning_rate": 0.00017311370882040384,
      "loss": 0.223,
      "step": 516
    },
    {
      "epoch": 0.41080651569328563,
      "grad_norm": 0.21220611035823822,
      "learning_rate": 0.0001730605738575983,
      "loss": 0.1906,
      "step": 517
    },
    {
      "epoch": 0.411601112435439,
      "grad_norm": 0.16713078320026398,
      "learning_rate": 0.00017300743889479277,
      "loss": 0.167,
      "step": 518
    },
    {
      "epoch": 0.4123957091775924,
      "grad_norm": 0.16015617549419403,
      "learning_rate": 0.00017295430393198726,
      "loss": 0.1873,
      "step": 519
    },
    {
      "epoch": 0.4131903059197457,
      "grad_norm": 0.17407336831092834,
      "learning_rate": 0.00017290116896918174,
      "loss": 0.1765,
      "step": 520
    },
    {
      "epoch": 0.4139849026618991,
      "grad_norm": 0.18234583735466003,
      "learning_rate": 0.00017284803400637622,
      "loss": 0.1597,
      "step": 521
    },
    {
      "epoch": 0.41477949940405245,
      "grad_norm": 0.12813718616962433,
      "learning_rate": 0.00017279489904357067,
      "loss": 0.1334,
      "step": 522
    },
    {
      "epoch": 0.4155740961462058,
      "grad_norm": 0.15167921781539917,
      "learning_rate": 0.00017274176408076515,
      "loss": 0.1368,
      "step": 523
    },
    {
      "epoch": 0.41636869288835915,
      "grad_norm": 0.19684261083602905,
      "learning_rate": 0.00017268862911795963,
      "loss": 0.131,
      "step": 524
    },
    {
      "epoch": 0.4171632896305125,
      "grad_norm": 0.1527683585882187,
      "learning_rate": 0.0001726354941551541,
      "loss": 0.1338,
      "step": 525
    },
    {
      "epoch": 0.4179578863726659,
      "grad_norm": 0.16256847977638245,
      "learning_rate": 0.00017258235919234856,
      "loss": 0.1209,
      "step": 526
    },
    {
      "epoch": 0.4187524831148192,
      "grad_norm": 0.1611175239086151,
      "learning_rate": 0.00017252922422954304,
      "loss": 0.1342,
      "step": 527
    },
    {
      "epoch": 0.4195470798569726,
      "grad_norm": 0.15386508405208588,
      "learning_rate": 0.00017247608926673752,
      "loss": 0.1349,
      "step": 528
    },
    {
      "epoch": 0.42034167659912597,
      "grad_norm": 0.13807763159275055,
      "learning_rate": 0.000172422954303932,
      "loss": 0.1045,
      "step": 529
    },
    {
      "epoch": 0.4211362733412793,
      "grad_norm": 0.15460006892681122,
      "learning_rate": 0.00017236981934112646,
      "loss": 0.1167,
      "step": 530
    },
    {
      "epoch": 0.42193087008343266,
      "grad_norm": 0.16303877532482147,
      "learning_rate": 0.00017231668437832094,
      "loss": 0.144,
      "step": 531
    },
    {
      "epoch": 0.42272546682558604,
      "grad_norm": 0.11480847746133804,
      "learning_rate": 0.00017226354941551542,
      "loss": 0.09,
      "step": 532
    },
    {
      "epoch": 0.42352006356773936,
      "grad_norm": 0.1374879777431488,
      "learning_rate": 0.0001722104144527099,
      "loss": 0.1146,
      "step": 533
    },
    {
      "epoch": 0.42431466030989273,
      "grad_norm": 0.15954264998435974,
      "learning_rate": 0.00017215727948990438,
      "loss": 0.1043,
      "step": 534
    },
    {
      "epoch": 0.4251092570520461,
      "grad_norm": 0.11786545813083649,
      "learning_rate": 0.00017210414452709883,
      "loss": 0.1048,
      "step": 535
    },
    {
      "epoch": 0.4259038537941994,
      "grad_norm": 0.13134117424488068,
      "learning_rate": 0.0001720510095642933,
      "loss": 0.104,
      "step": 536
    },
    {
      "epoch": 0.4266984505363528,
      "grad_norm": 0.11975903809070587,
      "learning_rate": 0.0001719978746014878,
      "loss": 0.0904,
      "step": 537
    },
    {
      "epoch": 0.4274930472785062,
      "grad_norm": 0.11612091958522797,
      "learning_rate": 0.00017194473963868224,
      "loss": 0.0733,
      "step": 538
    },
    {
      "epoch": 0.4282876440206595,
      "grad_norm": 0.10892501473426819,
      "learning_rate": 0.00017189160467587672,
      "loss": 0.0782,
      "step": 539
    },
    {
      "epoch": 0.42908224076281287,
      "grad_norm": 0.11306760460138321,
      "learning_rate": 0.0001718384697130712,
      "loss": 0.062,
      "step": 540
    },
    {
      "epoch": 0.42987683750496625,
      "grad_norm": 0.14252065122127533,
      "learning_rate": 0.00017178533475026568,
      "loss": 0.0675,
      "step": 541
    },
    {
      "epoch": 0.43067143424711957,
      "grad_norm": 0.10595011711120605,
      "learning_rate": 0.00017173219978746016,
      "loss": 0.0756,
      "step": 542
    },
    {
      "epoch": 0.43146603098927294,
      "grad_norm": 0.12251737713813782,
      "learning_rate": 0.00017167906482465464,
      "loss": 0.05,
      "step": 543
    },
    {
      "epoch": 0.4322606277314263,
      "grad_norm": 0.11248825490474701,
      "learning_rate": 0.0001716259298618491,
      "loss": 0.0419,
      "step": 544
    },
    {
      "epoch": 0.43305522447357964,
      "grad_norm": 0.09291782230138779,
      "learning_rate": 0.00017157279489904358,
      "loss": 0.0525,
      "step": 545
    },
    {
      "epoch": 0.433849821215733,
      "grad_norm": 0.0952376201748848,
      "learning_rate": 0.00017151965993623803,
      "loss": 0.0483,
      "step": 546
    },
    {
      "epoch": 0.4346444179578864,
      "grad_norm": 0.09954146295785904,
      "learning_rate": 0.0001714665249734325,
      "loss": 0.0538,
      "step": 547
    },
    {
      "epoch": 0.4354390147000397,
      "grad_norm": 0.09918416291475296,
      "learning_rate": 0.000171413390010627,
      "loss": 0.0406,
      "step": 548
    },
    {
      "epoch": 0.4362336114421931,
      "grad_norm": 0.09304721653461456,
      "learning_rate": 0.00017136025504782147,
      "loss": 0.0334,
      "step": 549
    },
    {
      "epoch": 0.43702820818434646,
      "grad_norm": 0.11751978099346161,
      "learning_rate": 0.00017130712008501595,
      "loss": 0.0422,
      "step": 550
    },
    {
      "epoch": 0.4378228049264998,
      "grad_norm": 0.6405899524688721,
      "learning_rate": 0.00017125398512221043,
      "loss": 0.6003,
      "step": 551
    },
    {
      "epoch": 0.43861740166865315,
      "grad_norm": 0.5206952691078186,
      "learning_rate": 0.0001712008501594049,
      "loss": 0.5092,
      "step": 552
    },
    {
      "epoch": 0.4394119984108065,
      "grad_norm": 0.4875452518463135,
      "learning_rate": 0.0001711477151965994,
      "loss": 0.4131,
      "step": 553
    },
    {
      "epoch": 0.44020659515295985,
      "grad_norm": 0.29319000244140625,
      "learning_rate": 0.00017109458023379384,
      "loss": 0.4218,
      "step": 554
    },
    {
      "epoch": 0.4410011918951132,
      "grad_norm": 0.2839665710926056,
      "learning_rate": 0.0001710414452709883,
      "loss": 0.357,
      "step": 555
    },
    {
      "epoch": 0.4417957886372666,
      "grad_norm": 0.29354360699653625,
      "learning_rate": 0.00017098831030818278,
      "loss": 0.3146,
      "step": 556
    },
    {
      "epoch": 0.44259038537941997,
      "grad_norm": 0.2391386777162552,
      "learning_rate": 0.00017093517534537726,
      "loss": 0.3074,
      "step": 557
    },
    {
      "epoch": 0.4433849821215733,
      "grad_norm": 0.2562192678451538,
      "learning_rate": 0.00017088204038257174,
      "loss": 0.3362,
      "step": 558
    },
    {
      "epoch": 0.44417957886372667,
      "grad_norm": 0.23815485835075378,
      "learning_rate": 0.00017082890541976622,
      "loss": 0.2741,
      "step": 559
    },
    {
      "epoch": 0.44497417560588004,
      "grad_norm": 0.22830252349376678,
      "learning_rate": 0.0001707757704569607,
      "loss": 0.2739,
      "step": 560
    },
    {
      "epoch": 0.44576877234803336,
      "grad_norm": 0.24359333515167236,
      "learning_rate": 0.00017072263549415518,
      "loss": 0.277,
      "step": 561
    },
    {
      "epoch": 0.44656336909018673,
      "grad_norm": 0.24038057029247284,
      "learning_rate": 0.00017066950053134966,
      "loss": 0.2617,
      "step": 562
    },
    {
      "epoch": 0.4473579658323401,
      "grad_norm": 0.2074430137872696,
      "learning_rate": 0.0001706163655685441,
      "loss": 0.2167,
      "step": 563
    },
    {
      "epoch": 0.44815256257449343,
      "grad_norm": 0.17728093266487122,
      "learning_rate": 0.00017056323060573857,
      "loss": 0.2097,
      "step": 564
    },
    {
      "epoch": 0.4489471593166468,
      "grad_norm": 0.21102969348430634,
      "learning_rate": 0.00017051009564293305,
      "loss": 0.2182,
      "step": 565
    },
    {
      "epoch": 0.4497417560588002,
      "grad_norm": 0.20054851472377777,
      "learning_rate": 0.00017045696068012753,
      "loss": 0.2052,
      "step": 566
    },
    {
      "epoch": 0.4505363528009535,
      "grad_norm": 0.21039541065692902,
      "learning_rate": 0.000170403825717322,
      "loss": 0.2092,
      "step": 567
    },
    {
      "epoch": 0.4513309495431069,
      "grad_norm": 0.19313572347164154,
      "learning_rate": 0.00017035069075451649,
      "loss": 0.2008,
      "step": 568
    },
    {
      "epoch": 0.45212554628526025,
      "grad_norm": 0.18476566672325134,
      "learning_rate": 0.00017029755579171097,
      "loss": 0.1973,
      "step": 569
    },
    {
      "epoch": 0.45292014302741357,
      "grad_norm": 0.17767760157585144,
      "learning_rate": 0.00017024442082890545,
      "loss": 0.1688,
      "step": 570
    },
    {
      "epoch": 0.45371473976956694,
      "grad_norm": 0.16385503113269806,
      "learning_rate": 0.0001701912858660999,
      "loss": 0.1476,
      "step": 571
    },
    {
      "epoch": 0.4545093365117203,
      "grad_norm": 0.1573036164045334,
      "learning_rate": 0.00017013815090329438,
      "loss": 0.1403,
      "step": 572
    },
    {
      "epoch": 0.45530393325387364,
      "grad_norm": 0.14095774292945862,
      "learning_rate": 0.00017008501594048883,
      "loss": 0.1451,
      "step": 573
    },
    {
      "epoch": 0.456098529996027,
      "grad_norm": 0.15940800309181213,
      "learning_rate": 0.0001700318809776833,
      "loss": 0.1597,
      "step": 574
    },
    {
      "epoch": 0.4568931267381804,
      "grad_norm": 0.18398946523666382,
      "learning_rate": 0.0001699787460148778,
      "loss": 0.1677,
      "step": 575
    },
    {
      "epoch": 0.4576877234803337,
      "grad_norm": 0.17532724142074585,
      "learning_rate": 0.00016992561105207227,
      "loss": 0.1526,
      "step": 576
    },
    {
      "epoch": 0.4584823202224871,
      "grad_norm": 0.13531874120235443,
      "learning_rate": 0.00016987247608926675,
      "loss": 0.1347,
      "step": 577
    },
    {
      "epoch": 0.45927691696464046,
      "grad_norm": 0.14403687417507172,
      "learning_rate": 0.00016981934112646123,
      "loss": 0.1355,
      "step": 578
    },
    {
      "epoch": 0.4600715137067938,
      "grad_norm": 0.1270918846130371,
      "learning_rate": 0.0001697662061636557,
      "loss": 0.1322,
      "step": 579
    },
    {
      "epoch": 0.46086611044894715,
      "grad_norm": 0.14452433586120605,
      "learning_rate": 0.00016971307120085017,
      "loss": 0.1227,
      "step": 580
    },
    {
      "epoch": 0.46166070719110053,
      "grad_norm": 0.1376820057630539,
      "learning_rate": 0.00016965993623804465,
      "loss": 0.122,
      "step": 581
    },
    {
      "epoch": 0.46245530393325385,
      "grad_norm": 0.10571511089801788,
      "learning_rate": 0.0001696068012752391,
      "loss": 0.0859,
      "step": 582
    },
    {
      "epoch": 0.4632499006754072,
      "grad_norm": 0.12337704747915268,
      "learning_rate": 0.00016955366631243358,
      "loss": 0.1025,
      "step": 583
    },
    {
      "epoch": 0.4640444974175606,
      "grad_norm": 0.12889760732650757,
      "learning_rate": 0.00016950053134962806,
      "loss": 0.1081,
      "step": 584
    },
    {
      "epoch": 0.464839094159714,
      "grad_norm": 0.10969044268131256,
      "learning_rate": 0.00016944739638682254,
      "loss": 0.0828,
      "step": 585
    },
    {
      "epoch": 0.4656336909018673,
      "grad_norm": 0.12754599750041962,
      "learning_rate": 0.00016939426142401702,
      "loss": 0.0798,
      "step": 586
    },
    {
      "epoch": 0.46642828764402067,
      "grad_norm": 0.10851863771677017,
      "learning_rate": 0.00016934112646121147,
      "loss": 0.0943,
      "step": 587
    },
    {
      "epoch": 0.46722288438617404,
      "grad_norm": 0.10078829526901245,
      "learning_rate": 0.00016928799149840595,
      "loss": 0.0733,
      "step": 588
    },
    {
      "epoch": 0.46801748112832736,
      "grad_norm": 0.12145230174064636,
      "learning_rate": 0.00016923485653560043,
      "loss": 0.0854,
      "step": 589
    },
    {
      "epoch": 0.46881207787048074,
      "grad_norm": 0.10914859920740128,
      "learning_rate": 0.00016918172157279491,
      "loss": 0.0778,
      "step": 590
    },
    {
      "epoch": 0.4696066746126341,
      "grad_norm": 0.10615339130163193,
      "learning_rate": 0.00016912858660998937,
      "loss": 0.0791,
      "step": 591
    },
    {
      "epoch": 0.47040127135478743,
      "grad_norm": 0.11796209961175919,
      "learning_rate": 0.00016907545164718385,
      "loss": 0.0729,
      "step": 592
    },
    {
      "epoch": 0.4711958680969408,
      "grad_norm": 0.10080892592668533,
      "learning_rate": 0.00016902231668437833,
      "loss": 0.0812,
      "step": 593
    },
    {
      "epoch": 0.4719904648390942,
      "grad_norm": 0.07477190345525742,
      "learning_rate": 0.0001689691817215728,
      "loss": 0.0478,
      "step": 594
    },
    {
      "epoch": 0.4727850615812475,
      "grad_norm": 0.08991693705320358,
      "learning_rate": 0.0001689160467587673,
      "loss": 0.0629,
      "step": 595
    },
    {
      "epoch": 0.4735796583234009,
      "grad_norm": 0.08759844303131104,
      "learning_rate": 0.00016886291179596174,
      "loss": 0.0395,
      "step": 596
    },
    {
      "epoch": 0.47437425506555425,
      "grad_norm": 0.08860986679792404,
      "learning_rate": 0.00016880977683315622,
      "loss": 0.0398,
      "step": 597
    },
    {
      "epoch": 0.47516885180770757,
      "grad_norm": 0.08822234719991684,
      "learning_rate": 0.0001687566418703507,
      "loss": 0.0336,
      "step": 598
    },
    {
      "epoch": 0.47596344854986095,
      "grad_norm": 0.08599933236837387,
      "learning_rate": 0.00016870350690754518,
      "loss": 0.037,
      "step": 599
    },
    {
      "epoch": 0.4767580452920143,
      "grad_norm": 0.08145996928215027,
      "learning_rate": 0.00016865037194473964,
      "loss": 0.032,
      "step": 600
    },
    {
      "epoch": 0.47755264203416764,
      "grad_norm": 0.6958491206169128,
      "learning_rate": 0.00016859723698193412,
      "loss": 0.7426,
      "step": 601
    },
    {
      "epoch": 0.478347238776321,
      "grad_norm": 0.5394800901412964,
      "learning_rate": 0.0001685441020191286,
      "loss": 0.4991,
      "step": 602
    },
    {
      "epoch": 0.4791418355184744,
      "grad_norm": 0.4228609502315521,
      "learning_rate": 0.00016849096705632308,
      "loss": 0.4661,
      "step": 603
    },
    {
      "epoch": 0.4799364322606277,
      "grad_norm": 0.3216509222984314,
      "learning_rate": 0.00016843783209351753,
      "loss": 0.3719,
      "step": 604
    },
    {
      "epoch": 0.4807310290027811,
      "grad_norm": 0.25649672746658325,
      "learning_rate": 0.000168384697130712,
      "loss": 0.3248,
      "step": 605
    },
    {
      "epoch": 0.48152562574493446,
      "grad_norm": 0.22266343235969543,
      "learning_rate": 0.0001683315621679065,
      "loss": 0.2933,
      "step": 606
    },
    {
      "epoch": 0.4823202224870878,
      "grad_norm": 0.27043452858924866,
      "learning_rate": 0.00016827842720510097,
      "loss": 0.2962,
      "step": 607
    },
    {
      "epoch": 0.48311481922924115,
      "grad_norm": 0.20372124016284943,
      "learning_rate": 0.00016822529224229545,
      "loss": 0.2698,
      "step": 608
    },
    {
      "epoch": 0.48390941597139453,
      "grad_norm": 0.21176624298095703,
      "learning_rate": 0.00016817215727948993,
      "loss": 0.2722,
      "step": 609
    },
    {
      "epoch": 0.48470401271354785,
      "grad_norm": 0.20498968660831451,
      "learning_rate": 0.00016811902231668438,
      "loss": 0.2362,
      "step": 610
    },
    {
      "epoch": 0.4854986094557012,
      "grad_norm": 0.26487547159194946,
      "learning_rate": 0.00016806588735387886,
      "loss": 0.2383,
      "step": 611
    },
    {
      "epoch": 0.4862932061978546,
      "grad_norm": 0.18695175647735596,
      "learning_rate": 0.00016801275239107332,
      "loss": 0.2038,
      "step": 612
    },
    {
      "epoch": 0.4870878029400079,
      "grad_norm": 0.19320477545261383,
      "learning_rate": 0.0001679596174282678,
      "loss": 0.2098,
      "step": 613
    },
    {
      "epoch": 0.4878823996821613,
      "grad_norm": 0.19279474020004272,
      "learning_rate": 0.00016790648246546228,
      "loss": 0.1873,
      "step": 614
    },
    {
      "epoch": 0.48867699642431467,
      "grad_norm": 0.16420583426952362,
      "learning_rate": 0.00016785334750265676,
      "loss": 0.1888,
      "step": 615
    },
    {
      "epoch": 0.48947159316646804,
      "grad_norm": 0.18118548393249512,
      "learning_rate": 0.00016780021253985124,
      "loss": 0.1962,
      "step": 616
    },
    {
      "epoch": 0.49026618990862136,
      "grad_norm": 0.1687295138835907,
      "learning_rate": 0.00016774707757704572,
      "loss": 0.1623,
      "step": 617
    },
    {
      "epoch": 0.49106078665077474,
      "grad_norm": 0.19524388015270233,
      "learning_rate": 0.0001676939426142402,
      "loss": 0.1553,
      "step": 618
    },
    {
      "epoch": 0.4918553833929281,
      "grad_norm": 0.1852264702320099,
      "learning_rate": 0.00016764080765143465,
      "loss": 0.1568,
      "step": 619
    },
    {
      "epoch": 0.49264998013508143,
      "grad_norm": 0.16435974836349487,
      "learning_rate": 0.0001675876726886291,
      "loss": 0.1701,
      "step": 620
    },
    {
      "epoch": 0.4934445768772348,
      "grad_norm": 0.19954049587249756,
      "learning_rate": 0.00016753453772582358,
      "loss": 0.1509,
      "step": 621
    },
    {
      "epoch": 0.4942391736193882,
      "grad_norm": 0.16141052544116974,
      "learning_rate": 0.00016748140276301806,
      "loss": 0.1539,
      "step": 622
    },
    {
      "epoch": 0.4950337703615415,
      "grad_norm": 0.16733233630657196,
      "learning_rate": 0.00016742826780021254,
      "loss": 0.1313,
      "step": 623
    },
    {
      "epoch": 0.4958283671036949,
      "grad_norm": 0.141399547457695,
      "learning_rate": 0.00016737513283740702,
      "loss": 0.1253,
      "step": 624
    },
    {
      "epoch": 0.49662296384584825,
      "grad_norm": 0.2049967348575592,
      "learning_rate": 0.0001673219978746015,
      "loss": 0.1458,
      "step": 625
    },
    {
      "epoch": 0.4974175605880016,
      "grad_norm": 0.1681317538022995,
      "learning_rate": 0.00016726886291179598,
      "loss": 0.122,
      "step": 626
    },
    {
      "epoch": 0.49821215733015495,
      "grad_norm": 0.16735979914665222,
      "learning_rate": 0.00016721572794899047,
      "loss": 0.1244,
      "step": 627
    },
    {
      "epoch": 0.4990067540723083,
      "grad_norm": 0.21398334205150604,
      "learning_rate": 0.00016716259298618492,
      "loss": 0.1404,
      "step": 628
    },
    {
      "epoch": 0.49980135081446164,
      "grad_norm": 0.12683236598968506,
      "learning_rate": 0.00016710945802337937,
      "loss": 0.1078,
      "step": 629
    },
    {
      "epoch": 0.5005959475566151,
      "grad_norm": 0.12924201786518097,
      "learning_rate": 0.00016705632306057385,
      "loss": 0.0995,
      "step": 630
    },
    {
      "epoch": 0.5013905442987684,
      "grad_norm": 0.12943007051944733,
      "learning_rate": 0.00016700318809776833,
      "loss": 0.1131,
      "step": 631
    },
    {
      "epoch": 0.5021851410409217,
      "grad_norm": 0.1379767805337906,
      "learning_rate": 0.0001669500531349628,
      "loss": 0.0896,
      "step": 632
    },
    {
      "epoch": 0.5029797377830751,
      "grad_norm": 0.1383536458015442,
      "learning_rate": 0.0001668969181721573,
      "loss": 0.0935,
      "step": 633
    },
    {
      "epoch": 0.5037743345252285,
      "grad_norm": 0.11201071739196777,
      "learning_rate": 0.00016684378320935177,
      "loss": 0.09,
      "step": 634
    },
    {
      "epoch": 0.5045689312673818,
      "grad_norm": 0.13013210892677307,
      "learning_rate": 0.00016679064824654625,
      "loss": 0.0977,
      "step": 635
    },
    {
      "epoch": 0.5053635280095352,
      "grad_norm": 0.1170118898153305,
      "learning_rate": 0.0001667375132837407,
      "loss": 0.0714,
      "step": 636
    },
    {
      "epoch": 0.5061581247516885,
      "grad_norm": 0.10926445573568344,
      "learning_rate": 0.00016668437832093519,
      "loss": 0.0813,
      "step": 637
    },
    {
      "epoch": 0.5069527214938419,
      "grad_norm": 0.11709357053041458,
      "learning_rate": 0.00016663124335812964,
      "loss": 0.0665,
      "step": 638
    },
    {
      "epoch": 0.5077473182359953,
      "grad_norm": 0.11044281721115112,
      "learning_rate": 0.00016657810839532412,
      "loss": 0.0818,
      "step": 639
    },
    {
      "epoch": 0.5085419149781486,
      "grad_norm": 0.10964597761631012,
      "learning_rate": 0.0001665249734325186,
      "loss": 0.089,
      "step": 640
    },
    {
      "epoch": 0.5093365117203019,
      "grad_norm": 0.10428038239479065,
      "learning_rate": 0.00016647183846971308,
      "loss": 0.0539,
      "step": 641
    },
    {
      "epoch": 0.5101311084624554,
      "grad_norm": 0.10011354833841324,
      "learning_rate": 0.00016641870350690756,
      "loss": 0.0565,
      "step": 642
    },
    {
      "epoch": 0.5109257052046087,
      "grad_norm": 0.09789957106113434,
      "learning_rate": 0.00016636556854410204,
      "loss": 0.0534,
      "step": 643
    },
    {
      "epoch": 0.511720301946762,
      "grad_norm": 0.15499535202980042,
      "learning_rate": 0.00016631243358129652,
      "loss": 0.0609,
      "step": 644
    },
    {
      "epoch": 0.5125148986889154,
      "grad_norm": 0.0898820161819458,
      "learning_rate": 0.00016625929861849097,
      "loss": 0.0505,
      "step": 645
    },
    {
      "epoch": 0.5133094954310687,
      "grad_norm": 0.09114237874746323,
      "learning_rate": 0.00016620616365568545,
      "loss": 0.0509,
      "step": 646
    },
    {
      "epoch": 0.5141040921732221,
      "grad_norm": 0.08712676167488098,
      "learning_rate": 0.0001661530286928799,
      "loss": 0.0435,
      "step": 647
    },
    {
      "epoch": 0.5148986889153755,
      "grad_norm": 0.08626683801412582,
      "learning_rate": 0.0001660998937300744,
      "loss": 0.0386,
      "step": 648
    },
    {
      "epoch": 0.5156932856575288,
      "grad_norm": 0.08220765739679337,
      "learning_rate": 0.00016604675876726887,
      "loss": 0.0358,
      "step": 649
    },
    {
      "epoch": 0.5164878823996821,
      "grad_norm": 0.0968245342373848,
      "learning_rate": 0.00016599362380446335,
      "loss": 0.0421,
      "step": 650
    },
    {
      "epoch": 0.5172824791418356,
      "grad_norm": 0.6951093673706055,
      "learning_rate": 0.00016594048884165783,
      "loss": 0.5908,
      "step": 651
    },
    {
      "epoch": 0.5180770758839889,
      "grad_norm": 0.7204357981681824,
      "learning_rate": 0.0001658873538788523,
      "loss": 0.4778,
      "step": 652
    },
    {
      "epoch": 0.5188716726261422,
      "grad_norm": 0.5625940561294556,
      "learning_rate": 0.00016583421891604676,
      "loss": 0.4331,
      "step": 653
    },
    {
      "epoch": 0.5196662693682956,
      "grad_norm": 0.3845457136631012,
      "learning_rate": 0.00016578108395324124,
      "loss": 0.4383,
      "step": 654
    },
    {
      "epoch": 0.520460866110449,
      "grad_norm": 0.31672534346580505,
      "learning_rate": 0.00016572794899043572,
      "loss": 0.3628,
      "step": 655
    },
    {
      "epoch": 0.5212554628526023,
      "grad_norm": 0.2632070779800415,
      "learning_rate": 0.0001656748140276302,
      "loss": 0.2884,
      "step": 656
    },
    {
      "epoch": 0.5220500595947557,
      "grad_norm": 0.1956564038991928,
      "learning_rate": 0.00016562167906482465,
      "loss": 0.2295,
      "step": 657
    },
    {
      "epoch": 0.522844656336909,
      "grad_norm": 0.20164911448955536,
      "learning_rate": 0.00016556854410201913,
      "loss": 0.2922,
      "step": 658
    },
    {
      "epoch": 0.5236392530790623,
      "grad_norm": 0.210745170712471,
      "learning_rate": 0.00016551540913921361,
      "loss": 0.2527,
      "step": 659
    },
    {
      "epoch": 0.5244338498212158,
      "grad_norm": 0.23250262439250946,
      "learning_rate": 0.0001654622741764081,
      "loss": 0.2292,
      "step": 660
    },
    {
      "epoch": 0.5252284465633691,
      "grad_norm": 0.2170725017786026,
      "learning_rate": 0.00016540913921360255,
      "loss": 0.2539,
      "step": 661
    },
    {
      "epoch": 0.5260230433055224,
      "grad_norm": 0.2121613621711731,
      "learning_rate": 0.00016535600425079703,
      "loss": 0.2289,
      "step": 662
    },
    {
      "epoch": 0.5268176400476758,
      "grad_norm": 0.2281489372253418,
      "learning_rate": 0.0001653028692879915,
      "loss": 0.2548,
      "step": 663
    },
    {
      "epoch": 0.5276122367898292,
      "grad_norm": 0.16878920793533325,
      "learning_rate": 0.000165249734325186,
      "loss": 0.1884,
      "step": 664
    },
    {
      "epoch": 0.5284068335319825,
      "grad_norm": 0.19336383044719696,
      "learning_rate": 0.00016519659936238047,
      "loss": 0.1892,
      "step": 665
    },
    {
      "epoch": 0.5292014302741359,
      "grad_norm": 0.18649530410766602,
      "learning_rate": 0.00016514346439957492,
      "loss": 0.1982,
      "step": 666
    },
    {
      "epoch": 0.5299960270162892,
      "grad_norm": 0.17531649768352509,
      "learning_rate": 0.0001650903294367694,
      "loss": 0.1833,
      "step": 667
    },
    {
      "epoch": 0.5307906237584425,
      "grad_norm": 0.19203226268291473,
      "learning_rate": 0.00016503719447396388,
      "loss": 0.1822,
      "step": 668
    },
    {
      "epoch": 0.531585220500596,
      "grad_norm": 0.20249062776565552,
      "learning_rate": 0.00016498405951115834,
      "loss": 0.1652,
      "step": 669
    },
    {
      "epoch": 0.5323798172427493,
      "grad_norm": 0.1944902241230011,
      "learning_rate": 0.00016493092454835282,
      "loss": 0.1774,
      "step": 670
    },
    {
      "epoch": 0.5331744139849026,
      "grad_norm": 0.22583326697349548,
      "learning_rate": 0.0001648777895855473,
      "loss": 0.1456,
      "step": 671
    },
    {
      "epoch": 0.533969010727056,
      "grad_norm": 0.17457987368106842,
      "learning_rate": 0.00016482465462274178,
      "loss": 0.1584,
      "step": 672
    },
    {
      "epoch": 0.5347636074692094,
      "grad_norm": 0.18834908306598663,
      "learning_rate": 0.00016477151965993626,
      "loss": 0.1716,
      "step": 673
    },
    {
      "epoch": 0.5355582042113627,
      "grad_norm": 0.1352584958076477,
      "learning_rate": 0.00016471838469713074,
      "loss": 0.1285,
      "step": 674
    },
    {
      "epoch": 0.5363528009535161,
      "grad_norm": 0.14262254536151886,
      "learning_rate": 0.0001646652497343252,
      "loss": 0.1206,
      "step": 675
    },
    {
      "epoch": 0.5371473976956694,
      "grad_norm": 0.15499314665794373,
      "learning_rate": 0.00016461211477151967,
      "loss": 0.141,
      "step": 676
    },
    {
      "epoch": 0.5379419944378228,
      "grad_norm": 0.15070171654224396,
      "learning_rate": 0.00016455897980871412,
      "loss": 0.1502,
      "step": 677
    },
    {
      "epoch": 0.5387365911799762,
      "grad_norm": 0.159584641456604,
      "learning_rate": 0.0001645058448459086,
      "loss": 0.1287,
      "step": 678
    },
    {
      "epoch": 0.5395311879221295,
      "grad_norm": 0.13231301307678223,
      "learning_rate": 0.00016445270988310308,
      "loss": 0.1283,
      "step": 679
    },
    {
      "epoch": 0.5403257846642828,
      "grad_norm": 0.127310112118721,
      "learning_rate": 0.00016439957492029756,
      "loss": 0.1215,
      "step": 680
    },
    {
      "epoch": 0.5411203814064363,
      "grad_norm": 0.1596185714006424,
      "learning_rate": 0.00016434643995749204,
      "loss": 0.1423,
      "step": 681
    },
    {
      "epoch": 0.5419149781485896,
      "grad_norm": 0.11697930842638016,
      "learning_rate": 0.00016429330499468652,
      "loss": 0.0917,
      "step": 682
    },
    {
      "epoch": 0.5427095748907429,
      "grad_norm": 0.12343541532754898,
      "learning_rate": 0.000164240170031881,
      "loss": 0.1197,
      "step": 683
    },
    {
      "epoch": 0.5435041716328963,
      "grad_norm": 0.14299806952476501,
      "learning_rate": 0.00016418703506907546,
      "loss": 0.1206,
      "step": 684
    },
    {
      "epoch": 0.5442987683750496,
      "grad_norm": 0.11287374794483185,
      "learning_rate": 0.00016413390010626994,
      "loss": 0.0966,
      "step": 685
    },
    {
      "epoch": 0.545093365117203,
      "grad_norm": 0.1287161409854889,
      "learning_rate": 0.0001640807651434644,
      "loss": 0.0962,
      "step": 686
    },
    {
      "epoch": 0.5458879618593564,
      "grad_norm": 0.14539708197116852,
      "learning_rate": 0.00016402763018065887,
      "loss": 0.1205,
      "step": 687
    },
    {
      "epoch": 0.5466825586015097,
      "grad_norm": 0.12033561617136002,
      "learning_rate": 0.00016397449521785335,
      "loss": 0.0887,
      "step": 688
    },
    {
      "epoch": 0.547477155343663,
      "grad_norm": 0.10007776319980621,
      "learning_rate": 0.00016392136025504783,
      "loss": 0.0768,
      "step": 689
    },
    {
      "epoch": 0.5482717520858165,
      "grad_norm": 0.10888214409351349,
      "learning_rate": 0.0001638682252922423,
      "loss": 0.0806,
      "step": 690
    },
    {
      "epoch": 0.5490663488279698,
      "grad_norm": 0.09900055080652237,
      "learning_rate": 0.0001638150903294368,
      "loss": 0.0847,
      "step": 691
    },
    {
      "epoch": 0.5498609455701232,
      "grad_norm": 0.10102885216474533,
      "learning_rate": 0.00016376195536663127,
      "loss": 0.0815,
      "step": 692
    },
    {
      "epoch": 0.5506555423122765,
      "grad_norm": 0.09336532652378082,
      "learning_rate": 0.00016370882040382572,
      "loss": 0.0601,
      "step": 693
    },
    {
      "epoch": 0.5514501390544299,
      "grad_norm": 0.09683185070753098,
      "learning_rate": 0.00016365568544102018,
      "loss": 0.0495,
      "step": 694
    },
    {
      "epoch": 0.5522447357965833,
      "grad_norm": 0.09547777473926544,
      "learning_rate": 0.00016360255047821466,
      "loss": 0.0536,
      "step": 695
    },
    {
      "epoch": 0.5530393325387366,
      "grad_norm": 0.07931177318096161,
      "learning_rate": 0.00016354941551540914,
      "loss": 0.0632,
      "step": 696
    },
    {
      "epoch": 0.5538339292808899,
      "grad_norm": 0.07589127868413925,
      "learning_rate": 0.00016349628055260362,
      "loss": 0.0386,
      "step": 697
    },
    {
      "epoch": 0.5546285260230434,
      "grad_norm": 0.10413055121898651,
      "learning_rate": 0.0001634431455897981,
      "loss": 0.044,
      "step": 698
    },
    {
      "epoch": 0.5554231227651967,
      "grad_norm": 0.09753414988517761,
      "learning_rate": 0.00016339001062699258,
      "loss": 0.0373,
      "step": 699
    },
    {
      "epoch": 0.55621771950735,
      "grad_norm": 0.08821544796228409,
      "learning_rate": 0.00016333687566418706,
      "loss": 0.0395,
      "step": 700
    },
    {
      "epoch": 0.5570123162495034,
      "grad_norm": 1.132065773010254,
      "learning_rate": 0.00016328374070138154,
      "loss": 0.7612,
      "step": 701
    },
    {
      "epoch": 0.5578069129916567,
      "grad_norm": 0.6405693292617798,
      "learning_rate": 0.000163230605738576,
      "loss": 0.5343,
      "step": 702
    },
    {
      "epoch": 0.5586015097338101,
      "grad_norm": 0.3990688621997833,
      "learning_rate": 0.00016317747077577044,
      "loss": 0.3777,
      "step": 703
    },
    {
      "epoch": 0.5593961064759635,
      "grad_norm": 0.3034796714782715,
      "learning_rate": 0.00016312433581296492,
      "loss": 0.3349,
      "step": 704
    },
    {
      "epoch": 0.5601907032181168,
      "grad_norm": 0.33816733956336975,
      "learning_rate": 0.0001630712008501594,
      "loss": 0.3643,
      "step": 705
    },
    {
      "epoch": 0.5609852999602701,
      "grad_norm": 0.24680352210998535,
      "learning_rate": 0.00016301806588735389,
      "loss": 0.3043,
      "step": 706
    },
    {
      "epoch": 0.5617798967024236,
      "grad_norm": 0.246956005692482,
      "learning_rate": 0.00016296493092454837,
      "loss": 0.3314,
      "step": 707
    },
    {
      "epoch": 0.5625744934445769,
      "grad_norm": 0.22934018075466156,
      "learning_rate": 0.00016291179596174285,
      "loss": 0.2829,
      "step": 708
    },
    {
      "epoch": 0.5633690901867302,
      "grad_norm": 0.22942501306533813,
      "learning_rate": 0.00016285866099893733,
      "loss": 0.2778,
      "step": 709
    },
    {
      "epoch": 0.5641636869288836,
      "grad_norm": 0.2229616492986679,
      "learning_rate": 0.00016280552603613178,
      "loss": 0.2583,
      "step": 710
    },
    {
      "epoch": 0.564958283671037,
      "grad_norm": 0.2174660712480545,
      "learning_rate": 0.00016275239107332626,
      "loss": 0.2101,
      "step": 711
    },
    {
      "epoch": 0.5657528804131903,
      "grad_norm": 0.24063381552696228,
      "learning_rate": 0.00016269925611052074,
      "loss": 0.2696,
      "step": 712
    },
    {
      "epoch": 0.5665474771553437,
      "grad_norm": 0.4026258885860443,
      "learning_rate": 0.0001626461211477152,
      "loss": 0.2265,
      "step": 713
    },
    {
      "epoch": 0.567342073897497,
      "grad_norm": 1.2688955068588257,
      "learning_rate": 0.00016259298618490967,
      "loss": 0.2426,
      "step": 714
    },
    {
      "epoch": 0.5681366706396503,
      "grad_norm": 0.19637733697891235,
      "learning_rate": 0.00016253985122210415,
      "loss": 0.1727,
      "step": 715
    },
    {
      "epoch": 0.5689312673818038,
      "grad_norm": 2.4659271240234375,
      "learning_rate": 0.00016248671625929863,
      "loss": 0.3198,
      "step": 716
    },
    {
      "epoch": 0.5697258641239571,
      "grad_norm": 0.2315731793642044,
      "learning_rate": 0.0001624335812964931,
      "loss": 0.1954,
      "step": 717
    },
    {
      "epoch": 0.5705204608661104,
      "grad_norm": 0.2035675048828125,
      "learning_rate": 0.00016238044633368757,
      "loss": 0.1589,
      "step": 718
    },
    {
      "epoch": 0.5713150576082638,
      "grad_norm": 0.1927490085363388,
      "learning_rate": 0.00016232731137088205,
      "loss": 0.1496,
      "step": 719
    },
    {
      "epoch": 0.5721096543504172,
      "grad_norm": 0.18121927976608276,
      "learning_rate": 0.00016227417640807653,
      "loss": 0.1826,
      "step": 720
    },
    {
      "epoch": 0.5729042510925705,
      "grad_norm": 0.2265416979789734,
      "learning_rate": 0.000162221041445271,
      "loss": 0.1663,
      "step": 721
    },
    {
      "epoch": 0.5736988478347239,
      "grad_norm": 0.17162835597991943,
      "learning_rate": 0.00016216790648246546,
      "loss": 0.1604,
      "step": 722
    },
    {
      "epoch": 0.5744934445768772,
      "grad_norm": 0.20011308789253235,
      "learning_rate": 0.00016211477151965994,
      "loss": 0.1611,
      "step": 723
    },
    {
      "epoch": 0.5752880413190306,
      "grad_norm": 0.1709226816892624,
      "learning_rate": 0.00016206163655685442,
      "loss": 0.1107,
      "step": 724
    },
    {
      "epoch": 0.576082638061184,
      "grad_norm": 0.18157874047756195,
      "learning_rate": 0.0001620085015940489,
      "loss": 0.1592,
      "step": 725
    },
    {
      "epoch": 0.5768772348033373,
      "grad_norm": 0.20633453130722046,
      "learning_rate": 0.00016195536663124335,
      "loss": 0.1764,
      "step": 726
    },
    {
      "epoch": 0.5776718315454906,
      "grad_norm": 0.16789625585079193,
      "learning_rate": 0.00016190223166843783,
      "loss": 0.1423,
      "step": 727
    },
    {
      "epoch": 0.578466428287644,
      "grad_norm": 0.15606899559497833,
      "learning_rate": 0.00016184909670563231,
      "loss": 0.1297,
      "step": 728
    },
    {
      "epoch": 0.5792610250297974,
      "grad_norm": 0.19828598201274872,
      "learning_rate": 0.0001617959617428268,
      "loss": 0.1231,
      "step": 729
    },
    {
      "epoch": 0.5800556217719507,
      "grad_norm": 0.15548956394195557,
      "learning_rate": 0.00016174282678002127,
      "loss": 0.1267,
      "step": 730
    },
    {
      "epoch": 0.5808502185141041,
      "grad_norm": 0.20172038674354553,
      "learning_rate": 0.00016168969181721573,
      "loss": 0.1211,
      "step": 731
    },
    {
      "epoch": 0.5816448152562574,
      "grad_norm": 0.16032472252845764,
      "learning_rate": 0.0001616365568544102,
      "loss": 0.1211,
      "step": 732
    },
    {
      "epoch": 0.5824394119984108,
      "grad_norm": 0.18731114268302917,
      "learning_rate": 0.0001615834218916047,
      "loss": 0.1132,
      "step": 733
    },
    {
      "epoch": 0.5832340087405642,
      "grad_norm": 0.20096363127231598,
      "learning_rate": 0.00016153028692879917,
      "loss": 0.1185,
      "step": 734
    },
    {
      "epoch": 0.5840286054827175,
      "grad_norm": 0.13719375431537628,
      "learning_rate": 0.00016147715196599362,
      "loss": 0.1198,
      "step": 735
    },
    {
      "epoch": 0.5848232022248708,
      "grad_norm": 0.1686636060476303,
      "learning_rate": 0.0001614240170031881,
      "loss": 0.1139,
      "step": 736
    },
    {
      "epoch": 0.5856177989670243,
      "grad_norm": 0.14287561178207397,
      "learning_rate": 0.00016137088204038258,
      "loss": 0.0855,
      "step": 737
    },
    {
      "epoch": 0.5864123957091776,
      "grad_norm": 0.14618562161922455,
      "learning_rate": 0.00016131774707757706,
      "loss": 0.0845,
      "step": 738
    },
    {
      "epoch": 0.5872069924513309,
      "grad_norm": 0.1274416297674179,
      "learning_rate": 0.00016126461211477154,
      "loss": 0.0931,
      "step": 739
    },
    {
      "epoch": 0.5880015891934843,
      "grad_norm": 0.12459716945886612,
      "learning_rate": 0.000161211477151966,
      "loss": 0.0907,
      "step": 740
    },
    {
      "epoch": 0.5887961859356377,
      "grad_norm": 0.1482650488615036,
      "learning_rate": 0.00016115834218916048,
      "loss": 0.0864,
      "step": 741
    },
    {
      "epoch": 0.589590782677791,
      "grad_norm": 0.1332787722349167,
      "learning_rate": 0.00016110520722635496,
      "loss": 0.0722,
      "step": 742
    },
    {
      "epoch": 0.5903853794199444,
      "grad_norm": 0.16148334741592407,
      "learning_rate": 0.0001610520722635494,
      "loss": 0.0779,
      "step": 743
    },
    {
      "epoch": 0.5911799761620977,
      "grad_norm": 0.11318366229534149,
      "learning_rate": 0.0001609989373007439,
      "loss": 0.0672,
      "step": 744
    },
    {
      "epoch": 0.591974572904251,
      "grad_norm": 0.16228853166103363,
      "learning_rate": 0.00016094580233793837,
      "loss": 0.0499,
      "step": 745
    },
    {
      "epoch": 0.5927691696464045,
      "grad_norm": 0.1373501718044281,
      "learning_rate": 0.00016089266737513285,
      "loss": 0.0584,
      "step": 746
    },
    {
      "epoch": 0.5935637663885578,
      "grad_norm": 0.10946714133024216,
      "learning_rate": 0.00016083953241232733,
      "loss": 0.0437,
      "step": 747
    },
    {
      "epoch": 0.5943583631307111,
      "grad_norm": 0.10562935471534729,
      "learning_rate": 0.0001607863974495218,
      "loss": 0.0384,
      "step": 748
    },
    {
      "epoch": 0.5951529598728645,
      "grad_norm": 0.10019563883543015,
      "learning_rate": 0.00016073326248671626,
      "loss": 0.0395,
      "step": 749
    },
    {
      "epoch": 0.5959475566150179,
      "grad_norm": 0.1085941269993782,
      "learning_rate": 0.00016068012752391074,
      "loss": 0.0492,
      "step": 750
    },
    {
      "epoch": 0.5967421533571713,
      "grad_norm": 1.1121731996536255,
      "learning_rate": 0.0001606269925611052,
      "loss": 0.5234,
      "step": 751
    },
    {
      "epoch": 0.5975367500993246,
      "grad_norm": 0.5160964727401733,
      "learning_rate": 0.00016057385759829968,
      "loss": 0.5035,
      "step": 752
    },
    {
      "epoch": 0.5983313468414779,
      "grad_norm": 0.4228474795818329,
      "learning_rate": 0.00016052072263549416,
      "loss": 0.4175,
      "step": 753
    },
    {
      "epoch": 0.5991259435836314,
      "grad_norm": 0.41054975986480713,
      "learning_rate": 0.00016046758767268864,
      "loss": 0.3674,
      "step": 754
    },
    {
      "epoch": 0.5999205403257847,
      "grad_norm": 0.3304517865180969,
      "learning_rate": 0.00016041445270988312,
      "loss": 0.3742,
      "step": 755
    },
    {
      "epoch": 0.5999205403257847,
      "eval_loss": 0.15635450184345245,
      "eval_runtime": 300.8581,
      "eval_samples_per_second": 3.171,
      "eval_steps_per_second": 1.057,
      "step": 755
    },
    {
      "epoch": 0.600715137067938,
      "grad_norm": 0.2887241542339325,
      "learning_rate": 0.0001603613177470776,
      "loss": 0.3677,
      "step": 756
    },
    {
      "epoch": 0.6015097338100914,
      "grad_norm": 0.33603569865226746,
      "learning_rate": 0.00016030818278427208,
      "loss": 0.3095,
      "step": 757
    },
    {
      "epoch": 0.6023043305522447,
      "grad_norm": 0.33944064378738403,
      "learning_rate": 0.00016025504782146653,
      "loss": 0.275,
      "step": 758
    },
    {
      "epoch": 0.6030989272943981,
      "grad_norm": 0.31103232502937317,
      "learning_rate": 0.00016020191285866098,
      "loss": 0.2625,
      "step": 759
    },
    {
      "epoch": 0.6038935240365515,
      "grad_norm": 0.2857193946838379,
      "learning_rate": 0.00016014877789585546,
      "loss": 0.2309,
      "step": 760
    },
    {
      "epoch": 0.6046881207787048,
      "grad_norm": 0.252677857875824,
      "learning_rate": 0.00016009564293304994,
      "loss": 0.231,
      "step": 761
    },
    {
      "epoch": 0.6054827175208581,
      "grad_norm": 0.2156064808368683,
      "learning_rate": 0.00016004250797024442,
      "loss": 0.2222,
      "step": 762
    },
    {
      "epoch": 0.6062773142630116,
      "grad_norm": 0.22114630043506622,
      "learning_rate": 0.0001599893730074389,
      "loss": 0.2331,
      "step": 763
    },
    {
      "epoch": 0.6070719110051649,
      "grad_norm": 0.171585351228714,
      "learning_rate": 0.00015993623804463338,
      "loss": 0.1904,
      "step": 764
    },
    {
      "epoch": 0.6078665077473182,
      "grad_norm": 0.21495848894119263,
      "learning_rate": 0.00015988310308182786,
      "loss": 0.212,
      "step": 765
    },
    {
      "epoch": 0.6086611044894716,
      "grad_norm": 0.2126864194869995,
      "learning_rate": 0.00015982996811902234,
      "loss": 0.2236,
      "step": 766
    },
    {
      "epoch": 0.609455701231625,
      "grad_norm": 0.18560358881950378,
      "learning_rate": 0.0001597768331562168,
      "loss": 0.2098,
      "step": 767
    },
    {
      "epoch": 0.6102502979737783,
      "grad_norm": 0.16898883879184723,
      "learning_rate": 0.00015972369819341128,
      "loss": 0.1695,
      "step": 768
    },
    {
      "epoch": 0.6110448947159317,
      "grad_norm": 0.23021505773067474,
      "learning_rate": 0.00015967056323060573,
      "loss": 0.166,
      "step": 769
    },
    {
      "epoch": 0.611839491458085,
      "grad_norm": 0.20647498965263367,
      "learning_rate": 0.0001596174282678002,
      "loss": 0.1702,
      "step": 770
    },
    {
      "epoch": 0.6126340882002383,
      "grad_norm": 0.1607869565486908,
      "learning_rate": 0.0001595642933049947,
      "loss": 0.1568,
      "step": 771
    },
    {
      "epoch": 0.6134286849423918,
      "grad_norm": 0.1519125998020172,
      "learning_rate": 0.00015951115834218917,
      "loss": 0.1451,
      "step": 772
    },
    {
      "epoch": 0.6142232816845451,
      "grad_norm": 0.14512525498867035,
      "learning_rate": 0.00015945802337938365,
      "loss": 0.1428,
      "step": 773
    },
    {
      "epoch": 0.6150178784266984,
      "grad_norm": 0.16227658092975616,
      "learning_rate": 0.00015940488841657813,
      "loss": 0.1573,
      "step": 774
    },
    {
      "epoch": 0.6158124751688518,
      "grad_norm": 0.17023614048957825,
      "learning_rate": 0.0001593517534537726,
      "loss": 0.1523,
      "step": 775
    },
    {
      "epoch": 0.6166070719110052,
      "grad_norm": 0.1583867371082306,
      "learning_rate": 0.00015929861849096706,
      "loss": 0.1242,
      "step": 776
    },
    {
      "epoch": 0.6174016686531585,
      "grad_norm": 0.1608862429857254,
      "learning_rate": 0.00015924548352816155,
      "loss": 0.1215,
      "step": 777
    },
    {
      "epoch": 0.6181962653953119,
      "grad_norm": 0.1336755007505417,
      "learning_rate": 0.000159192348565356,
      "loss": 0.1222,
      "step": 778
    },
    {
      "epoch": 0.6189908621374652,
      "grad_norm": 0.19663363695144653,
      "learning_rate": 0.00015913921360255048,
      "loss": 0.1375,
      "step": 779
    },
    {
      "epoch": 0.6197854588796186,
      "grad_norm": 0.15700852870941162,
      "learning_rate": 0.00015908607863974496,
      "loss": 0.1188,
      "step": 780
    },
    {
      "epoch": 0.620580055621772,
      "grad_norm": 0.14438439905643463,
      "learning_rate": 0.00015903294367693944,
      "loss": 0.1041,
      "step": 781
    },
    {
      "epoch": 0.6213746523639253,
      "grad_norm": 0.1315022110939026,
      "learning_rate": 0.00015897980871413392,
      "loss": 0.1029,
      "step": 782
    },
    {
      "epoch": 0.6221692491060786,
      "grad_norm": 0.14895732700824738,
      "learning_rate": 0.0001589266737513284,
      "loss": 0.1002,
      "step": 783
    },
    {
      "epoch": 0.6229638458482321,
      "grad_norm": 0.13309700787067413,
      "learning_rate": 0.00015887353878852285,
      "loss": 0.1048,
      "step": 784
    },
    {
      "epoch": 0.6237584425903854,
      "grad_norm": 0.1379583775997162,
      "learning_rate": 0.00015882040382571733,
      "loss": 0.1027,
      "step": 785
    },
    {
      "epoch": 0.6245530393325387,
      "grad_norm": 0.12613476812839508,
      "learning_rate": 0.0001587672688629118,
      "loss": 0.11,
      "step": 786
    },
    {
      "epoch": 0.6253476360746921,
      "grad_norm": 0.17210915684700012,
      "learning_rate": 0.00015871413390010627,
      "loss": 0.116,
      "step": 787
    },
    {
      "epoch": 0.6261422328168454,
      "grad_norm": 0.16118450462818146,
      "learning_rate": 0.00015866099893730075,
      "loss": 0.1233,
      "step": 788
    },
    {
      "epoch": 0.6269368295589988,
      "grad_norm": 0.1353677213191986,
      "learning_rate": 0.00015860786397449523,
      "loss": 0.0964,
      "step": 789
    },
    {
      "epoch": 0.6277314263011522,
      "grad_norm": 0.10656167566776276,
      "learning_rate": 0.0001585547290116897,
      "loss": 0.0832,
      "step": 790
    },
    {
      "epoch": 0.6285260230433055,
      "grad_norm": 0.1553148776292801,
      "learning_rate": 0.00015850159404888419,
      "loss": 0.0836,
      "step": 791
    },
    {
      "epoch": 0.6293206197854588,
      "grad_norm": 0.09940006583929062,
      "learning_rate": 0.00015844845908607864,
      "loss": 0.0694,
      "step": 792
    },
    {
      "epoch": 0.6301152165276123,
      "grad_norm": 0.11613394320011139,
      "learning_rate": 0.00015839532412327312,
      "loss": 0.0857,
      "step": 793
    },
    {
      "epoch": 0.6309098132697656,
      "grad_norm": 0.10293091088533401,
      "learning_rate": 0.0001583421891604676,
      "loss": 0.0745,
      "step": 794
    },
    {
      "epoch": 0.6317044100119189,
      "grad_norm": 0.08698353171348572,
      "learning_rate": 0.00015828905419766208,
      "loss": 0.0569,
      "step": 795
    },
    {
      "epoch": 0.6324990067540723,
      "grad_norm": 0.09978461265563965,
      "learning_rate": 0.00015823591923485653,
      "loss": 0.0568,
      "step": 796
    },
    {
      "epoch": 0.6332936034962257,
      "grad_norm": 0.088238924741745,
      "learning_rate": 0.000158182784272051,
      "loss": 0.0517,
      "step": 797
    },
    {
      "epoch": 0.634088200238379,
      "grad_norm": 0.10042648017406464,
      "learning_rate": 0.0001581296493092455,
      "loss": 0.0524,
      "step": 798
    },
    {
      "epoch": 0.6348827969805324,
      "grad_norm": 0.11305853724479675,
      "learning_rate": 0.00015807651434643997,
      "loss": 0.0381,
      "step": 799
    },
    {
      "epoch": 0.6356773937226857,
      "grad_norm": 0.09357500821352005,
      "learning_rate": 0.00015802337938363443,
      "loss": 0.0486,
      "step": 800
    },
    {
      "epoch": 0.636471990464839,
      "grad_norm": 0.541445255279541,
      "learning_rate": 0.0001579702444208289,
      "loss": 0.571,
      "step": 801
    },
    {
      "epoch": 0.6372665872069925,
      "grad_norm": 0.4609028995037079,
      "learning_rate": 0.0001579171094580234,
      "loss": 0.4654,
      "step": 802
    },
    {
      "epoch": 0.6380611839491458,
      "grad_norm": 0.48764342069625854,
      "learning_rate": 0.00015786397449521787,
      "loss": 0.4822,
      "step": 803
    },
    {
      "epoch": 0.6388557806912991,
      "grad_norm": 0.3877433240413666,
      "learning_rate": 0.00015781083953241235,
      "loss": 0.3934,
      "step": 804
    },
    {
      "epoch": 0.6396503774334525,
      "grad_norm": 2.7081058025360107,
      "learning_rate": 0.0001577577045696068,
      "loss": 0.4499,
      "step": 805
    },
    {
      "epoch": 0.6404449741756059,
      "grad_norm": 1.2951513528823853,
      "learning_rate": 0.00015770456960680128,
      "loss": 0.3071,
      "step": 806
    },
    {
      "epoch": 0.6412395709177592,
      "grad_norm": 0.32757440209388733,
      "learning_rate": 0.00015765143464399576,
      "loss": 0.3173,
      "step": 807
    },
    {
      "epoch": 0.6420341676599126,
      "grad_norm": 0.2470272034406662,
      "learning_rate": 0.00015759829968119021,
      "loss": 0.2644,
      "step": 808
    },
    {
      "epoch": 0.6428287644020659,
      "grad_norm": 0.20689138770103455,
      "learning_rate": 0.0001575451647183847,
      "loss": 0.2475,
      "step": 809
    },
    {
      "epoch": 0.6436233611442194,
      "grad_norm": 0.2183491438627243,
      "learning_rate": 0.00015749202975557917,
      "loss": 0.2497,
      "step": 810
    },
    {
      "epoch": 0.6444179578863727,
      "grad_norm": 0.203469917178154,
      "learning_rate": 0.00015743889479277365,
      "loss": 0.217,
      "step": 811
    },
    {
      "epoch": 0.645212554628526,
      "grad_norm": 0.19532045722007751,
      "learning_rate": 0.00015738575982996813,
      "loss": 0.209,
      "step": 812
    },
    {
      "epoch": 0.6460071513706794,
      "grad_norm": 0.22475062310695648,
      "learning_rate": 0.00015733262486716262,
      "loss": 0.1791,
      "step": 813
    },
    {
      "epoch": 0.6468017481128328,
      "grad_norm": 0.20370744168758392,
      "learning_rate": 0.00015727948990435707,
      "loss": 0.2238,
      "step": 814
    },
    {
      "epoch": 0.6475963448549861,
      "grad_norm": 0.1931508630514145,
      "learning_rate": 0.00015722635494155155,
      "loss": 0.2168,
      "step": 815
    },
    {
      "epoch": 0.6483909415971395,
      "grad_norm": 0.17058776319026947,
      "learning_rate": 0.000157173219978746,
      "loss": 0.183,
      "step": 816
    },
    {
      "epoch": 0.6491855383392928,
      "grad_norm": 0.19255024194717407,
      "learning_rate": 0.00015712008501594048,
      "loss": 0.2077,
      "step": 817
    },
    {
      "epoch": 0.6499801350814461,
      "grad_norm": 0.15157672762870789,
      "learning_rate": 0.00015706695005313496,
      "loss": 0.1548,
      "step": 818
    },
    {
      "epoch": 0.6507747318235996,
      "grad_norm": 0.19671405851840973,
      "learning_rate": 0.00015701381509032944,
      "loss": 0.1796,
      "step": 819
    },
    {
      "epoch": 0.6515693285657529,
      "grad_norm": 0.14942291378974915,
      "learning_rate": 0.00015696068012752392,
      "loss": 0.1498,
      "step": 820
    },
    {
      "epoch": 0.6523639253079062,
      "grad_norm": 0.14758694171905518,
      "learning_rate": 0.0001569075451647184,
      "loss": 0.153,
      "step": 821
    },
    {
      "epoch": 0.6531585220500596,
      "grad_norm": 0.16902419924736023,
      "learning_rate": 0.00015685441020191288,
      "loss": 0.1462,
      "step": 822
    },
    {
      "epoch": 0.653953118792213,
      "grad_norm": 0.14098156988620758,
      "learning_rate": 0.00015680127523910734,
      "loss": 0.1592,
      "step": 823
    },
    {
      "epoch": 0.6547477155343663,
      "grad_norm": 0.16769036650657654,
      "learning_rate": 0.00015674814027630182,
      "loss": 0.1471,
      "step": 824
    },
    {
      "epoch": 0.6555423122765197,
      "grad_norm": 0.15487892925739288,
      "learning_rate": 0.00015669500531349627,
      "loss": 0.1533,
      "step": 825
    },
    {
      "epoch": 0.656336909018673,
      "grad_norm": 0.12373887747526169,
      "learning_rate": 0.00015664187035069075,
      "loss": 0.1354,
      "step": 826
    },
    {
      "epoch": 0.6571315057608264,
      "grad_norm": 0.12332677096128464,
      "learning_rate": 0.00015658873538788523,
      "loss": 0.1183,
      "step": 827
    },
    {
      "epoch": 0.6579261025029798,
      "grad_norm": 0.14650800824165344,
      "learning_rate": 0.0001565356004250797,
      "loss": 0.1317,
      "step": 828
    },
    {
      "epoch": 0.6587206992451331,
      "grad_norm": 0.1608908474445343,
      "learning_rate": 0.0001564824654622742,
      "loss": 0.1177,
      "step": 829
    },
    {
      "epoch": 0.6595152959872864,
      "grad_norm": 0.14412282407283783,
      "learning_rate": 0.00015642933049946867,
      "loss": 0.1043,
      "step": 830
    },
    {
      "epoch": 0.6603098927294399,
      "grad_norm": 0.10838835686445236,
      "learning_rate": 0.00015637619553666315,
      "loss": 0.0997,
      "step": 831
    },
    {
      "epoch": 0.6611044894715932,
      "grad_norm": 0.14359134435653687,
      "learning_rate": 0.0001563230605738576,
      "loss": 0.1201,
      "step": 832
    },
    {
      "epoch": 0.6618990862137465,
      "grad_norm": 0.09975313395261765,
      "learning_rate": 0.00015626992561105208,
      "loss": 0.0842,
      "step": 833
    },
    {
      "epoch": 0.6626936829558999,
      "grad_norm": 0.10912968218326569,
      "learning_rate": 0.00015621679064824654,
      "loss": 0.0827,
      "step": 834
    },
    {
      "epoch": 0.6634882796980532,
      "grad_norm": 0.1177326887845993,
      "learning_rate": 0.00015616365568544102,
      "loss": 0.1012,
      "step": 835
    },
    {
      "epoch": 0.6642828764402066,
      "grad_norm": 0.10228876769542694,
      "learning_rate": 0.0001561105207226355,
      "loss": 0.0938,
      "step": 836
    },
    {
      "epoch": 0.66507747318236,
      "grad_norm": 0.10407134145498276,
      "learning_rate": 0.00015605738575982998,
      "loss": 0.081,
      "step": 837
    },
    {
      "epoch": 0.6658720699245133,
      "grad_norm": 0.09947404265403748,
      "learning_rate": 0.00015600425079702446,
      "loss": 0.0771,
      "step": 838
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.10491498559713364,
      "learning_rate": 0.00015595111583421894,
      "loss": 0.0741,
      "step": 839
    },
    {
      "epoch": 0.6674612634088201,
      "grad_norm": 0.09212974458932877,
      "learning_rate": 0.00015589798087141342,
      "loss": 0.0735,
      "step": 840
    },
    {
      "epoch": 0.6682558601509734,
      "grad_norm": 0.1042516678571701,
      "learning_rate": 0.00015584484590860787,
      "loss": 0.0751,
      "step": 841
    },
    {
      "epoch": 0.6690504568931267,
      "grad_norm": 0.1004156842827797,
      "learning_rate": 0.00015579171094580235,
      "loss": 0.0701,
      "step": 842
    },
    {
      "epoch": 0.6698450536352801,
      "grad_norm": 0.10128264129161835,
      "learning_rate": 0.0001557385759829968,
      "loss": 0.0561,
      "step": 843
    },
    {
      "epoch": 0.6706396503774334,
      "grad_norm": 0.07632110267877579,
      "learning_rate": 0.00015568544102019128,
      "loss": 0.0581,
      "step": 844
    },
    {
      "epoch": 0.6714342471195868,
      "grad_norm": 0.07227075099945068,
      "learning_rate": 0.00015563230605738576,
      "loss": 0.0522,
      "step": 845
    },
    {
      "epoch": 0.6722288438617402,
      "grad_norm": 0.0899520218372345,
      "learning_rate": 0.00015557917109458024,
      "loss": 0.0529,
      "step": 846
    },
    {
      "epoch": 0.6730234406038935,
      "grad_norm": 0.09612081199884415,
      "learning_rate": 0.00015552603613177472,
      "loss": 0.0465,
      "step": 847
    },
    {
      "epoch": 0.6738180373460468,
      "grad_norm": 0.09274063259363174,
      "learning_rate": 0.0001554729011689692,
      "loss": 0.0469,
      "step": 848
    },
    {
      "epoch": 0.6746126340882003,
      "grad_norm": 0.08494765311479568,
      "learning_rate": 0.00015541976620616366,
      "loss": 0.0384,
      "step": 849
    },
    {
      "epoch": 0.6754072308303536,
      "grad_norm": 0.07244168221950531,
      "learning_rate": 0.00015536663124335814,
      "loss": 0.036,
      "step": 850
    },
    {
      "epoch": 0.6762018275725069,
      "grad_norm": 0.6010459065437317,
      "learning_rate": 0.00015531349628055262,
      "loss": 0.6212,
      "step": 851
    },
    {
      "epoch": 0.6769964243146603,
      "grad_norm": 0.5257092118263245,
      "learning_rate": 0.00015526036131774707,
      "loss": 0.5556,
      "step": 852
    },
    {
      "epoch": 0.6777910210568137,
      "grad_norm": 0.3934745788574219,
      "learning_rate": 0.00015520722635494155,
      "loss": 0.4329,
      "step": 853
    },
    {
      "epoch": 0.678585617798967,
      "grad_norm": 0.2981342077255249,
      "learning_rate": 0.00015515409139213603,
      "loss": 0.3525,
      "step": 854
    },
    {
      "epoch": 0.6793802145411204,
      "grad_norm": 0.281978577375412,
      "learning_rate": 0.0001551009564293305,
      "loss": 0.3726,
      "step": 855
    },
    {
      "epoch": 0.6801748112832737,
      "grad_norm": 0.24806897342205048,
      "learning_rate": 0.000155047821466525,
      "loss": 0.3183,
      "step": 856
    },
    {
      "epoch": 0.680969408025427,
      "grad_norm": 0.21533361077308655,
      "learning_rate": 0.00015499468650371945,
      "loss": 0.3042,
      "step": 857
    },
    {
      "epoch": 0.6817640047675805,
      "grad_norm": 0.20488591492176056,
      "learning_rate": 0.00015494155154091393,
      "loss": 0.3059,
      "step": 858
    },
    {
      "epoch": 0.6825586015097338,
      "grad_norm": 0.18938808143138885,
      "learning_rate": 0.0001548884165781084,
      "loss": 0.2499,
      "step": 859
    },
    {
      "epoch": 0.6833531982518871,
      "grad_norm": 0.1756967455148697,
      "learning_rate": 0.00015483528161530289,
      "loss": 0.241,
      "step": 860
    },
    {
      "epoch": 0.6841477949940405,
      "grad_norm": 0.17692844569683075,
      "learning_rate": 0.00015478214665249734,
      "loss": 0.2335,
      "step": 861
    },
    {
      "epoch": 0.6849423917361939,
      "grad_norm": 0.1890135109424591,
      "learning_rate": 0.00015472901168969182,
      "loss": 0.2274,
      "step": 862
    },
    {
      "epoch": 0.6857369884783472,
      "grad_norm": 0.20004349946975708,
      "learning_rate": 0.0001546758767268863,
      "loss": 0.222,
      "step": 863
    },
    {
      "epoch": 0.6865315852205006,
      "grad_norm": 0.21096085011959076,
      "learning_rate": 0.00015462274176408078,
      "loss": 0.2513,
      "step": 864
    },
    {
      "epoch": 0.6873261819626539,
      "grad_norm": 0.2102903574705124,
      "learning_rate": 0.00015456960680127526,
      "loss": 0.2501,
      "step": 865
    },
    {
      "epoch": 0.6881207787048073,
      "grad_norm": 0.18359464406967163,
      "learning_rate": 0.0001545164718384697,
      "loss": 0.2014,
      "step": 866
    },
    {
      "epoch": 0.6889153754469607,
      "grad_norm": 0.17571285367012024,
      "learning_rate": 0.0001544633368756642,
      "loss": 0.194,
      "step": 867
    },
    {
      "epoch": 0.689709972189114,
      "grad_norm": 0.18363459408283234,
      "learning_rate": 0.00015441020191285867,
      "loss": 0.2043,
      "step": 868
    },
    {
      "epoch": 0.6905045689312674,
      "grad_norm": 0.14756089448928833,
      "learning_rate": 0.00015435706695005315,
      "loss": 0.1839,
      "step": 869
    },
    {
      "epoch": 0.6912991656734208,
      "grad_norm": 0.1605052351951599,
      "learning_rate": 0.0001543039319872476,
      "loss": 0.1465,
      "step": 870
    },
    {
      "epoch": 0.6920937624155741,
      "grad_norm": 0.17290230095386505,
      "learning_rate": 0.0001542507970244421,
      "loss": 0.1636,
      "step": 871
    },
    {
      "epoch": 0.6928883591577275,
      "grad_norm": 0.14696426689624786,
      "learning_rate": 0.00015419766206163657,
      "loss": 0.1592,
      "step": 872
    },
    {
      "epoch": 0.6936829558998808,
      "grad_norm": 0.15093308687210083,
      "learning_rate": 0.00015414452709883105,
      "loss": 0.1526,
      "step": 873
    },
    {
      "epoch": 0.6944775526420341,
      "grad_norm": 0.13490872085094452,
      "learning_rate": 0.0001540913921360255,
      "loss": 0.1288,
      "step": 874
    },
    {
      "epoch": 0.6952721493841876,
      "grad_norm": 0.17242932319641113,
      "learning_rate": 0.00015403825717321998,
      "loss": 0.1434,
      "step": 875
    },
    {
      "epoch": 0.6960667461263409,
      "grad_norm": 0.13226795196533203,
      "learning_rate": 0.00015398512221041446,
      "loss": 0.1451,
      "step": 876
    },
    {
      "epoch": 0.6968613428684942,
      "grad_norm": 0.1453486829996109,
      "learning_rate": 0.00015393198724760894,
      "loss": 0.1367,
      "step": 877
    },
    {
      "epoch": 0.6976559396106476,
      "grad_norm": 0.14889761805534363,
      "learning_rate": 0.00015387885228480342,
      "loss": 0.1316,
      "step": 878
    },
    {
      "epoch": 0.698450536352801,
      "grad_norm": 0.1764766126871109,
      "learning_rate": 0.00015382571732199787,
      "loss": 0.1497,
      "step": 879
    },
    {
      "epoch": 0.6992451330949543,
      "grad_norm": 0.14940160512924194,
      "learning_rate": 0.00015377258235919235,
      "loss": 0.1196,
      "step": 880
    },
    {
      "epoch": 0.7000397298371077,
      "grad_norm": 0.12179233133792877,
      "learning_rate": 0.00015371944739638683,
      "loss": 0.0999,
      "step": 881
    },
    {
      "epoch": 0.700834326579261,
      "grad_norm": 0.11307493597269058,
      "learning_rate": 0.0001536663124335813,
      "loss": 0.1109,
      "step": 882
    },
    {
      "epoch": 0.7016289233214144,
      "grad_norm": 0.129949152469635,
      "learning_rate": 0.00015361317747077577,
      "loss": 0.1031,
      "step": 883
    },
    {
      "epoch": 0.7024235200635678,
      "grad_norm": 0.12910374999046326,
      "learning_rate": 0.00015356004250797025,
      "loss": 0.1106,
      "step": 884
    },
    {
      "epoch": 0.7032181168057211,
      "grad_norm": 0.1278323084115982,
      "learning_rate": 0.00015350690754516473,
      "loss": 0.0879,
      "step": 885
    },
    {
      "epoch": 0.7040127135478744,
      "grad_norm": 0.11691456288099289,
      "learning_rate": 0.0001534537725823592,
      "loss": 0.105,
      "step": 886
    },
    {
      "epoch": 0.7048073102900279,
      "grad_norm": 0.11593248695135117,
      "learning_rate": 0.0001534006376195537,
      "loss": 0.0912,
      "step": 887
    },
    {
      "epoch": 0.7056019070321812,
      "grad_norm": 0.11432858556509018,
      "learning_rate": 0.00015334750265674814,
      "loss": 0.0993,
      "step": 888
    },
    {
      "epoch": 0.7063965037743345,
      "grad_norm": 0.1033988669514656,
      "learning_rate": 0.00015329436769394262,
      "loss": 0.0773,
      "step": 889
    },
    {
      "epoch": 0.7071911005164879,
      "grad_norm": 0.1578824669122696,
      "learning_rate": 0.00015324123273113707,
      "loss": 0.0793,
      "step": 890
    },
    {
      "epoch": 0.7079856972586412,
      "grad_norm": 0.10812303423881531,
      "learning_rate": 0.00015318809776833155,
      "loss": 0.0893,
      "step": 891
    },
    {
      "epoch": 0.7087802940007946,
      "grad_norm": 0.10565468668937683,
      "learning_rate": 0.00015313496280552604,
      "loss": 0.0821,
      "step": 892
    },
    {
      "epoch": 0.709574890742948,
      "grad_norm": 0.09359944611787796,
      "learning_rate": 0.00015308182784272052,
      "loss": 0.0641,
      "step": 893
    },
    {
      "epoch": 0.7103694874851013,
      "grad_norm": 0.08907123655080795,
      "learning_rate": 0.000153028692879915,
      "loss": 0.0406,
      "step": 894
    },
    {
      "epoch": 0.7111640842272546,
      "grad_norm": 0.0890696793794632,
      "learning_rate": 0.00015297555791710948,
      "loss": 0.0527,
      "step": 895
    },
    {
      "epoch": 0.7119586809694081,
      "grad_norm": 0.09487970918416977,
      "learning_rate": 0.00015292242295430396,
      "loss": 0.0638,
      "step": 896
    },
    {
      "epoch": 0.7127532777115614,
      "grad_norm": 0.07490167766809464,
      "learning_rate": 0.0001528692879914984,
      "loss": 0.0524,
      "step": 897
    },
    {
      "epoch": 0.7135478744537147,
      "grad_norm": 0.09016551822423935,
      "learning_rate": 0.0001528161530286929,
      "loss": 0.0422,
      "step": 898
    },
    {
      "epoch": 0.7143424711958681,
      "grad_norm": 0.10610288381576538,
      "learning_rate": 0.00015276301806588734,
      "loss": 0.0388,
      "step": 899
    },
    {
      "epoch": 0.7151370679380215,
      "grad_norm": 0.10187870264053345,
      "learning_rate": 0.00015270988310308182,
      "loss": 0.0395,
      "step": 900
    },
    {
      "epoch": 0.7159316646801748,
      "grad_norm": 0.5524362325668335,
      "learning_rate": 0.0001526567481402763,
      "loss": 0.6549,
      "step": 901
    },
    {
      "epoch": 0.7167262614223282,
      "grad_norm": 0.44480687379837036,
      "learning_rate": 0.00015260361317747078,
      "loss": 0.5276,
      "step": 902
    },
    {
      "epoch": 0.7175208581644815,
      "grad_norm": 0.3485763967037201,
      "learning_rate": 0.00015255047821466526,
      "loss": 0.3978,
      "step": 903
    },
    {
      "epoch": 0.7183154549066348,
      "grad_norm": 0.31222444772720337,
      "learning_rate": 0.00015249734325185974,
      "loss": 0.3519,
      "step": 904
    },
    {
      "epoch": 0.7191100516487883,
      "grad_norm": 0.3030798137187958,
      "learning_rate": 0.00015244420828905422,
      "loss": 0.3438,
      "step": 905
    },
    {
      "epoch": 0.7199046483909416,
      "grad_norm": 0.28741419315338135,
      "learning_rate": 0.00015239107332624868,
      "loss": 0.2776,
      "step": 906
    },
    {
      "epoch": 0.7206992451330949,
      "grad_norm": 0.25087860226631165,
      "learning_rate": 0.00015233793836344316,
      "loss": 0.3094,
      "step": 907
    },
    {
      "epoch": 0.7214938418752483,
      "grad_norm": 0.23296049237251282,
      "learning_rate": 0.0001522848034006376,
      "loss": 0.3148,
      "step": 908
    },
    {
      "epoch": 0.7222884386174017,
      "grad_norm": 0.22973895072937012,
      "learning_rate": 0.0001522316684378321,
      "loss": 0.278,
      "step": 909
    },
    {
      "epoch": 0.723083035359555,
      "grad_norm": 0.25666916370391846,
      "learning_rate": 0.00015217853347502657,
      "loss": 0.2893,
      "step": 910
    },
    {
      "epoch": 0.7238776321017084,
      "grad_norm": 0.18476302921772003,
      "learning_rate": 0.00015212539851222105,
      "loss": 0.2144,
      "step": 911
    },
    {
      "epoch": 0.7246722288438617,
      "grad_norm": 0.17033450305461884,
      "learning_rate": 0.00015207226354941553,
      "loss": 0.2178,
      "step": 912
    },
    {
      "epoch": 0.725466825586015,
      "grad_norm": 0.22406944632530212,
      "learning_rate": 0.00015201912858661,
      "loss": 0.2232,
      "step": 913
    },
    {
      "epoch": 0.7262614223281685,
      "grad_norm": 0.1855459064245224,
      "learning_rate": 0.0001519659936238045,
      "loss": 0.2238,
      "step": 914
    },
    {
      "epoch": 0.7270560190703218,
      "grad_norm": 0.17534932494163513,
      "learning_rate": 0.00015191285866099894,
      "loss": 0.1829,
      "step": 915
    },
    {
      "epoch": 0.7278506158124751,
      "grad_norm": 0.17830877006053925,
      "learning_rate": 0.00015185972369819342,
      "loss": 0.1911,
      "step": 916
    },
    {
      "epoch": 0.7286452125546286,
      "grad_norm": 0.18996073305606842,
      "learning_rate": 0.00015180658873538788,
      "loss": 0.2083,
      "step": 917
    },
    {
      "epoch": 0.7294398092967819,
      "grad_norm": 0.16163833439350128,
      "learning_rate": 0.00015175345377258236,
      "loss": 0.1688,
      "step": 918
    },
    {
      "epoch": 0.7302344060389352,
      "grad_norm": 0.18759693205356598,
      "learning_rate": 0.00015170031880977684,
      "loss": 0.1892,
      "step": 919
    },
    {
      "epoch": 0.7310290027810886,
      "grad_norm": 0.1666620522737503,
      "learning_rate": 0.00015164718384697132,
      "loss": 0.2028,
      "step": 920
    },
    {
      "epoch": 0.7318235995232419,
      "grad_norm": 0.15551327168941498,
      "learning_rate": 0.0001515940488841658,
      "loss": 0.149,
      "step": 921
    },
    {
      "epoch": 0.7326181962653953,
      "grad_norm": 0.13187117874622345,
      "learning_rate": 0.00015154091392136028,
      "loss": 0.1631,
      "step": 922
    },
    {
      "epoch": 0.7334127930075487,
      "grad_norm": 0.15222927927970886,
      "learning_rate": 0.00015148777895855473,
      "loss": 0.1646,
      "step": 923
    },
    {
      "epoch": 0.734207389749702,
      "grad_norm": 0.15013282001018524,
      "learning_rate": 0.0001514346439957492,
      "loss": 0.1451,
      "step": 924
    },
    {
      "epoch": 0.7350019864918553,
      "grad_norm": 0.15980419516563416,
      "learning_rate": 0.0001513815090329437,
      "loss": 0.145,
      "step": 925
    },
    {
      "epoch": 0.7357965832340088,
      "grad_norm": 0.13976669311523438,
      "learning_rate": 0.00015132837407013814,
      "loss": 0.1323,
      "step": 926
    },
    {
      "epoch": 0.7365911799761621,
      "grad_norm": 0.18615901470184326,
      "learning_rate": 0.00015127523910733262,
      "loss": 0.1249,
      "step": 927
    },
    {
      "epoch": 0.7373857767183154,
      "grad_norm": 0.12407401204109192,
      "learning_rate": 0.0001512221041445271,
      "loss": 0.1229,
      "step": 928
    },
    {
      "epoch": 0.7381803734604688,
      "grad_norm": 0.12538042664527893,
      "learning_rate": 0.00015116896918172159,
      "loss": 0.1241,
      "step": 929
    },
    {
      "epoch": 0.7389749702026222,
      "grad_norm": 0.12717090547084808,
      "learning_rate": 0.00015111583421891607,
      "loss": 0.1328,
      "step": 930
    },
    {
      "epoch": 0.7397695669447756,
      "grad_norm": 0.12394813448190689,
      "learning_rate": 0.00015106269925611052,
      "loss": 0.1178,
      "step": 931
    },
    {
      "epoch": 0.7405641636869289,
      "grad_norm": 0.14310665428638458,
      "learning_rate": 0.000151009564293305,
      "loss": 0.1102,
      "step": 932
    },
    {
      "epoch": 0.7413587604290822,
      "grad_norm": 0.11430836468935013,
      "learning_rate": 0.00015095642933049948,
      "loss": 0.1067,
      "step": 933
    },
    {
      "epoch": 0.7421533571712357,
      "grad_norm": 0.10837774723768234,
      "learning_rate": 0.00015090329436769396,
      "loss": 0.114,
      "step": 934
    },
    {
      "epoch": 0.742947953913389,
      "grad_norm": 0.13309143483638763,
      "learning_rate": 0.0001508501594048884,
      "loss": 0.0963,
      "step": 935
    },
    {
      "epoch": 0.7437425506555423,
      "grad_norm": 0.1250973790884018,
      "learning_rate": 0.0001507970244420829,
      "loss": 0.0984,
      "step": 936
    },
    {
      "epoch": 0.7445371473976957,
      "grad_norm": 0.09105899930000305,
      "learning_rate": 0.00015074388947927737,
      "loss": 0.0866,
      "step": 937
    },
    {
      "epoch": 0.745331744139849,
      "grad_norm": 0.12140386551618576,
      "learning_rate": 0.00015069075451647185,
      "loss": 0.0848,
      "step": 938
    },
    {
      "epoch": 0.7461263408820024,
      "grad_norm": 0.11533164232969284,
      "learning_rate": 0.0001506376195536663,
      "loss": 0.086,
      "step": 939
    },
    {
      "epoch": 0.7469209376241558,
      "grad_norm": 0.12977193295955658,
      "learning_rate": 0.00015058448459086079,
      "loss": 0.0817,
      "step": 940
    },
    {
      "epoch": 0.7477155343663091,
      "grad_norm": 0.11200359463691711,
      "learning_rate": 0.00015053134962805527,
      "loss": 0.0737,
      "step": 941
    },
    {
      "epoch": 0.7485101311084624,
      "grad_norm": 0.11996280401945114,
      "learning_rate": 0.00015047821466524975,
      "loss": 0.0775,
      "step": 942
    },
    {
      "epoch": 0.7493047278506159,
      "grad_norm": 0.09982285648584366,
      "learning_rate": 0.00015042507970244423,
      "loss": 0.0632,
      "step": 943
    },
    {
      "epoch": 0.7500993245927692,
      "grad_norm": 0.1250828206539154,
      "learning_rate": 0.00015037194473963868,
      "loss": 0.0766,
      "step": 944
    },
    {
      "epoch": 0.7508939213349225,
      "grad_norm": 0.09339133650064468,
      "learning_rate": 0.00015031880977683316,
      "loss": 0.0527,
      "step": 945
    },
    {
      "epoch": 0.7516885180770759,
      "grad_norm": 0.1033630222082138,
      "learning_rate": 0.00015026567481402764,
      "loss": 0.0608,
      "step": 946
    },
    {
      "epoch": 0.7524831148192292,
      "grad_norm": 0.07617101073265076,
      "learning_rate": 0.0001502125398512221,
      "loss": 0.0425,
      "step": 947
    },
    {
      "epoch": 0.7532777115613826,
      "grad_norm": 0.08297976106405258,
      "learning_rate": 0.00015015940488841657,
      "loss": 0.0374,
      "step": 948
    },
    {
      "epoch": 0.754072308303536,
      "grad_norm": 0.08770835399627686,
      "learning_rate": 0.00015010626992561105,
      "loss": 0.0401,
      "step": 949
    },
    {
      "epoch": 0.7548669050456893,
      "grad_norm": 0.08744286000728607,
      "learning_rate": 0.00015005313496280553,
      "loss": 0.041,
      "step": 950
    },
    {
      "epoch": 0.7556615017878426,
      "grad_norm": 1.1166263818740845,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.8082,
      "step": 951
    },
    {
      "epoch": 0.7564560985299961,
      "grad_norm": 0.6174643039703369,
      "learning_rate": 0.0001499468650371945,
      "loss": 0.5286,
      "step": 952
    },
    {
      "epoch": 0.7572506952721494,
      "grad_norm": 0.4276581406593323,
      "learning_rate": 0.00014989373007438895,
      "loss": 0.4409,
      "step": 953
    },
    {
      "epoch": 0.7580452920143027,
      "grad_norm": 0.3548150062561035,
      "learning_rate": 0.00014984059511158343,
      "loss": 0.3916,
      "step": 954
    },
    {
      "epoch": 0.7588398887564561,
      "grad_norm": 0.262378454208374,
      "learning_rate": 0.0001497874601487779,
      "loss": 0.3464,
      "step": 955
    },
    {
      "epoch": 0.7596344854986095,
      "grad_norm": 0.22821268439292908,
      "learning_rate": 0.00014973432518597236,
      "loss": 0.3627,
      "step": 956
    },
    {
      "epoch": 0.7604290822407628,
      "grad_norm": 0.21509510278701782,
      "learning_rate": 0.00014968119022316684,
      "loss": 0.2743,
      "step": 957
    },
    {
      "epoch": 0.7612236789829162,
      "grad_norm": 0.22652535140514374,
      "learning_rate": 0.00014962805526036132,
      "loss": 0.2851,
      "step": 958
    },
    {
      "epoch": 0.7620182757250695,
      "grad_norm": 0.21353915333747864,
      "learning_rate": 0.0001495749202975558,
      "loss": 0.256,
      "step": 959
    },
    {
      "epoch": 0.7628128724672228,
      "grad_norm": 0.20576009154319763,
      "learning_rate": 0.00014952178533475028,
      "loss": 0.266,
      "step": 960
    },
    {
      "epoch": 0.7636074692093763,
      "grad_norm": 0.22500286996364594,
      "learning_rate": 0.00014946865037194476,
      "loss": 0.2472,
      "step": 961
    },
    {
      "epoch": 0.7644020659515296,
      "grad_norm": 0.1894240379333496,
      "learning_rate": 0.00014941551540913924,
      "loss": 0.2137,
      "step": 962
    },
    {
      "epoch": 0.7651966626936829,
      "grad_norm": 0.21805627644062042,
      "learning_rate": 0.0001493623804463337,
      "loss": 0.2256,
      "step": 963
    },
    {
      "epoch": 0.7659912594358363,
      "grad_norm": 0.1821945160627365,
      "learning_rate": 0.00014930924548352815,
      "loss": 0.1857,
      "step": 964
    },
    {
      "epoch": 0.7667858561779897,
      "grad_norm": 0.1960521787405014,
      "learning_rate": 0.00014925611052072263,
      "loss": 0.2085,
      "step": 965
    },
    {
      "epoch": 0.767580452920143,
      "grad_norm": 0.18112990260124207,
      "learning_rate": 0.0001492029755579171,
      "loss": 0.1661,
      "step": 966
    },
    {
      "epoch": 0.7683750496622964,
      "grad_norm": 0.18633733689785004,
      "learning_rate": 0.0001491498405951116,
      "loss": 0.201,
      "step": 967
    },
    {
      "epoch": 0.7691696464044497,
      "grad_norm": 0.1491515338420868,
      "learning_rate": 0.00014909670563230607,
      "loss": 0.151,
      "step": 968
    },
    {
      "epoch": 0.7699642431466031,
      "grad_norm": 0.17171186208724976,
      "learning_rate": 0.00014904357066950055,
      "loss": 0.1425,
      "step": 969
    },
    {
      "epoch": 0.7707588398887565,
      "grad_norm": 0.20572105050086975,
      "learning_rate": 0.00014899043570669503,
      "loss": 0.1675,
      "step": 970
    },
    {
      "epoch": 0.7715534366309098,
      "grad_norm": 0.16931883990764618,
      "learning_rate": 0.0001489373007438895,
      "loss": 0.1773,
      "step": 971
    },
    {
      "epoch": 0.7723480333730631,
      "grad_norm": 0.15122970938682556,
      "learning_rate": 0.00014888416578108396,
      "loss": 0.1387,
      "step": 972
    },
    {
      "epoch": 0.7731426301152166,
      "grad_norm": 0.14790543913841248,
      "learning_rate": 0.00014883103081827842,
      "loss": 0.1327,
      "step": 973
    },
    {
      "epoch": 0.7739372268573699,
      "grad_norm": 0.15891239047050476,
      "learning_rate": 0.0001487778958554729,
      "loss": 0.1631,
      "step": 974
    },
    {
      "epoch": 0.7747318235995232,
      "grad_norm": 0.1503559947013855,
      "learning_rate": 0.00014872476089266738,
      "loss": 0.1287,
      "step": 975
    },
    {
      "epoch": 0.7755264203416766,
      "grad_norm": 0.13630451261997223,
      "learning_rate": 0.00014867162592986186,
      "loss": 0.1273,
      "step": 976
    },
    {
      "epoch": 0.77632101708383,
      "grad_norm": 0.15950947999954224,
      "learning_rate": 0.00014861849096705634,
      "loss": 0.1259,
      "step": 977
    },
    {
      "epoch": 0.7771156138259833,
      "grad_norm": 0.1692427545785904,
      "learning_rate": 0.00014856535600425082,
      "loss": 0.1383,
      "step": 978
    },
    {
      "epoch": 0.7779102105681367,
      "grad_norm": 0.17961415648460388,
      "learning_rate": 0.0001485122210414453,
      "loss": 0.1135,
      "step": 979
    },
    {
      "epoch": 0.77870480731029,
      "grad_norm": 0.18368515372276306,
      "learning_rate": 0.00014845908607863975,
      "loss": 0.1225,
      "step": 980
    },
    {
      "epoch": 0.7794994040524433,
      "grad_norm": 0.14315643906593323,
      "learning_rate": 0.00014840595111583423,
      "loss": 0.115,
      "step": 981
    },
    {
      "epoch": 0.7802940007945968,
      "grad_norm": 0.12215398252010345,
      "learning_rate": 0.00014835281615302868,
      "loss": 0.0922,
      "step": 982
    },
    {
      "epoch": 0.7810885975367501,
      "grad_norm": 0.14005695283412933,
      "learning_rate": 0.00014829968119022316,
      "loss": 0.1104,
      "step": 983
    },
    {
      "epoch": 0.7818831942789034,
      "grad_norm": 0.13215531408786774,
      "learning_rate": 0.00014824654622741764,
      "loss": 0.1129,
      "step": 984
    },
    {
      "epoch": 0.7826777910210568,
      "grad_norm": 0.14320342242717743,
      "learning_rate": 0.00014819341126461212,
      "loss": 0.1195,
      "step": 985
    },
    {
      "epoch": 0.7834723877632102,
      "grad_norm": 0.11862819641828537,
      "learning_rate": 0.0001481402763018066,
      "loss": 0.09,
      "step": 986
    },
    {
      "epoch": 0.7842669845053635,
      "grad_norm": 0.14149117469787598,
      "learning_rate": 0.00014808714133900108,
      "loss": 0.0917,
      "step": 987
    },
    {
      "epoch": 0.7850615812475169,
      "grad_norm": 0.1347786784172058,
      "learning_rate": 0.00014803400637619554,
      "loss": 0.0923,
      "step": 988
    },
    {
      "epoch": 0.7858561779896702,
      "grad_norm": 0.1400180608034134,
      "learning_rate": 0.00014798087141339002,
      "loss": 0.0774,
      "step": 989
    },
    {
      "epoch": 0.7866507747318237,
      "grad_norm": 0.12244634330272675,
      "learning_rate": 0.0001479277364505845,
      "loss": 0.0861,
      "step": 990
    },
    {
      "epoch": 0.787445371473977,
      "grad_norm": 0.14861556887626648,
      "learning_rate": 0.00014787460148777895,
      "loss": 0.0809,
      "step": 991
    },
    {
      "epoch": 0.7882399682161303,
      "grad_norm": 0.1157178208231926,
      "learning_rate": 0.00014782146652497343,
      "loss": 0.0702,
      "step": 992
    },
    {
      "epoch": 0.7890345649582837,
      "grad_norm": 0.11569633334875107,
      "learning_rate": 0.0001477683315621679,
      "loss": 0.0866,
      "step": 993
    },
    {
      "epoch": 0.789829161700437,
      "grad_norm": 0.10847916454076767,
      "learning_rate": 0.0001477151965993624,
      "loss": 0.0821,
      "step": 994
    },
    {
      "epoch": 0.7906237584425904,
      "grad_norm": 0.09860499203205109,
      "learning_rate": 0.00014766206163655687,
      "loss": 0.058,
      "step": 995
    },
    {
      "epoch": 0.7914183551847438,
      "grad_norm": 0.14462539553642273,
      "learning_rate": 0.00014760892667375135,
      "loss": 0.053,
      "step": 996
    },
    {
      "epoch": 0.7922129519268971,
      "grad_norm": 0.09537909924983978,
      "learning_rate": 0.0001475557917109458,
      "loss": 0.047,
      "step": 997
    },
    {
      "epoch": 0.7930075486690504,
      "grad_norm": 0.10279659181833267,
      "learning_rate": 0.00014750265674814028,
      "loss": 0.0408,
      "step": 998
    },
    {
      "epoch": 0.7938021454112039,
      "grad_norm": 0.09692683815956116,
      "learning_rate": 0.00014744952178533476,
      "loss": 0.0323,
      "step": 999
    },
    {
      "epoch": 0.7945967421533572,
      "grad_norm": 0.09459809958934784,
      "learning_rate": 0.00014739638682252922,
      "loss": 0.0445,
      "step": 1000
    },
    {
      "epoch": 0.7953913388955105,
      "grad_norm": 0.7982057929039001,
      "learning_rate": 0.0001473432518597237,
      "loss": 0.6368,
      "step": 1001
    },
    {
      "epoch": 0.7961859356376639,
      "grad_norm": 0.5440145134925842,
      "learning_rate": 0.00014729011689691818,
      "loss": 0.539,
      "step": 1002
    },
    {
      "epoch": 0.7969805323798173,
      "grad_norm": 0.3930254578590393,
      "learning_rate": 0.00014723698193411266,
      "loss": 0.5243,
      "step": 1003
    },
    {
      "epoch": 0.7977751291219706,
      "grad_norm": 0.44504302740097046,
      "learning_rate": 0.00014718384697130714,
      "loss": 0.4442,
      "step": 1004
    },
    {
      "epoch": 0.798569725864124,
      "grad_norm": 0.29309555888175964,
      "learning_rate": 0.0001471307120085016,
      "loss": 0.3715,
      "step": 1005
    },
    {
      "epoch": 0.7993643226062773,
      "grad_norm": 0.26763471961021423,
      "learning_rate": 0.00014707757704569607,
      "loss": 0.3059,
      "step": 1006
    },
    {
      "epoch": 0.8001589193484306,
      "grad_norm": 0.23269227147102356,
      "learning_rate": 0.00014702444208289055,
      "loss": 0.3197,
      "step": 1007
    },
    {
      "epoch": 0.8009535160905841,
      "grad_norm": 0.2193998098373413,
      "learning_rate": 0.00014697130712008503,
      "loss": 0.2883,
      "step": 1008
    },
    {
      "epoch": 0.8017481128327374,
      "grad_norm": 0.22889013588428497,
      "learning_rate": 0.00014691817215727949,
      "loss": 0.2888,
      "step": 1009
    },
    {
      "epoch": 0.8025427095748907,
      "grad_norm": 0.24885441362857819,
      "learning_rate": 0.00014686503719447397,
      "loss": 0.3199,
      "step": 1010
    },
    {
      "epoch": 0.8033373063170441,
      "grad_norm": 0.22908234596252441,
      "learning_rate": 0.00014681190223166845,
      "loss": 0.27,
      "step": 1011
    },
    {
      "epoch": 0.8041319030591975,
      "grad_norm": 0.19936750829219818,
      "learning_rate": 0.00014675876726886293,
      "loss": 0.259,
      "step": 1012
    },
    {
      "epoch": 0.8049264998013508,
      "grad_norm": 0.20632779598236084,
      "learning_rate": 0.00014670563230605738,
      "loss": 0.2332,
      "step": 1013
    },
    {
      "epoch": 0.8057210965435042,
      "grad_norm": 0.1910337507724762,
      "learning_rate": 0.00014665249734325186,
      "loss": 0.2073,
      "step": 1014
    },
    {
      "epoch": 0.8065156932856575,
      "grad_norm": 0.19770681858062744,
      "learning_rate": 0.00014659936238044634,
      "loss": 0.186,
      "step": 1015
    },
    {
      "epoch": 0.8073102900278109,
      "grad_norm": 0.23315271735191345,
      "learning_rate": 0.00014654622741764082,
      "loss": 0.2304,
      "step": 1016
    },
    {
      "epoch": 0.8081048867699643,
      "grad_norm": 0.1712750345468521,
      "learning_rate": 0.0001464930924548353,
      "loss": 0.1845,
      "step": 1017
    },
    {
      "epoch": 0.8088994835121176,
      "grad_norm": 0.1857125163078308,
      "learning_rate": 0.00014643995749202978,
      "loss": 0.1754,
      "step": 1018
    },
    {
      "epoch": 0.8096940802542709,
      "grad_norm": 0.14829452335834503,
      "learning_rate": 0.00014638682252922423,
      "loss": 0.1644,
      "step": 1019
    },
    {
      "epoch": 0.8104886769964244,
      "grad_norm": 0.15219083428382874,
      "learning_rate": 0.0001463336875664187,
      "loss": 0.1667,
      "step": 1020
    },
    {
      "epoch": 0.8112832737385777,
      "grad_norm": 0.15679706633090973,
      "learning_rate": 0.00014628055260361317,
      "loss": 0.1664,
      "step": 1021
    },
    {
      "epoch": 0.812077870480731,
      "grad_norm": 0.1533365100622177,
      "learning_rate": 0.00014622741764080765,
      "loss": 0.1452,
      "step": 1022
    },
    {
      "epoch": 0.8128724672228844,
      "grad_norm": 0.14555476605892181,
      "learning_rate": 0.00014617428267800213,
      "loss": 0.1435,
      "step": 1023
    },
    {
      "epoch": 0.8136670639650377,
      "grad_norm": 0.16744375228881836,
      "learning_rate": 0.0001461211477151966,
      "loss": 0.1459,
      "step": 1024
    },
    {
      "epoch": 0.8144616607071911,
      "grad_norm": 0.14782918989658356,
      "learning_rate": 0.0001460680127523911,
      "loss": 0.1454,
      "step": 1025
    },
    {
      "epoch": 0.8152562574493445,
      "grad_norm": 0.14104869961738586,
      "learning_rate": 0.00014601487778958557,
      "loss": 0.1527,
      "step": 1026
    },
    {
      "epoch": 0.8160508541914978,
      "grad_norm": 0.14225347340106964,
      "learning_rate": 0.00014596174282678005,
      "loss": 0.1401,
      "step": 1027
    },
    {
      "epoch": 0.8168454509336511,
      "grad_norm": 0.1839175671339035,
      "learning_rate": 0.0001459086078639745,
      "loss": 0.1499,
      "step": 1028
    },
    {
      "epoch": 0.8176400476758046,
      "grad_norm": 0.15583886206150055,
      "learning_rate": 0.00014585547290116895,
      "loss": 0.1453,
      "step": 1029
    },
    {
      "epoch": 0.8184346444179579,
      "grad_norm": 0.12764005362987518,
      "learning_rate": 0.00014580233793836343,
      "loss": 0.1141,
      "step": 1030
    },
    {
      "epoch": 0.8192292411601112,
      "grad_norm": 0.12769784033298492,
      "learning_rate": 0.00014574920297555791,
      "loss": 0.1276,
      "step": 1031
    },
    {
      "epoch": 0.8200238379022646,
      "grad_norm": 0.11799757927656174,
      "learning_rate": 0.0001456960680127524,
      "loss": 0.1191,
      "step": 1032
    },
    {
      "epoch": 0.820818434644418,
      "grad_norm": 0.12257808446884155,
      "learning_rate": 0.00014564293304994687,
      "loss": 0.1085,
      "step": 1033
    },
    {
      "epoch": 0.8216130313865713,
      "grad_norm": 0.13945569097995758,
      "learning_rate": 0.00014558979808714135,
      "loss": 0.1318,
      "step": 1034
    },
    {
      "epoch": 0.8224076281287247,
      "grad_norm": 0.11521019041538239,
      "learning_rate": 0.00014553666312433583,
      "loss": 0.1021,
      "step": 1035
    },
    {
      "epoch": 0.823202224870878,
      "grad_norm": 0.12664729356765747,
      "learning_rate": 0.00014548352816153032,
      "loss": 0.1013,
      "step": 1036
    },
    {
      "epoch": 0.8239968216130313,
      "grad_norm": 0.11767835170030594,
      "learning_rate": 0.00014543039319872477,
      "loss": 0.0871,
      "step": 1037
    },
    {
      "epoch": 0.8247914183551848,
      "grad_norm": 0.11087784916162491,
      "learning_rate": 0.00014537725823591922,
      "loss": 0.0885,
      "step": 1038
    },
    {
      "epoch": 0.8255860150973381,
      "grad_norm": 0.1129642128944397,
      "learning_rate": 0.0001453241232731137,
      "loss": 0.0875,
      "step": 1039
    },
    {
      "epoch": 0.8263806118394914,
      "grad_norm": 0.09900479018688202,
      "learning_rate": 0.00014527098831030818,
      "loss": 0.0902,
      "step": 1040
    },
    {
      "epoch": 0.8271752085816448,
      "grad_norm": 0.11332940310239792,
      "learning_rate": 0.00014521785334750266,
      "loss": 0.0814,
      "step": 1041
    },
    {
      "epoch": 0.8279698053237982,
      "grad_norm": 0.12198956310749054,
      "learning_rate": 0.00014516471838469714,
      "loss": 0.0727,
      "step": 1042
    },
    {
      "epoch": 0.8287644020659515,
      "grad_norm": 0.1000734269618988,
      "learning_rate": 0.00014511158342189162,
      "loss": 0.0651,
      "step": 1043
    },
    {
      "epoch": 0.8295589988081049,
      "grad_norm": 0.1066126674413681,
      "learning_rate": 0.0001450584484590861,
      "loss": 0.046,
      "step": 1044
    },
    {
      "epoch": 0.8303535955502582,
      "grad_norm": 0.09324541687965393,
      "learning_rate": 0.00014500531349628058,
      "loss": 0.062,
      "step": 1045
    },
    {
      "epoch": 0.8311481922924115,
      "grad_norm": 0.0716303214430809,
      "learning_rate": 0.00014495217853347504,
      "loss": 0.0438,
      "step": 1046
    },
    {
      "epoch": 0.831942789034565,
      "grad_norm": 0.11921781301498413,
      "learning_rate": 0.0001448990435706695,
      "loss": 0.0783,
      "step": 1047
    },
    {
      "epoch": 0.8327373857767183,
      "grad_norm": 0.08967143297195435,
      "learning_rate": 0.00014484590860786397,
      "loss": 0.0415,
      "step": 1048
    },
    {
      "epoch": 0.8335319825188717,
      "grad_norm": 0.10161249339580536,
      "learning_rate": 0.00014479277364505845,
      "loss": 0.0417,
      "step": 1049
    },
    {
      "epoch": 0.834326579261025,
      "grad_norm": 0.08159292489290237,
      "learning_rate": 0.00014473963868225293,
      "loss": 0.0381,
      "step": 1050
    },
    {
      "epoch": 0.8351211760031784,
      "grad_norm": 0.6631872057914734,
      "learning_rate": 0.0001446865037194474,
      "loss": 0.5721,
      "step": 1051
    },
    {
      "epoch": 0.8359157727453318,
      "grad_norm": 0.4195860028266907,
      "learning_rate": 0.0001446333687566419,
      "loss": 0.4029,
      "step": 1052
    },
    {
      "epoch": 0.8367103694874851,
      "grad_norm": 0.4346793591976166,
      "learning_rate": 0.00014458023379383637,
      "loss": 0.4051,
      "step": 1053
    },
    {
      "epoch": 0.8375049662296384,
      "grad_norm": 0.45292389392852783,
      "learning_rate": 0.00014452709883103082,
      "loss": 0.378,
      "step": 1054
    },
    {
      "epoch": 0.8382995629717919,
      "grad_norm": 0.2776213586330414,
      "learning_rate": 0.0001444739638682253,
      "loss": 0.3498,
      "step": 1055
    },
    {
      "epoch": 0.8390941597139452,
      "grad_norm": 0.2694806456565857,
      "learning_rate": 0.00014442082890541976,
      "loss": 0.3325,
      "step": 1056
    },
    {
      "epoch": 0.8398887564560985,
      "grad_norm": 0.27965253591537476,
      "learning_rate": 0.00014436769394261424,
      "loss": 0.3251,
      "step": 1057
    },
    {
      "epoch": 0.8406833531982519,
      "grad_norm": 0.20152422785758972,
      "learning_rate": 0.00014431455897980872,
      "loss": 0.2366,
      "step": 1058
    },
    {
      "epoch": 0.8414779499404053,
      "grad_norm": 0.21280209720134735,
      "learning_rate": 0.0001442614240170032,
      "loss": 0.2497,
      "step": 1059
    },
    {
      "epoch": 0.8422725466825586,
      "grad_norm": 0.22521112859249115,
      "learning_rate": 0.00014420828905419768,
      "loss": 0.253,
      "step": 1060
    },
    {
      "epoch": 0.843067143424712,
      "grad_norm": 0.18187753856182098,
      "learning_rate": 0.00014415515409139216,
      "loss": 0.1983,
      "step": 1061
    },
    {
      "epoch": 0.8438617401668653,
      "grad_norm": 0.18814246356487274,
      "learning_rate": 0.0001441020191285866,
      "loss": 0.2249,
      "step": 1062
    },
    {
      "epoch": 0.8446563369090186,
      "grad_norm": 0.19105903804302216,
      "learning_rate": 0.0001440488841657811,
      "loss": 0.2233,
      "step": 1063
    },
    {
      "epoch": 0.8454509336511721,
      "grad_norm": 0.17570634186267853,
      "learning_rate": 0.00014399574920297557,
      "loss": 0.1956,
      "step": 1064
    },
    {
      "epoch": 0.8462455303933254,
      "grad_norm": 0.18091823160648346,
      "learning_rate": 0.00014394261424017002,
      "loss": 0.1953,
      "step": 1065
    },
    {
      "epoch": 0.8470401271354787,
      "grad_norm": 0.16988641023635864,
      "learning_rate": 0.0001438894792773645,
      "loss": 0.1724,
      "step": 1066
    },
    {
      "epoch": 0.8478347238776321,
      "grad_norm": 0.1885702908039093,
      "learning_rate": 0.00014383634431455898,
      "loss": 0.1942,
      "step": 1067
    },
    {
      "epoch": 0.8486293206197855,
      "grad_norm": 0.2065356969833374,
      "learning_rate": 0.00014378320935175346,
      "loss": 0.186,
      "step": 1068
    },
    {
      "epoch": 0.8494239173619388,
      "grad_norm": 0.14351029694080353,
      "learning_rate": 0.00014373007438894794,
      "loss": 0.1564,
      "step": 1069
    },
    {
      "epoch": 0.8502185141040922,
      "grad_norm": 0.15378205478191376,
      "learning_rate": 0.0001436769394261424,
      "loss": 0.1571,
      "step": 1070
    },
    {
      "epoch": 0.8510131108462455,
      "grad_norm": 0.1464133858680725,
      "learning_rate": 0.00014362380446333688,
      "loss": 0.1611,
      "step": 1071
    },
    {
      "epoch": 0.8518077075883989,
      "grad_norm": 0.16091503202915192,
      "learning_rate": 0.00014357066950053136,
      "loss": 0.1468,
      "step": 1072
    },
    {
      "epoch": 0.8526023043305523,
      "grad_norm": 0.16093412041664124,
      "learning_rate": 0.00014351753453772584,
      "loss": 0.1592,
      "step": 1073
    },
    {
      "epoch": 0.8533969010727056,
      "grad_norm": 0.1451140195131302,
      "learning_rate": 0.00014346439957492032,
      "loss": 0.154,
      "step": 1074
    },
    {
      "epoch": 0.8541914978148589,
      "grad_norm": 0.15880471467971802,
      "learning_rate": 0.00014341126461211477,
      "loss": 0.1487,
      "step": 1075
    },
    {
      "epoch": 0.8549860945570124,
      "grad_norm": 0.18335765600204468,
      "learning_rate": 0.00014335812964930925,
      "loss": 0.134,
      "step": 1076
    },
    {
      "epoch": 0.8557806912991657,
      "grad_norm": 0.1859458088874817,
      "learning_rate": 0.00014330499468650373,
      "loss": 0.1531,
      "step": 1077
    },
    {
      "epoch": 0.856575288041319,
      "grad_norm": 0.16956055164337158,
      "learning_rate": 0.00014325185972369818,
      "loss": 0.1486,
      "step": 1078
    },
    {
      "epoch": 0.8573698847834724,
      "grad_norm": 0.17626875638961792,
      "learning_rate": 0.00014319872476089267,
      "loss": 0.1328,
      "step": 1079
    },
    {
      "epoch": 0.8581644815256257,
      "grad_norm": 0.1506943553686142,
      "learning_rate": 0.00014314558979808715,
      "loss": 0.118,
      "step": 1080
    },
    {
      "epoch": 0.8589590782677791,
      "grad_norm": 0.15119488537311554,
      "learning_rate": 0.00014309245483528163,
      "loss": 0.1065,
      "step": 1081
    },
    {
      "epoch": 0.8597536750099325,
      "grad_norm": 0.15247370302677155,
      "learning_rate": 0.0001430393198724761,
      "loss": 0.1255,
      "step": 1082
    },
    {
      "epoch": 0.8605482717520858,
      "grad_norm": 0.12036538869142532,
      "learning_rate": 0.00014298618490967059,
      "loss": 0.1189,
      "step": 1083
    },
    {
      "epoch": 0.8613428684942391,
      "grad_norm": 0.12185975164175034,
      "learning_rate": 0.00014293304994686504,
      "loss": 0.1168,
      "step": 1084
    },
    {
      "epoch": 0.8621374652363926,
      "grad_norm": 0.1401035189628601,
      "learning_rate": 0.00014287991498405952,
      "loss": 0.1202,
      "step": 1085
    },
    {
      "epoch": 0.8629320619785459,
      "grad_norm": 0.10936564952135086,
      "learning_rate": 0.000142826780021254,
      "loss": 0.0785,
      "step": 1086
    },
    {
      "epoch": 0.8637266587206992,
      "grad_norm": 0.11652134358882904,
      "learning_rate": 0.00014277364505844845,
      "loss": 0.0954,
      "step": 1087
    },
    {
      "epoch": 0.8645212554628526,
      "grad_norm": 0.09883921593427658,
      "learning_rate": 0.00014272051009564293,
      "loss": 0.0799,
      "step": 1088
    },
    {
      "epoch": 0.865315852205006,
      "grad_norm": 0.10390195995569229,
      "learning_rate": 0.0001426673751328374,
      "loss": 0.0844,
      "step": 1089
    },
    {
      "epoch": 0.8661104489471593,
      "grad_norm": 0.12115893512964249,
      "learning_rate": 0.0001426142401700319,
      "loss": 0.0957,
      "step": 1090
    },
    {
      "epoch": 0.8669050456893127,
      "grad_norm": 0.11780912429094315,
      "learning_rate": 0.00014256110520722637,
      "loss": 0.0891,
      "step": 1091
    },
    {
      "epoch": 0.867699642431466,
      "grad_norm": 0.0913640558719635,
      "learning_rate": 0.00014250797024442085,
      "loss": 0.067,
      "step": 1092
    },
    {
      "epoch": 0.8684942391736193,
      "grad_norm": 0.0991712436079979,
      "learning_rate": 0.0001424548352816153,
      "loss": 0.0688,
      "step": 1093
    },
    {
      "epoch": 0.8692888359157728,
      "grad_norm": 0.0848195031285286,
      "learning_rate": 0.0001424017003188098,
      "loss": 0.0569,
      "step": 1094
    },
    {
      "epoch": 0.8700834326579261,
      "grad_norm": 0.12434939295053482,
      "learning_rate": 0.00014234856535600424,
      "loss": 0.0609,
      "step": 1095
    },
    {
      "epoch": 0.8708780294000794,
      "grad_norm": 0.10383394360542297,
      "learning_rate": 0.00014229543039319872,
      "loss": 0.049,
      "step": 1096
    },
    {
      "epoch": 0.8716726261422328,
      "grad_norm": 0.10085881501436234,
      "learning_rate": 0.0001422422954303932,
      "loss": 0.0619,
      "step": 1097
    },
    {
      "epoch": 0.8724672228843862,
      "grad_norm": 0.1061105877161026,
      "learning_rate": 0.00014218916046758768,
      "loss": 0.0351,
      "step": 1098
    },
    {
      "epoch": 0.8732618196265395,
      "grad_norm": 0.0841752290725708,
      "learning_rate": 0.00014213602550478216,
      "loss": 0.0404,
      "step": 1099
    },
    {
      "epoch": 0.8740564163686929,
      "grad_norm": 0.08387070894241333,
      "learning_rate": 0.00014208289054197664,
      "loss": 0.046,
      "step": 1100
    },
    {
      "epoch": 0.8748510131108462,
      "grad_norm": 0.5461241602897644,
      "learning_rate": 0.00014202975557917112,
      "loss": 0.5732,
      "step": 1101
    },
    {
      "epoch": 0.8756456098529996,
      "grad_norm": 0.48525452613830566,
      "learning_rate": 0.00014197662061636557,
      "loss": 0.5159,
      "step": 1102
    },
    {
      "epoch": 0.876440206595153,
      "grad_norm": 0.34251725673675537,
      "learning_rate": 0.00014192348565356003,
      "loss": 0.4136,
      "step": 1103
    },
    {
      "epoch": 0.8772348033373063,
      "grad_norm": 0.3426162004470825,
      "learning_rate": 0.0001418703506907545,
      "loss": 0.3857,
      "step": 1104
    },
    {
      "epoch": 0.8780294000794596,
      "grad_norm": 0.33425837755203247,
      "learning_rate": 0.000141817215727949,
      "loss": 0.3404,
      "step": 1105
    },
    {
      "epoch": 0.878823996821613,
      "grad_norm": 0.4458531141281128,
      "learning_rate": 0.00014176408076514347,
      "loss": 0.3827,
      "step": 1106
    },
    {
      "epoch": 0.8796185935637664,
      "grad_norm": 0.2702980935573578,
      "learning_rate": 0.00014171094580233795,
      "loss": 0.2948,
      "step": 1107
    },
    {
      "epoch": 0.8804131903059197,
      "grad_norm": 0.24143941700458527,
      "learning_rate": 0.00014165781083953243,
      "loss": 0.2939,
      "step": 1108
    },
    {
      "epoch": 0.8812077870480731,
      "grad_norm": 0.23816315829753876,
      "learning_rate": 0.0001416046758767269,
      "loss": 0.3165,
      "step": 1109
    },
    {
      "epoch": 0.8820023837902264,
      "grad_norm": 0.18365263938903809,
      "learning_rate": 0.0001415515409139214,
      "loss": 0.2387,
      "step": 1110
    },
    {
      "epoch": 0.8827969805323799,
      "grad_norm": 0.24342700839042664,
      "learning_rate": 0.00014149840595111584,
      "loss": 0.2731,
      "step": 1111
    },
    {
      "epoch": 0.8835915772745332,
      "grad_norm": 0.17425644397735596,
      "learning_rate": 0.0001414452709883103,
      "loss": 0.2221,
      "step": 1112
    },
    {
      "epoch": 0.8843861740166865,
      "grad_norm": 0.19942860305309296,
      "learning_rate": 0.00014139213602550477,
      "loss": 0.2527,
      "step": 1113
    },
    {
      "epoch": 0.8851807707588399,
      "grad_norm": 0.22895166277885437,
      "learning_rate": 0.00014133900106269925,
      "loss": 0.2588,
      "step": 1114
    },
    {
      "epoch": 0.8859753675009933,
      "grad_norm": 0.18675294518470764,
      "learning_rate": 0.00014128586609989374,
      "loss": 0.2204,
      "step": 1115
    },
    {
      "epoch": 0.8867699642431466,
      "grad_norm": 0.18545156717300415,
      "learning_rate": 0.00014123273113708822,
      "loss": 0.2417,
      "step": 1116
    },
    {
      "epoch": 0.8875645609853,
      "grad_norm": 0.17004534602165222,
      "learning_rate": 0.0001411795961742827,
      "loss": 0.2067,
      "step": 1117
    },
    {
      "epoch": 0.8883591577274533,
      "grad_norm": 0.1652783900499344,
      "learning_rate": 0.00014112646121147718,
      "loss": 0.181,
      "step": 1118
    },
    {
      "epoch": 0.8891537544696066,
      "grad_norm": 0.19285008311271667,
      "learning_rate": 0.00014107332624867163,
      "loss": 0.183,
      "step": 1119
    },
    {
      "epoch": 0.8899483512117601,
      "grad_norm": 0.17197032272815704,
      "learning_rate": 0.0001410201912858661,
      "loss": 0.1731,
      "step": 1120
    },
    {
      "epoch": 0.8907429479539134,
      "grad_norm": 0.1408628225326538,
      "learning_rate": 0.00014096705632306056,
      "loss": 0.1566,
      "step": 1121
    },
    {
      "epoch": 0.8915375446960667,
      "grad_norm": 0.1624501347541809,
      "learning_rate": 0.00014091392136025504,
      "loss": 0.1647,
      "step": 1122
    },
    {
      "epoch": 0.8923321414382201,
      "grad_norm": 0.16039304435253143,
      "learning_rate": 0.00014086078639744952,
      "loss": 0.1898,
      "step": 1123
    },
    {
      "epoch": 0.8931267381803735,
      "grad_norm": 0.16658678650856018,
      "learning_rate": 0.000140807651434644,
      "loss": 0.1761,
      "step": 1124
    },
    {
      "epoch": 0.8939213349225268,
      "grad_norm": 0.1530534327030182,
      "learning_rate": 0.00014075451647183848,
      "loss": 0.1771,
      "step": 1125
    },
    {
      "epoch": 0.8947159316646802,
      "grad_norm": 0.14544551074504852,
      "learning_rate": 0.00014070138150903296,
      "loss": 0.1489,
      "step": 1126
    },
    {
      "epoch": 0.8955105284068335,
      "grad_norm": 0.1477578580379486,
      "learning_rate": 0.00014064824654622742,
      "loss": 0.1459,
      "step": 1127
    },
    {
      "epoch": 0.8963051251489869,
      "grad_norm": 0.12880726158618927,
      "learning_rate": 0.0001405951115834219,
      "loss": 0.1345,
      "step": 1128
    },
    {
      "epoch": 0.8970997218911403,
      "grad_norm": 0.14756016433238983,
      "learning_rate": 0.00014054197662061638,
      "loss": 0.1503,
      "step": 1129
    },
    {
      "epoch": 0.8978943186332936,
      "grad_norm": 0.11035463213920593,
      "learning_rate": 0.00014048884165781086,
      "loss": 0.107,
      "step": 1130
    },
    {
      "epoch": 0.8986889153754469,
      "grad_norm": 0.12776942551136017,
      "learning_rate": 0.0001404357066950053,
      "loss": 0.1298,
      "step": 1131
    },
    {
      "epoch": 0.8994835121176004,
      "grad_norm": 0.10418325662612915,
      "learning_rate": 0.0001403825717321998,
      "loss": 0.1027,
      "step": 1132
    },
    {
      "epoch": 0.9002781088597537,
      "grad_norm": 0.14172440767288208,
      "learning_rate": 0.00014032943676939427,
      "loss": 0.1214,
      "step": 1133
    },
    {
      "epoch": 0.901072705601907,
      "grad_norm": 0.10435676574707031,
      "learning_rate": 0.00014027630180658875,
      "loss": 0.0964,
      "step": 1134
    },
    {
      "epoch": 0.9018673023440604,
      "grad_norm": 0.13357378542423248,
      "learning_rate": 0.00014022316684378323,
      "loss": 0.1178,
      "step": 1135
    },
    {
      "epoch": 0.9026618990862137,
      "grad_norm": 0.11534975469112396,
      "learning_rate": 0.00014017003188097768,
      "loss": 0.1068,
      "step": 1136
    },
    {
      "epoch": 0.9034564958283671,
      "grad_norm": 0.10476747900247574,
      "learning_rate": 0.00014011689691817216,
      "loss": 0.1002,
      "step": 1137
    },
    {
      "epoch": 0.9042510925705205,
      "grad_norm": 0.11095428466796875,
      "learning_rate": 0.00014006376195536664,
      "loss": 0.1008,
      "step": 1138
    },
    {
      "epoch": 0.9050456893126738,
      "grad_norm": 0.07671906799077988,
      "learning_rate": 0.00014001062699256112,
      "loss": 0.0687,
      "step": 1139
    },
    {
      "epoch": 0.9058402860548271,
      "grad_norm": 0.10547593235969543,
      "learning_rate": 0.00013995749202975558,
      "loss": 0.0873,
      "step": 1140
    },
    {
      "epoch": 0.9066348827969806,
      "grad_norm": 0.0935574397444725,
      "learning_rate": 0.00013990435706695006,
      "loss": 0.064,
      "step": 1141
    },
    {
      "epoch": 0.9074294795391339,
      "grad_norm": 0.09743662178516388,
      "learning_rate": 0.00013985122210414454,
      "loss": 0.0755,
      "step": 1142
    },
    {
      "epoch": 0.9082240762812872,
      "grad_norm": 0.08330310881137848,
      "learning_rate": 0.00013979808714133902,
      "loss": 0.0497,
      "step": 1143
    },
    {
      "epoch": 0.9090186730234406,
      "grad_norm": 0.09813280403614044,
      "learning_rate": 0.00013974495217853347,
      "loss": 0.0451,
      "step": 1144
    },
    {
      "epoch": 0.909813269765594,
      "grad_norm": 0.0807059183716774,
      "learning_rate": 0.00013969181721572795,
      "loss": 0.0472,
      "step": 1145
    },
    {
      "epoch": 0.9106078665077473,
      "grad_norm": 0.09884028136730194,
      "learning_rate": 0.00013963868225292243,
      "loss": 0.0459,
      "step": 1146
    },
    {
      "epoch": 0.9114024632499007,
      "grad_norm": 0.06826390326023102,
      "learning_rate": 0.0001395855472901169,
      "loss": 0.0424,
      "step": 1147
    },
    {
      "epoch": 0.912197059992054,
      "grad_norm": 0.08763972669839859,
      "learning_rate": 0.0001395324123273114,
      "loss": 0.0426,
      "step": 1148
    },
    {
      "epoch": 0.9129916567342073,
      "grad_norm": 0.083426833152771,
      "learning_rate": 0.00013947927736450584,
      "loss": 0.0463,
      "step": 1149
    },
    {
      "epoch": 0.9137862534763608,
      "grad_norm": 0.08774559944868088,
      "learning_rate": 0.00013942614240170033,
      "loss": 0.0446,
      "step": 1150
    },
    {
      "epoch": 0.9145808502185141,
      "grad_norm": 0.4939640760421753,
      "learning_rate": 0.0001393730074388948,
      "loss": 0.6283,
      "step": 1151
    },
    {
      "epoch": 0.9153754469606674,
      "grad_norm": 0.4638272821903229,
      "learning_rate": 0.00013931987247608926,
      "loss": 0.565,
      "step": 1152
    },
    {
      "epoch": 0.9161700437028208,
      "grad_norm": 0.38020092248916626,
      "learning_rate": 0.00013926673751328374,
      "loss": 0.4382,
      "step": 1153
    },
    {
      "epoch": 0.9169646404449742,
      "grad_norm": 0.34330567717552185,
      "learning_rate": 0.00013921360255047822,
      "loss": 0.4321,
      "step": 1154
    },
    {
      "epoch": 0.9177592371871275,
      "grad_norm": 0.3320833146572113,
      "learning_rate": 0.0001391604675876727,
      "loss": 0.443,
      "step": 1155
    },
    {
      "epoch": 0.9185538339292809,
      "grad_norm": 0.3074623942375183,
      "learning_rate": 0.00013910733262486718,
      "loss": 0.353,
      "step": 1156
    },
    {
      "epoch": 0.9193484306714342,
      "grad_norm": 0.2404792606830597,
      "learning_rate": 0.00013905419766206166,
      "loss": 0.2695,
      "step": 1157
    },
    {
      "epoch": 0.9201430274135876,
      "grad_norm": 0.25678321719169617,
      "learning_rate": 0.0001390010626992561,
      "loss": 0.3049,
      "step": 1158
    },
    {
      "epoch": 0.920937624155741,
      "grad_norm": 0.2123294472694397,
      "learning_rate": 0.0001389479277364506,
      "loss": 0.2636,
      "step": 1159
    },
    {
      "epoch": 0.9217322208978943,
      "grad_norm": 0.20267823338508606,
      "learning_rate": 0.00013889479277364505,
      "loss": 0.2864,
      "step": 1160
    },
    {
      "epoch": 0.9225268176400476,
      "grad_norm": 0.18853454291820526,
      "learning_rate": 0.00013884165781083953,
      "loss": 0.2446,
      "step": 1161
    },
    {
      "epoch": 0.9233214143822011,
      "grad_norm": 0.21049664914608002,
      "learning_rate": 0.000138788522848034,
      "loss": 0.2596,
      "step": 1162
    },
    {
      "epoch": 0.9241160111243544,
      "grad_norm": 0.1599644273519516,
      "learning_rate": 0.00013873538788522849,
      "loss": 0.1932,
      "step": 1163
    },
    {
      "epoch": 0.9249106078665077,
      "grad_norm": 0.18216587603092194,
      "learning_rate": 0.00013868225292242297,
      "loss": 0.2229,
      "step": 1164
    },
    {
      "epoch": 0.9257052046086611,
      "grad_norm": 0.17472875118255615,
      "learning_rate": 0.00013862911795961745,
      "loss": 0.174,
      "step": 1165
    },
    {
      "epoch": 0.9264998013508144,
      "grad_norm": 0.16963282227516174,
      "learning_rate": 0.00013857598299681193,
      "loss": 0.1683,
      "step": 1166
    },
    {
      "epoch": 0.9272943980929678,
      "grad_norm": 0.1480587124824524,
      "learning_rate": 0.00013852284803400638,
      "loss": 0.1762,
      "step": 1167
    },
    {
      "epoch": 0.9280889948351212,
      "grad_norm": 0.18424291908740997,
      "learning_rate": 0.00013846971307120083,
      "loss": 0.211,
      "step": 1168
    },
    {
      "epoch": 0.9288835915772745,
      "grad_norm": 0.15253572165966034,
      "learning_rate": 0.0001384165781083953,
      "loss": 0.1826,
      "step": 1169
    },
    {
      "epoch": 0.929678188319428,
      "grad_norm": 0.14457212388515472,
      "learning_rate": 0.0001383634431455898,
      "loss": 0.1758,
      "step": 1170
    },
    {
      "epoch": 0.9304727850615813,
      "grad_norm": 0.2129662036895752,
      "learning_rate": 0.00013831030818278427,
      "loss": 0.1816,
      "step": 1171
    },
    {
      "epoch": 0.9312673818037346,
      "grad_norm": 0.14901253581047058,
      "learning_rate": 0.00013825717321997875,
      "loss": 0.1531,
      "step": 1172
    },
    {
      "epoch": 0.932061978545888,
      "grad_norm": 0.13766159117221832,
      "learning_rate": 0.00013820403825717323,
      "loss": 0.1329,
      "step": 1173
    },
    {
      "epoch": 0.9328565752880413,
      "grad_norm": 0.15506432950496674,
      "learning_rate": 0.00013815090329436771,
      "loss": 0.153,
      "step": 1174
    },
    {
      "epoch": 0.9336511720301947,
      "grad_norm": 0.11569711565971375,
      "learning_rate": 0.0001380977683315622,
      "loss": 0.1105,
      "step": 1175
    },
    {
      "epoch": 0.9344457687723481,
      "grad_norm": 0.13544242084026337,
      "learning_rate": 0.00013804463336875665,
      "loss": 0.1439,
      "step": 1176
    },
    {
      "epoch": 0.9352403655145014,
      "grad_norm": 0.14121797680854797,
      "learning_rate": 0.00013799149840595113,
      "loss": 0.1277,
      "step": 1177
    },
    {
      "epoch": 0.9360349622566547,
      "grad_norm": 0.13597089052200317,
      "learning_rate": 0.00013793836344314558,
      "loss": 0.1311,
      "step": 1178
    },
    {
      "epoch": 0.9368295589988082,
      "grad_norm": 0.13596493005752563,
      "learning_rate": 0.00013788522848034006,
      "loss": 0.1193,
      "step": 1179
    },
    {
      "epoch": 0.9376241557409615,
      "grad_norm": 0.14825543761253357,
      "learning_rate": 0.00013783209351753454,
      "loss": 0.1135,
      "step": 1180
    },
    {
      "epoch": 0.9384187524831148,
      "grad_norm": 0.11195217818021774,
      "learning_rate": 0.00013777895855472902,
      "loss": 0.0983,
      "step": 1181
    },
    {
      "epoch": 0.9392133492252682,
      "grad_norm": 0.09982459247112274,
      "learning_rate": 0.0001377258235919235,
      "loss": 0.0836,
      "step": 1182
    },
    {
      "epoch": 0.9400079459674215,
      "grad_norm": 0.14236551523208618,
      "learning_rate": 0.00013767268862911798,
      "loss": 0.1219,
      "step": 1183
    },
    {
      "epoch": 0.9408025427095749,
      "grad_norm": 0.14885559678077698,
      "learning_rate": 0.00013761955366631246,
      "loss": 0.116,
      "step": 1184
    },
    {
      "epoch": 0.9415971394517283,
      "grad_norm": 0.11927012354135513,
      "learning_rate": 0.00013756641870350691,
      "loss": 0.0932,
      "step": 1185
    },
    {
      "epoch": 0.9423917361938816,
      "grad_norm": 0.09626646339893341,
      "learning_rate": 0.0001375132837407014,
      "loss": 0.0769,
      "step": 1186
    },
    {
      "epoch": 0.9431863329360349,
      "grad_norm": 0.07796096056699753,
      "learning_rate": 0.00013746014877789585,
      "loss": 0.0643,
      "step": 1187
    },
    {
      "epoch": 0.9439809296781884,
      "grad_norm": 0.11311522871255875,
      "learning_rate": 0.00013740701381509033,
      "loss": 0.0982,
      "step": 1188
    },
    {
      "epoch": 0.9447755264203417,
      "grad_norm": 0.09546688199043274,
      "learning_rate": 0.0001373538788522848,
      "loss": 0.0875,
      "step": 1189
    },
    {
      "epoch": 0.945570123162495,
      "grad_norm": 0.0935685783624649,
      "learning_rate": 0.0001373007438894793,
      "loss": 0.0746,
      "step": 1190
    },
    {
      "epoch": 0.9463647199046484,
      "grad_norm": 0.10311561077833176,
      "learning_rate": 0.00013724760892667377,
      "loss": 0.0945,
      "step": 1191
    },
    {
      "epoch": 0.9471593166468018,
      "grad_norm": 0.12913458049297333,
      "learning_rate": 0.00013719447396386825,
      "loss": 0.0776,
      "step": 1192
    },
    {
      "epoch": 0.9479539133889551,
      "grad_norm": 0.09048736095428467,
      "learning_rate": 0.0001371413390010627,
      "loss": 0.0554,
      "step": 1193
    },
    {
      "epoch": 0.9487485101311085,
      "grad_norm": 0.08983369171619415,
      "learning_rate": 0.00013708820403825718,
      "loss": 0.0503,
      "step": 1194
    },
    {
      "epoch": 0.9495431068732618,
      "grad_norm": 0.09041696786880493,
      "learning_rate": 0.00013703506907545166,
      "loss": 0.0384,
      "step": 1195
    },
    {
      "epoch": 0.9503377036154151,
      "grad_norm": 0.0826612338423729,
      "learning_rate": 0.00013698193411264612,
      "loss": 0.0605,
      "step": 1196
    },
    {
      "epoch": 0.9511323003575686,
      "grad_norm": 0.06866651773452759,
      "learning_rate": 0.0001369287991498406,
      "loss": 0.0632,
      "step": 1197
    },
    {
      "epoch": 0.9519268970997219,
      "grad_norm": 0.07331424206495285,
      "learning_rate": 0.00013687566418703508,
      "loss": 0.0454,
      "step": 1198
    },
    {
      "epoch": 0.9527214938418752,
      "grad_norm": 0.0895390436053276,
      "learning_rate": 0.00013682252922422956,
      "loss": 0.0496,
      "step": 1199
    },
    {
      "epoch": 0.9535160905840286,
      "grad_norm": 0.10769344866275787,
      "learning_rate": 0.00013676939426142404,
      "loss": 0.0374,
      "step": 1200
    },
    {
      "epoch": 0.954310687326182,
      "grad_norm": 0.5129445791244507,
      "learning_rate": 0.0001367162592986185,
      "loss": 0.6155,
      "step": 1201
    },
    {
      "epoch": 0.9551052840683353,
      "grad_norm": 0.4313046634197235,
      "learning_rate": 0.00013666312433581297,
      "loss": 0.4479,
      "step": 1202
    },
    {
      "epoch": 0.9558998808104887,
      "grad_norm": 0.3024410009384155,
      "learning_rate": 0.00013660998937300745,
      "loss": 0.385,
      "step": 1203
    },
    {
      "epoch": 0.956694477552642,
      "grad_norm": 0.2969221770763397,
      "learning_rate": 0.00013655685441020193,
      "loss": 0.3443,
      "step": 1204
    },
    {
      "epoch": 0.9574890742947954,
      "grad_norm": 0.26552000641822815,
      "learning_rate": 0.00013650371944739638,
      "loss": 0.3636,
      "step": 1205
    },
    {
      "epoch": 0.9582836710369488,
      "grad_norm": 0.22930151224136353,
      "learning_rate": 0.00013645058448459086,
      "loss": 0.3062,
      "step": 1206
    },
    {
      "epoch": 0.9590782677791021,
      "grad_norm": 0.22458049654960632,
      "learning_rate": 0.00013639744952178534,
      "loss": 0.2759,
      "step": 1207
    },
    {
      "epoch": 0.9598728645212554,
      "grad_norm": 0.269520103931427,
      "learning_rate": 0.00013634431455897982,
      "loss": 0.3137,
      "step": 1208
    },
    {
      "epoch": 0.9606674612634089,
      "grad_norm": 0.1973971724510193,
      "learning_rate": 0.00013629117959617428,
      "loss": 0.2669,
      "step": 1209
    },
    {
      "epoch": 0.9614620580055622,
      "grad_norm": 0.20871977508068085,
      "learning_rate": 0.00013623804463336876,
      "loss": 0.2644,
      "step": 1210
    },
    {
      "epoch": 0.9622566547477155,
      "grad_norm": 0.2362547069787979,
      "learning_rate": 0.00013618490967056324,
      "loss": 0.2439,
      "step": 1211
    },
    {
      "epoch": 0.9630512514898689,
      "grad_norm": 0.201205313205719,
      "learning_rate": 0.00013613177470775772,
      "loss": 0.214,
      "step": 1212
    },
    {
      "epoch": 0.9638458482320222,
      "grad_norm": 0.16771584749221802,
      "learning_rate": 0.0001360786397449522,
      "loss": 0.1703,
      "step": 1213
    },
    {
      "epoch": 0.9646404449741756,
      "grad_norm": 0.15818171203136444,
      "learning_rate": 0.00013602550478214665,
      "loss": 0.2115,
      "step": 1214
    },
    {
      "epoch": 0.965435041716329,
      "grad_norm": 0.1635342389345169,
      "learning_rate": 0.00013597236981934113,
      "loss": 0.1841,
      "step": 1215
    },
    {
      "epoch": 0.9662296384584823,
      "grad_norm": 0.1578637808561325,
      "learning_rate": 0.0001359192348565356,
      "loss": 0.1809,
      "step": 1216
    },
    {
      "epoch": 0.9670242352006356,
      "grad_norm": 0.16282540559768677,
      "learning_rate": 0.00013586609989373006,
      "loss": 0.1882,
      "step": 1217
    },
    {
      "epoch": 0.9678188319427891,
      "grad_norm": 0.1485735923051834,
      "learning_rate": 0.00013581296493092454,
      "loss": 0.166,
      "step": 1218
    },
    {
      "epoch": 0.9686134286849424,
      "grad_norm": 0.15639083087444305,
      "learning_rate": 0.00013575982996811902,
      "loss": 0.1602,
      "step": 1219
    },
    {
      "epoch": 0.9694080254270957,
      "grad_norm": 0.15406735241413116,
      "learning_rate": 0.0001357066950053135,
      "loss": 0.1616,
      "step": 1220
    },
    {
      "epoch": 0.9702026221692491,
      "grad_norm": 0.1696178913116455,
      "learning_rate": 0.00013565356004250798,
      "loss": 0.1637,
      "step": 1221
    },
    {
      "epoch": 0.9709972189114024,
      "grad_norm": 0.19708207249641418,
      "learning_rate": 0.00013560042507970247,
      "loss": 0.1943,
      "step": 1222
    },
    {
      "epoch": 0.9717918156535558,
      "grad_norm": 0.14182953536510468,
      "learning_rate": 0.00013554729011689692,
      "loss": 0.1552,
      "step": 1223
    },
    {
      "epoch": 0.9725864123957092,
      "grad_norm": 0.14024819433689117,
      "learning_rate": 0.0001354941551540914,
      "loss": 0.1437,
      "step": 1224
    },
    {
      "epoch": 0.9733810091378625,
      "grad_norm": 0.13247135281562805,
      "learning_rate": 0.00013544102019128588,
      "loss": 0.1338,
      "step": 1225
    },
    {
      "epoch": 0.9741756058800158,
      "grad_norm": 0.12053976953029633,
      "learning_rate": 0.00013538788522848033,
      "loss": 0.1163,
      "step": 1226
    },
    {
      "epoch": 0.9749702026221693,
      "grad_norm": 0.12326852232217789,
      "learning_rate": 0.0001353347502656748,
      "loss": 0.0988,
      "step": 1227
    },
    {
      "epoch": 0.9757647993643226,
      "grad_norm": 0.13557927310466766,
      "learning_rate": 0.0001352816153028693,
      "loss": 0.1151,
      "step": 1228
    },
    {
      "epoch": 0.976559396106476,
      "grad_norm": 0.13764473795890808,
      "learning_rate": 0.00013522848034006377,
      "loss": 0.1431,
      "step": 1229
    },
    {
      "epoch": 0.9773539928486293,
      "grad_norm": 0.16549527645111084,
      "learning_rate": 0.00013517534537725825,
      "loss": 0.1189,
      "step": 1230
    },
    {
      "epoch": 0.9781485895907827,
      "grad_norm": 0.10945237427949905,
      "learning_rate": 0.00013512221041445273,
      "loss": 0.1058,
      "step": 1231
    },
    {
      "epoch": 0.9789431863329361,
      "grad_norm": 0.1467214971780777,
      "learning_rate": 0.00013506907545164719,
      "loss": 0.1354,
      "step": 1232
    },
    {
      "epoch": 0.9797377830750894,
      "grad_norm": 0.1251613050699234,
      "learning_rate": 0.00013501594048884167,
      "loss": 0.1029,
      "step": 1233
    },
    {
      "epoch": 0.9805323798172427,
      "grad_norm": 0.1394180804491043,
      "learning_rate": 0.00013496280552603612,
      "loss": 0.1139,
      "step": 1234
    },
    {
      "epoch": 0.9813269765593962,
      "grad_norm": 0.10053374618291855,
      "learning_rate": 0.0001349096705632306,
      "loss": 0.0925,
      "step": 1235
    },
    {
      "epoch": 0.9821215733015495,
      "grad_norm": 0.0859265848994255,
      "learning_rate": 0.00013485653560042508,
      "loss": 0.0769,
      "step": 1236
    },
    {
      "epoch": 0.9829161700437028,
      "grad_norm": 0.1170683279633522,
      "learning_rate": 0.00013480340063761956,
      "loss": 0.0863,
      "step": 1237
    },
    {
      "epoch": 0.9837107667858562,
      "grad_norm": 0.10413280874490738,
      "learning_rate": 0.00013475026567481404,
      "loss": 0.071,
      "step": 1238
    },
    {
      "epoch": 0.9845053635280095,
      "grad_norm": 0.11033487319946289,
      "learning_rate": 0.00013469713071200852,
      "loss": 0.0932,
      "step": 1239
    },
    {
      "epoch": 0.9852999602701629,
      "grad_norm": 0.09573941677808762,
      "learning_rate": 0.000134643995749203,
      "loss": 0.0767,
      "step": 1240
    },
    {
      "epoch": 0.9860945570123163,
      "grad_norm": 0.09211397916078568,
      "learning_rate": 0.00013459086078639745,
      "loss": 0.0683,
      "step": 1241
    },
    {
      "epoch": 0.9868891537544696,
      "grad_norm": 0.09450456500053406,
      "learning_rate": 0.00013453772582359193,
      "loss": 0.0631,
      "step": 1242
    },
    {
      "epoch": 0.9876837504966229,
      "grad_norm": 0.10360462963581085,
      "learning_rate": 0.00013448459086078639,
      "loss": 0.0698,
      "step": 1243
    },
    {
      "epoch": 0.9884783472387764,
      "grad_norm": 0.07886791974306107,
      "learning_rate": 0.00013443145589798087,
      "loss": 0.0455,
      "step": 1244
    },
    {
      "epoch": 0.9892729439809297,
      "grad_norm": 0.09990602731704712,
      "learning_rate": 0.00013437832093517535,
      "loss": 0.0535,
      "step": 1245
    },
    {
      "epoch": 0.990067540723083,
      "grad_norm": 0.07039719074964523,
      "learning_rate": 0.00013432518597236983,
      "loss": 0.0433,
      "step": 1246
    },
    {
      "epoch": 0.9908621374652364,
      "grad_norm": 0.0886400118470192,
      "learning_rate": 0.0001342720510095643,
      "loss": 0.0407,
      "step": 1247
    },
    {
      "epoch": 0.9916567342073898,
      "grad_norm": 0.09763384610414505,
      "learning_rate": 0.0001342189160467588,
      "loss": 0.0421,
      "step": 1248
    },
    {
      "epoch": 0.9924513309495431,
      "grad_norm": 0.09699507057666779,
      "learning_rate": 0.00013416578108395327,
      "loss": 0.0359,
      "step": 1249
    },
    {
      "epoch": 0.9932459276916965,
      "grad_norm": 0.07834824174642563,
      "learning_rate": 0.00013411264612114772,
      "loss": 0.0411,
      "step": 1250
    },
    {
      "epoch": 0.9940405244338498,
      "grad_norm": 0.49543359875679016,
      "learning_rate": 0.0001340595111583422,
      "loss": 0.4361,
      "step": 1251
    },
    {
      "epoch": 0.9948351211760031,
      "grad_norm": 0.297643780708313,
      "learning_rate": 0.00013400637619553665,
      "loss": 0.3235,
      "step": 1252
    },
    {
      "epoch": 0.9956297179181566,
      "grad_norm": 0.27088603377342224,
      "learning_rate": 0.00013395324123273113,
      "loss": 0.2057,
      "step": 1253
    },
    {
      "epoch": 0.9964243146603099,
      "grad_norm": 0.1763683408498764,
      "learning_rate": 0.00013390010626992561,
      "loss": 0.154,
      "step": 1254
    },
    {
      "epoch": 0.9972189114024632,
      "grad_norm": 0.16318531334400177,
      "learning_rate": 0.0001338469713071201,
      "loss": 0.1592,
      "step": 1255
    },
    {
      "epoch": 0.9980135081446166,
      "grad_norm": 0.16667817533016205,
      "learning_rate": 0.00013379383634431457,
      "loss": 0.1255,
      "step": 1256
    },
    {
      "epoch": 0.99880810488677,
      "grad_norm": 0.10401593893766403,
      "learning_rate": 0.00013374070138150905,
      "loss": 0.0834,
      "step": 1257
    },
    {
      "epoch": 0.9996027016289233,
      "grad_norm": 0.10194851458072662,
      "learning_rate": 0.0001336875664187035,
      "loss": 0.0517,
      "step": 1258
    },
    {
      "epoch": 1.0003972983710767,
      "grad_norm": 0.2597169280052185,
      "learning_rate": 0.000133634431455898,
      "loss": 0.3201,
      "step": 1259
    },
    {
      "epoch": 1.0011918951132301,
      "grad_norm": 0.32392850518226624,
      "learning_rate": 0.00013358129649309247,
      "loss": 0.478,
      "step": 1260
    },
    {
      "epoch": 1.0019864918553834,
      "grad_norm": 0.2571159601211548,
      "learning_rate": 0.00013352816153028692,
      "loss": 0.3563,
      "step": 1261
    },
    {
      "epoch": 1.0027810885975368,
      "grad_norm": 0.24153581261634827,
      "learning_rate": 0.0001334750265674814,
      "loss": 0.3193,
      "step": 1262
    },
    {
      "epoch": 1.0035756853396902,
      "grad_norm": 0.25654569268226624,
      "learning_rate": 0.00013342189160467588,
      "loss": 0.3214,
      "step": 1263
    },
    {
      "epoch": 1.0043702820818434,
      "grad_norm": 0.21213844418525696,
      "learning_rate": 0.00013336875664187036,
      "loss": 0.2635,
      "step": 1264
    },
    {
      "epoch": 1.0051648788239969,
      "grad_norm": 0.21936768293380737,
      "learning_rate": 0.00013331562167906484,
      "loss": 0.2537,
      "step": 1265
    },
    {
      "epoch": 1.0059594755661503,
      "grad_norm": 0.31671762466430664,
      "learning_rate": 0.00013326248671625932,
      "loss": 0.2394,
      "step": 1266
    },
    {
      "epoch": 1.0067540723083035,
      "grad_norm": 0.25091177225112915,
      "learning_rate": 0.00013320935175345378,
      "loss": 0.2515,
      "step": 1267
    },
    {
      "epoch": 1.007548669050457,
      "grad_norm": 0.2938635051250458,
      "learning_rate": 0.00013315621679064826,
      "loss": 0.2267,
      "step": 1268
    },
    {
      "epoch": 1.0083432657926104,
      "grad_norm": 0.2273813635110855,
      "learning_rate": 0.00013310308182784274,
      "loss": 0.2178,
      "step": 1269
    },
    {
      "epoch": 1.0091378625347636,
      "grad_norm": 0.24819894134998322,
      "learning_rate": 0.0001330499468650372,
      "loss": 0.2124,
      "step": 1270
    },
    {
      "epoch": 1.009932459276917,
      "grad_norm": 0.20537133514881134,
      "learning_rate": 0.00013299681190223167,
      "loss": 0.2122,
      "step": 1271
    },
    {
      "epoch": 1.0107270560190704,
      "grad_norm": 0.22734080255031586,
      "learning_rate": 0.00013294367693942615,
      "loss": 0.1868,
      "step": 1272
    },
    {
      "epoch": 1.0115216527612236,
      "grad_norm": 0.21568851172924042,
      "learning_rate": 0.00013289054197662063,
      "loss": 0.1918,
      "step": 1273
    },
    {
      "epoch": 1.012316249503377,
      "grad_norm": 0.2289736121892929,
      "learning_rate": 0.0001328374070138151,
      "loss": 0.1962,
      "step": 1274
    },
    {
      "epoch": 1.0131108462455305,
      "grad_norm": 0.21707667410373688,
      "learning_rate": 0.00013278427205100956,
      "loss": 0.195,
      "step": 1275
    },
    {
      "epoch": 1.0139054429876837,
      "grad_norm": 0.21377602219581604,
      "learning_rate": 0.00013273113708820404,
      "loss": 0.187,
      "step": 1276
    },
    {
      "epoch": 1.0147000397298371,
      "grad_norm": 0.20722344517707825,
      "learning_rate": 0.00013267800212539852,
      "loss": 0.1599,
      "step": 1277
    },
    {
      "epoch": 1.0154946364719906,
      "grad_norm": 0.27440929412841797,
      "learning_rate": 0.000132624867162593,
      "loss": 0.1741,
      "step": 1278
    },
    {
      "epoch": 1.0162892332141438,
      "grad_norm": 0.2962803542613983,
      "learning_rate": 0.00013257173219978746,
      "loss": 0.1619,
      "step": 1279
    },
    {
      "epoch": 1.0170838299562972,
      "grad_norm": 0.20626434683799744,
      "learning_rate": 0.00013251859723698194,
      "loss": 0.1434,
      "step": 1280
    },
    {
      "epoch": 1.0178784266984506,
      "grad_norm": 0.18189795315265656,
      "learning_rate": 0.00013246546227417642,
      "loss": 0.151,
      "step": 1281
    },
    {
      "epoch": 1.0186730234406038,
      "grad_norm": 0.1804772913455963,
      "learning_rate": 0.0001324123273113709,
      "loss": 0.1504,
      "step": 1282
    },
    {
      "epoch": 1.0194676201827573,
      "grad_norm": 0.15606512129306793,
      "learning_rate": 0.00013235919234856535,
      "loss": 0.1326,
      "step": 1283
    },
    {
      "epoch": 1.0202622169249107,
      "grad_norm": 0.16483387351036072,
      "learning_rate": 0.00013230605738575983,
      "loss": 0.1358,
      "step": 1284
    },
    {
      "epoch": 1.021056813667064,
      "grad_norm": 0.15826180577278137,
      "learning_rate": 0.0001322529224229543,
      "loss": 0.1221,
      "step": 1285
    },
    {
      "epoch": 1.0218514104092173,
      "grad_norm": 0.13444122672080994,
      "learning_rate": 0.0001321997874601488,
      "loss": 0.0982,
      "step": 1286
    },
    {
      "epoch": 1.0226460071513708,
      "grad_norm": 0.14937280118465424,
      "learning_rate": 0.00013214665249734327,
      "loss": 0.1205,
      "step": 1287
    },
    {
      "epoch": 1.023440603893524,
      "grad_norm": 0.15241263806819916,
      "learning_rate": 0.00013209351753453772,
      "loss": 0.0988,
      "step": 1288
    },
    {
      "epoch": 1.0242352006356774,
      "grad_norm": 0.17868942022323608,
      "learning_rate": 0.0001320403825717322,
      "loss": 0.1236,
      "step": 1289
    },
    {
      "epoch": 1.0250297973778308,
      "grad_norm": 0.13261350989341736,
      "learning_rate": 0.00013198724760892668,
      "loss": 0.1025,
      "step": 1290
    },
    {
      "epoch": 1.025824394119984,
      "grad_norm": 0.14546562731266022,
      "learning_rate": 0.00013193411264612114,
      "loss": 0.1046,
      "step": 1291
    },
    {
      "epoch": 1.0266189908621375,
      "grad_norm": 0.14056451618671417,
      "learning_rate": 0.00013188097768331562,
      "loss": 0.0939,
      "step": 1292
    },
    {
      "epoch": 1.027413587604291,
      "grad_norm": 0.17143014073371887,
      "learning_rate": 0.0001318278427205101,
      "loss": 0.111,
      "step": 1293
    },
    {
      "epoch": 1.0282081843464441,
      "grad_norm": 0.16019673645496368,
      "learning_rate": 0.00013177470775770458,
      "loss": 0.1108,
      "step": 1294
    },
    {
      "epoch": 1.0290027810885976,
      "grad_norm": 0.14711317420005798,
      "learning_rate": 0.00013172157279489906,
      "loss": 0.0966,
      "step": 1295
    },
    {
      "epoch": 1.029797377830751,
      "grad_norm": 0.13216646015644073,
      "learning_rate": 0.00013166843783209354,
      "loss": 0.0812,
      "step": 1296
    },
    {
      "epoch": 1.0305919745729042,
      "grad_norm": 0.11143606156110764,
      "learning_rate": 0.000131615302869288,
      "loss": 0.0715,
      "step": 1297
    },
    {
      "epoch": 1.0313865713150576,
      "grad_norm": 0.11317430436611176,
      "learning_rate": 0.00013156216790648247,
      "loss": 0.0793,
      "step": 1298
    },
    {
      "epoch": 1.032181168057211,
      "grad_norm": 0.11751356720924377,
      "learning_rate": 0.00013150903294367692,
      "loss": 0.0755,
      "step": 1299
    },
    {
      "epoch": 1.0329757647993643,
      "grad_norm": 0.1108931452035904,
      "learning_rate": 0.0001314558979808714,
      "loss": 0.0698,
      "step": 1300
    },
    {
      "epoch": 1.0337703615415177,
      "grad_norm": 0.08251899480819702,
      "learning_rate": 0.00013140276301806589,
      "loss": 0.0563,
      "step": 1301
    },
    {
      "epoch": 1.0345649582836711,
      "grad_norm": 0.10271679610013962,
      "learning_rate": 0.00013134962805526037,
      "loss": 0.0618,
      "step": 1302
    },
    {
      "epoch": 1.0353595550258243,
      "grad_norm": 0.10374701768159866,
      "learning_rate": 0.00013129649309245485,
      "loss": 0.0515,
      "step": 1303
    },
    {
      "epoch": 1.0361541517679778,
      "grad_norm": 0.08635757863521576,
      "learning_rate": 0.00013124335812964933,
      "loss": 0.0492,
      "step": 1304
    },
    {
      "epoch": 1.0369487485101312,
      "grad_norm": 0.08025088161230087,
      "learning_rate": 0.0001311902231668438,
      "loss": 0.047,
      "step": 1305
    },
    {
      "epoch": 1.0377433452522844,
      "grad_norm": 0.07624372094869614,
      "learning_rate": 0.00013113708820403826,
      "loss": 0.0489,
      "step": 1306
    },
    {
      "epoch": 1.0385379419944378,
      "grad_norm": 0.07481830567121506,
      "learning_rate": 0.00013108395324123274,
      "loss": 0.0374,
      "step": 1307
    },
    {
      "epoch": 1.0393325387365913,
      "grad_norm": 0.07772351056337357,
      "learning_rate": 0.0001310308182784272,
      "loss": 0.0385,
      "step": 1308
    },
    {
      "epoch": 1.0401271354787445,
      "grad_norm": 0.44078949093818665,
      "learning_rate": 0.00013097768331562167,
      "loss": 0.2931,
      "step": 1309
    },
    {
      "epoch": 1.040921732220898,
      "grad_norm": 0.6264327764511108,
      "learning_rate": 0.00013092454835281615,
      "loss": 0.4295,
      "step": 1310
    },
    {
      "epoch": 1.0417163289630513,
      "grad_norm": 0.4178972542285919,
      "learning_rate": 0.00013087141339001063,
      "loss": 0.366,
      "step": 1311
    },
    {
      "epoch": 1.0425109257052045,
      "grad_norm": 0.3702310621738434,
      "learning_rate": 0.0001308182784272051,
      "loss": 0.3301,
      "step": 1312
    },
    {
      "epoch": 1.043305522447358,
      "grad_norm": 0.32433968782424927,
      "learning_rate": 0.0001307651434643996,
      "loss": 0.3334,
      "step": 1313
    },
    {
      "epoch": 1.0441001191895114,
      "grad_norm": 0.33574458956718445,
      "learning_rate": 0.00013071200850159407,
      "loss": 0.2747,
      "step": 1314
    },
    {
      "epoch": 1.0448947159316646,
      "grad_norm": 0.2578916847705841,
      "learning_rate": 0.00013065887353878853,
      "loss": 0.2759,
      "step": 1315
    },
    {
      "epoch": 1.045689312673818,
      "grad_norm": 0.28512245416641235,
      "learning_rate": 0.000130605738575983,
      "loss": 0.2635,
      "step": 1316
    },
    {
      "epoch": 1.0464839094159715,
      "grad_norm": 0.25784486532211304,
      "learning_rate": 0.00013055260361317746,
      "loss": 0.2382,
      "step": 1317
    },
    {
      "epoch": 1.0472785061581247,
      "grad_norm": 0.2182462066411972,
      "learning_rate": 0.00013049946865037194,
      "loss": 0.2213,
      "step": 1318
    },
    {
      "epoch": 1.048073102900278,
      "grad_norm": 0.22624938189983368,
      "learning_rate": 0.00013044633368756642,
      "loss": 0.2395,
      "step": 1319
    },
    {
      "epoch": 1.0488676996424315,
      "grad_norm": 0.20465196669101715,
      "learning_rate": 0.0001303931987247609,
      "loss": 0.2205,
      "step": 1320
    },
    {
      "epoch": 1.0496622963845847,
      "grad_norm": 0.20721422135829926,
      "learning_rate": 0.00013034006376195538,
      "loss": 0.1989,
      "step": 1321
    },
    {
      "epoch": 1.0504568931267382,
      "grad_norm": 0.20716363191604614,
      "learning_rate": 0.00013028692879914986,
      "loss": 0.2039,
      "step": 1322
    },
    {
      "epoch": 1.0512514898688916,
      "grad_norm": 0.19372236728668213,
      "learning_rate": 0.00013023379383634434,
      "loss": 0.187,
      "step": 1323
    },
    {
      "epoch": 1.0520460866110448,
      "grad_norm": 0.1990106850862503,
      "learning_rate": 0.0001301806588735388,
      "loss": 0.1699,
      "step": 1324
    },
    {
      "epoch": 1.0528406833531982,
      "grad_norm": 0.18515798449516296,
      "learning_rate": 0.00013012752391073327,
      "loss": 0.1733,
      "step": 1325
    },
    {
      "epoch": 1.0536352800953517,
      "grad_norm": 0.18662065267562866,
      "learning_rate": 0.00013007438894792773,
      "loss": 0.171,
      "step": 1326
    },
    {
      "epoch": 1.0544298768375049,
      "grad_norm": 0.1786520928144455,
      "learning_rate": 0.0001300212539851222,
      "loss": 0.1483,
      "step": 1327
    },
    {
      "epoch": 1.0552244735796583,
      "grad_norm": 0.2055073380470276,
      "learning_rate": 0.0001299681190223167,
      "loss": 0.1572,
      "step": 1328
    },
    {
      "epoch": 1.0560190703218117,
      "grad_norm": 0.1953820288181305,
      "learning_rate": 0.00012991498405951117,
      "loss": 0.1497,
      "step": 1329
    },
    {
      "epoch": 1.056813667063965,
      "grad_norm": 0.19657209515571594,
      "learning_rate": 0.00012986184909670565,
      "loss": 0.1338,
      "step": 1330
    },
    {
      "epoch": 1.0576082638061184,
      "grad_norm": 0.16474713385105133,
      "learning_rate": 0.00012980871413390013,
      "loss": 0.1383,
      "step": 1331
    },
    {
      "epoch": 1.0584028605482718,
      "grad_norm": 0.1737191081047058,
      "learning_rate": 0.00012975557917109458,
      "loss": 0.1397,
      "step": 1332
    },
    {
      "epoch": 1.059197457290425,
      "grad_norm": 0.15851610898971558,
      "learning_rate": 0.00012970244420828906,
      "loss": 0.1156,
      "step": 1333
    },
    {
      "epoch": 1.0599920540325785,
      "grad_norm": 0.16169790923595428,
      "learning_rate": 0.00012964930924548354,
      "loss": 0.1329,
      "step": 1334
    },
    {
      "epoch": 1.0607866507747319,
      "grad_norm": 0.22902734577655792,
      "learning_rate": 0.000129596174282678,
      "loss": 0.1348,
      "step": 1335
    },
    {
      "epoch": 1.061581247516885,
      "grad_norm": 0.15716123580932617,
      "learning_rate": 0.00012954303931987247,
      "loss": 0.1234,
      "step": 1336
    },
    {
      "epoch": 1.0623758442590385,
      "grad_norm": 0.1672956645488739,
      "learning_rate": 0.00012948990435706696,
      "loss": 0.1248,
      "step": 1337
    },
    {
      "epoch": 1.063170441001192,
      "grad_norm": 0.12012085318565369,
      "learning_rate": 0.00012943676939426144,
      "loss": 0.0891,
      "step": 1338
    },
    {
      "epoch": 1.0639650377433452,
      "grad_norm": 0.1448824554681778,
      "learning_rate": 0.00012938363443145592,
      "loss": 0.0979,
      "step": 1339
    },
    {
      "epoch": 1.0647596344854986,
      "grad_norm": 0.1348317414522171,
      "learning_rate": 0.00012933049946865037,
      "loss": 0.1101,
      "step": 1340
    },
    {
      "epoch": 1.065554231227652,
      "grad_norm": 0.14210854470729828,
      "learning_rate": 0.00012927736450584485,
      "loss": 0.09,
      "step": 1341
    },
    {
      "epoch": 1.0663488279698052,
      "grad_norm": 0.12395056337118149,
      "learning_rate": 0.00012922422954303933,
      "loss": 0.0895,
      "step": 1342
    },
    {
      "epoch": 1.0671434247119587,
      "grad_norm": 0.12445136159658432,
      "learning_rate": 0.0001291710945802338,
      "loss": 0.0871,
      "step": 1343
    },
    {
      "epoch": 1.067938021454112,
      "grad_norm": 0.1101786345243454,
      "learning_rate": 0.00012911795961742826,
      "loss": 0.0817,
      "step": 1344
    },
    {
      "epoch": 1.0687326181962653,
      "grad_norm": 0.11779917031526566,
      "learning_rate": 0.00012906482465462274,
      "loss": 0.0962,
      "step": 1345
    },
    {
      "epoch": 1.0695272149384187,
      "grad_norm": 0.10148341208696365,
      "learning_rate": 0.00012901168969181722,
      "loss": 0.0804,
      "step": 1346
    },
    {
      "epoch": 1.0703218116805722,
      "grad_norm": 0.09880863130092621,
      "learning_rate": 0.0001289585547290117,
      "loss": 0.0675,
      "step": 1347
    },
    {
      "epoch": 1.0711164084227254,
      "grad_norm": 0.14226968586444855,
      "learning_rate": 0.00012890541976620616,
      "loss": 0.0862,
      "step": 1348
    },
    {
      "epoch": 1.0719110051648788,
      "grad_norm": 0.10096202790737152,
      "learning_rate": 0.00012885228480340064,
      "loss": 0.0781,
      "step": 1349
    },
    {
      "epoch": 1.0727056019070322,
      "grad_norm": 0.08866731077432632,
      "learning_rate": 0.00012879914984059512,
      "loss": 0.0676,
      "step": 1350
    },
    {
      "epoch": 1.0735001986491854,
      "grad_norm": 0.09614696353673935,
      "learning_rate": 0.0001287460148777896,
      "loss": 0.0555,
      "step": 1351
    },
    {
      "epoch": 1.0742947953913389,
      "grad_norm": 0.0857664942741394,
      "learning_rate": 0.00012869287991498408,
      "loss": 0.0587,
      "step": 1352
    },
    {
      "epoch": 1.0750893921334923,
      "grad_norm": 0.10097575932741165,
      "learning_rate": 0.00012863974495217853,
      "loss": 0.0621,
      "step": 1353
    },
    {
      "epoch": 1.0758839888756455,
      "grad_norm": 0.10778728127479553,
      "learning_rate": 0.000128586609989373,
      "loss": 0.0495,
      "step": 1354
    },
    {
      "epoch": 1.076678585617799,
      "grad_norm": 0.07861951738595963,
      "learning_rate": 0.0001285334750265675,
      "loss": 0.0495,
      "step": 1355
    },
    {
      "epoch": 1.0774731823599524,
      "grad_norm": 0.08079054206609726,
      "learning_rate": 0.00012848034006376197,
      "loss": 0.036,
      "step": 1356
    },
    {
      "epoch": 1.0782677791021056,
      "grad_norm": 0.09136535972356796,
      "learning_rate": 0.00012842720510095642,
      "loss": 0.0474,
      "step": 1357
    },
    {
      "epoch": 1.079062375844259,
      "grad_norm": 0.08877480775117874,
      "learning_rate": 0.0001283740701381509,
      "loss": 0.0354,
      "step": 1358
    },
    {
      "epoch": 1.0798569725864124,
      "grad_norm": 0.383513867855072,
      "learning_rate": 0.00012832093517534538,
      "loss": 0.2889,
      "step": 1359
    },
    {
      "epoch": 1.0806515693285657,
      "grad_norm": 0.4319961667060852,
      "learning_rate": 0.00012826780021253986,
      "loss": 0.4093,
      "step": 1360
    },
    {
      "epoch": 1.081446166070719,
      "grad_norm": 0.38963666558265686,
      "learning_rate": 0.00012821466524973434,
      "loss": 0.3408,
      "step": 1361
    },
    {
      "epoch": 1.0822407628128725,
      "grad_norm": 0.4013315737247467,
      "learning_rate": 0.0001281615302869288,
      "loss": 0.3321,
      "step": 1362
    },
    {
      "epoch": 1.0830353595550257,
      "grad_norm": 0.37869247794151306,
      "learning_rate": 0.00012810839532412328,
      "loss": 0.2718,
      "step": 1363
    },
    {
      "epoch": 1.0838299562971792,
      "grad_norm": 0.37256819009780884,
      "learning_rate": 0.00012805526036131776,
      "loss": 0.2885,
      "step": 1364
    },
    {
      "epoch": 1.0846245530393326,
      "grad_norm": 0.3259843587875366,
      "learning_rate": 0.0001280021253985122,
      "loss": 0.2717,
      "step": 1365
    },
    {
      "epoch": 1.0854191497814858,
      "grad_norm": 0.3254108428955078,
      "learning_rate": 0.0001279489904357067,
      "loss": 0.256,
      "step": 1366
    },
    {
      "epoch": 1.0862137465236392,
      "grad_norm": 0.3139190971851349,
      "learning_rate": 0.00012789585547290117,
      "loss": 0.2537,
      "step": 1367
    },
    {
      "epoch": 1.0870083432657927,
      "grad_norm": 0.2413432002067566,
      "learning_rate": 0.00012784272051009565,
      "loss": 0.2272,
      "step": 1368
    },
    {
      "epoch": 1.087802940007946,
      "grad_norm": 0.23584647476673126,
      "learning_rate": 0.00012778958554729013,
      "loss": 0.2172,
      "step": 1369
    },
    {
      "epoch": 1.0885975367500993,
      "grad_norm": 0.26558881998062134,
      "learning_rate": 0.0001277364505844846,
      "loss": 0.2093,
      "step": 1370
    },
    {
      "epoch": 1.0893921334922527,
      "grad_norm": 0.20201145112514496,
      "learning_rate": 0.00012768331562167906,
      "loss": 0.1721,
      "step": 1371
    },
    {
      "epoch": 1.090186730234406,
      "grad_norm": 0.2269126921892166,
      "learning_rate": 0.00012763018065887354,
      "loss": 0.18,
      "step": 1372
    },
    {
      "epoch": 1.0909813269765594,
      "grad_norm": 0.20868735015392303,
      "learning_rate": 0.000127577045696068,
      "loss": 0.1925,
      "step": 1373
    },
    {
      "epoch": 1.0917759237187128,
      "grad_norm": 0.17371201515197754,
      "learning_rate": 0.00012752391073326248,
      "loss": 0.175,
      "step": 1374
    },
    {
      "epoch": 1.0925705204608662,
      "grad_norm": 0.2055584341287613,
      "learning_rate": 0.00012747077577045696,
      "loss": 0.1683,
      "step": 1375
    },
    {
      "epoch": 1.0933651172030194,
      "grad_norm": 0.1614067703485489,
      "learning_rate": 0.00012741764080765144,
      "loss": 0.1342,
      "step": 1376
    },
    {
      "epoch": 1.0941597139451729,
      "grad_norm": 0.23472550511360168,
      "learning_rate": 0.00012736450584484592,
      "loss": 0.1665,
      "step": 1377
    },
    {
      "epoch": 1.094954310687326,
      "grad_norm": 0.16702800989151,
      "learning_rate": 0.0001273113708820404,
      "loss": 0.1437,
      "step": 1378
    },
    {
      "epoch": 1.0957489074294795,
      "grad_norm": 0.18118098378181458,
      "learning_rate": 0.00012725823591923488,
      "loss": 0.1537,
      "step": 1379
    },
    {
      "epoch": 1.096543504171633,
      "grad_norm": 0.17861618101596832,
      "learning_rate": 0.00012720510095642936,
      "loss": 0.1524,
      "step": 1380
    },
    {
      "epoch": 1.0973381009137864,
      "grad_norm": 0.13871270418167114,
      "learning_rate": 0.0001271519659936238,
      "loss": 0.1152,
      "step": 1381
    },
    {
      "epoch": 1.0981326976559396,
      "grad_norm": 0.19086851179599762,
      "learning_rate": 0.00012709883103081827,
      "loss": 0.1499,
      "step": 1382
    },
    {
      "epoch": 1.098927294398093,
      "grad_norm": 0.1772942692041397,
      "learning_rate": 0.00012704569606801275,
      "loss": 0.1312,
      "step": 1383
    },
    {
      "epoch": 1.0997218911402462,
      "grad_norm": 0.1275821030139923,
      "learning_rate": 0.00012699256110520723,
      "loss": 0.0981,
      "step": 1384
    },
    {
      "epoch": 1.1005164878823996,
      "grad_norm": 0.15250644087791443,
      "learning_rate": 0.0001269394261424017,
      "loss": 0.117,
      "step": 1385
    },
    {
      "epoch": 1.101311084624553,
      "grad_norm": 0.16616810858249664,
      "learning_rate": 0.00012688629117959619,
      "loss": 0.1132,
      "step": 1386
    },
    {
      "epoch": 1.1021056813667065,
      "grad_norm": 0.14622637629508972,
      "learning_rate": 0.00012683315621679067,
      "loss": 0.1058,
      "step": 1387
    },
    {
      "epoch": 1.1029002781088597,
      "grad_norm": 0.11939410120248795,
      "learning_rate": 0.00012678002125398515,
      "loss": 0.0998,
      "step": 1388
    },
    {
      "epoch": 1.1036948748510131,
      "grad_norm": 0.15440689027309418,
      "learning_rate": 0.0001267268862911796,
      "loss": 0.1019,
      "step": 1389
    },
    {
      "epoch": 1.1044894715931666,
      "grad_norm": 0.17241963744163513,
      "learning_rate": 0.00012667375132837408,
      "loss": 0.1075,
      "step": 1390
    },
    {
      "epoch": 1.1052840683353198,
      "grad_norm": 0.1533646136522293,
      "learning_rate": 0.00012662061636556853,
      "loss": 0.1012,
      "step": 1391
    },
    {
      "epoch": 1.1060786650774732,
      "grad_norm": 0.19105641543865204,
      "learning_rate": 0.000126567481402763,
      "loss": 0.1171,
      "step": 1392
    },
    {
      "epoch": 1.1068732618196266,
      "grad_norm": 0.14499078691005707,
      "learning_rate": 0.0001265143464399575,
      "loss": 0.0986,
      "step": 1393
    },
    {
      "epoch": 1.1076678585617799,
      "grad_norm": 0.15739423036575317,
      "learning_rate": 0.00012646121147715197,
      "loss": 0.0989,
      "step": 1394
    },
    {
      "epoch": 1.1084624553039333,
      "grad_norm": 0.17634834349155426,
      "learning_rate": 0.00012640807651434645,
      "loss": 0.0804,
      "step": 1395
    },
    {
      "epoch": 1.1092570520460867,
      "grad_norm": 0.14165262877941132,
      "learning_rate": 0.00012635494155154093,
      "loss": 0.0771,
      "step": 1396
    },
    {
      "epoch": 1.11005164878824,
      "grad_norm": 0.14703723788261414,
      "learning_rate": 0.0001263018065887354,
      "loss": 0.0976,
      "step": 1397
    },
    {
      "epoch": 1.1108462455303934,
      "grad_norm": 0.09967680275440216,
      "learning_rate": 0.00012624867162592987,
      "loss": 0.064,
      "step": 1398
    },
    {
      "epoch": 1.1116408422725468,
      "grad_norm": 0.15304213762283325,
      "learning_rate": 0.00012619553666312435,
      "loss": 0.0772,
      "step": 1399
    },
    {
      "epoch": 1.1124354390147,
      "grad_norm": 0.12827518582344055,
      "learning_rate": 0.0001261424017003188,
      "loss": 0.0714,
      "step": 1400
    },
    {
      "epoch": 1.1132300357568534,
      "grad_norm": 0.10702899843454361,
      "learning_rate": 0.00012608926673751328,
      "loss": 0.0746,
      "step": 1401
    },
    {
      "epoch": 1.1140246324990069,
      "grad_norm": 0.09933944791555405,
      "learning_rate": 0.00012603613177470776,
      "loss": 0.0585,
      "step": 1402
    },
    {
      "epoch": 1.11481922924116,
      "grad_norm": 0.08960972726345062,
      "learning_rate": 0.00012598299681190224,
      "loss": 0.0521,
      "step": 1403
    },
    {
      "epoch": 1.1156138259833135,
      "grad_norm": 0.14735078811645508,
      "learning_rate": 0.00012592986184909672,
      "loss": 0.0632,
      "step": 1404
    },
    {
      "epoch": 1.116408422725467,
      "grad_norm": 0.10856363922357559,
      "learning_rate": 0.0001258767268862912,
      "loss": 0.0558,
      "step": 1405
    },
    {
      "epoch": 1.1172030194676201,
      "grad_norm": 0.09076007455587387,
      "learning_rate": 0.00012582359192348565,
      "loss": 0.0478,
      "step": 1406
    },
    {
      "epoch": 1.1179976162097736,
      "grad_norm": 0.08276970684528351,
      "learning_rate": 0.00012577045696068013,
      "loss": 0.0345,
      "step": 1407
    },
    {
      "epoch": 1.118792212951927,
      "grad_norm": 0.08642856776714325,
      "learning_rate": 0.00012571732199787461,
      "loss": 0.0367,
      "step": 1408
    },
    {
      "epoch": 1.1195868096940802,
      "grad_norm": 0.5143133997917175,
      "learning_rate": 0.00012566418703506907,
      "loss": 0.3499,
      "step": 1409
    },
    {
      "epoch": 1.1203814064362336,
      "grad_norm": 0.612343430519104,
      "learning_rate": 0.00012561105207226355,
      "loss": 0.4898,
      "step": 1410
    },
    {
      "epoch": 1.121176003178387,
      "grad_norm": 0.43754616379737854,
      "learning_rate": 0.00012555791710945803,
      "loss": 0.3333,
      "step": 1411
    },
    {
      "epoch": 1.1219705999205403,
      "grad_norm": 0.4023694097995758,
      "learning_rate": 0.0001255047821466525,
      "loss": 0.3548,
      "step": 1412
    },
    {
      "epoch": 1.1227651966626937,
      "grad_norm": 0.41297975182533264,
      "learning_rate": 0.000125451647183847,
      "loss": 0.3268,
      "step": 1413
    },
    {
      "epoch": 1.1235597934048471,
      "grad_norm": 0.35763081908226013,
      "learning_rate": 0.00012539851222104144,
      "loss": 0.3228,
      "step": 1414
    },
    {
      "epoch": 1.1243543901470003,
      "grad_norm": 0.3006933927536011,
      "learning_rate": 0.00012534537725823592,
      "loss": 0.2574,
      "step": 1415
    },
    {
      "epoch": 1.1251489868891538,
      "grad_norm": 0.29093772172927856,
      "learning_rate": 0.0001252922422954304,
      "loss": 0.3051,
      "step": 1416
    },
    {
      "epoch": 1.1259435836313072,
      "grad_norm": 0.2954963147640228,
      "learning_rate": 0.00012523910733262488,
      "loss": 0.2698,
      "step": 1417
    },
    {
      "epoch": 1.1267381803734604,
      "grad_norm": 0.27410441637039185,
      "learning_rate": 0.00012518597236981934,
      "loss": 0.2521,
      "step": 1418
    },
    {
      "epoch": 1.1275327771156138,
      "grad_norm": 0.25719931721687317,
      "learning_rate": 0.00012513283740701382,
      "loss": 0.2352,
      "step": 1419
    },
    {
      "epoch": 1.1283273738577673,
      "grad_norm": 0.19553397595882416,
      "learning_rate": 0.0001250797024442083,
      "loss": 0.2163,
      "step": 1420
    },
    {
      "epoch": 1.1291219705999205,
      "grad_norm": 0.23584306240081787,
      "learning_rate": 0.00012502656748140278,
      "loss": 0.1851,
      "step": 1421
    },
    {
      "epoch": 1.129916567342074,
      "grad_norm": 0.22894205152988434,
      "learning_rate": 0.00012497343251859723,
      "loss": 0.215,
      "step": 1422
    },
    {
      "epoch": 1.1307111640842273,
      "grad_norm": 0.2095794379711151,
      "learning_rate": 0.0001249202975557917,
      "loss": 0.169,
      "step": 1423
    },
    {
      "epoch": 1.1315057608263805,
      "grad_norm": 0.21444246172904968,
      "learning_rate": 0.0001248671625929862,
      "loss": 0.1868,
      "step": 1424
    },
    {
      "epoch": 1.132300357568534,
      "grad_norm": 0.20830973982810974,
      "learning_rate": 0.00012481402763018067,
      "loss": 0.1793,
      "step": 1425
    },
    {
      "epoch": 1.1330949543106874,
      "grad_norm": 0.20433580875396729,
      "learning_rate": 0.00012476089266737515,
      "loss": 0.1583,
      "step": 1426
    },
    {
      "epoch": 1.1338895510528406,
      "grad_norm": 0.21063722670078278,
      "learning_rate": 0.00012470775770456963,
      "loss": 0.1796,
      "step": 1427
    },
    {
      "epoch": 1.134684147794994,
      "grad_norm": 0.18073420226573944,
      "learning_rate": 0.00012465462274176408,
      "loss": 0.1394,
      "step": 1428
    },
    {
      "epoch": 1.1354787445371475,
      "grad_norm": 0.18632349371910095,
      "learning_rate": 0.00012460148777895856,
      "loss": 0.1515,
      "step": 1429
    },
    {
      "epoch": 1.1362733412793007,
      "grad_norm": 0.2014894038438797,
      "learning_rate": 0.00012454835281615302,
      "loss": 0.1521,
      "step": 1430
    },
    {
      "epoch": 1.1370679380214541,
      "grad_norm": 0.21789850294589996,
      "learning_rate": 0.0001244952178533475,
      "loss": 0.163,
      "step": 1431
    },
    {
      "epoch": 1.1378625347636075,
      "grad_norm": 0.2023848444223404,
      "learning_rate": 0.00012444208289054198,
      "loss": 0.1519,
      "step": 1432
    },
    {
      "epoch": 1.1386571315057608,
      "grad_norm": 0.20174479484558105,
      "learning_rate": 0.00012438894792773646,
      "loss": 0.1378,
      "step": 1433
    },
    {
      "epoch": 1.1394517282479142,
      "grad_norm": 0.16985879838466644,
      "learning_rate": 0.00012433581296493094,
      "loss": 0.1145,
      "step": 1434
    },
    {
      "epoch": 1.1402463249900676,
      "grad_norm": 0.21331381797790527,
      "learning_rate": 0.00012428267800212542,
      "loss": 0.1447,
      "step": 1435
    },
    {
      "epoch": 1.1410409217322208,
      "grad_norm": 0.20556271076202393,
      "learning_rate": 0.0001242295430393199,
      "loss": 0.1298,
      "step": 1436
    },
    {
      "epoch": 1.1418355184743743,
      "grad_norm": 0.134752556681633,
      "learning_rate": 0.00012417640807651435,
      "loss": 0.0981,
      "step": 1437
    },
    {
      "epoch": 1.1426301152165277,
      "grad_norm": 0.1754498928785324,
      "learning_rate": 0.0001241232731137088,
      "loss": 0.112,
      "step": 1438
    },
    {
      "epoch": 1.143424711958681,
      "grad_norm": 0.14783890545368195,
      "learning_rate": 0.00012407013815090328,
      "loss": 0.1115,
      "step": 1439
    },
    {
      "epoch": 1.1442193087008343,
      "grad_norm": 0.17270737886428833,
      "learning_rate": 0.00012401700318809776,
      "loss": 0.1383,
      "step": 1440
    },
    {
      "epoch": 1.1450139054429878,
      "grad_norm": 0.13373976945877075,
      "learning_rate": 0.00012396386822529224,
      "loss": 0.0874,
      "step": 1441
    },
    {
      "epoch": 1.145808502185141,
      "grad_norm": 0.16251569986343384,
      "learning_rate": 0.00012391073326248672,
      "loss": 0.108,
      "step": 1442
    },
    {
      "epoch": 1.1466030989272944,
      "grad_norm": 0.13469530642032623,
      "learning_rate": 0.0001238575982996812,
      "loss": 0.0847,
      "step": 1443
    },
    {
      "epoch": 1.1473976956694478,
      "grad_norm": 0.13279210031032562,
      "learning_rate": 0.00012380446333687568,
      "loss": 0.0782,
      "step": 1444
    },
    {
      "epoch": 1.148192292411601,
      "grad_norm": 0.12971536815166473,
      "learning_rate": 0.00012375132837407017,
      "loss": 0.0825,
      "step": 1445
    },
    {
      "epoch": 1.1489868891537545,
      "grad_norm": 0.12107512354850769,
      "learning_rate": 0.00012369819341126462,
      "loss": 0.0703,
      "step": 1446
    },
    {
      "epoch": 1.149781485895908,
      "grad_norm": 0.151823028922081,
      "learning_rate": 0.00012364505844845907,
      "loss": 0.0827,
      "step": 1447
    },
    {
      "epoch": 1.150576082638061,
      "grad_norm": 0.13025394082069397,
      "learning_rate": 0.00012359192348565355,
      "loss": 0.0805,
      "step": 1448
    },
    {
      "epoch": 1.1513706793802145,
      "grad_norm": 0.10811062157154083,
      "learning_rate": 0.00012353878852284803,
      "loss": 0.0665,
      "step": 1449
    },
    {
      "epoch": 1.152165276122368,
      "grad_norm": 0.1476633995771408,
      "learning_rate": 0.0001234856535600425,
      "loss": 0.0794,
      "step": 1450
    },
    {
      "epoch": 1.1529598728645212,
      "grad_norm": 0.10893158614635468,
      "learning_rate": 0.000123432518597237,
      "loss": 0.0521,
      "step": 1451
    },
    {
      "epoch": 1.1537544696066746,
      "grad_norm": 0.11777999252080917,
      "learning_rate": 0.00012337938363443147,
      "loss": 0.06,
      "step": 1452
    },
    {
      "epoch": 1.154549066348828,
      "grad_norm": 0.08092537522315979,
      "learning_rate": 0.00012332624867162595,
      "loss": 0.0521,
      "step": 1453
    },
    {
      "epoch": 1.1553436630909812,
      "grad_norm": 0.10535918176174164,
      "learning_rate": 0.00012327311370882043,
      "loss": 0.0612,
      "step": 1454
    },
    {
      "epoch": 1.1561382598331347,
      "grad_norm": 0.08061953634023666,
      "learning_rate": 0.00012321997874601489,
      "loss": 0.0502,
      "step": 1455
    },
    {
      "epoch": 1.156932856575288,
      "grad_norm": 0.10563195496797562,
      "learning_rate": 0.00012316684378320934,
      "loss": 0.0465,
      "step": 1456
    },
    {
      "epoch": 1.1577274533174413,
      "grad_norm": 0.09234337508678436,
      "learning_rate": 0.00012311370882040382,
      "loss": 0.0466,
      "step": 1457
    },
    {
      "epoch": 1.1585220500595947,
      "grad_norm": 0.09709915518760681,
      "learning_rate": 0.0001230605738575983,
      "loss": 0.038,
      "step": 1458
    },
    {
      "epoch": 1.1593166468017482,
      "grad_norm": 0.45033395290374756,
      "learning_rate": 0.00012300743889479278,
      "loss": 0.2792,
      "step": 1459
    },
    {
      "epoch": 1.1601112435439014,
      "grad_norm": 0.7783058881759644,
      "learning_rate": 0.00012295430393198726,
      "loss": 0.4021,
      "step": 1460
    },
    {
      "epoch": 1.1609058402860548,
      "grad_norm": 0.5030250549316406,
      "learning_rate": 0.00012290116896918174,
      "loss": 0.4173,
      "step": 1461
    },
    {
      "epoch": 1.1617004370282082,
      "grad_norm": 0.5032122731208801,
      "learning_rate": 0.00012284803400637622,
      "loss": 0.3719,
      "step": 1462
    },
    {
      "epoch": 1.1624950337703615,
      "grad_norm": 0.38438844680786133,
      "learning_rate": 0.00012279489904357067,
      "loss": 0.2818,
      "step": 1463
    },
    {
      "epoch": 1.1632896305125149,
      "grad_norm": 0.35916587710380554,
      "learning_rate": 0.00012274176408076515,
      "loss": 0.2738,
      "step": 1464
    },
    {
      "epoch": 1.1640842272546683,
      "grad_norm": 0.32680806517601013,
      "learning_rate": 0.0001226886291179596,
      "loss": 0.2276,
      "step": 1465
    },
    {
      "epoch": 1.1648788239968215,
      "grad_norm": 0.3080199360847473,
      "learning_rate": 0.0001226354941551541,
      "loss": 0.2411,
      "step": 1466
    },
    {
      "epoch": 1.165673420738975,
      "grad_norm": 0.23585626482963562,
      "learning_rate": 0.00012258235919234857,
      "loss": 0.221,
      "step": 1467
    },
    {
      "epoch": 1.1664680174811284,
      "grad_norm": 0.23979654908180237,
      "learning_rate": 0.00012252922422954305,
      "loss": 0.2139,
      "step": 1468
    },
    {
      "epoch": 1.1672626142232816,
      "grad_norm": 0.24084970355033875,
      "learning_rate": 0.00012247608926673753,
      "loss": 0.1889,
      "step": 1469
    },
    {
      "epoch": 1.168057210965435,
      "grad_norm": 0.269894540309906,
      "learning_rate": 0.000122422954303932,
      "loss": 0.2093,
      "step": 1470
    },
    {
      "epoch": 1.1688518077075885,
      "grad_norm": 0.24497583508491516,
      "learning_rate": 0.00012236981934112646,
      "loss": 0.2085,
      "step": 1471
    },
    {
      "epoch": 1.1696464044497417,
      "grad_norm": 0.23008699715137482,
      "learning_rate": 0.00012231668437832094,
      "loss": 0.1663,
      "step": 1472
    },
    {
      "epoch": 1.170441001191895,
      "grad_norm": 0.2285328507423401,
      "learning_rate": 0.00012226354941551542,
      "loss": 0.1846,
      "step": 1473
    },
    {
      "epoch": 1.1712355979340485,
      "grad_norm": 0.19763939082622528,
      "learning_rate": 0.00012221041445270987,
      "loss": 0.1564,
      "step": 1474
    },
    {
      "epoch": 1.1720301946762017,
      "grad_norm": 0.23172350227832794,
      "learning_rate": 0.00012215727948990435,
      "loss": 0.179,
      "step": 1475
    },
    {
      "epoch": 1.1728247914183552,
      "grad_norm": 0.2543832063674927,
      "learning_rate": 0.00012210414452709883,
      "loss": 0.1796,
      "step": 1476
    },
    {
      "epoch": 1.1736193881605086,
      "grad_norm": 0.19087745249271393,
      "learning_rate": 0.00012205100956429331,
      "loss": 0.1458,
      "step": 1477
    },
    {
      "epoch": 1.174413984902662,
      "grad_norm": 0.20850227773189545,
      "learning_rate": 0.0001219978746014878,
      "loss": 0.1647,
      "step": 1478
    },
    {
      "epoch": 1.1752085816448152,
      "grad_norm": 0.23810802400112152,
      "learning_rate": 0.00012194473963868225,
      "loss": 0.1551,
      "step": 1479
    },
    {
      "epoch": 1.1760031783869687,
      "grad_norm": 0.1932220160961151,
      "learning_rate": 0.00012189160467587673,
      "loss": 0.1572,
      "step": 1480
    },
    {
      "epoch": 1.1767977751291219,
      "grad_norm": 0.17286504805088043,
      "learning_rate": 0.00012183846971307121,
      "loss": 0.1306,
      "step": 1481
    },
    {
      "epoch": 1.1775923718712753,
      "grad_norm": 0.17873713374137878,
      "learning_rate": 0.00012178533475026567,
      "loss": 0.1366,
      "step": 1482
    },
    {
      "epoch": 1.1783869686134287,
      "grad_norm": 0.19128242135047913,
      "learning_rate": 0.00012173219978746015,
      "loss": 0.1245,
      "step": 1483
    },
    {
      "epoch": 1.1791815653555822,
      "grad_norm": 0.15981166064739227,
      "learning_rate": 0.00012167906482465464,
      "loss": 0.1296,
      "step": 1484
    },
    {
      "epoch": 1.1799761620977354,
      "grad_norm": 0.19066596031188965,
      "learning_rate": 0.0001216259298618491,
      "loss": 0.1456,
      "step": 1485
    },
    {
      "epoch": 1.1807707588398888,
      "grad_norm": 0.18181578814983368,
      "learning_rate": 0.00012157279489904358,
      "loss": 0.1304,
      "step": 1486
    },
    {
      "epoch": 1.181565355582042,
      "grad_norm": 0.16915863752365112,
      "learning_rate": 0.00012151965993623803,
      "loss": 0.1048,
      "step": 1487
    },
    {
      "epoch": 1.1823599523241954,
      "grad_norm": 0.15410266816616058,
      "learning_rate": 0.00012146652497343252,
      "loss": 0.0939,
      "step": 1488
    },
    {
      "epoch": 1.1831545490663489,
      "grad_norm": 0.1649097204208374,
      "learning_rate": 0.000121413390010627,
      "loss": 0.1203,
      "step": 1489
    },
    {
      "epoch": 1.1839491458085023,
      "grad_norm": 0.13214807212352753,
      "learning_rate": 0.00012136025504782148,
      "loss": 0.1014,
      "step": 1490
    },
    {
      "epoch": 1.1847437425506555,
      "grad_norm": 0.12198685109615326,
      "learning_rate": 0.00012130712008501594,
      "loss": 0.0955,
      "step": 1491
    },
    {
      "epoch": 1.185538339292809,
      "grad_norm": 0.12705695629119873,
      "learning_rate": 0.00012125398512221042,
      "loss": 0.1004,
      "step": 1492
    },
    {
      "epoch": 1.1863329360349621,
      "grad_norm": 0.16360023617744446,
      "learning_rate": 0.0001212008501594049,
      "loss": 0.1131,
      "step": 1493
    },
    {
      "epoch": 1.1871275327771156,
      "grad_norm": 0.15332600474357605,
      "learning_rate": 0.00012114771519659938,
      "loss": 0.1002,
      "step": 1494
    },
    {
      "epoch": 1.187922129519269,
      "grad_norm": 0.14393369853496552,
      "learning_rate": 0.00012109458023379385,
      "loss": 0.1051,
      "step": 1495
    },
    {
      "epoch": 1.1887167262614224,
      "grad_norm": 0.11349387466907501,
      "learning_rate": 0.0001210414452709883,
      "loss": 0.0912,
      "step": 1496
    },
    {
      "epoch": 1.1895113230035756,
      "grad_norm": 0.12356997281312943,
      "learning_rate": 0.00012098831030818278,
      "loss": 0.0874,
      "step": 1497
    },
    {
      "epoch": 1.190305919745729,
      "grad_norm": 0.17533154785633087,
      "learning_rate": 0.00012093517534537726,
      "loss": 0.0875,
      "step": 1498
    },
    {
      "epoch": 1.1911005164878823,
      "grad_norm": 0.10799466073513031,
      "learning_rate": 0.00012088204038257174,
      "loss": 0.0773,
      "step": 1499
    },
    {
      "epoch": 1.1918951132300357,
      "grad_norm": 0.12254850566387177,
      "learning_rate": 0.00012082890541976621,
      "loss": 0.0778,
      "step": 1500
    },
    {
      "epoch": 1.1926897099721891,
      "grad_norm": 0.13085035979747772,
      "learning_rate": 0.00012077577045696069,
      "loss": 0.088,
      "step": 1501
    },
    {
      "epoch": 1.1934843067143426,
      "grad_norm": 0.09874401986598969,
      "learning_rate": 0.00012072263549415517,
      "loss": 0.0462,
      "step": 1502
    },
    {
      "epoch": 1.1942789034564958,
      "grad_norm": 0.09817245602607727,
      "learning_rate": 0.00012066950053134965,
      "loss": 0.0573,
      "step": 1503
    },
    {
      "epoch": 1.1950735001986492,
      "grad_norm": 0.07806755602359772,
      "learning_rate": 0.0001206163655685441,
      "loss": 0.0584,
      "step": 1504
    },
    {
      "epoch": 1.1958680969408024,
      "grad_norm": 0.08267638832330704,
      "learning_rate": 0.00012056323060573857,
      "loss": 0.0425,
      "step": 1505
    },
    {
      "epoch": 1.1966626936829559,
      "grad_norm": 0.08749381452798843,
      "learning_rate": 0.00012051009564293305,
      "loss": 0.0347,
      "step": 1506
    },
    {
      "epoch": 1.1974572904251093,
      "grad_norm": 0.0841686949133873,
      "learning_rate": 0.00012045696068012753,
      "loss": 0.0406,
      "step": 1507
    },
    {
      "epoch": 1.1982518871672627,
      "grad_norm": 0.07826800644397736,
      "learning_rate": 0.00012040382571732201,
      "loss": 0.0439,
      "step": 1508
    },
    {
      "epoch": 1.199046483909416,
      "grad_norm": 0.45710375905036926,
      "learning_rate": 0.00012035069075451648,
      "loss": 0.3052,
      "step": 1509
    },
    {
      "epoch": 1.1998410806515694,
      "grad_norm": 0.5765721201896667,
      "learning_rate": 0.00012029755579171096,
      "loss": 0.3982,
      "step": 1510
    },
    {
      "epoch": 1.1998410806515694,
      "eval_loss": 0.15971313416957855,
      "eval_runtime": 302.7946,
      "eval_samples_per_second": 3.151,
      "eval_steps_per_second": 1.05,
      "step": 1510
    },
    {
      "epoch": 1.2006356773937226,
      "grad_norm": 0.43556496500968933,
      "learning_rate": 0.00012024442082890544,
      "loss": 0.3279,
      "step": 1511
    },
    {
      "epoch": 1.201430274135876,
      "grad_norm": 0.3900793790817261,
      "learning_rate": 0.00012019128586609989,
      "loss": 0.3004,
      "step": 1512
    },
    {
      "epoch": 1.2022248708780294,
      "grad_norm": 0.3689850866794586,
      "learning_rate": 0.00012013815090329437,
      "loss": 0.2831,
      "step": 1513
    },
    {
      "epoch": 1.2030194676201829,
      "grad_norm": 0.3418510854244232,
      "learning_rate": 0.00012008501594048884,
      "loss": 0.2493,
      "step": 1514
    },
    {
      "epoch": 1.203814064362336,
      "grad_norm": 0.3010208010673523,
      "learning_rate": 0.00012003188097768332,
      "loss": 0.2176,
      "step": 1515
    },
    {
      "epoch": 1.2046086611044895,
      "grad_norm": 0.3023667633533478,
      "learning_rate": 0.0001199787460148778,
      "loss": 0.2442,
      "step": 1516
    },
    {
      "epoch": 1.2054032578466427,
      "grad_norm": 0.2821812927722931,
      "learning_rate": 0.00011992561105207228,
      "loss": 0.2186,
      "step": 1517
    },
    {
      "epoch": 1.2061978545887961,
      "grad_norm": 0.28297531604766846,
      "learning_rate": 0.00011987247608926674,
      "loss": 0.1983,
      "step": 1518
    },
    {
      "epoch": 1.2069924513309496,
      "grad_norm": 0.26990610361099243,
      "learning_rate": 0.00011981934112646122,
      "loss": 0.1956,
      "step": 1519
    },
    {
      "epoch": 1.207787048073103,
      "grad_norm": 0.2749321460723877,
      "learning_rate": 0.00011976620616365568,
      "loss": 0.1847,
      "step": 1520
    },
    {
      "epoch": 1.2085816448152562,
      "grad_norm": 0.2800828218460083,
      "learning_rate": 0.00011971307120085016,
      "loss": 0.1754,
      "step": 1521
    },
    {
      "epoch": 1.2093762415574096,
      "grad_norm": 0.25708380341529846,
      "learning_rate": 0.00011965993623804464,
      "loss": 0.1919,
      "step": 1522
    },
    {
      "epoch": 1.210170838299563,
      "grad_norm": 0.20212334394454956,
      "learning_rate": 0.0001196068012752391,
      "loss": 0.1374,
      "step": 1523
    },
    {
      "epoch": 1.2109654350417163,
      "grad_norm": 0.20243953168392181,
      "learning_rate": 0.00011955366631243359,
      "loss": 0.1448,
      "step": 1524
    },
    {
      "epoch": 1.2117600317838697,
      "grad_norm": 0.26122772693634033,
      "learning_rate": 0.00011950053134962807,
      "loss": 0.1597,
      "step": 1525
    },
    {
      "epoch": 1.2125546285260231,
      "grad_norm": 0.246907040476799,
      "learning_rate": 0.00011944739638682255,
      "loss": 0.167,
      "step": 1526
    },
    {
      "epoch": 1.2133492252681763,
      "grad_norm": 0.1875414401292801,
      "learning_rate": 0.00011939426142401701,
      "loss": 0.1515,
      "step": 1527
    },
    {
      "epoch": 1.2141438220103298,
      "grad_norm": 0.28128549456596375,
      "learning_rate": 0.00011934112646121148,
      "loss": 0.1652,
      "step": 1528
    },
    {
      "epoch": 1.2149384187524832,
      "grad_norm": 0.18976081907749176,
      "learning_rate": 0.00011928799149840595,
      "loss": 0.1259,
      "step": 1529
    },
    {
      "epoch": 1.2157330154946364,
      "grad_norm": 0.20986215770244598,
      "learning_rate": 0.00011923485653560043,
      "loss": 0.1449,
      "step": 1530
    },
    {
      "epoch": 1.2165276122367898,
      "grad_norm": 0.19042523205280304,
      "learning_rate": 0.0001191817215727949,
      "loss": 0.12,
      "step": 1531
    },
    {
      "epoch": 1.2173222089789433,
      "grad_norm": 0.15997692942619324,
      "learning_rate": 0.00011912858660998937,
      "loss": 0.121,
      "step": 1532
    },
    {
      "epoch": 1.2181168057210965,
      "grad_norm": 0.19278131425380707,
      "learning_rate": 0.00011907545164718385,
      "loss": 0.1303,
      "step": 1533
    },
    {
      "epoch": 1.21891140246325,
      "grad_norm": 0.20524218678474426,
      "learning_rate": 0.00011902231668437833,
      "loss": 0.1366,
      "step": 1534
    },
    {
      "epoch": 1.2197059992054033,
      "grad_norm": 0.15813596546649933,
      "learning_rate": 0.00011896918172157281,
      "loss": 0.1084,
      "step": 1535
    },
    {
      "epoch": 1.2205005959475566,
      "grad_norm": 0.1514078825712204,
      "learning_rate": 0.00011891604675876728,
      "loss": 0.1317,
      "step": 1536
    },
    {
      "epoch": 1.22129519268971,
      "grad_norm": 0.16152307391166687,
      "learning_rate": 0.00011886291179596175,
      "loss": 0.1049,
      "step": 1537
    },
    {
      "epoch": 1.2220897894318634,
      "grad_norm": 0.17751234769821167,
      "learning_rate": 0.00011880977683315621,
      "loss": 0.1167,
      "step": 1538
    },
    {
      "epoch": 1.2228843861740166,
      "grad_norm": 0.17400693893432617,
      "learning_rate": 0.00011875664187035069,
      "loss": 0.1353,
      "step": 1539
    },
    {
      "epoch": 1.22367898291617,
      "grad_norm": 0.2374996393918991,
      "learning_rate": 0.00011870350690754517,
      "loss": 0.1207,
      "step": 1540
    },
    {
      "epoch": 1.2244735796583235,
      "grad_norm": 0.1480349600315094,
      "learning_rate": 0.00011865037194473964,
      "loss": 0.1018,
      "step": 1541
    },
    {
      "epoch": 1.2252681764004767,
      "grad_norm": 0.14408716559410095,
      "learning_rate": 0.00011859723698193412,
      "loss": 0.097,
      "step": 1542
    },
    {
      "epoch": 1.2260627731426301,
      "grad_norm": 0.12044498324394226,
      "learning_rate": 0.0001185441020191286,
      "loss": 0.0948,
      "step": 1543
    },
    {
      "epoch": 1.2268573698847836,
      "grad_norm": 0.14475758373737335,
      "learning_rate": 0.00011849096705632308,
      "loss": 0.1054,
      "step": 1544
    },
    {
      "epoch": 1.2276519666269368,
      "grad_norm": 0.12327535450458527,
      "learning_rate": 0.00011843783209351753,
      "loss": 0.083,
      "step": 1545
    },
    {
      "epoch": 1.2284465633690902,
      "grad_norm": 0.13265539705753326,
      "learning_rate": 0.00011838469713071201,
      "loss": 0.0913,
      "step": 1546
    },
    {
      "epoch": 1.2292411601112436,
      "grad_norm": 0.11241400986909866,
      "learning_rate": 0.00011833156216790648,
      "loss": 0.0686,
      "step": 1547
    },
    {
      "epoch": 1.2300357568533968,
      "grad_norm": 0.1332373321056366,
      "learning_rate": 0.00011827842720510096,
      "loss": 0.1071,
      "step": 1548
    },
    {
      "epoch": 1.2308303535955503,
      "grad_norm": 0.11924038827419281,
      "learning_rate": 0.00011822529224229544,
      "loss": 0.0744,
      "step": 1549
    },
    {
      "epoch": 1.2316249503377037,
      "grad_norm": 0.12424436211585999,
      "learning_rate": 0.00011817215727948992,
      "loss": 0.0839,
      "step": 1550
    },
    {
      "epoch": 1.232419547079857,
      "grad_norm": 0.10307753831148148,
      "learning_rate": 0.00011811902231668439,
      "loss": 0.0731,
      "step": 1551
    },
    {
      "epoch": 1.2332141438220103,
      "grad_norm": 0.09323548525571823,
      "learning_rate": 0.00011806588735387887,
      "loss": 0.0552,
      "step": 1552
    },
    {
      "epoch": 1.2340087405641638,
      "grad_norm": 0.1206483244895935,
      "learning_rate": 0.00011801275239107332,
      "loss": 0.0539,
      "step": 1553
    },
    {
      "epoch": 1.234803337306317,
      "grad_norm": 0.10332068800926208,
      "learning_rate": 0.0001179596174282678,
      "loss": 0.0574,
      "step": 1554
    },
    {
      "epoch": 1.2355979340484704,
      "grad_norm": 0.09737995266914368,
      "learning_rate": 0.00011790648246546228,
      "loss": 0.0493,
      "step": 1555
    },
    {
      "epoch": 1.2363925307906238,
      "grad_norm": 0.08217529207468033,
      "learning_rate": 0.00011785334750265675,
      "loss": 0.0379,
      "step": 1556
    },
    {
      "epoch": 1.237187127532777,
      "grad_norm": 0.07649563997983932,
      "learning_rate": 0.00011780021253985123,
      "loss": 0.0389,
      "step": 1557
    },
    {
      "epoch": 1.2379817242749305,
      "grad_norm": 0.06905321776866913,
      "learning_rate": 0.00011774707757704571,
      "loss": 0.0379,
      "step": 1558
    },
    {
      "epoch": 1.238776321017084,
      "grad_norm": 0.4610266387462616,
      "learning_rate": 0.00011769394261424019,
      "loss": 0.2595,
      "step": 1559
    },
    {
      "epoch": 1.2395709177592371,
      "grad_norm": 0.5635904669761658,
      "learning_rate": 0.00011764080765143466,
      "loss": 0.4075,
      "step": 1560
    },
    {
      "epoch": 1.2403655145013905,
      "grad_norm": 0.4589340388774872,
      "learning_rate": 0.00011758767268862911,
      "loss": 0.34,
      "step": 1561
    },
    {
      "epoch": 1.241160111243544,
      "grad_norm": 0.4986928701400757,
      "learning_rate": 0.00011753453772582359,
      "loss": 0.3808,
      "step": 1562
    },
    {
      "epoch": 1.2419547079856972,
      "grad_norm": 0.3690356910228729,
      "learning_rate": 0.00011748140276301807,
      "loss": 0.3407,
      "step": 1563
    },
    {
      "epoch": 1.2427493047278506,
      "grad_norm": 0.3197893500328064,
      "learning_rate": 0.00011742826780021255,
      "loss": 0.2988,
      "step": 1564
    },
    {
      "epoch": 1.243543901470004,
      "grad_norm": 0.37584415078163147,
      "learning_rate": 0.00011737513283740702,
      "loss": 0.2777,
      "step": 1565
    },
    {
      "epoch": 1.2443384982121573,
      "grad_norm": 0.33653736114501953,
      "learning_rate": 0.0001173219978746015,
      "loss": 0.256,
      "step": 1566
    },
    {
      "epoch": 1.2451330949543107,
      "grad_norm": 0.28806188702583313,
      "learning_rate": 0.00011726886291179598,
      "loss": 0.2676,
      "step": 1567
    },
    {
      "epoch": 1.2459276916964641,
      "grad_norm": 0.2781263291835785,
      "learning_rate": 0.00011721572794899046,
      "loss": 0.248,
      "step": 1568
    },
    {
      "epoch": 1.2467222884386173,
      "grad_norm": 0.2531437575817108,
      "learning_rate": 0.00011716259298618491,
      "loss": 0.2104,
      "step": 1569
    },
    {
      "epoch": 1.2475168851807708,
      "grad_norm": 0.2716637849807739,
      "learning_rate": 0.00011710945802337938,
      "loss": 0.2304,
      "step": 1570
    },
    {
      "epoch": 1.2483114819229242,
      "grad_norm": 0.30235055088996887,
      "learning_rate": 0.00011705632306057386,
      "loss": 0.1921,
      "step": 1571
    },
    {
      "epoch": 1.2491060786650774,
      "grad_norm": 0.24232591688632965,
      "learning_rate": 0.00011700318809776834,
      "loss": 0.1918,
      "step": 1572
    },
    {
      "epoch": 1.2499006754072308,
      "grad_norm": 0.20054641366004944,
      "learning_rate": 0.00011695005313496282,
      "loss": 0.1794,
      "step": 1573
    },
    {
      "epoch": 1.2506952721493843,
      "grad_norm": 0.2960893511772156,
      "learning_rate": 0.00011689691817215728,
      "loss": 0.2305,
      "step": 1574
    },
    {
      "epoch": 1.2514898688915377,
      "grad_norm": 0.20979183912277222,
      "learning_rate": 0.00011684378320935176,
      "loss": 0.1677,
      "step": 1575
    },
    {
      "epoch": 1.252284465633691,
      "grad_norm": 0.20456713438034058,
      "learning_rate": 0.00011679064824654624,
      "loss": 0.1872,
      "step": 1576
    },
    {
      "epoch": 1.2530790623758443,
      "grad_norm": 0.196288600564003,
      "learning_rate": 0.0001167375132837407,
      "loss": 0.1671,
      "step": 1577
    },
    {
      "epoch": 1.2538736591179975,
      "grad_norm": 0.28711411356925964,
      "learning_rate": 0.00011668437832093518,
      "loss": 0.1754,
      "step": 1578
    },
    {
      "epoch": 1.254668255860151,
      "grad_norm": 0.19070471823215485,
      "learning_rate": 0.00011663124335812964,
      "loss": 0.1551,
      "step": 1579
    },
    {
      "epoch": 1.2554628526023044,
      "grad_norm": 0.19371972978115082,
      "learning_rate": 0.00011657810839532412,
      "loss": 0.1461,
      "step": 1580
    },
    {
      "epoch": 1.2562574493444578,
      "grad_norm": 0.21041350066661835,
      "learning_rate": 0.0001165249734325186,
      "loss": 0.1724,
      "step": 1581
    },
    {
      "epoch": 1.257052046086611,
      "grad_norm": 0.19195346534252167,
      "learning_rate": 0.00011647183846971308,
      "loss": 0.1436,
      "step": 1582
    },
    {
      "epoch": 1.2578466428287645,
      "grad_norm": 0.16908776760101318,
      "learning_rate": 0.00011641870350690755,
      "loss": 0.1212,
      "step": 1583
    },
    {
      "epoch": 1.2586412395709177,
      "grad_norm": 0.2284228503704071,
      "learning_rate": 0.00011636556854410203,
      "loss": 0.1319,
      "step": 1584
    },
    {
      "epoch": 1.259435836313071,
      "grad_norm": 0.16103722155094147,
      "learning_rate": 0.00011631243358129651,
      "loss": 0.1117,
      "step": 1585
    },
    {
      "epoch": 1.2602304330552245,
      "grad_norm": 0.20755325257778168,
      "learning_rate": 0.00011625929861849096,
      "loss": 0.1227,
      "step": 1586
    },
    {
      "epoch": 1.261025029797378,
      "grad_norm": 0.20230555534362793,
      "learning_rate": 0.00011620616365568544,
      "loss": 0.1263,
      "step": 1587
    },
    {
      "epoch": 1.2618196265395312,
      "grad_norm": 0.1507282257080078,
      "learning_rate": 0.00011615302869287991,
      "loss": 0.1107,
      "step": 1588
    },
    {
      "epoch": 1.2626142232816846,
      "grad_norm": 0.1744290441274643,
      "learning_rate": 0.00011609989373007439,
      "loss": 0.1222,
      "step": 1589
    },
    {
      "epoch": 1.2634088200238378,
      "grad_norm": 0.1901697814464569,
      "learning_rate": 0.00011604675876726887,
      "loss": 0.1196,
      "step": 1590
    },
    {
      "epoch": 1.2642034167659912,
      "grad_norm": 0.16412508487701416,
      "learning_rate": 0.00011599362380446335,
      "loss": 0.1131,
      "step": 1591
    },
    {
      "epoch": 1.2649980135081447,
      "grad_norm": 0.15796835720539093,
      "learning_rate": 0.00011594048884165782,
      "loss": 0.1074,
      "step": 1592
    },
    {
      "epoch": 1.265792610250298,
      "grad_norm": 0.17506921291351318,
      "learning_rate": 0.0001158873538788523,
      "loss": 0.1172,
      "step": 1593
    },
    {
      "epoch": 1.2665872069924513,
      "grad_norm": 0.13117974996566772,
      "learning_rate": 0.00011583421891604675,
      "loss": 0.0953,
      "step": 1594
    },
    {
      "epoch": 1.2673818037346047,
      "grad_norm": 0.13309122622013092,
      "learning_rate": 0.00011578108395324123,
      "loss": 0.0793,
      "step": 1595
    },
    {
      "epoch": 1.268176400476758,
      "grad_norm": 0.12717360258102417,
      "learning_rate": 0.00011572794899043571,
      "loss": 0.0695,
      "step": 1596
    },
    {
      "epoch": 1.2689709972189114,
      "grad_norm": 0.12112308293581009,
      "learning_rate": 0.00011567481402763019,
      "loss": 0.0748,
      "step": 1597
    },
    {
      "epoch": 1.2697655939610648,
      "grad_norm": 0.11357192695140839,
      "learning_rate": 0.00011562167906482466,
      "loss": 0.0827,
      "step": 1598
    },
    {
      "epoch": 1.2705601907032182,
      "grad_norm": 0.10081163048744202,
      "learning_rate": 0.00011556854410201914,
      "loss": 0.068,
      "step": 1599
    },
    {
      "epoch": 1.2713547874453714,
      "grad_norm": 0.10995761305093765,
      "learning_rate": 0.00011551540913921362,
      "loss": 0.0694,
      "step": 1600
    },
    {
      "epoch": 1.2721493841875249,
      "grad_norm": 0.10844412446022034,
      "learning_rate": 0.00011546227417640809,
      "loss": 0.0637,
      "step": 1601
    },
    {
      "epoch": 1.272943980929678,
      "grad_norm": 0.13303785026073456,
      "learning_rate": 0.00011540913921360255,
      "loss": 0.0867,
      "step": 1602
    },
    {
      "epoch": 1.2737385776718315,
      "grad_norm": 0.1118783950805664,
      "learning_rate": 0.00011535600425079702,
      "loss": 0.0607,
      "step": 1603
    },
    {
      "epoch": 1.274533174413985,
      "grad_norm": 0.09496083110570908,
      "learning_rate": 0.0001153028692879915,
      "loss": 0.0499,
      "step": 1604
    },
    {
      "epoch": 1.2753277711561384,
      "grad_norm": 0.08925227075815201,
      "learning_rate": 0.00011524973432518598,
      "loss": 0.0601,
      "step": 1605
    },
    {
      "epoch": 1.2761223678982916,
      "grad_norm": 0.10715141147375107,
      "learning_rate": 0.00011519659936238046,
      "loss": 0.0559,
      "step": 1606
    },
    {
      "epoch": 1.276916964640445,
      "grad_norm": 0.0893774926662445,
      "learning_rate": 0.00011514346439957493,
      "loss": 0.0391,
      "step": 1607
    },
    {
      "epoch": 1.2777115613825982,
      "grad_norm": 0.0881904810667038,
      "learning_rate": 0.0001150903294367694,
      "loss": 0.04,
      "step": 1608
    },
    {
      "epoch": 1.2785061581247517,
      "grad_norm": 0.4409450590610504,
      "learning_rate": 0.00011503719447396389,
      "loss": 0.324,
      "step": 1609
    },
    {
      "epoch": 1.279300754866905,
      "grad_norm": 0.6632673740386963,
      "learning_rate": 0.00011498405951115834,
      "loss": 0.417,
      "step": 1610
    },
    {
      "epoch": 1.2800953516090585,
      "grad_norm": 0.5054203867912292,
      "learning_rate": 0.00011493092454835282,
      "loss": 0.33,
      "step": 1611
    },
    {
      "epoch": 1.2808899483512117,
      "grad_norm": 0.4651276767253876,
      "learning_rate": 0.00011487778958554729,
      "loss": 0.2979,
      "step": 1612
    },
    {
      "epoch": 1.2816845450933652,
      "grad_norm": 0.4695003926753998,
      "learning_rate": 0.00011482465462274177,
      "loss": 0.3144,
      "step": 1613
    },
    {
      "epoch": 1.2824791418355184,
      "grad_norm": 0.3952433168888092,
      "learning_rate": 0.00011477151965993625,
      "loss": 0.3016,
      "step": 1614
    },
    {
      "epoch": 1.2832737385776718,
      "grad_norm": 0.3369606137275696,
      "learning_rate": 0.00011471838469713073,
      "loss": 0.2597,
      "step": 1615
    },
    {
      "epoch": 1.2840683353198252,
      "grad_norm": 0.2789277732372284,
      "learning_rate": 0.0001146652497343252,
      "loss": 0.2367,
      "step": 1616
    },
    {
      "epoch": 1.2848629320619787,
      "grad_norm": 0.29508283734321594,
      "learning_rate": 0.00011461211477151967,
      "loss": 0.2548,
      "step": 1617
    },
    {
      "epoch": 1.2856575288041319,
      "grad_norm": 0.26962634921073914,
      "learning_rate": 0.00011455897980871413,
      "loss": 0.261,
      "step": 1618
    },
    {
      "epoch": 1.2864521255462853,
      "grad_norm": 0.24681958556175232,
      "learning_rate": 0.00011450584484590861,
      "loss": 0.1885,
      "step": 1619
    },
    {
      "epoch": 1.2872467222884385,
      "grad_norm": 0.25412750244140625,
      "learning_rate": 0.00011445270988310309,
      "loss": 0.2121,
      "step": 1620
    },
    {
      "epoch": 1.288041319030592,
      "grad_norm": 0.3293687701225281,
      "learning_rate": 0.00011439957492029755,
      "loss": 0.2139,
      "step": 1621
    },
    {
      "epoch": 1.2888359157727454,
      "grad_norm": 0.240535706281662,
      "learning_rate": 0.00011434643995749203,
      "loss": 0.1762,
      "step": 1622
    },
    {
      "epoch": 1.2896305125148988,
      "grad_norm": 0.21313384175300598,
      "learning_rate": 0.00011429330499468651,
      "loss": 0.1574,
      "step": 1623
    },
    {
      "epoch": 1.290425109257052,
      "grad_norm": 0.23805077373981476,
      "learning_rate": 0.000114240170031881,
      "loss": 0.2002,
      "step": 1624
    },
    {
      "epoch": 1.2912197059992054,
      "grad_norm": 0.19681723415851593,
      "learning_rate": 0.00011418703506907546,
      "loss": 0.1531,
      "step": 1625
    },
    {
      "epoch": 1.2920143027413586,
      "grad_norm": 0.19506332278251648,
      "learning_rate": 0.00011413390010626994,
      "loss": 0.1749,
      "step": 1626
    },
    {
      "epoch": 1.292808899483512,
      "grad_norm": 0.1766539216041565,
      "learning_rate": 0.0001140807651434644,
      "loss": 0.1343,
      "step": 1627
    },
    {
      "epoch": 1.2936034962256655,
      "grad_norm": 0.18386349081993103,
      "learning_rate": 0.00011402763018065887,
      "loss": 0.1304,
      "step": 1628
    },
    {
      "epoch": 1.294398092967819,
      "grad_norm": 0.16991198062896729,
      "learning_rate": 0.00011397449521785335,
      "loss": 0.137,
      "step": 1629
    },
    {
      "epoch": 1.2951926897099721,
      "grad_norm": 0.16081738471984863,
      "learning_rate": 0.00011392136025504782,
      "loss": 0.1121,
      "step": 1630
    },
    {
      "epoch": 1.2959872864521256,
      "grad_norm": 0.18396033346652985,
      "learning_rate": 0.0001138682252922423,
      "loss": 0.1259,
      "step": 1631
    },
    {
      "epoch": 1.2967818831942788,
      "grad_norm": 0.21394944190979004,
      "learning_rate": 0.00011381509032943678,
      "loss": 0.1367,
      "step": 1632
    },
    {
      "epoch": 1.2975764799364322,
      "grad_norm": 0.15169495344161987,
      "learning_rate": 0.00011376195536663126,
      "loss": 0.1067,
      "step": 1633
    },
    {
      "epoch": 1.2983710766785856,
      "grad_norm": 0.17540723085403442,
      "learning_rate": 0.00011370882040382573,
      "loss": 0.1117,
      "step": 1634
    },
    {
      "epoch": 1.299165673420739,
      "grad_norm": 0.1582397073507309,
      "learning_rate": 0.00011365568544102018,
      "loss": 0.1088,
      "step": 1635
    },
    {
      "epoch": 1.2999602701628923,
      "grad_norm": 0.16747920215129852,
      "learning_rate": 0.00011360255047821466,
      "loss": 0.1145,
      "step": 1636
    },
    {
      "epoch": 1.3007548669050457,
      "grad_norm": 0.16025030612945557,
      "learning_rate": 0.00011354941551540914,
      "loss": 0.1116,
      "step": 1637
    },
    {
      "epoch": 1.301549463647199,
      "grad_norm": 0.1727427840232849,
      "learning_rate": 0.00011349628055260362,
      "loss": 0.1117,
      "step": 1638
    },
    {
      "epoch": 1.3023440603893524,
      "grad_norm": 0.15802563726902008,
      "learning_rate": 0.00011344314558979809,
      "loss": 0.0916,
      "step": 1639
    },
    {
      "epoch": 1.3031386571315058,
      "grad_norm": 0.1772061288356781,
      "learning_rate": 0.00011339001062699257,
      "loss": 0.1211,
      "step": 1640
    },
    {
      "epoch": 1.3039332538736592,
      "grad_norm": 0.13943228125572205,
      "learning_rate": 0.00011333687566418705,
      "loss": 0.0865,
      "step": 1641
    },
    {
      "epoch": 1.3047278506158124,
      "grad_norm": 0.1439262181520462,
      "learning_rate": 0.00011328374070138153,
      "loss": 0.0989,
      "step": 1642
    },
    {
      "epoch": 1.3055224473579659,
      "grad_norm": 0.11793555319309235,
      "learning_rate": 0.00011323060573857598,
      "loss": 0.0802,
      "step": 1643
    },
    {
      "epoch": 1.306317044100119,
      "grad_norm": 0.12729735672473907,
      "learning_rate": 0.00011317747077577045,
      "loss": 0.0804,
      "step": 1644
    },
    {
      "epoch": 1.3071116408422725,
      "grad_norm": 0.11363290995359421,
      "learning_rate": 0.00011312433581296493,
      "loss": 0.0723,
      "step": 1645
    },
    {
      "epoch": 1.307906237584426,
      "grad_norm": 0.13019709289073944,
      "learning_rate": 0.00011307120085015941,
      "loss": 0.0762,
      "step": 1646
    },
    {
      "epoch": 1.3087008343265794,
      "grad_norm": 0.14283408224582672,
      "learning_rate": 0.00011301806588735389,
      "loss": 0.0778,
      "step": 1647
    },
    {
      "epoch": 1.3094954310687326,
      "grad_norm": 0.10365277528762817,
      "learning_rate": 0.00011296493092454836,
      "loss": 0.0751,
      "step": 1648
    },
    {
      "epoch": 1.310290027810886,
      "grad_norm": 0.09723062068223953,
      "learning_rate": 0.00011291179596174284,
      "loss": 0.0702,
      "step": 1649
    },
    {
      "epoch": 1.3110846245530392,
      "grad_norm": 0.10146412253379822,
      "learning_rate": 0.00011285866099893732,
      "loss": 0.0661,
      "step": 1650
    },
    {
      "epoch": 1.3118792212951926,
      "grad_norm": 0.11126869171857834,
      "learning_rate": 0.00011280552603613177,
      "loss": 0.0685,
      "step": 1651
    },
    {
      "epoch": 1.312673818037346,
      "grad_norm": 0.0974181517958641,
      "learning_rate": 0.00011275239107332625,
      "loss": 0.0543,
      "step": 1652
    },
    {
      "epoch": 1.3134684147794995,
      "grad_norm": 0.10330390185117722,
      "learning_rate": 0.00011269925611052073,
      "loss": 0.0615,
      "step": 1653
    },
    {
      "epoch": 1.3142630115216527,
      "grad_norm": 0.11038700491189957,
      "learning_rate": 0.0001126461211477152,
      "loss": 0.0617,
      "step": 1654
    },
    {
      "epoch": 1.3150576082638061,
      "grad_norm": 0.07386607676744461,
      "learning_rate": 0.00011259298618490968,
      "loss": 0.0454,
      "step": 1655
    },
    {
      "epoch": 1.3158522050059593,
      "grad_norm": 0.09047484397888184,
      "learning_rate": 0.00011253985122210416,
      "loss": 0.0464,
      "step": 1656
    },
    {
      "epoch": 1.3166468017481128,
      "grad_norm": 0.08799221366643906,
      "learning_rate": 0.00011248671625929862,
      "loss": 0.0372,
      "step": 1657
    },
    {
      "epoch": 1.3174413984902662,
      "grad_norm": 0.07908966392278671,
      "learning_rate": 0.0001124335812964931,
      "loss": 0.0409,
      "step": 1658
    },
    {
      "epoch": 1.3182359952324196,
      "grad_norm": 0.42458364367485046,
      "learning_rate": 0.00011238044633368756,
      "loss": 0.3009,
      "step": 1659
    },
    {
      "epoch": 1.3190305919745728,
      "grad_norm": 0.5000807046890259,
      "learning_rate": 0.00011232731137088204,
      "loss": 0.3527,
      "step": 1660
    },
    {
      "epoch": 1.3198251887167263,
      "grad_norm": 0.42015185952186584,
      "learning_rate": 0.00011227417640807652,
      "loss": 0.2905,
      "step": 1661
    },
    {
      "epoch": 1.3206197854588795,
      "grad_norm": 0.41084882616996765,
      "learning_rate": 0.000112221041445271,
      "loss": 0.3565,
      "step": 1662
    },
    {
      "epoch": 1.321414382201033,
      "grad_norm": 0.4015779197216034,
      "learning_rate": 0.00011216790648246546,
      "loss": 0.2982,
      "step": 1663
    },
    {
      "epoch": 1.3222089789431863,
      "grad_norm": 0.30499720573425293,
      "learning_rate": 0.00011211477151965994,
      "loss": 0.2558,
      "step": 1664
    },
    {
      "epoch": 1.3230035756853398,
      "grad_norm": 0.32485806941986084,
      "learning_rate": 0.00011206163655685442,
      "loss": 0.2372,
      "step": 1665
    },
    {
      "epoch": 1.323798172427493,
      "grad_norm": 0.27566227316856384,
      "learning_rate": 0.00011200850159404889,
      "loss": 0.2491,
      "step": 1666
    },
    {
      "epoch": 1.3245927691696464,
      "grad_norm": 0.2724522352218628,
      "learning_rate": 0.00011195536663124336,
      "loss": 0.2264,
      "step": 1667
    },
    {
      "epoch": 1.3253873659117998,
      "grad_norm": 0.22583793103694916,
      "learning_rate": 0.00011190223166843782,
      "loss": 0.2185,
      "step": 1668
    },
    {
      "epoch": 1.326181962653953,
      "grad_norm": 0.2370501011610031,
      "learning_rate": 0.0001118490967056323,
      "loss": 0.1874,
      "step": 1669
    },
    {
      "epoch": 1.3269765593961065,
      "grad_norm": 0.20729133486747742,
      "learning_rate": 0.00011179596174282678,
      "loss": 0.2013,
      "step": 1670
    },
    {
      "epoch": 1.32777115613826,
      "grad_norm": 0.18496668338775635,
      "learning_rate": 0.00011174282678002127,
      "loss": 0.1665,
      "step": 1671
    },
    {
      "epoch": 1.3285657528804131,
      "grad_norm": 0.19641734659671783,
      "learning_rate": 0.00011168969181721573,
      "loss": 0.163,
      "step": 1672
    },
    {
      "epoch": 1.3293603496225666,
      "grad_norm": 0.1845356523990631,
      "learning_rate": 0.00011163655685441021,
      "loss": 0.1544,
      "step": 1673
    },
    {
      "epoch": 1.33015494636472,
      "grad_norm": 0.19416415691375732,
      "learning_rate": 0.00011158342189160469,
      "loss": 0.1555,
      "step": 1674
    },
    {
      "epoch": 1.3309495431068732,
      "grad_norm": 0.21190685033798218,
      "learning_rate": 0.00011153028692879917,
      "loss": 0.1523,
      "step": 1675
    },
    {
      "epoch": 1.3317441398490266,
      "grad_norm": 0.20596079528331757,
      "learning_rate": 0.00011147715196599363,
      "loss": 0.1662,
      "step": 1676
    },
    {
      "epoch": 1.33253873659118,
      "grad_norm": 0.1556750237941742,
      "learning_rate": 0.00011142401700318809,
      "loss": 0.1446,
      "step": 1677
    },
    {
      "epoch": 1.3333333333333333,
      "grad_norm": 0.2119244784116745,
      "learning_rate": 0.00011137088204038257,
      "loss": 0.1431,
      "step": 1678
    },
    {
      "epoch": 1.3341279300754867,
      "grad_norm": 0.17626513540744781,
      "learning_rate": 0.00011131774707757705,
      "loss": 0.1519,
      "step": 1679
    },
    {
      "epoch": 1.3349225268176401,
      "grad_norm": 0.21521155536174774,
      "learning_rate": 0.00011126461211477153,
      "loss": 0.17,
      "step": 1680
    },
    {
      "epoch": 1.3357171235597933,
      "grad_norm": 0.186436265707016,
      "learning_rate": 0.000111211477151966,
      "loss": 0.1431,
      "step": 1681
    },
    {
      "epoch": 1.3365117203019468,
      "grad_norm": 0.16064102947711945,
      "learning_rate": 0.00011115834218916048,
      "loss": 0.1189,
      "step": 1682
    },
    {
      "epoch": 1.3373063170441002,
      "grad_norm": 0.2296784371137619,
      "learning_rate": 0.00011110520722635496,
      "loss": 0.1217,
      "step": 1683
    },
    {
      "epoch": 1.3381009137862534,
      "grad_norm": 0.16878163814544678,
      "learning_rate": 0.00011105207226354941,
      "loss": 0.1109,
      "step": 1684
    },
    {
      "epoch": 1.3388955105284068,
      "grad_norm": 0.16485919058322906,
      "learning_rate": 0.00011099893730074389,
      "loss": 0.1179,
      "step": 1685
    },
    {
      "epoch": 1.3396901072705603,
      "grad_norm": 0.16926351189613342,
      "learning_rate": 0.00011094580233793836,
      "loss": 0.1164,
      "step": 1686
    },
    {
      "epoch": 1.3404847040127135,
      "grad_norm": 0.14267680048942566,
      "learning_rate": 0.00011089266737513284,
      "loss": 0.0975,
      "step": 1687
    },
    {
      "epoch": 1.341279300754867,
      "grad_norm": 0.15051084756851196,
      "learning_rate": 0.00011083953241232732,
      "loss": 0.1085,
      "step": 1688
    },
    {
      "epoch": 1.3420738974970203,
      "grad_norm": 0.1536230891942978,
      "learning_rate": 0.0001107863974495218,
      "loss": 0.1018,
      "step": 1689
    },
    {
      "epoch": 1.3428684942391735,
      "grad_norm": 0.14714539051055908,
      "learning_rate": 0.00011073326248671627,
      "loss": 0.1035,
      "step": 1690
    },
    {
      "epoch": 1.343663090981327,
      "grad_norm": 0.12971211969852448,
      "learning_rate": 0.00011068012752391075,
      "loss": 0.0938,
      "step": 1691
    },
    {
      "epoch": 1.3444576877234804,
      "grad_norm": 0.13334442675113678,
      "learning_rate": 0.0001106269925611052,
      "loss": 0.083,
      "step": 1692
    },
    {
      "epoch": 1.3452522844656336,
      "grad_norm": 0.15938831865787506,
      "learning_rate": 0.00011057385759829968,
      "loss": 0.1091,
      "step": 1693
    },
    {
      "epoch": 1.346046881207787,
      "grad_norm": 0.11024398356676102,
      "learning_rate": 0.00011052072263549416,
      "loss": 0.0708,
      "step": 1694
    },
    {
      "epoch": 1.3468414779499405,
      "grad_norm": 0.11323071271181107,
      "learning_rate": 0.00011046758767268863,
      "loss": 0.0779,
      "step": 1695
    },
    {
      "epoch": 1.347636074692094,
      "grad_norm": 0.10548748075962067,
      "learning_rate": 0.00011041445270988311,
      "loss": 0.0746,
      "step": 1696
    },
    {
      "epoch": 1.348430671434247,
      "grad_norm": 0.12813059985637665,
      "learning_rate": 0.00011036131774707759,
      "loss": 0.0851,
      "step": 1697
    },
    {
      "epoch": 1.3492252681764005,
      "grad_norm": 0.12368360161781311,
      "learning_rate": 0.00011030818278427207,
      "loss": 0.0742,
      "step": 1698
    },
    {
      "epoch": 1.3500198649185537,
      "grad_norm": 0.12329541146755219,
      "learning_rate": 0.00011025504782146653,
      "loss": 0.0705,
      "step": 1699
    },
    {
      "epoch": 1.3508144616607072,
      "grad_norm": 0.12510350346565247,
      "learning_rate": 0.00011020191285866099,
      "loss": 0.0718,
      "step": 1700
    },
    {
      "epoch": 1.3516090584028606,
      "grad_norm": 0.14573241770267487,
      "learning_rate": 0.00011014877789585547,
      "loss": 0.0784,
      "step": 1701
    },
    {
      "epoch": 1.352403655145014,
      "grad_norm": 0.11333685368299484,
      "learning_rate": 0.00011009564293304995,
      "loss": 0.0515,
      "step": 1702
    },
    {
      "epoch": 1.3531982518871672,
      "grad_norm": 0.09039903432130814,
      "learning_rate": 0.00011004250797024443,
      "loss": 0.0516,
      "step": 1703
    },
    {
      "epoch": 1.3539928486293207,
      "grad_norm": 0.0886264517903328,
      "learning_rate": 0.0001099893730074389,
      "loss": 0.0522,
      "step": 1704
    },
    {
      "epoch": 1.3547874453714739,
      "grad_norm": 0.09716791659593582,
      "learning_rate": 0.00010993623804463337,
      "loss": 0.0483,
      "step": 1705
    },
    {
      "epoch": 1.3555820421136273,
      "grad_norm": 0.07099860906600952,
      "learning_rate": 0.00010988310308182785,
      "loss": 0.0384,
      "step": 1706
    },
    {
      "epoch": 1.3563766388557807,
      "grad_norm": 0.12479002773761749,
      "learning_rate": 0.00010982996811902234,
      "loss": 0.0442,
      "step": 1707
    },
    {
      "epoch": 1.3571712355979342,
      "grad_norm": 0.13122111558914185,
      "learning_rate": 0.00010977683315621679,
      "loss": 0.0489,
      "step": 1708
    },
    {
      "epoch": 1.3579658323400874,
      "grad_norm": 0.4334108233451843,
      "learning_rate": 0.00010972369819341127,
      "loss": 0.3095,
      "step": 1709
    },
    {
      "epoch": 1.3587604290822408,
      "grad_norm": 0.526380181312561,
      "learning_rate": 0.00010967056323060574,
      "loss": 0.4398,
      "step": 1710
    },
    {
      "epoch": 1.359555025824394,
      "grad_norm": 0.46928346157073975,
      "learning_rate": 0.00010961742826780022,
      "loss": 0.3763,
      "step": 1711
    },
    {
      "epoch": 1.3603496225665475,
      "grad_norm": 0.42142874002456665,
      "learning_rate": 0.0001095642933049947,
      "loss": 0.343,
      "step": 1712
    },
    {
      "epoch": 1.3611442193087009,
      "grad_norm": 0.6466464400291443,
      "learning_rate": 0.00010951115834218916,
      "loss": 0.2562,
      "step": 1713
    },
    {
      "epoch": 1.3619388160508543,
      "grad_norm": 0.3438433110713959,
      "learning_rate": 0.00010945802337938364,
      "loss": 0.2888,
      "step": 1714
    },
    {
      "epoch": 1.3627334127930075,
      "grad_norm": 0.2664477527141571,
      "learning_rate": 0.00010940488841657812,
      "loss": 0.2622,
      "step": 1715
    },
    {
      "epoch": 1.363528009535161,
      "grad_norm": 0.27775564789772034,
      "learning_rate": 0.0001093517534537726,
      "loss": 0.2562,
      "step": 1716
    },
    {
      "epoch": 1.3643226062773142,
      "grad_norm": 0.25942283868789673,
      "learning_rate": 0.00010929861849096706,
      "loss": 0.2208,
      "step": 1717
    },
    {
      "epoch": 1.3651172030194676,
      "grad_norm": 0.2890969514846802,
      "learning_rate": 0.00010924548352816154,
      "loss": 0.1997,
      "step": 1718
    },
    {
      "epoch": 1.365911799761621,
      "grad_norm": 0.3193235695362091,
      "learning_rate": 0.000109192348565356,
      "loss": 0.2297,
      "step": 1719
    },
    {
      "epoch": 1.3667063965037745,
      "grad_norm": 0.27248504757881165,
      "learning_rate": 0.00010913921360255048,
      "loss": 0.2087,
      "step": 1720
    },
    {
      "epoch": 1.3675009932459277,
      "grad_norm": 0.21848753094673157,
      "learning_rate": 0.00010908607863974496,
      "loss": 0.165,
      "step": 1721
    },
    {
      "epoch": 1.368295589988081,
      "grad_norm": 0.22435247898101807,
      "learning_rate": 0.00010903294367693944,
      "loss": 0.1739,
      "step": 1722
    },
    {
      "epoch": 1.3690901867302343,
      "grad_norm": 0.22648680210113525,
      "learning_rate": 0.00010897980871413391,
      "loss": 0.173,
      "step": 1723
    },
    {
      "epoch": 1.3698847834723877,
      "grad_norm": 0.18352875113487244,
      "learning_rate": 0.00010892667375132839,
      "loss": 0.1344,
      "step": 1724
    },
    {
      "epoch": 1.3706793802145412,
      "grad_norm": 0.21859878301620483,
      "learning_rate": 0.00010887353878852284,
      "loss": 0.1422,
      "step": 1725
    },
    {
      "epoch": 1.3714739769566946,
      "grad_norm": 0.1636037975549698,
      "learning_rate": 0.00010882040382571732,
      "loss": 0.1424,
      "step": 1726
    },
    {
      "epoch": 1.3722685736988478,
      "grad_norm": 0.19983558356761932,
      "learning_rate": 0.0001087672688629118,
      "loss": 0.1368,
      "step": 1727
    },
    {
      "epoch": 1.3730631704410012,
      "grad_norm": 0.22745750844478607,
      "learning_rate": 0.00010871413390010627,
      "loss": 0.1554,
      "step": 1728
    },
    {
      "epoch": 1.3738577671831544,
      "grad_norm": 0.18748386204242706,
      "learning_rate": 0.00010866099893730075,
      "loss": 0.1397,
      "step": 1729
    },
    {
      "epoch": 1.3746523639253079,
      "grad_norm": 0.17161758244037628,
      "learning_rate": 0.00010860786397449523,
      "loss": 0.1204,
      "step": 1730
    },
    {
      "epoch": 1.3754469606674613,
      "grad_norm": 0.17384256422519684,
      "learning_rate": 0.00010855472901168971,
      "loss": 0.1075,
      "step": 1731
    },
    {
      "epoch": 1.3762415574096147,
      "grad_norm": 0.2108636349439621,
      "learning_rate": 0.00010850159404888418,
      "loss": 0.1316,
      "step": 1732
    },
    {
      "epoch": 1.377036154151768,
      "grad_norm": 0.16475705802440643,
      "learning_rate": 0.00010844845908607863,
      "loss": 0.1141,
      "step": 1733
    },
    {
      "epoch": 1.3778307508939214,
      "grad_norm": 0.16076260805130005,
      "learning_rate": 0.00010839532412327311,
      "loss": 0.1286,
      "step": 1734
    },
    {
      "epoch": 1.3786253476360746,
      "grad_norm": 0.16888277232646942,
      "learning_rate": 0.00010834218916046759,
      "loss": 0.1184,
      "step": 1735
    },
    {
      "epoch": 1.379419944378228,
      "grad_norm": 0.1696004420518875,
      "learning_rate": 0.00010828905419766207,
      "loss": 0.114,
      "step": 1736
    },
    {
      "epoch": 1.3802145411203814,
      "grad_norm": 0.14430823922157288,
      "learning_rate": 0.00010823591923485654,
      "loss": 0.0929,
      "step": 1737
    },
    {
      "epoch": 1.3810091378625349,
      "grad_norm": 0.1362844556570053,
      "learning_rate": 0.00010818278427205102,
      "loss": 0.0865,
      "step": 1738
    },
    {
      "epoch": 1.381803734604688,
      "grad_norm": 0.19616185128688812,
      "learning_rate": 0.0001081296493092455,
      "loss": 0.123,
      "step": 1739
    },
    {
      "epoch": 1.3825983313468415,
      "grad_norm": 0.16481471061706543,
      "learning_rate": 0.00010807651434643998,
      "loss": 0.1254,
      "step": 1740
    },
    {
      "epoch": 1.3833929280889947,
      "grad_norm": 0.14392003417015076,
      "learning_rate": 0.00010802337938363443,
      "loss": 0.0947,
      "step": 1741
    },
    {
      "epoch": 1.3841875248311482,
      "grad_norm": 0.10880153626203537,
      "learning_rate": 0.0001079702444208289,
      "loss": 0.0865,
      "step": 1742
    },
    {
      "epoch": 1.3849821215733016,
      "grad_norm": 0.10815045982599258,
      "learning_rate": 0.00010791710945802338,
      "loss": 0.0781,
      "step": 1743
    },
    {
      "epoch": 1.385776718315455,
      "grad_norm": 0.1291939914226532,
      "learning_rate": 0.00010786397449521786,
      "loss": 0.0779,
      "step": 1744
    },
    {
      "epoch": 1.3865713150576082,
      "grad_norm": 0.09646078944206238,
      "learning_rate": 0.00010781083953241234,
      "loss": 0.0658,
      "step": 1745
    },
    {
      "epoch": 1.3873659117997617,
      "grad_norm": 0.12187962234020233,
      "learning_rate": 0.0001077577045696068,
      "loss": 0.0921,
      "step": 1746
    },
    {
      "epoch": 1.3881605085419149,
      "grad_norm": 0.0993303582072258,
      "learning_rate": 0.00010770456960680129,
      "loss": 0.0676,
      "step": 1747
    },
    {
      "epoch": 1.3889551052840683,
      "grad_norm": 0.10383143275976181,
      "learning_rate": 0.00010765143464399577,
      "loss": 0.0614,
      "step": 1748
    },
    {
      "epoch": 1.3897497020262217,
      "grad_norm": 0.11849558353424072,
      "learning_rate": 0.00010759829968119022,
      "loss": 0.0696,
      "step": 1749
    },
    {
      "epoch": 1.3905442987683752,
      "grad_norm": 0.09457309544086456,
      "learning_rate": 0.0001075451647183847,
      "loss": 0.0607,
      "step": 1750
    },
    {
      "epoch": 1.3913388955105284,
      "grad_norm": 0.11822275817394257,
      "learning_rate": 0.00010749202975557917,
      "loss": 0.0659,
      "step": 1751
    },
    {
      "epoch": 1.3921334922526818,
      "grad_norm": 0.0639188215136528,
      "learning_rate": 0.00010743889479277365,
      "loss": 0.0494,
      "step": 1752
    },
    {
      "epoch": 1.392928088994835,
      "grad_norm": 0.0885019302368164,
      "learning_rate": 0.00010738575982996813,
      "loss": 0.0577,
      "step": 1753
    },
    {
      "epoch": 1.3937226857369884,
      "grad_norm": 0.3244626224040985,
      "learning_rate": 0.0001073326248671626,
      "loss": 0.0526,
      "step": 1754
    },
    {
      "epoch": 1.3945172824791419,
      "grad_norm": 0.09691137075424194,
      "learning_rate": 0.00010727948990435707,
      "loss": 0.0441,
      "step": 1755
    },
    {
      "epoch": 1.3953118792212953,
      "grad_norm": 0.09399890154600143,
      "learning_rate": 0.00010722635494155155,
      "loss": 0.0439,
      "step": 1756
    },
    {
      "epoch": 1.3961064759634485,
      "grad_norm": 0.09666115045547485,
      "learning_rate": 0.000107173219978746,
      "loss": 0.0448,
      "step": 1757
    },
    {
      "epoch": 1.396901072705602,
      "grad_norm": 0.08425918221473694,
      "learning_rate": 0.00010712008501594049,
      "loss": 0.0394,
      "step": 1758
    },
    {
      "epoch": 1.3976956694477551,
      "grad_norm": 0.44855180382728577,
      "learning_rate": 0.00010706695005313497,
      "loss": 0.3159,
      "step": 1759
    },
    {
      "epoch": 1.3984902661899086,
      "grad_norm": 0.5717088580131531,
      "learning_rate": 0.00010701381509032943,
      "loss": 0.4912,
      "step": 1760
    },
    {
      "epoch": 1.399284862932062,
      "grad_norm": 0.497831255197525,
      "learning_rate": 0.00010696068012752391,
      "loss": 0.3507,
      "step": 1761
    },
    {
      "epoch": 1.4000794596742154,
      "grad_norm": 0.3752988576889038,
      "learning_rate": 0.00010690754516471839,
      "loss": 0.3088,
      "step": 1762
    },
    {
      "epoch": 1.4008740564163686,
      "grad_norm": 0.38759011030197144,
      "learning_rate": 0.00010685441020191287,
      "loss": 0.3104,
      "step": 1763
    },
    {
      "epoch": 1.401668653158522,
      "grad_norm": 0.3546904921531677,
      "learning_rate": 0.00010680127523910734,
      "loss": 0.2902,
      "step": 1764
    },
    {
      "epoch": 1.4024632499006753,
      "grad_norm": 0.3169706463813782,
      "learning_rate": 0.00010674814027630182,
      "loss": 0.2786,
      "step": 1765
    },
    {
      "epoch": 1.4032578466428287,
      "grad_norm": 0.3209465742111206,
      "learning_rate": 0.00010669500531349627,
      "loss": 0.2642,
      "step": 1766
    },
    {
      "epoch": 1.4040524433849821,
      "grad_norm": 0.28920823335647583,
      "learning_rate": 0.00010664187035069075,
      "loss": 0.2324,
      "step": 1767
    },
    {
      "epoch": 1.4048470401271356,
      "grad_norm": 0.2662298083305359,
      "learning_rate": 0.00010658873538788523,
      "loss": 0.2295,
      "step": 1768
    },
    {
      "epoch": 1.4056416368692888,
      "grad_norm": 0.2760794758796692,
      "learning_rate": 0.0001065356004250797,
      "loss": 0.2324,
      "step": 1769
    },
    {
      "epoch": 1.4064362336114422,
      "grad_norm": 0.21915753185749054,
      "learning_rate": 0.00010648246546227418,
      "loss": 0.1998,
      "step": 1770
    },
    {
      "epoch": 1.4072308303535954,
      "grad_norm": 0.27565455436706543,
      "learning_rate": 0.00010642933049946866,
      "loss": 0.2193,
      "step": 1771
    },
    {
      "epoch": 1.4080254270957488,
      "grad_norm": 0.24069646000862122,
      "learning_rate": 0.00010637619553666314,
      "loss": 0.1537,
      "step": 1772
    },
    {
      "epoch": 1.4088200238379023,
      "grad_norm": 0.22627206146717072,
      "learning_rate": 0.00010632306057385761,
      "loss": 0.1834,
      "step": 1773
    },
    {
      "epoch": 1.4096146205800557,
      "grad_norm": 0.2072916328907013,
      "learning_rate": 0.00010626992561105207,
      "loss": 0.1718,
      "step": 1774
    },
    {
      "epoch": 1.410409217322209,
      "grad_norm": 0.24290090799331665,
      "learning_rate": 0.00010621679064824654,
      "loss": 0.1686,
      "step": 1775
    },
    {
      "epoch": 1.4112038140643623,
      "grad_norm": 0.24952863156795502,
      "learning_rate": 0.00010616365568544102,
      "loss": 0.1824,
      "step": 1776
    },
    {
      "epoch": 1.4119984108065156,
      "grad_norm": 0.27182242274284363,
      "learning_rate": 0.0001061105207226355,
      "loss": 0.1752,
      "step": 1777
    },
    {
      "epoch": 1.412793007548669,
      "grad_norm": 0.20185968279838562,
      "learning_rate": 0.00010605738575982998,
      "loss": 0.1457,
      "step": 1778
    },
    {
      "epoch": 1.4135876042908224,
      "grad_norm": 0.220296710729599,
      "learning_rate": 0.00010600425079702445,
      "loss": 0.1507,
      "step": 1779
    },
    {
      "epoch": 1.4143822010329758,
      "grad_norm": 0.22017084062099457,
      "learning_rate": 0.00010595111583421893,
      "loss": 0.1525,
      "step": 1780
    },
    {
      "epoch": 1.415176797775129,
      "grad_norm": 0.19022318720817566,
      "learning_rate": 0.00010589798087141341,
      "loss": 0.1188,
      "step": 1781
    },
    {
      "epoch": 1.4159713945172825,
      "grad_norm": 0.1860502064228058,
      "learning_rate": 0.00010584484590860786,
      "loss": 0.1335,
      "step": 1782
    },
    {
      "epoch": 1.416765991259436,
      "grad_norm": 0.19124794006347656,
      "learning_rate": 0.00010579171094580234,
      "loss": 0.1647,
      "step": 1783
    },
    {
      "epoch": 1.4175605880015891,
      "grad_norm": 0.1576899141073227,
      "learning_rate": 0.00010573857598299681,
      "loss": 0.1237,
      "step": 1784
    },
    {
      "epoch": 1.4183551847437426,
      "grad_norm": 0.1931982934474945,
      "learning_rate": 0.00010568544102019129,
      "loss": 0.1147,
      "step": 1785
    },
    {
      "epoch": 1.419149781485896,
      "grad_norm": 0.17776291072368622,
      "learning_rate": 0.00010563230605738577,
      "loss": 0.1043,
      "step": 1786
    },
    {
      "epoch": 1.4199443782280492,
      "grad_norm": 0.16073109209537506,
      "learning_rate": 0.00010557917109458025,
      "loss": 0.1156,
      "step": 1787
    },
    {
      "epoch": 1.4207389749702026,
      "grad_norm": 0.1469729244709015,
      "learning_rate": 0.00010552603613177472,
      "loss": 0.078,
      "step": 1788
    },
    {
      "epoch": 1.421533571712356,
      "grad_norm": 0.2107628583908081,
      "learning_rate": 0.0001054729011689692,
      "loss": 0.1021,
      "step": 1789
    },
    {
      "epoch": 1.4223281684545093,
      "grad_norm": 0.17267970740795135,
      "learning_rate": 0.00010541976620616365,
      "loss": 0.1166,
      "step": 1790
    },
    {
      "epoch": 1.4231227651966627,
      "grad_norm": 0.18164287507534027,
      "learning_rate": 0.00010536663124335813,
      "loss": 0.1076,
      "step": 1791
    },
    {
      "epoch": 1.4239173619388161,
      "grad_norm": 0.1565868854522705,
      "learning_rate": 0.00010531349628055261,
      "loss": 0.0958,
      "step": 1792
    },
    {
      "epoch": 1.4247119586809693,
      "grad_norm": 0.1436653882265091,
      "learning_rate": 0.00010526036131774708,
      "loss": 0.095,
      "step": 1793
    },
    {
      "epoch": 1.4255065554231228,
      "grad_norm": 0.17047861218452454,
      "learning_rate": 0.00010520722635494156,
      "loss": 0.1129,
      "step": 1794
    },
    {
      "epoch": 1.4263011521652762,
      "grad_norm": 0.17492778599262238,
      "learning_rate": 0.00010515409139213604,
      "loss": 0.0884,
      "step": 1795
    },
    {
      "epoch": 1.4270957489074294,
      "grad_norm": 0.14084267616271973,
      "learning_rate": 0.00010510095642933052,
      "loss": 0.0768,
      "step": 1796
    },
    {
      "epoch": 1.4278903456495828,
      "grad_norm": 0.11948296427726746,
      "learning_rate": 0.00010504782146652498,
      "loss": 0.0699,
      "step": 1797
    },
    {
      "epoch": 1.4286849423917363,
      "grad_norm": 0.11687116324901581,
      "learning_rate": 0.00010499468650371944,
      "loss": 0.0736,
      "step": 1798
    },
    {
      "epoch": 1.4294795391338895,
      "grad_norm": 0.12694145739078522,
      "learning_rate": 0.00010494155154091392,
      "loss": 0.0831,
      "step": 1799
    },
    {
      "epoch": 1.430274135876043,
      "grad_norm": 0.13132423162460327,
      "learning_rate": 0.0001048884165781084,
      "loss": 0.0848,
      "step": 1800
    },
    {
      "epoch": 1.4310687326181963,
      "grad_norm": 0.11423645168542862,
      "learning_rate": 0.00010483528161530288,
      "loss": 0.0733,
      "step": 1801
    },
    {
      "epoch": 1.4318633293603495,
      "grad_norm": 0.11974798887968063,
      "learning_rate": 0.00010478214665249734,
      "loss": 0.0683,
      "step": 1802
    },
    {
      "epoch": 1.432657926102503,
      "grad_norm": 0.11411245912313461,
      "learning_rate": 0.00010472901168969182,
      "loss": 0.0563,
      "step": 1803
    },
    {
      "epoch": 1.4334525228446564,
      "grad_norm": 0.07645759731531143,
      "learning_rate": 0.0001046758767268863,
      "loss": 0.0483,
      "step": 1804
    },
    {
      "epoch": 1.4342471195868096,
      "grad_norm": 0.08665395528078079,
      "learning_rate": 0.00010462274176408078,
      "loss": 0.0451,
      "step": 1805
    },
    {
      "epoch": 1.435041716328963,
      "grad_norm": 0.09991767257452011,
      "learning_rate": 0.00010456960680127525,
      "loss": 0.0448,
      "step": 1806
    },
    {
      "epoch": 1.4358363130711165,
      "grad_norm": 0.09520377963781357,
      "learning_rate": 0.0001045164718384697,
      "loss": 0.033,
      "step": 1807
    },
    {
      "epoch": 1.4366309098132697,
      "grad_norm": 0.09031979739665985,
      "learning_rate": 0.00010446333687566418,
      "loss": 0.0439,
      "step": 1808
    },
    {
      "epoch": 1.4374255065554231,
      "grad_norm": 0.36098894476890564,
      "learning_rate": 0.00010441020191285866,
      "loss": 0.2697,
      "step": 1809
    },
    {
      "epoch": 1.4382201032975765,
      "grad_norm": 0.5583683252334595,
      "learning_rate": 0.00010435706695005314,
      "loss": 0.3805,
      "step": 1810
    },
    {
      "epoch": 1.4390147000397298,
      "grad_norm": 0.48403066396713257,
      "learning_rate": 0.00010430393198724761,
      "loss": 0.3586,
      "step": 1811
    },
    {
      "epoch": 1.4398092967818832,
      "grad_norm": 0.37443631887435913,
      "learning_rate": 0.00010425079702444209,
      "loss": 0.2812,
      "step": 1812
    },
    {
      "epoch": 1.4406038935240366,
      "grad_norm": 0.3841714560985565,
      "learning_rate": 0.00010419766206163657,
      "loss": 0.2825,
      "step": 1813
    },
    {
      "epoch": 1.4413984902661898,
      "grad_norm": 0.3619273900985718,
      "learning_rate": 0.00010414452709883105,
      "loss": 0.2787,
      "step": 1814
    },
    {
      "epoch": 1.4421930870083433,
      "grad_norm": 0.27810654044151306,
      "learning_rate": 0.0001040913921360255,
      "loss": 0.2192,
      "step": 1815
    },
    {
      "epoch": 1.4429876837504967,
      "grad_norm": 0.27689123153686523,
      "learning_rate": 0.00010403825717321997,
      "loss": 0.2328,
      "step": 1816
    },
    {
      "epoch": 1.4437822804926501,
      "grad_norm": 0.3506720960140228,
      "learning_rate": 0.00010398512221041445,
      "loss": 0.216,
      "step": 1817
    },
    {
      "epoch": 1.4445768772348033,
      "grad_norm": 0.2289414256811142,
      "learning_rate": 0.00010393198724760893,
      "loss": 0.2081,
      "step": 1818
    },
    {
      "epoch": 1.4453714739769568,
      "grad_norm": 0.28263238072395325,
      "learning_rate": 0.00010387885228480341,
      "loss": 0.2284,
      "step": 1819
    },
    {
      "epoch": 1.44616607071911,
      "grad_norm": 0.26762643456459045,
      "learning_rate": 0.00010382571732199788,
      "loss": 0.2545,
      "step": 1820
    },
    {
      "epoch": 1.4469606674612634,
      "grad_norm": 0.28101053833961487,
      "learning_rate": 0.00010377258235919236,
      "loss": 0.2046,
      "step": 1821
    },
    {
      "epoch": 1.4477552642034168,
      "grad_norm": 0.30215829610824585,
      "learning_rate": 0.00010371944739638684,
      "loss": 0.1948,
      "step": 1822
    },
    {
      "epoch": 1.4485498609455703,
      "grad_norm": 0.19200792908668518,
      "learning_rate": 0.00010366631243358129,
      "loss": 0.1649,
      "step": 1823
    },
    {
      "epoch": 1.4493444576877235,
      "grad_norm": 0.20170637965202332,
      "learning_rate": 0.00010361317747077577,
      "loss": 0.1668,
      "step": 1824
    },
    {
      "epoch": 1.450139054429877,
      "grad_norm": 0.17064017057418823,
      "learning_rate": 0.00010356004250797024,
      "loss": 0.1455,
      "step": 1825
    },
    {
      "epoch": 1.45093365117203,
      "grad_norm": 0.19485515356063843,
      "learning_rate": 0.00010350690754516472,
      "loss": 0.1635,
      "step": 1826
    },
    {
      "epoch": 1.4517282479141835,
      "grad_norm": 0.19597384333610535,
      "learning_rate": 0.0001034537725823592,
      "loss": 0.1449,
      "step": 1827
    },
    {
      "epoch": 1.452522844656337,
      "grad_norm": 0.21846871078014374,
      "learning_rate": 0.00010340063761955368,
      "loss": 0.1496,
      "step": 1828
    },
    {
      "epoch": 1.4533174413984904,
      "grad_norm": 0.17871536314487457,
      "learning_rate": 0.00010334750265674815,
      "loss": 0.16,
      "step": 1829
    },
    {
      "epoch": 1.4541120381406436,
      "grad_norm": 0.16481953859329224,
      "learning_rate": 0.00010329436769394263,
      "loss": 0.1472,
      "step": 1830
    },
    {
      "epoch": 1.454906634882797,
      "grad_norm": 0.19566233456134796,
      "learning_rate": 0.00010324123273113708,
      "loss": 0.1337,
      "step": 1831
    },
    {
      "epoch": 1.4557012316249502,
      "grad_norm": 0.1945657730102539,
      "learning_rate": 0.00010318809776833156,
      "loss": 0.1466,
      "step": 1832
    },
    {
      "epoch": 1.4564958283671037,
      "grad_norm": 0.15227726101875305,
      "learning_rate": 0.00010313496280552604,
      "loss": 0.1214,
      "step": 1833
    },
    {
      "epoch": 1.457290425109257,
      "grad_norm": 0.16794632375240326,
      "learning_rate": 0.00010308182784272052,
      "loss": 0.1081,
      "step": 1834
    },
    {
      "epoch": 1.4580850218514105,
      "grad_norm": 0.15155775845050812,
      "learning_rate": 0.00010302869287991499,
      "loss": 0.1285,
      "step": 1835
    },
    {
      "epoch": 1.4588796185935637,
      "grad_norm": 0.16844891011714935,
      "learning_rate": 0.00010297555791710947,
      "loss": 0.1002,
      "step": 1836
    },
    {
      "epoch": 1.4596742153357172,
      "grad_norm": 0.17869259417057037,
      "learning_rate": 0.00010292242295430395,
      "loss": 0.1156,
      "step": 1837
    },
    {
      "epoch": 1.4604688120778704,
      "grad_norm": 0.14946159720420837,
      "learning_rate": 0.00010286928799149841,
      "loss": 0.0977,
      "step": 1838
    },
    {
      "epoch": 1.4612634088200238,
      "grad_norm": 0.18531984090805054,
      "learning_rate": 0.00010281615302869288,
      "loss": 0.128,
      "step": 1839
    },
    {
      "epoch": 1.4620580055621772,
      "grad_norm": 0.1387147456407547,
      "learning_rate": 0.00010276301806588735,
      "loss": 0.1056,
      "step": 1840
    },
    {
      "epoch": 1.4628526023043307,
      "grad_norm": 0.15917780995368958,
      "learning_rate": 0.00010270988310308183,
      "loss": 0.1059,
      "step": 1841
    },
    {
      "epoch": 1.4636471990464839,
      "grad_norm": 0.1115134134888649,
      "learning_rate": 0.00010265674814027631,
      "loss": 0.0862,
      "step": 1842
    },
    {
      "epoch": 1.4644417957886373,
      "grad_norm": 0.13956964015960693,
      "learning_rate": 0.00010260361317747079,
      "loss": 0.0823,
      "step": 1843
    },
    {
      "epoch": 1.4652363925307905,
      "grad_norm": 0.11906440556049347,
      "learning_rate": 0.00010255047821466525,
      "loss": 0.0857,
      "step": 1844
    },
    {
      "epoch": 1.466030989272944,
      "grad_norm": 0.13681159913539886,
      "learning_rate": 0.00010249734325185973,
      "loss": 0.0783,
      "step": 1845
    },
    {
      "epoch": 1.4668255860150974,
      "grad_norm": 0.12309448421001434,
      "learning_rate": 0.00010244420828905421,
      "loss": 0.0848,
      "step": 1846
    },
    {
      "epoch": 1.4676201827572508,
      "grad_norm": 0.10236959904432297,
      "learning_rate": 0.00010239107332624867,
      "loss": 0.0645,
      "step": 1847
    },
    {
      "epoch": 1.468414779499404,
      "grad_norm": 0.14245754480361938,
      "learning_rate": 0.00010233793836344315,
      "loss": 0.0892,
      "step": 1848
    },
    {
      "epoch": 1.4692093762415575,
      "grad_norm": 0.11464456468820572,
      "learning_rate": 0.00010228480340063761,
      "loss": 0.0712,
      "step": 1849
    },
    {
      "epoch": 1.4700039729837107,
      "grad_norm": 0.13368985056877136,
      "learning_rate": 0.0001022316684378321,
      "loss": 0.0737,
      "step": 1850
    },
    {
      "epoch": 1.470798569725864,
      "grad_norm": 0.14306817948818207,
      "learning_rate": 0.00010217853347502657,
      "loss": 0.072,
      "step": 1851
    },
    {
      "epoch": 1.4715931664680175,
      "grad_norm": 0.09881056100130081,
      "learning_rate": 0.00010212539851222105,
      "loss": 0.0433,
      "step": 1852
    },
    {
      "epoch": 1.472387763210171,
      "grad_norm": 0.10587453842163086,
      "learning_rate": 0.00010207226354941552,
      "loss": 0.0469,
      "step": 1853
    },
    {
      "epoch": 1.4731823599523242,
      "grad_norm": 0.09992574900388718,
      "learning_rate": 0.00010201912858661,
      "loss": 0.0432,
      "step": 1854
    },
    {
      "epoch": 1.4739769566944776,
      "grad_norm": 0.08402948826551437,
      "learning_rate": 0.00010196599362380448,
      "loss": 0.05,
      "step": 1855
    },
    {
      "epoch": 1.4747715534366308,
      "grad_norm": 0.11008875072002411,
      "learning_rate": 0.00010191285866099893,
      "loss": 0.0551,
      "step": 1856
    },
    {
      "epoch": 1.4755661501787842,
      "grad_norm": 0.08505100011825562,
      "learning_rate": 0.00010185972369819341,
      "loss": 0.0341,
      "step": 1857
    },
    {
      "epoch": 1.4763607469209377,
      "grad_norm": 0.0740329846739769,
      "learning_rate": 0.00010180658873538788,
      "loss": 0.0361,
      "step": 1858
    },
    {
      "epoch": 1.477155343663091,
      "grad_norm": 0.3027743697166443,
      "learning_rate": 0.00010175345377258236,
      "loss": 0.2137,
      "step": 1859
    },
    {
      "epoch": 1.4779499404052443,
      "grad_norm": 0.4751088321208954,
      "learning_rate": 0.00010170031880977684,
      "loss": 0.4103,
      "step": 1860
    },
    {
      "epoch": 1.4787445371473977,
      "grad_norm": 0.4679569602012634,
      "learning_rate": 0.00010164718384697132,
      "loss": 0.3166,
      "step": 1861
    },
    {
      "epoch": 1.479539133889551,
      "grad_norm": 0.4181194603443146,
      "learning_rate": 0.00010159404888416579,
      "loss": 0.2779,
      "step": 1862
    },
    {
      "epoch": 1.4803337306317044,
      "grad_norm": 0.3884417712688446,
      "learning_rate": 0.00010154091392136027,
      "loss": 0.2794,
      "step": 1863
    },
    {
      "epoch": 1.4811283273738578,
      "grad_norm": 0.30227869749069214,
      "learning_rate": 0.00010148777895855472,
      "loss": 0.2357,
      "step": 1864
    },
    {
      "epoch": 1.4819229241160112,
      "grad_norm": 0.3764917850494385,
      "learning_rate": 0.0001014346439957492,
      "loss": 0.2944,
      "step": 1865
    },
    {
      "epoch": 1.4827175208581644,
      "grad_norm": 0.2943996787071228,
      "learning_rate": 0.00010138150903294368,
      "loss": 0.2325,
      "step": 1866
    },
    {
      "epoch": 1.4835121176003179,
      "grad_norm": 0.27916210889816284,
      "learning_rate": 0.00010132837407013815,
      "loss": 0.216,
      "step": 1867
    },
    {
      "epoch": 1.484306714342471,
      "grad_norm": 0.3343515396118164,
      "learning_rate": 0.00010127523910733263,
      "loss": 0.2052,
      "step": 1868
    },
    {
      "epoch": 1.4851013110846245,
      "grad_norm": 0.24808543920516968,
      "learning_rate": 0.00010122210414452711,
      "loss": 0.1952,
      "step": 1869
    },
    {
      "epoch": 1.485895907826778,
      "grad_norm": 0.2039777934551239,
      "learning_rate": 0.00010116896918172159,
      "loss": 0.1709,
      "step": 1870
    },
    {
      "epoch": 1.4866905045689314,
      "grad_norm": 0.200648695230484,
      "learning_rate": 0.00010111583421891606,
      "loss": 0.1808,
      "step": 1871
    },
    {
      "epoch": 1.4874851013110846,
      "grad_norm": 0.19755028188228607,
      "learning_rate": 0.00010106269925611051,
      "loss": 0.1639,
      "step": 1872
    },
    {
      "epoch": 1.488279698053238,
      "grad_norm": 0.18833407759666443,
      "learning_rate": 0.00010100956429330499,
      "loss": 0.1477,
      "step": 1873
    },
    {
      "epoch": 1.4890742947953912,
      "grad_norm": 0.1789969950914383,
      "learning_rate": 0.00010095642933049947,
      "loss": 0.1519,
      "step": 1874
    },
    {
      "epoch": 1.4898688915375446,
      "grad_norm": 0.19360727071762085,
      "learning_rate": 0.00010090329436769395,
      "loss": 0.1709,
      "step": 1875
    },
    {
      "epoch": 1.490663488279698,
      "grad_norm": 0.1843319535255432,
      "learning_rate": 0.00010085015940488842,
      "loss": 0.1475,
      "step": 1876
    },
    {
      "epoch": 1.4914580850218515,
      "grad_norm": 0.20594437420368195,
      "learning_rate": 0.0001007970244420829,
      "loss": 0.1337,
      "step": 1877
    },
    {
      "epoch": 1.4922526817640047,
      "grad_norm": 0.17462189495563507,
      "learning_rate": 0.00010074388947927738,
      "loss": 0.1373,
      "step": 1878
    },
    {
      "epoch": 1.4930472785061581,
      "grad_norm": 0.16096565127372742,
      "learning_rate": 0.00010069075451647186,
      "loss": 0.1363,
      "step": 1879
    },
    {
      "epoch": 1.4938418752483114,
      "grad_norm": 0.22536903619766235,
      "learning_rate": 0.00010063761955366631,
      "loss": 0.1498,
      "step": 1880
    },
    {
      "epoch": 1.4946364719904648,
      "grad_norm": 0.16080714762210846,
      "learning_rate": 0.00010058448459086078,
      "loss": 0.125,
      "step": 1881
    },
    {
      "epoch": 1.4954310687326182,
      "grad_norm": 0.17677180469036102,
      "learning_rate": 0.00010053134962805526,
      "loss": 0.1276,
      "step": 1882
    },
    {
      "epoch": 1.4962256654747716,
      "grad_norm": 0.16920353472232819,
      "learning_rate": 0.00010047821466524974,
      "loss": 0.1093,
      "step": 1883
    },
    {
      "epoch": 1.4970202622169249,
      "grad_norm": 0.14221996068954468,
      "learning_rate": 0.00010042507970244422,
      "loss": 0.1009,
      "step": 1884
    },
    {
      "epoch": 1.4978148589590783,
      "grad_norm": 0.1507238894701004,
      "learning_rate": 0.00010037194473963868,
      "loss": 0.1051,
      "step": 1885
    },
    {
      "epoch": 1.4986094557012315,
      "grad_norm": 0.18472661077976227,
      "learning_rate": 0.00010031880977683316,
      "loss": 0.1119,
      "step": 1886
    },
    {
      "epoch": 1.499404052443385,
      "grad_norm": 0.17030799388885498,
      "learning_rate": 0.00010026567481402764,
      "loss": 0.1122,
      "step": 1887
    },
    {
      "epoch": 1.5001986491855384,
      "grad_norm": 0.16750748455524445,
      "learning_rate": 0.0001002125398512221,
      "loss": 0.1012,
      "step": 1888
    },
    {
      "epoch": 1.5009932459276918,
      "grad_norm": 0.14712749421596527,
      "learning_rate": 0.00010015940488841658,
      "loss": 0.1087,
      "step": 1889
    },
    {
      "epoch": 1.5017878426698452,
      "grad_norm": 0.19836261868476868,
      "learning_rate": 0.00010010626992561106,
      "loss": 0.1204,
      "step": 1890
    },
    {
      "epoch": 1.5025824394119984,
      "grad_norm": 0.2421334832906723,
      "learning_rate": 0.00010005313496280552,
      "loss": 0.0911,
      "step": 1891
    },
    {
      "epoch": 1.5033770361541516,
      "grad_norm": 0.13918587565422058,
      "learning_rate": 0.0001,
      "loss": 0.1096,
      "step": 1892
    },
    {
      "epoch": 1.504171632896305,
      "grad_norm": 0.11295383423566818,
      "learning_rate": 9.994686503719448e-05,
      "loss": 0.0831,
      "step": 1893
    },
    {
      "epoch": 1.5049662296384585,
      "grad_norm": 0.16000278294086456,
      "learning_rate": 9.989373007438895e-05,
      "loss": 0.0939,
      "step": 1894
    },
    {
      "epoch": 1.505760826380612,
      "grad_norm": 0.1346665918827057,
      "learning_rate": 9.984059511158342e-05,
      "loss": 0.0986,
      "step": 1895
    },
    {
      "epoch": 1.5065554231227654,
      "grad_norm": 0.11157955974340439,
      "learning_rate": 9.97874601487779e-05,
      "loss": 0.0792,
      "step": 1896
    },
    {
      "epoch": 1.5073500198649186,
      "grad_norm": 0.11423365026712418,
      "learning_rate": 9.973432518597238e-05,
      "loss": 0.0775,
      "step": 1897
    },
    {
      "epoch": 1.5081446166070718,
      "grad_norm": 0.09964938461780548,
      "learning_rate": 9.968119022316686e-05,
      "loss": 0.073,
      "step": 1898
    },
    {
      "epoch": 1.5089392133492252,
      "grad_norm": 0.10837557911872864,
      "learning_rate": 9.962805526036133e-05,
      "loss": 0.0691,
      "step": 1899
    },
    {
      "epoch": 1.5097338100913786,
      "grad_norm": 0.12103981524705887,
      "learning_rate": 9.957492029755579e-05,
      "loss": 0.0641,
      "step": 1900
    },
    {
      "epoch": 1.510528406833532,
      "grad_norm": 0.09711472690105438,
      "learning_rate": 9.952178533475027e-05,
      "loss": 0.0614,
      "step": 1901
    },
    {
      "epoch": 1.5113230035756855,
      "grad_norm": 0.0914839506149292,
      "learning_rate": 9.946865037194475e-05,
      "loss": 0.0666,
      "step": 1902
    },
    {
      "epoch": 1.5121176003178387,
      "grad_norm": 0.08722277730703354,
      "learning_rate": 9.941551540913922e-05,
      "loss": 0.0533,
      "step": 1903
    },
    {
      "epoch": 1.512912197059992,
      "grad_norm": 0.08474480360746384,
      "learning_rate": 9.936238044633369e-05,
      "loss": 0.0605,
      "step": 1904
    },
    {
      "epoch": 1.5137067938021453,
      "grad_norm": 0.09452471137046814,
      "learning_rate": 9.930924548352817e-05,
      "loss": 0.0618,
      "step": 1905
    },
    {
      "epoch": 1.5145013905442988,
      "grad_norm": 0.08195935934782028,
      "learning_rate": 9.925611052072265e-05,
      "loss": 0.041,
      "step": 1906
    },
    {
      "epoch": 1.5152959872864522,
      "grad_norm": 0.0696176066994667,
      "learning_rate": 9.920297555791711e-05,
      "loss": 0.0479,
      "step": 1907
    },
    {
      "epoch": 1.5160905840286056,
      "grad_norm": 0.06708072125911713,
      "learning_rate": 9.914984059511159e-05,
      "loss": 0.0396,
      "step": 1908
    },
    {
      "epoch": 1.5168851807707588,
      "grad_norm": 0.4119442403316498,
      "learning_rate": 9.909670563230606e-05,
      "loss": 0.3293,
      "step": 1909
    },
    {
      "epoch": 1.517679777512912,
      "grad_norm": 0.3828265070915222,
      "learning_rate": 9.904357066950054e-05,
      "loss": 0.3505,
      "step": 1910
    },
    {
      "epoch": 1.5184743742550655,
      "grad_norm": 0.37777331471443176,
      "learning_rate": 9.8990435706695e-05,
      "loss": 0.4028,
      "step": 1911
    },
    {
      "epoch": 1.519268970997219,
      "grad_norm": 0.3435329794883728,
      "learning_rate": 9.893730074388949e-05,
      "loss": 0.3293,
      "step": 1912
    },
    {
      "epoch": 1.5200635677393723,
      "grad_norm": 0.31762707233428955,
      "learning_rate": 9.888416578108395e-05,
      "loss": 0.2806,
      "step": 1913
    },
    {
      "epoch": 1.5208581644815258,
      "grad_norm": 0.32553762197494507,
      "learning_rate": 9.883103081827843e-05,
      "loss": 0.2935,
      "step": 1914
    },
    {
      "epoch": 1.521652761223679,
      "grad_norm": 0.28625723719596863,
      "learning_rate": 9.87778958554729e-05,
      "loss": 0.2577,
      "step": 1915
    },
    {
      "epoch": 1.5224473579658322,
      "grad_norm": 0.3254409730434418,
      "learning_rate": 9.872476089266738e-05,
      "loss": 0.2633,
      "step": 1916
    },
    {
      "epoch": 1.5232419547079856,
      "grad_norm": 0.2698502242565155,
      "learning_rate": 9.867162592986186e-05,
      "loss": 0.2328,
      "step": 1917
    },
    {
      "epoch": 1.524036551450139,
      "grad_norm": 0.26901087164878845,
      "learning_rate": 9.861849096705633e-05,
      "loss": 0.207,
      "step": 1918
    },
    {
      "epoch": 1.5248311481922925,
      "grad_norm": 0.2982967793941498,
      "learning_rate": 9.85653560042508e-05,
      "loss": 0.1869,
      "step": 1919
    },
    {
      "epoch": 1.525625744934446,
      "grad_norm": 0.22770234942436218,
      "learning_rate": 9.851222104144527e-05,
      "loss": 0.1877,
      "step": 1920
    },
    {
      "epoch": 1.5264203416765991,
      "grad_norm": 0.27034813165664673,
      "learning_rate": 9.845908607863975e-05,
      "loss": 0.2001,
      "step": 1921
    },
    {
      "epoch": 1.5272149384187523,
      "grad_norm": 0.2823440730571747,
      "learning_rate": 9.840595111583422e-05,
      "loss": 0.1945,
      "step": 1922
    },
    {
      "epoch": 1.5280095351609058,
      "grad_norm": 0.2518804371356964,
      "learning_rate": 9.835281615302869e-05,
      "loss": 0.1903,
      "step": 1923
    },
    {
      "epoch": 1.5288041319030592,
      "grad_norm": 0.2994501292705536,
      "learning_rate": 9.829968119022317e-05,
      "loss": 0.1876,
      "step": 1924
    },
    {
      "epoch": 1.5295987286452126,
      "grad_norm": 0.24304188787937164,
      "learning_rate": 9.824654622741765e-05,
      "loss": 0.193,
      "step": 1925
    },
    {
      "epoch": 1.530393325387366,
      "grad_norm": 0.2230536788702011,
      "learning_rate": 9.819341126461213e-05,
      "loss": 0.1506,
      "step": 1926
    },
    {
      "epoch": 1.5311879221295193,
      "grad_norm": 0.26833462715148926,
      "learning_rate": 9.81402763018066e-05,
      "loss": 0.1798,
      "step": 1927
    },
    {
      "epoch": 1.5319825188716725,
      "grad_norm": 0.21953463554382324,
      "learning_rate": 9.808714133900106e-05,
      "loss": 0.1712,
      "step": 1928
    },
    {
      "epoch": 1.532777115613826,
      "grad_norm": 0.21555013954639435,
      "learning_rate": 9.803400637619554e-05,
      "loss": 0.1843,
      "step": 1929
    },
    {
      "epoch": 1.5335717123559793,
      "grad_norm": 0.2871797978878021,
      "learning_rate": 9.798087141339002e-05,
      "loss": 0.1349,
      "step": 1930
    },
    {
      "epoch": 1.5343663090981328,
      "grad_norm": 0.2035917490720749,
      "learning_rate": 9.792773645058449e-05,
      "loss": 0.1357,
      "step": 1931
    },
    {
      "epoch": 1.5351609058402862,
      "grad_norm": 0.21401791274547577,
      "learning_rate": 9.787460148777895e-05,
      "loss": 0.1349,
      "step": 1932
    },
    {
      "epoch": 1.5359555025824394,
      "grad_norm": 0.16516591608524323,
      "learning_rate": 9.782146652497344e-05,
      "loss": 0.1243,
      "step": 1933
    },
    {
      "epoch": 1.5367500993245926,
      "grad_norm": 0.16583122313022614,
      "learning_rate": 9.776833156216792e-05,
      "loss": 0.13,
      "step": 1934
    },
    {
      "epoch": 1.537544696066746,
      "grad_norm": 0.18711107969284058,
      "learning_rate": 9.771519659936238e-05,
      "loss": 0.1032,
      "step": 1935
    },
    {
      "epoch": 1.5383392928088995,
      "grad_norm": 0.1991272121667862,
      "learning_rate": 9.766206163655686e-05,
      "loss": 0.1257,
      "step": 1936
    },
    {
      "epoch": 1.539133889551053,
      "grad_norm": 0.16038133203983307,
      "learning_rate": 9.760892667375133e-05,
      "loss": 0.0979,
      "step": 1937
    },
    {
      "epoch": 1.5399284862932063,
      "grad_norm": 0.16577932238578796,
      "learning_rate": 9.755579171094581e-05,
      "loss": 0.1177,
      "step": 1938
    },
    {
      "epoch": 1.5407230830353595,
      "grad_norm": 0.15659236907958984,
      "learning_rate": 9.750265674814029e-05,
      "loss": 0.108,
      "step": 1939
    },
    {
      "epoch": 1.5415176797775128,
      "grad_norm": 0.23831512033939362,
      "learning_rate": 9.744952178533476e-05,
      "loss": 0.1234,
      "step": 1940
    },
    {
      "epoch": 1.5423122765196662,
      "grad_norm": 0.14938470721244812,
      "learning_rate": 9.739638682252922e-05,
      "loss": 0.1058,
      "step": 1941
    },
    {
      "epoch": 1.5431068732618196,
      "grad_norm": 0.15151657164096832,
      "learning_rate": 9.73432518597237e-05,
      "loss": 0.0966,
      "step": 1942
    },
    {
      "epoch": 1.543901470003973,
      "grad_norm": 0.13989150524139404,
      "learning_rate": 9.729011689691818e-05,
      "loss": 0.0835,
      "step": 1943
    },
    {
      "epoch": 1.5446960667461265,
      "grad_norm": 0.1310347020626068,
      "learning_rate": 9.723698193411265e-05,
      "loss": 0.0858,
      "step": 1944
    },
    {
      "epoch": 1.5454906634882797,
      "grad_norm": 0.2105664610862732,
      "learning_rate": 9.718384697130713e-05,
      "loss": 0.1116,
      "step": 1945
    },
    {
      "epoch": 1.546285260230433,
      "grad_norm": 0.1062600389122963,
      "learning_rate": 9.71307120085016e-05,
      "loss": 0.0729,
      "step": 1946
    },
    {
      "epoch": 1.5470798569725863,
      "grad_norm": 0.1447303742170334,
      "learning_rate": 9.707757704569608e-05,
      "loss": 0.088,
      "step": 1947
    },
    {
      "epoch": 1.5478744537147398,
      "grad_norm": 0.11387696862220764,
      "learning_rate": 9.702444208289054e-05,
      "loss": 0.0651,
      "step": 1948
    },
    {
      "epoch": 1.5486690504568932,
      "grad_norm": 0.12874779105186462,
      "learning_rate": 9.697130712008502e-05,
      "loss": 0.0857,
      "step": 1949
    },
    {
      "epoch": 1.5494636471990466,
      "grad_norm": 0.12221506237983704,
      "learning_rate": 9.691817215727949e-05,
      "loss": 0.0786,
      "step": 1950
    },
    {
      "epoch": 1.5502582439411998,
      "grad_norm": 0.06958828121423721,
      "learning_rate": 9.686503719447397e-05,
      "loss": 0.041,
      "step": 1951
    },
    {
      "epoch": 1.5510528406833533,
      "grad_norm": 0.07724390178918839,
      "learning_rate": 9.681190223166844e-05,
      "loss": 0.0531,
      "step": 1952
    },
    {
      "epoch": 1.5518474374255065,
      "grad_norm": 0.07809963077306747,
      "learning_rate": 9.675876726886292e-05,
      "loss": 0.0458,
      "step": 1953
    },
    {
      "epoch": 1.55264203416766,
      "grad_norm": 0.07415401935577393,
      "learning_rate": 9.67056323060574e-05,
      "loss": 0.0386,
      "step": 1954
    },
    {
      "epoch": 1.5534366309098133,
      "grad_norm": 0.07770246267318726,
      "learning_rate": 9.665249734325186e-05,
      "loss": 0.0486,
      "step": 1955
    },
    {
      "epoch": 1.5542312276519668,
      "grad_norm": 0.09335271269083023,
      "learning_rate": 9.659936238044633e-05,
      "loss": 0.0422,
      "step": 1956
    },
    {
      "epoch": 1.55502582439412,
      "grad_norm": 0.08787047117948532,
      "learning_rate": 9.654622741764081e-05,
      "loss": 0.0405,
      "step": 1957
    },
    {
      "epoch": 1.5558204211362734,
      "grad_norm": 0.08771529793739319,
      "learning_rate": 9.649309245483529e-05,
      "loss": 0.0429,
      "step": 1958
    },
    {
      "epoch": 1.5566150178784266,
      "grad_norm": 0.3699583411216736,
      "learning_rate": 9.643995749202977e-05,
      "loss": 0.2776,
      "step": 1959
    },
    {
      "epoch": 1.55740961462058,
      "grad_norm": 0.4196649193763733,
      "learning_rate": 9.638682252922422e-05,
      "loss": 0.458,
      "step": 1960
    },
    {
      "epoch": 1.5582042113627335,
      "grad_norm": 0.41596487164497375,
      "learning_rate": 9.63336875664187e-05,
      "loss": 0.3741,
      "step": 1961
    },
    {
      "epoch": 1.558998808104887,
      "grad_norm": 0.38115453720092773,
      "learning_rate": 9.628055260361318e-05,
      "loss": 0.3023,
      "step": 1962
    },
    {
      "epoch": 1.55979340484704,
      "grad_norm": 0.35853320360183716,
      "learning_rate": 9.622741764080766e-05,
      "loss": 0.2884,
      "step": 1963
    },
    {
      "epoch": 1.5605880015891935,
      "grad_norm": 0.35076963901519775,
      "learning_rate": 9.617428267800213e-05,
      "loss": 0.2809,
      "step": 1964
    },
    {
      "epoch": 1.5613825983313467,
      "grad_norm": 0.2763517498970032,
      "learning_rate": 9.61211477151966e-05,
      "loss": 0.2485,
      "step": 1965
    },
    {
      "epoch": 1.5621771950735002,
      "grad_norm": 0.28909021615982056,
      "learning_rate": 9.606801275239108e-05,
      "loss": 0.2262,
      "step": 1966
    },
    {
      "epoch": 1.5629717918156536,
      "grad_norm": 0.2433355450630188,
      "learning_rate": 9.601487778958556e-05,
      "loss": 0.2106,
      "step": 1967
    },
    {
      "epoch": 1.563766388557807,
      "grad_norm": 0.24598954617977142,
      "learning_rate": 9.596174282678002e-05,
      "loss": 0.2135,
      "step": 1968
    },
    {
      "epoch": 1.5645609852999602,
      "grad_norm": 0.22511227428913116,
      "learning_rate": 9.590860786397449e-05,
      "loss": 0.1615,
      "step": 1969
    },
    {
      "epoch": 1.5653555820421137,
      "grad_norm": 0.2284441441297531,
      "learning_rate": 9.585547290116897e-05,
      "loss": 0.1787,
      "step": 1970
    },
    {
      "epoch": 1.5661501787842669,
      "grad_norm": 0.2047015279531479,
      "learning_rate": 9.580233793836345e-05,
      "loss": 0.1582,
      "step": 1971
    },
    {
      "epoch": 1.5669447755264203,
      "grad_norm": 0.8513358235359192,
      "learning_rate": 9.574920297555792e-05,
      "loss": 0.1996,
      "step": 1972
    },
    {
      "epoch": 1.5677393722685737,
      "grad_norm": 14.037805557250977,
      "learning_rate": 9.56960680127524e-05,
      "loss": 0.3017,
      "step": 1973
    },
    {
      "epoch": 1.5685339690107272,
      "grad_norm": 0.2178461104631424,
      "learning_rate": 9.564293304994687e-05,
      "loss": 0.1441,
      "step": 1974
    },
    {
      "epoch": 1.5693285657528804,
      "grad_norm": 15.672645568847656,
      "learning_rate": 9.558979808714135e-05,
      "loss": 0.2687,
      "step": 1975
    },
    {
      "epoch": 1.5701231624950338,
      "grad_norm": 4.4589715003967285,
      "learning_rate": 9.553666312433581e-05,
      "loss": 0.1942,
      "step": 1976
    },
    {
      "epoch": 1.570917759237187,
      "grad_norm": 5.80436897277832,
      "learning_rate": 9.548352816153029e-05,
      "loss": 0.1525,
      "step": 1977
    },
    {
      "epoch": 1.5717123559793404,
      "grad_norm": 2.117056131362915,
      "learning_rate": 9.543039319872476e-05,
      "loss": 0.1792,
      "step": 1978
    },
    {
      "epoch": 1.5725069527214939,
      "grad_norm": 0.40959489345550537,
      "learning_rate": 9.537725823591924e-05,
      "loss": 0.1379,
      "step": 1979
    },
    {
      "epoch": 1.5733015494636473,
      "grad_norm": 0.5133061408996582,
      "learning_rate": 9.53241232731137e-05,
      "loss": 0.1218,
      "step": 1980
    },
    {
      "epoch": 1.5740961462058005,
      "grad_norm": 0.27578437328338623,
      "learning_rate": 9.527098831030819e-05,
      "loss": 0.1311,
      "step": 1981
    },
    {
      "epoch": 1.574890742947954,
      "grad_norm": 2.467292547225952,
      "learning_rate": 9.521785334750267e-05,
      "loss": 0.1286,
      "step": 1982
    },
    {
      "epoch": 1.5756853396901072,
      "grad_norm": 0.1355445384979248,
      "learning_rate": 9.516471838469713e-05,
      "loss": 0.1046,
      "step": 1983
    },
    {
      "epoch": 1.5764799364322606,
      "grad_norm": 0.36536478996276855,
      "learning_rate": 9.511158342189161e-05,
      "loss": 0.1316,
      "step": 1984
    },
    {
      "epoch": 1.577274533174414,
      "grad_norm": 0.2355547845363617,
      "learning_rate": 9.505844845908608e-05,
      "loss": 0.1322,
      "step": 1985
    },
    {
      "epoch": 1.5780691299165674,
      "grad_norm": 0.16733577847480774,
      "learning_rate": 9.500531349628056e-05,
      "loss": 0.1138,
      "step": 1986
    },
    {
      "epoch": 1.5788637266587207,
      "grad_norm": 0.14731821417808533,
      "learning_rate": 9.495217853347504e-05,
      "loss": 0.0906,
      "step": 1987
    },
    {
      "epoch": 1.579658323400874,
      "grad_norm": 0.14282920956611633,
      "learning_rate": 9.48990435706695e-05,
      "loss": 0.0956,
      "step": 1988
    },
    {
      "epoch": 1.5804529201430273,
      "grad_norm": 0.1791669726371765,
      "learning_rate": 9.484590860786397e-05,
      "loss": 0.1181,
      "step": 1989
    },
    {
      "epoch": 1.5812475168851807,
      "grad_norm": 0.13420595228672028,
      "learning_rate": 9.479277364505845e-05,
      "loss": 0.0912,
      "step": 1990
    },
    {
      "epoch": 1.5820421136273342,
      "grad_norm": 0.139102041721344,
      "learning_rate": 9.473963868225293e-05,
      "loss": 0.0801,
      "step": 1991
    },
    {
      "epoch": 1.5828367103694876,
      "grad_norm": 0.19922903180122375,
      "learning_rate": 9.46865037194474e-05,
      "loss": 0.0983,
      "step": 1992
    },
    {
      "epoch": 1.5836313071116408,
      "grad_norm": 0.16187793016433716,
      "learning_rate": 9.463336875664187e-05,
      "loss": 0.0989,
      "step": 1993
    },
    {
      "epoch": 1.5844259038537942,
      "grad_norm": 0.11626304686069489,
      "learning_rate": 9.458023379383635e-05,
      "loss": 0.0944,
      "step": 1994
    },
    {
      "epoch": 1.5852205005959474,
      "grad_norm": 0.15233317017555237,
      "learning_rate": 9.452709883103083e-05,
      "loss": 0.0909,
      "step": 1995
    },
    {
      "epoch": 1.5860150973381009,
      "grad_norm": 0.1277313083410263,
      "learning_rate": 9.447396386822531e-05,
      "loss": 0.0887,
      "step": 1996
    },
    {
      "epoch": 1.5868096940802543,
      "grad_norm": 0.10602018237113953,
      "learning_rate": 9.442082890541976e-05,
      "loss": 0.07,
      "step": 1997
    },
    {
      "epoch": 1.5876042908224077,
      "grad_norm": 0.10385135561227798,
      "learning_rate": 9.436769394261424e-05,
      "loss": 0.0717,
      "step": 1998
    },
    {
      "epoch": 1.588398887564561,
      "grad_norm": 0.12645725905895233,
      "learning_rate": 9.431455897980872e-05,
      "loss": 0.0755,
      "step": 1999
    },
    {
      "epoch": 1.5891934843067144,
      "grad_norm": 0.06517559289932251,
      "learning_rate": 9.42614240170032e-05,
      "loss": 0.0493,
      "step": 2000
    }
  ],
  "logging_steps": 1,
  "max_steps": 3774,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.240966765836288e+17,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
