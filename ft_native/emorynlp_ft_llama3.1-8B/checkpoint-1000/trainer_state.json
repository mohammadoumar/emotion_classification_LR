{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7945967421533572,
  "eval_steps": 755,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007945967421533572,
      "grad_norm": 5.095927715301514,
      "learning_rate": 2e-05,
      "loss": 3.0773,
      "step": 1
    },
    {
      "epoch": 0.0015891934843067143,
      "grad_norm": 5.295345783233643,
      "learning_rate": 4e-05,
      "loss": 3.0638,
      "step": 2
    },
    {
      "epoch": 0.0023837902264600714,
      "grad_norm": 3.9243922233581543,
      "learning_rate": 6e-05,
      "loss": 2.9447,
      "step": 3
    },
    {
      "epoch": 0.0031783869686134287,
      "grad_norm": 2.9748141765594482,
      "learning_rate": 8e-05,
      "loss": 2.7782,
      "step": 4
    },
    {
      "epoch": 0.003972983710766786,
      "grad_norm": 6.790021896362305,
      "learning_rate": 0.0001,
      "loss": 2.4546,
      "step": 5
    },
    {
      "epoch": 0.004767580452920143,
      "grad_norm": 2.016477584838867,
      "learning_rate": 0.00012,
      "loss": 2.2744,
      "step": 6
    },
    {
      "epoch": 0.0055621771950735005,
      "grad_norm": 1.9537750482559204,
      "learning_rate": 0.00014,
      "loss": 2.0014,
      "step": 7
    },
    {
      "epoch": 0.006356773937226857,
      "grad_norm": 1.7805707454681396,
      "learning_rate": 0.00016,
      "loss": 1.7197,
      "step": 8
    },
    {
      "epoch": 0.007151370679380214,
      "grad_norm": 1.8606624603271484,
      "learning_rate": 0.00018,
      "loss": 1.3944,
      "step": 9
    },
    {
      "epoch": 0.007945967421533572,
      "grad_norm": 5.094567775726318,
      "learning_rate": 0.0002,
      "loss": 1.068,
      "step": 10
    },
    {
      "epoch": 0.00874056416368693,
      "grad_norm": 7.3542938232421875,
      "learning_rate": 0.0001999468650371945,
      "loss": 0.8414,
      "step": 11
    },
    {
      "epoch": 0.009535160905840286,
      "grad_norm": 2.5358312129974365,
      "learning_rate": 0.00019989373007438897,
      "loss": 0.575,
      "step": 12
    },
    {
      "epoch": 0.010329757647993643,
      "grad_norm": 1.625028133392334,
      "learning_rate": 0.00019984059511158345,
      "loss": 0.4307,
      "step": 13
    },
    {
      "epoch": 0.011124354390147001,
      "grad_norm": 1.3451693058013916,
      "learning_rate": 0.0001997874601487779,
      "loss": 0.3319,
      "step": 14
    },
    {
      "epoch": 0.011918951132300357,
      "grad_norm": 1.885640025138855,
      "learning_rate": 0.00019973432518597238,
      "loss": 0.3196,
      "step": 15
    },
    {
      "epoch": 0.012713547874453715,
      "grad_norm": 1.2925968170166016,
      "learning_rate": 0.00019968119022316684,
      "loss": 0.2892,
      "step": 16
    },
    {
      "epoch": 0.013508144616607072,
      "grad_norm": 0.6818913221359253,
      "learning_rate": 0.00019962805526036132,
      "loss": 0.2906,
      "step": 17
    },
    {
      "epoch": 0.014302741358760428,
      "grad_norm": 1.793684482574463,
      "learning_rate": 0.0001995749202975558,
      "loss": 0.2529,
      "step": 18
    },
    {
      "epoch": 0.015097338100913786,
      "grad_norm": 1.2905668020248413,
      "learning_rate": 0.00019952178533475028,
      "loss": 0.2545,
      "step": 19
    },
    {
      "epoch": 0.015891934843067144,
      "grad_norm": 3.4983420372009277,
      "learning_rate": 0.00019946865037194476,
      "loss": 0.2479,
      "step": 20
    },
    {
      "epoch": 0.0166865315852205,
      "grad_norm": 1.3703911304473877,
      "learning_rate": 0.00019941551540913924,
      "loss": 0.2466,
      "step": 21
    },
    {
      "epoch": 0.01748112832737386,
      "grad_norm": 1.2794137001037598,
      "learning_rate": 0.00019936238044633372,
      "loss": 0.2425,
      "step": 22
    },
    {
      "epoch": 0.018275725069527213,
      "grad_norm": 4.779952049255371,
      "learning_rate": 0.00019930924548352817,
      "loss": 0.1825,
      "step": 23
    },
    {
      "epoch": 0.01907032181168057,
      "grad_norm": 6.491456985473633,
      "learning_rate": 0.00019925611052072265,
      "loss": 0.2387,
      "step": 24
    },
    {
      "epoch": 0.01986491855383393,
      "grad_norm": 5.676915645599365,
      "learning_rate": 0.0001992029755579171,
      "loss": 0.2204,
      "step": 25
    },
    {
      "epoch": 0.020659515295987287,
      "grad_norm": 0.3911113440990448,
      "learning_rate": 0.00019914984059511158,
      "loss": 0.1705,
      "step": 26
    },
    {
      "epoch": 0.021454112038140644,
      "grad_norm": 0.50523841381073,
      "learning_rate": 0.00019909670563230606,
      "loss": 0.1899,
      "step": 27
    },
    {
      "epoch": 0.022248708780294002,
      "grad_norm": 0.5957397222518921,
      "learning_rate": 0.00019904357066950054,
      "loss": 0.1861,
      "step": 28
    },
    {
      "epoch": 0.023043305522447356,
      "grad_norm": 0.775301992893219,
      "learning_rate": 0.00019899043570669502,
      "loss": 0.1574,
      "step": 29
    },
    {
      "epoch": 0.023837902264600714,
      "grad_norm": 1.0150798559188843,
      "learning_rate": 0.0001989373007438895,
      "loss": 0.1264,
      "step": 30
    },
    {
      "epoch": 0.02463249900675407,
      "grad_norm": 0.7563843131065369,
      "learning_rate": 0.00019888416578108396,
      "loss": 0.1429,
      "step": 31
    },
    {
      "epoch": 0.02542709574890743,
      "grad_norm": 0.4484422504901886,
      "learning_rate": 0.00019883103081827844,
      "loss": 0.1387,
      "step": 32
    },
    {
      "epoch": 0.026221692491060787,
      "grad_norm": 0.37793880701065063,
      "learning_rate": 0.00019877789585547292,
      "loss": 0.0917,
      "step": 33
    },
    {
      "epoch": 0.027016289233214145,
      "grad_norm": 0.6019443273544312,
      "learning_rate": 0.00019872476089266737,
      "loss": 0.123,
      "step": 34
    },
    {
      "epoch": 0.027810885975367503,
      "grad_norm": 0.48734718561172485,
      "learning_rate": 0.00019867162592986185,
      "loss": 0.131,
      "step": 35
    },
    {
      "epoch": 0.028605482717520857,
      "grad_norm": 0.5307793617248535,
      "learning_rate": 0.00019861849096705633,
      "loss": 0.0855,
      "step": 36
    },
    {
      "epoch": 0.029400079459674214,
      "grad_norm": 0.577855110168457,
      "learning_rate": 0.0001985653560042508,
      "loss": 0.082,
      "step": 37
    },
    {
      "epoch": 0.030194676201827572,
      "grad_norm": 0.37780269980430603,
      "learning_rate": 0.0001985122210414453,
      "loss": 0.0856,
      "step": 38
    },
    {
      "epoch": 0.03098927294398093,
      "grad_norm": 0.20342932641506195,
      "learning_rate": 0.00019845908607863975,
      "loss": 0.0824,
      "step": 39
    },
    {
      "epoch": 0.03178386968613429,
      "grad_norm": 0.36485666036605835,
      "learning_rate": 0.00019840595111583423,
      "loss": 0.0866,
      "step": 40
    },
    {
      "epoch": 0.03257846642828764,
      "grad_norm": 0.38292914628982544,
      "learning_rate": 0.0001983528161530287,
      "loss": 0.0813,
      "step": 41
    },
    {
      "epoch": 0.033373063170441,
      "grad_norm": 0.42815956473350525,
      "learning_rate": 0.00019829968119022319,
      "loss": 0.0777,
      "step": 42
    },
    {
      "epoch": 0.03416765991259436,
      "grad_norm": 0.42552876472473145,
      "learning_rate": 0.00019824654622741764,
      "loss": 0.0506,
      "step": 43
    },
    {
      "epoch": 0.03496225665474772,
      "grad_norm": 0.47190940380096436,
      "learning_rate": 0.00019819341126461212,
      "loss": 0.0624,
      "step": 44
    },
    {
      "epoch": 0.03575685339690107,
      "grad_norm": 0.2830035090446472,
      "learning_rate": 0.0001981402763018066,
      "loss": 0.0616,
      "step": 45
    },
    {
      "epoch": 0.03655145013905443,
      "grad_norm": 0.5423195362091064,
      "learning_rate": 0.00019808714133900108,
      "loss": 0.0614,
      "step": 46
    },
    {
      "epoch": 0.03734604688120779,
      "grad_norm": 0.20404629409313202,
      "learning_rate": 0.00019803400637619553,
      "loss": 0.0504,
      "step": 47
    },
    {
      "epoch": 0.03814064362336114,
      "grad_norm": 0.2916088402271271,
      "learning_rate": 0.00019798087141339,
      "loss": 0.0394,
      "step": 48
    },
    {
      "epoch": 0.038935240365514504,
      "grad_norm": 6.089454650878906,
      "learning_rate": 0.0001979277364505845,
      "loss": 0.0444,
      "step": 49
    },
    {
      "epoch": 0.03972983710766786,
      "grad_norm": 0.31506067514419556,
      "learning_rate": 0.00019787460148777897,
      "loss": 0.035,
      "step": 50
    },
    {
      "epoch": 0.04052443384982122,
      "grad_norm": 1.4184181690216064,
      "learning_rate": 0.00019782146652497345,
      "loss": 0.749,
      "step": 51
    },
    {
      "epoch": 0.04131903059197457,
      "grad_norm": 2.142108201980591,
      "learning_rate": 0.0001977683315621679,
      "loss": 0.5594,
      "step": 52
    },
    {
      "epoch": 0.04211362733412793,
      "grad_norm": 0.6221466064453125,
      "learning_rate": 0.0001977151965993624,
      "loss": 0.494,
      "step": 53
    },
    {
      "epoch": 0.04290822407628129,
      "grad_norm": 0.39055222272872925,
      "learning_rate": 0.00019766206163655687,
      "loss": 0.3899,
      "step": 54
    },
    {
      "epoch": 0.04370282081843464,
      "grad_norm": 0.4236755669116974,
      "learning_rate": 0.00019760892667375135,
      "loss": 0.3525,
      "step": 55
    },
    {
      "epoch": 0.044497417560588004,
      "grad_norm": 0.3338366746902466,
      "learning_rate": 0.0001975557917109458,
      "loss": 0.3568,
      "step": 56
    },
    {
      "epoch": 0.04529201430274136,
      "grad_norm": 0.34272903203964233,
      "learning_rate": 0.00019750265674814028,
      "loss": 0.3519,
      "step": 57
    },
    {
      "epoch": 0.04608661104489471,
      "grad_norm": 0.28583958745002747,
      "learning_rate": 0.00019744952178533476,
      "loss": 0.2993,
      "step": 58
    },
    {
      "epoch": 0.046881207787048074,
      "grad_norm": 0.3079226315021515,
      "learning_rate": 0.00019739638682252924,
      "loss": 0.3023,
      "step": 59
    },
    {
      "epoch": 0.04767580452920143,
      "grad_norm": 0.2857295572757721,
      "learning_rate": 0.00019734325185972372,
      "loss": 0.2599,
      "step": 60
    },
    {
      "epoch": 0.04847040127135479,
      "grad_norm": 0.26531514525413513,
      "learning_rate": 0.00019729011689691817,
      "loss": 0.2562,
      "step": 61
    },
    {
      "epoch": 0.04926499801350814,
      "grad_norm": 0.2522497773170471,
      "learning_rate": 0.00019723698193411265,
      "loss": 0.2805,
      "step": 62
    },
    {
      "epoch": 0.050059594755661505,
      "grad_norm": 0.19372712075710297,
      "learning_rate": 0.00019718384697130713,
      "loss": 0.2578,
      "step": 63
    },
    {
      "epoch": 0.05085419149781486,
      "grad_norm": 0.2371576875448227,
      "learning_rate": 0.0001971307120085016,
      "loss": 0.2443,
      "step": 64
    },
    {
      "epoch": 0.05164878823996821,
      "grad_norm": 0.2858528792858124,
      "learning_rate": 0.00019707757704569607,
      "loss": 0.2271,
      "step": 65
    },
    {
      "epoch": 0.052443384982121574,
      "grad_norm": 0.23870187997817993,
      "learning_rate": 0.00019702444208289055,
      "loss": 0.1795,
      "step": 66
    },
    {
      "epoch": 0.05323798172427493,
      "grad_norm": 0.22088764607906342,
      "learning_rate": 0.00019697130712008503,
      "loss": 0.2156,
      "step": 67
    },
    {
      "epoch": 0.05403257846642829,
      "grad_norm": 0.22069445252418518,
      "learning_rate": 0.0001969181721572795,
      "loss": 0.1977,
      "step": 68
    },
    {
      "epoch": 0.054827175208581644,
      "grad_norm": 0.2257574200630188,
      "learning_rate": 0.000196865037194474,
      "loss": 0.1556,
      "step": 69
    },
    {
      "epoch": 0.055621771950735005,
      "grad_norm": 0.24251490831375122,
      "learning_rate": 0.00019681190223166844,
      "loss": 0.1942,
      "step": 70
    },
    {
      "epoch": 0.05641636869288836,
      "grad_norm": 0.22877848148345947,
      "learning_rate": 0.00019675876726886292,
      "loss": 0.2078,
      "step": 71
    },
    {
      "epoch": 0.057210965435041714,
      "grad_norm": 0.2661331593990326,
      "learning_rate": 0.00019670563230605737,
      "loss": 0.1829,
      "step": 72
    },
    {
      "epoch": 0.058005562177195075,
      "grad_norm": 0.18543265759944916,
      "learning_rate": 0.00019665249734325185,
      "loss": 0.1766,
      "step": 73
    },
    {
      "epoch": 0.05880015891934843,
      "grad_norm": 0.19438567757606506,
      "learning_rate": 0.00019659936238044634,
      "loss": 0.1803,
      "step": 74
    },
    {
      "epoch": 0.05959475566150179,
      "grad_norm": 0.20390024781227112,
      "learning_rate": 0.00019654622741764082,
      "loss": 0.1298,
      "step": 75
    },
    {
      "epoch": 0.060389352403655144,
      "grad_norm": 0.18488165736198425,
      "learning_rate": 0.0001964930924548353,
      "loss": 0.1342,
      "step": 76
    },
    {
      "epoch": 0.061183949145808506,
      "grad_norm": 0.29963284730911255,
      "learning_rate": 0.00019643995749202978,
      "loss": 0.1634,
      "step": 77
    },
    {
      "epoch": 0.06197854588796186,
      "grad_norm": 0.1895529329776764,
      "learning_rate": 0.00019638682252922426,
      "loss": 0.1159,
      "step": 78
    },
    {
      "epoch": 0.06277314263011521,
      "grad_norm": 0.1705477386713028,
      "learning_rate": 0.0001963336875664187,
      "loss": 0.126,
      "step": 79
    },
    {
      "epoch": 0.06356773937226858,
      "grad_norm": 0.20234054327011108,
      "learning_rate": 0.0001962805526036132,
      "loss": 0.109,
      "step": 80
    },
    {
      "epoch": 0.06436233611442194,
      "grad_norm": 0.16916969418525696,
      "learning_rate": 0.00019622741764080764,
      "loss": 0.1424,
      "step": 81
    },
    {
      "epoch": 0.06515693285657528,
      "grad_norm": 0.17861494421958923,
      "learning_rate": 0.00019617428267800212,
      "loss": 0.1154,
      "step": 82
    },
    {
      "epoch": 0.06595152959872864,
      "grad_norm": 0.20245608687400818,
      "learning_rate": 0.0001961211477151966,
      "loss": 0.1337,
      "step": 83
    },
    {
      "epoch": 0.066746126340882,
      "grad_norm": 0.15971405804157257,
      "learning_rate": 0.00019606801275239108,
      "loss": 0.127,
      "step": 84
    },
    {
      "epoch": 0.06754072308303535,
      "grad_norm": 0.1884482502937317,
      "learning_rate": 0.00019601487778958556,
      "loss": 0.1215,
      "step": 85
    },
    {
      "epoch": 0.06833531982518871,
      "grad_norm": 1.080165147781372,
      "learning_rate": 0.00019596174282678004,
      "loss": 0.1195,
      "step": 86
    },
    {
      "epoch": 0.06912991656734208,
      "grad_norm": 0.1586466282606125,
      "learning_rate": 0.00019590860786397452,
      "loss": 0.0987,
      "step": 87
    },
    {
      "epoch": 0.06992451330949544,
      "grad_norm": 0.19644121825695038,
      "learning_rate": 0.00019585547290116898,
      "loss": 0.0735,
      "step": 88
    },
    {
      "epoch": 0.07071911005164878,
      "grad_norm": 0.15086694061756134,
      "learning_rate": 0.00019580233793836346,
      "loss": 0.0821,
      "step": 89
    },
    {
      "epoch": 0.07151370679380215,
      "grad_norm": 0.17575614154338837,
      "learning_rate": 0.0001957492029755579,
      "loss": 0.0847,
      "step": 90
    },
    {
      "epoch": 0.0723083035359555,
      "grad_norm": 0.15817910432815552,
      "learning_rate": 0.0001956960680127524,
      "loss": 0.0775,
      "step": 91
    },
    {
      "epoch": 0.07310290027810885,
      "grad_norm": 0.28526571393013,
      "learning_rate": 0.00019564293304994687,
      "loss": 0.0745,
      "step": 92
    },
    {
      "epoch": 0.07389749702026222,
      "grad_norm": 0.1360766887664795,
      "learning_rate": 0.00019558979808714135,
      "loss": 0.0637,
      "step": 93
    },
    {
      "epoch": 0.07469209376241558,
      "grad_norm": 0.13167662918567657,
      "learning_rate": 0.00019553666312433583,
      "loss": 0.0592,
      "step": 94
    },
    {
      "epoch": 0.07548669050456894,
      "grad_norm": 0.14108988642692566,
      "learning_rate": 0.0001954835281615303,
      "loss": 0.0381,
      "step": 95
    },
    {
      "epoch": 0.07628128724672228,
      "grad_norm": 0.09228210151195526,
      "learning_rate": 0.00019543039319872476,
      "loss": 0.0453,
      "step": 96
    },
    {
      "epoch": 0.07707588398887565,
      "grad_norm": 0.11658669263124466,
      "learning_rate": 0.00019537725823591924,
      "loss": 0.0418,
      "step": 97
    },
    {
      "epoch": 0.07787048073102901,
      "grad_norm": 0.13790568709373474,
      "learning_rate": 0.00019532412327311372,
      "loss": 0.0307,
      "step": 98
    },
    {
      "epoch": 0.07866507747318235,
      "grad_norm": 0.12323784083127975,
      "learning_rate": 0.00019527098831030818,
      "loss": 0.0473,
      "step": 99
    },
    {
      "epoch": 0.07945967421533572,
      "grad_norm": 0.10029814392328262,
      "learning_rate": 0.00019521785334750266,
      "loss": 0.033,
      "step": 100
    },
    {
      "epoch": 0.08025427095748908,
      "grad_norm": 2.2295618057250977,
      "learning_rate": 0.00019516471838469714,
      "loss": 0.8273,
      "step": 101
    },
    {
      "epoch": 0.08104886769964244,
      "grad_norm": 0.9544523358345032,
      "learning_rate": 0.00019511158342189162,
      "loss": 0.5899,
      "step": 102
    },
    {
      "epoch": 0.08184346444179579,
      "grad_norm": 0.7177934646606445,
      "learning_rate": 0.0001950584484590861,
      "loss": 0.4572,
      "step": 103
    },
    {
      "epoch": 0.08263806118394915,
      "grad_norm": 0.38069862127304077,
      "learning_rate": 0.00019500531349628058,
      "loss": 0.3659,
      "step": 104
    },
    {
      "epoch": 0.08343265792610251,
      "grad_norm": 0.3473935127258301,
      "learning_rate": 0.00019495217853347503,
      "loss": 0.3858,
      "step": 105
    },
    {
      "epoch": 0.08422725466825585,
      "grad_norm": 0.36638519167900085,
      "learning_rate": 0.0001948990435706695,
      "loss": 0.3261,
      "step": 106
    },
    {
      "epoch": 0.08502185141040922,
      "grad_norm": 0.5606485605239868,
      "learning_rate": 0.000194845908607864,
      "loss": 0.3292,
      "step": 107
    },
    {
      "epoch": 0.08581644815256258,
      "grad_norm": 2.4229931831359863,
      "learning_rate": 0.00019479277364505844,
      "loss": 0.419,
      "step": 108
    },
    {
      "epoch": 0.08661104489471594,
      "grad_norm": 5.446245193481445,
      "learning_rate": 0.00019473963868225293,
      "loss": 0.3847,
      "step": 109
    },
    {
      "epoch": 0.08740564163686929,
      "grad_norm": 0.5252211689949036,
      "learning_rate": 0.0001946865037194474,
      "loss": 0.3849,
      "step": 110
    },
    {
      "epoch": 0.08820023837902265,
      "grad_norm": 0.539202868938446,
      "learning_rate": 0.00019463336875664189,
      "loss": 0.309,
      "step": 111
    },
    {
      "epoch": 0.08899483512117601,
      "grad_norm": 0.27690789103507996,
      "learning_rate": 0.00019458023379383637,
      "loss": 0.2729,
      "step": 112
    },
    {
      "epoch": 0.08978943186332936,
      "grad_norm": 0.27345505356788635,
      "learning_rate": 0.00019452709883103082,
      "loss": 0.2915,
      "step": 113
    },
    {
      "epoch": 0.09058402860548272,
      "grad_norm": 0.30040210485458374,
      "learning_rate": 0.0001944739638682253,
      "loss": 0.2996,
      "step": 114
    },
    {
      "epoch": 0.09137862534763608,
      "grad_norm": 0.31928932666778564,
      "learning_rate": 0.00019442082890541978,
      "loss": 0.2556,
      "step": 115
    },
    {
      "epoch": 0.09217322208978942,
      "grad_norm": 0.2683221995830536,
      "learning_rate": 0.00019436769394261426,
      "loss": 0.2559,
      "step": 116
    },
    {
      "epoch": 0.09296781883194279,
      "grad_norm": 0.27670955657958984,
      "learning_rate": 0.0001943145589798087,
      "loss": 0.2535,
      "step": 117
    },
    {
      "epoch": 0.09376241557409615,
      "grad_norm": 0.2962208390235901,
      "learning_rate": 0.0001942614240170032,
      "loss": 0.2281,
      "step": 118
    },
    {
      "epoch": 0.09455701231624951,
      "grad_norm": 0.24815694987773895,
      "learning_rate": 0.00019420828905419767,
      "loss": 0.2095,
      "step": 119
    },
    {
      "epoch": 0.09535160905840286,
      "grad_norm": 0.3147117793560028,
      "learning_rate": 0.00019415515409139215,
      "loss": 0.2255,
      "step": 120
    },
    {
      "epoch": 0.09614620580055622,
      "grad_norm": 0.30625513195991516,
      "learning_rate": 0.0001941020191285866,
      "loss": 0.1762,
      "step": 121
    },
    {
      "epoch": 0.09694080254270958,
      "grad_norm": 0.38549360632896423,
      "learning_rate": 0.00019404888416578109,
      "loss": 0.182,
      "step": 122
    },
    {
      "epoch": 0.09773539928486293,
      "grad_norm": 0.34338653087615967,
      "learning_rate": 0.00019399574920297557,
      "loss": 0.1489,
      "step": 123
    },
    {
      "epoch": 0.09852999602701629,
      "grad_norm": 0.3558342456817627,
      "learning_rate": 0.00019394261424017005,
      "loss": 0.1786,
      "step": 124
    },
    {
      "epoch": 0.09932459276916965,
      "grad_norm": 0.39403101801872253,
      "learning_rate": 0.00019388947927736453,
      "loss": 0.1454,
      "step": 125
    },
    {
      "epoch": 0.10011918951132301,
      "grad_norm": 0.45242002606391907,
      "learning_rate": 0.00019383634431455898,
      "loss": 0.1497,
      "step": 126
    },
    {
      "epoch": 0.10091378625347636,
      "grad_norm": 0.66505366563797,
      "learning_rate": 0.00019378320935175346,
      "loss": 0.1607,
      "step": 127
    },
    {
      "epoch": 0.10170838299562972,
      "grad_norm": 1.2089029550552368,
      "learning_rate": 0.00019373007438894794,
      "loss": 0.1418,
      "step": 128
    },
    {
      "epoch": 0.10250297973778308,
      "grad_norm": 0.8559134602546692,
      "learning_rate": 0.0001936769394261424,
      "loss": 0.1274,
      "step": 129
    },
    {
      "epoch": 0.10329757647993643,
      "grad_norm": 0.29022493958473206,
      "learning_rate": 0.00019362380446333687,
      "loss": 0.0959,
      "step": 130
    },
    {
      "epoch": 0.10409217322208979,
      "grad_norm": 0.15817007422447205,
      "learning_rate": 0.00019357066950053135,
      "loss": 0.1159,
      "step": 131
    },
    {
      "epoch": 0.10488676996424315,
      "grad_norm": 0.178268700838089,
      "learning_rate": 0.00019351753453772583,
      "loss": 0.1063,
      "step": 132
    },
    {
      "epoch": 0.10568136670639651,
      "grad_norm": 0.19003534317016602,
      "learning_rate": 0.00019346439957492031,
      "loss": 0.136,
      "step": 133
    },
    {
      "epoch": 0.10647596344854986,
      "grad_norm": 0.1463114619255066,
      "learning_rate": 0.0001934112646121148,
      "loss": 0.1085,
      "step": 134
    },
    {
      "epoch": 0.10727056019070322,
      "grad_norm": 0.15220893919467926,
      "learning_rate": 0.00019335812964930927,
      "loss": 0.0957,
      "step": 135
    },
    {
      "epoch": 0.10806515693285658,
      "grad_norm": 0.13502717018127441,
      "learning_rate": 0.00019330499468650373,
      "loss": 0.0958,
      "step": 136
    },
    {
      "epoch": 0.10885975367500993,
      "grad_norm": 0.14319714903831482,
      "learning_rate": 0.00019325185972369818,
      "loss": 0.1062,
      "step": 137
    },
    {
      "epoch": 0.10965435041716329,
      "grad_norm": 0.15498748421669006,
      "learning_rate": 0.00019319872476089266,
      "loss": 0.0713,
      "step": 138
    },
    {
      "epoch": 0.11044894715931665,
      "grad_norm": 0.1616978794336319,
      "learning_rate": 0.00019314558979808714,
      "loss": 0.103,
      "step": 139
    },
    {
      "epoch": 0.11124354390147001,
      "grad_norm": 0.13654562830924988,
      "learning_rate": 0.00019309245483528162,
      "loss": 0.0786,
      "step": 140
    },
    {
      "epoch": 0.11203814064362336,
      "grad_norm": 0.1615074723958969,
      "learning_rate": 0.0001930393198724761,
      "loss": 0.075,
      "step": 141
    },
    {
      "epoch": 0.11283273738577672,
      "grad_norm": 0.10743191093206406,
      "learning_rate": 0.00019298618490967058,
      "loss": 0.0641,
      "step": 142
    },
    {
      "epoch": 0.11362733412793008,
      "grad_norm": 0.14171114563941956,
      "learning_rate": 0.00019293304994686506,
      "loss": 0.0518,
      "step": 143
    },
    {
      "epoch": 0.11442193087008343,
      "grad_norm": 0.11164332926273346,
      "learning_rate": 0.00019287991498405954,
      "loss": 0.048,
      "step": 144
    },
    {
      "epoch": 0.11521652761223679,
      "grad_norm": 0.10373976826667786,
      "learning_rate": 0.000192826780021254,
      "loss": 0.0583,
      "step": 145
    },
    {
      "epoch": 0.11601112435439015,
      "grad_norm": 0.13332654535770416,
      "learning_rate": 0.00019277364505844845,
      "loss": 0.0367,
      "step": 146
    },
    {
      "epoch": 0.11680572109654351,
      "grad_norm": 0.09105151146650314,
      "learning_rate": 0.00019272051009564293,
      "loss": 0.0506,
      "step": 147
    },
    {
      "epoch": 0.11760031783869686,
      "grad_norm": 0.07898186892271042,
      "learning_rate": 0.0001926673751328374,
      "loss": 0.0367,
      "step": 148
    },
    {
      "epoch": 0.11839491458085022,
      "grad_norm": 0.15004244446754456,
      "learning_rate": 0.0001926142401700319,
      "loss": 0.0397,
      "step": 149
    },
    {
      "epoch": 0.11918951132300358,
      "grad_norm": 0.10065368562936783,
      "learning_rate": 0.00019256110520722637,
      "loss": 0.0252,
      "step": 150
    },
    {
      "epoch": 0.11998410806515693,
      "grad_norm": 2.053464651107788,
      "learning_rate": 0.00019250797024442085,
      "loss": 0.911,
      "step": 151
    },
    {
      "epoch": 0.12077870480731029,
      "grad_norm": 1.0877612829208374,
      "learning_rate": 0.00019245483528161533,
      "loss": 0.5193,
      "step": 152
    },
    {
      "epoch": 0.12157330154946365,
      "grad_norm": 0.5766066908836365,
      "learning_rate": 0.0001924017003188098,
      "loss": 0.4419,
      "step": 153
    },
    {
      "epoch": 0.12236789829161701,
      "grad_norm": 0.3879833221435547,
      "learning_rate": 0.00019234856535600426,
      "loss": 0.3987,
      "step": 154
    },
    {
      "epoch": 0.12316249503377036,
      "grad_norm": 0.3370971083641052,
      "learning_rate": 0.00019229543039319872,
      "loss": 0.3516,
      "step": 155
    },
    {
      "epoch": 0.12395709177592372,
      "grad_norm": 0.3980254530906677,
      "learning_rate": 0.0001922422954303932,
      "loss": 0.3753,
      "step": 156
    },
    {
      "epoch": 0.12475168851807708,
      "grad_norm": 0.3528785705566406,
      "learning_rate": 0.00019218916046758768,
      "loss": 0.3268,
      "step": 157
    },
    {
      "epoch": 0.12554628526023043,
      "grad_norm": 0.3944130837917328,
      "learning_rate": 0.00019213602550478216,
      "loss": 0.3272,
      "step": 158
    },
    {
      "epoch": 0.1263408820023838,
      "grad_norm": 0.25333133339881897,
      "learning_rate": 0.00019208289054197664,
      "loss": 0.2303,
      "step": 159
    },
    {
      "epoch": 0.12713547874453715,
      "grad_norm": 0.3143738806247711,
      "learning_rate": 0.00019202975557917112,
      "loss": 0.3348,
      "step": 160
    },
    {
      "epoch": 0.1279300754866905,
      "grad_norm": 0.25687018036842346,
      "learning_rate": 0.0001919766206163656,
      "loss": 0.3177,
      "step": 161
    },
    {
      "epoch": 0.12872467222884387,
      "grad_norm": 0.27663454413414,
      "learning_rate": 0.00019192348565356005,
      "loss": 0.2664,
      "step": 162
    },
    {
      "epoch": 0.12951926897099722,
      "grad_norm": 0.26116472482681274,
      "learning_rate": 0.00019187035069075453,
      "loss": 0.2391,
      "step": 163
    },
    {
      "epoch": 0.13031386571315057,
      "grad_norm": 0.2535942792892456,
      "learning_rate": 0.00019181721572794898,
      "loss": 0.2354,
      "step": 164
    },
    {
      "epoch": 0.13110846245530394,
      "grad_norm": 0.30917888879776,
      "learning_rate": 0.00019176408076514346,
      "loss": 0.2492,
      "step": 165
    },
    {
      "epoch": 0.1319030591974573,
      "grad_norm": 0.22282648086547852,
      "learning_rate": 0.00019171094580233794,
      "loss": 0.1903,
      "step": 166
    },
    {
      "epoch": 0.13269765593961064,
      "grad_norm": 0.21345986425876617,
      "learning_rate": 0.00019165781083953242,
      "loss": 0.2048,
      "step": 167
    },
    {
      "epoch": 0.133492252681764,
      "grad_norm": 0.20483341813087463,
      "learning_rate": 0.0001916046758767269,
      "loss": 0.1784,
      "step": 168
    },
    {
      "epoch": 0.13428684942391736,
      "grad_norm": 0.23842015862464905,
      "learning_rate": 0.00019155154091392138,
      "loss": 0.1894,
      "step": 169
    },
    {
      "epoch": 0.1350814461660707,
      "grad_norm": 0.19351354241371155,
      "learning_rate": 0.00019149840595111584,
      "loss": 0.1733,
      "step": 170
    },
    {
      "epoch": 0.13587604290822408,
      "grad_norm": 0.1911739706993103,
      "learning_rate": 0.00019144527098831032,
      "loss": 0.1664,
      "step": 171
    },
    {
      "epoch": 0.13667063965037743,
      "grad_norm": 0.18692897260189056,
      "learning_rate": 0.0001913921360255048,
      "loss": 0.1592,
      "step": 172
    },
    {
      "epoch": 0.1374652363925308,
      "grad_norm": 0.19956745207309723,
      "learning_rate": 0.00019133900106269925,
      "loss": 0.1689,
      "step": 173
    },
    {
      "epoch": 0.13825983313468415,
      "grad_norm": 0.20633526146411896,
      "learning_rate": 0.00019128586609989373,
      "loss": 0.1447,
      "step": 174
    },
    {
      "epoch": 0.1390544298768375,
      "grad_norm": 0.1701001673936844,
      "learning_rate": 0.0001912327311370882,
      "loss": 0.1432,
      "step": 175
    },
    {
      "epoch": 0.13984902661899087,
      "grad_norm": 0.217815563082695,
      "learning_rate": 0.0001911795961742827,
      "loss": 0.1469,
      "step": 176
    },
    {
      "epoch": 0.14064362336114422,
      "grad_norm": 0.16553503274917603,
      "learning_rate": 0.00019112646121147717,
      "loss": 0.1273,
      "step": 177
    },
    {
      "epoch": 0.14143822010329757,
      "grad_norm": 0.1608160138130188,
      "learning_rate": 0.00019107332624867162,
      "loss": 0.107,
      "step": 178
    },
    {
      "epoch": 0.14223281684545094,
      "grad_norm": 0.15976440906524658,
      "learning_rate": 0.0001910201912858661,
      "loss": 0.1038,
      "step": 179
    },
    {
      "epoch": 0.1430274135876043,
      "grad_norm": 0.14395356178283691,
      "learning_rate": 0.00019096705632306058,
      "loss": 0.1176,
      "step": 180
    },
    {
      "epoch": 0.14382201032975764,
      "grad_norm": 0.1603294312953949,
      "learning_rate": 0.00019091392136025507,
      "loss": 0.1308,
      "step": 181
    },
    {
      "epoch": 0.144616607071911,
      "grad_norm": 0.1494060456752777,
      "learning_rate": 0.00019086078639744952,
      "loss": 0.1047,
      "step": 182
    },
    {
      "epoch": 0.14541120381406436,
      "grad_norm": 0.14104509353637695,
      "learning_rate": 0.000190807651434644,
      "loss": 0.1106,
      "step": 183
    },
    {
      "epoch": 0.1462058005562177,
      "grad_norm": 0.13810361921787262,
      "learning_rate": 0.00019075451647183848,
      "loss": 0.1022,
      "step": 184
    },
    {
      "epoch": 0.14700039729837108,
      "grad_norm": 0.1312577873468399,
      "learning_rate": 0.00019070138150903296,
      "loss": 0.0748,
      "step": 185
    },
    {
      "epoch": 0.14779499404052443,
      "grad_norm": 0.12264079600572586,
      "learning_rate": 0.0001906482465462274,
      "loss": 0.0969,
      "step": 186
    },
    {
      "epoch": 0.14858959078267778,
      "grad_norm": 0.1455862671136856,
      "learning_rate": 0.0001905951115834219,
      "loss": 0.1209,
      "step": 187
    },
    {
      "epoch": 0.14938418752483115,
      "grad_norm": 0.17013278603553772,
      "learning_rate": 0.00019054197662061637,
      "loss": 0.0815,
      "step": 188
    },
    {
      "epoch": 0.1501787842669845,
      "grad_norm": 0.12392491102218628,
      "learning_rate": 0.00019048884165781085,
      "loss": 0.0988,
      "step": 189
    },
    {
      "epoch": 0.15097338100913787,
      "grad_norm": 0.1342221349477768,
      "learning_rate": 0.00019043570669500533,
      "loss": 0.0677,
      "step": 190
    },
    {
      "epoch": 0.15176797775129122,
      "grad_norm": 0.12016434967517853,
      "learning_rate": 0.0001903825717321998,
      "loss": 0.0629,
      "step": 191
    },
    {
      "epoch": 0.15256257449344457,
      "grad_norm": 0.14949846267700195,
      "learning_rate": 0.00019032943676939427,
      "loss": 0.0847,
      "step": 192
    },
    {
      "epoch": 0.15335717123559794,
      "grad_norm": 0.11531606316566467,
      "learning_rate": 0.00019027630180658875,
      "loss": 0.0965,
      "step": 193
    },
    {
      "epoch": 0.1541517679777513,
      "grad_norm": 0.09808109700679779,
      "learning_rate": 0.00019022316684378323,
      "loss": 0.0548,
      "step": 194
    },
    {
      "epoch": 0.15494636471990464,
      "grad_norm": 0.090995192527771,
      "learning_rate": 0.00019017003188097768,
      "loss": 0.0583,
      "step": 195
    },
    {
      "epoch": 0.15574096146205801,
      "grad_norm": 0.10583622008562088,
      "learning_rate": 0.00019011689691817216,
      "loss": 0.0479,
      "step": 196
    },
    {
      "epoch": 0.15653555820421136,
      "grad_norm": 0.09739190340042114,
      "learning_rate": 0.00019006376195536664,
      "loss": 0.0562,
      "step": 197
    },
    {
      "epoch": 0.1573301549463647,
      "grad_norm": 0.0883338451385498,
      "learning_rate": 0.00019001062699256112,
      "loss": 0.0369,
      "step": 198
    },
    {
      "epoch": 0.15812475168851808,
      "grad_norm": 0.0873837023973465,
      "learning_rate": 0.0001899574920297556,
      "loss": 0.0472,
      "step": 199
    },
    {
      "epoch": 0.15891934843067143,
      "grad_norm": 0.10254475474357605,
      "learning_rate": 0.00018990435706695008,
      "loss": 0.045,
      "step": 200
    },
    {
      "epoch": 0.15971394517282478,
      "grad_norm": 1.2343448400497437,
      "learning_rate": 0.00018985122210414453,
      "loss": 0.7572,
      "step": 201
    },
    {
      "epoch": 0.16050854191497815,
      "grad_norm": 0.6580986380577087,
      "learning_rate": 0.000189798087141339,
      "loss": 0.4785,
      "step": 202
    },
    {
      "epoch": 0.1613031386571315,
      "grad_norm": 0.5708999633789062,
      "learning_rate": 0.00018974495217853347,
      "loss": 0.4374,
      "step": 203
    },
    {
      "epoch": 0.16209773539928488,
      "grad_norm": 0.4224390387535095,
      "learning_rate": 0.00018969181721572795,
      "loss": 0.3299,
      "step": 204
    },
    {
      "epoch": 0.16289233214143822,
      "grad_norm": 0.2796633541584015,
      "learning_rate": 0.00018963868225292243,
      "loss": 0.3331,
      "step": 205
    },
    {
      "epoch": 0.16368692888359157,
      "grad_norm": 0.323624849319458,
      "learning_rate": 0.0001895855472901169,
      "loss": 0.2753,
      "step": 206
    },
    {
      "epoch": 0.16448152562574495,
      "grad_norm": 0.31121471524238586,
      "learning_rate": 0.0001895324123273114,
      "loss": 0.2937,
      "step": 207
    },
    {
      "epoch": 0.1652761223678983,
      "grad_norm": 0.320325642824173,
      "learning_rate": 0.00018947927736450587,
      "loss": 0.3051,
      "step": 208
    },
    {
      "epoch": 0.16607071911005164,
      "grad_norm": 0.2650983929634094,
      "learning_rate": 0.00018942614240170035,
      "loss": 0.233,
      "step": 209
    },
    {
      "epoch": 0.16686531585220502,
      "grad_norm": 0.2619933485984802,
      "learning_rate": 0.0001893730074388948,
      "loss": 0.268,
      "step": 210
    },
    {
      "epoch": 0.16765991259435836,
      "grad_norm": 0.2706146538257599,
      "learning_rate": 0.00018931987247608925,
      "loss": 0.2425,
      "step": 211
    },
    {
      "epoch": 0.1684545093365117,
      "grad_norm": 0.26196661591529846,
      "learning_rate": 0.00018926673751328373,
      "loss": 0.2045,
      "step": 212
    },
    {
      "epoch": 0.16924910607866508,
      "grad_norm": 0.2148316353559494,
      "learning_rate": 0.00018921360255047821,
      "loss": 0.2268,
      "step": 213
    },
    {
      "epoch": 0.17004370282081843,
      "grad_norm": 0.24098920822143555,
      "learning_rate": 0.0001891604675876727,
      "loss": 0.2187,
      "step": 214
    },
    {
      "epoch": 0.17083829956297178,
      "grad_norm": 0.21333810687065125,
      "learning_rate": 0.00018910733262486717,
      "loss": 0.2266,
      "step": 215
    },
    {
      "epoch": 0.17163289630512515,
      "grad_norm": 0.20518247783184052,
      "learning_rate": 0.00018905419766206165,
      "loss": 0.2121,
      "step": 216
    },
    {
      "epoch": 0.1724274930472785,
      "grad_norm": 0.20147433876991272,
      "learning_rate": 0.00018900106269925614,
      "loss": 0.178,
      "step": 217
    },
    {
      "epoch": 0.17322208978943188,
      "grad_norm": 0.19207751750946045,
      "learning_rate": 0.00018894792773645062,
      "loss": 0.1983,
      "step": 218
    },
    {
      "epoch": 0.17401668653158522,
      "grad_norm": 0.18413332104682922,
      "learning_rate": 0.00018889479277364507,
      "loss": 0.1786,
      "step": 219
    },
    {
      "epoch": 0.17481128327373857,
      "grad_norm": 0.1892576813697815,
      "learning_rate": 0.00018884165781083952,
      "loss": 0.15,
      "step": 220
    },
    {
      "epoch": 0.17560588001589195,
      "grad_norm": 0.3022839426994324,
      "learning_rate": 0.000188788522848034,
      "loss": 0.1626,
      "step": 221
    },
    {
      "epoch": 0.1764004767580453,
      "grad_norm": 0.1650765836238861,
      "learning_rate": 0.00018873538788522848,
      "loss": 0.1732,
      "step": 222
    },
    {
      "epoch": 0.17719507350019864,
      "grad_norm": 0.1676028072834015,
      "learning_rate": 0.00018868225292242296,
      "loss": 0.1559,
      "step": 223
    },
    {
      "epoch": 0.17798967024235202,
      "grad_norm": 0.1494373381137848,
      "learning_rate": 0.00018862911795961744,
      "loss": 0.1356,
      "step": 224
    },
    {
      "epoch": 0.17878426698450536,
      "grad_norm": 0.1572408378124237,
      "learning_rate": 0.00018857598299681192,
      "loss": 0.1467,
      "step": 225
    },
    {
      "epoch": 0.1795788637266587,
      "grad_norm": 0.18835245072841644,
      "learning_rate": 0.0001885228480340064,
      "loss": 0.1344,
      "step": 226
    },
    {
      "epoch": 0.18037346046881209,
      "grad_norm": 0.19205854833126068,
      "learning_rate": 0.00018846971307120086,
      "loss": 0.1515,
      "step": 227
    },
    {
      "epoch": 0.18116805721096543,
      "grad_norm": 0.1536608785390854,
      "learning_rate": 0.00018841657810839534,
      "loss": 0.1243,
      "step": 228
    },
    {
      "epoch": 0.18196265395311878,
      "grad_norm": 0.16713586449623108,
      "learning_rate": 0.0001883634431455898,
      "loss": 0.1416,
      "step": 229
    },
    {
      "epoch": 0.18275725069527216,
      "grad_norm": 0.13304764032363892,
      "learning_rate": 0.00018831030818278427,
      "loss": 0.1065,
      "step": 230
    },
    {
      "epoch": 0.1835518474374255,
      "grad_norm": 0.14029183983802795,
      "learning_rate": 0.00018825717321997875,
      "loss": 0.105,
      "step": 231
    },
    {
      "epoch": 0.18434644417957885,
      "grad_norm": 0.14151990413665771,
      "learning_rate": 0.00018820403825717323,
      "loss": 0.1163,
      "step": 232
    },
    {
      "epoch": 0.18514104092173223,
      "grad_norm": 0.13942307233810425,
      "learning_rate": 0.0001881509032943677,
      "loss": 0.0947,
      "step": 233
    },
    {
      "epoch": 0.18593563766388557,
      "grad_norm": 0.12254980951547623,
      "learning_rate": 0.0001880977683315622,
      "loss": 0.0821,
      "step": 234
    },
    {
      "epoch": 0.18673023440603895,
      "grad_norm": 0.13873480260372162,
      "learning_rate": 0.00018804463336875667,
      "loss": 0.0944,
      "step": 235
    },
    {
      "epoch": 0.1875248311481923,
      "grad_norm": 0.1375495195388794,
      "learning_rate": 0.00018799149840595112,
      "loss": 0.0952,
      "step": 236
    },
    {
      "epoch": 0.18831942789034564,
      "grad_norm": 0.12424521148204803,
      "learning_rate": 0.0001879383634431456,
      "loss": 0.0786,
      "step": 237
    },
    {
      "epoch": 0.18911402463249902,
      "grad_norm": 0.18482917547225952,
      "learning_rate": 0.00018788522848034006,
      "loss": 0.0885,
      "step": 238
    },
    {
      "epoch": 0.18990862137465236,
      "grad_norm": 0.10911871492862701,
      "learning_rate": 0.00018783209351753454,
      "loss": 0.0759,
      "step": 239
    },
    {
      "epoch": 0.1907032181168057,
      "grad_norm": 0.11924850195646286,
      "learning_rate": 0.00018777895855472902,
      "loss": 0.0847,
      "step": 240
    },
    {
      "epoch": 0.1914978148589591,
      "grad_norm": 0.10834884643554688,
      "learning_rate": 0.0001877258235919235,
      "loss": 0.0754,
      "step": 241
    },
    {
      "epoch": 0.19229241160111243,
      "grad_norm": 0.09732711315155029,
      "learning_rate": 0.00018767268862911798,
      "loss": 0.0827,
      "step": 242
    },
    {
      "epoch": 0.19308700834326578,
      "grad_norm": 0.09349105507135391,
      "learning_rate": 0.00018761955366631246,
      "loss": 0.0579,
      "step": 243
    },
    {
      "epoch": 0.19388160508541916,
      "grad_norm": 0.11625517904758453,
      "learning_rate": 0.0001875664187035069,
      "loss": 0.0691,
      "step": 244
    },
    {
      "epoch": 0.1946762018275725,
      "grad_norm": 0.10142417252063751,
      "learning_rate": 0.0001875132837407014,
      "loss": 0.0634,
      "step": 245
    },
    {
      "epoch": 0.19547079856972585,
      "grad_norm": 0.11022219806909561,
      "learning_rate": 0.00018746014877789587,
      "loss": 0.0497,
      "step": 246
    },
    {
      "epoch": 0.19626539531187923,
      "grad_norm": 0.07341694831848145,
      "learning_rate": 0.00018740701381509035,
      "loss": 0.0478,
      "step": 247
    },
    {
      "epoch": 0.19705999205403257,
      "grad_norm": 0.08137212693691254,
      "learning_rate": 0.0001873538788522848,
      "loss": 0.0432,
      "step": 248
    },
    {
      "epoch": 0.19785458879618595,
      "grad_norm": 0.09138885140419006,
      "learning_rate": 0.00018730074388947928,
      "loss": 0.0424,
      "step": 249
    },
    {
      "epoch": 0.1986491855383393,
      "grad_norm": 0.09371628612279892,
      "learning_rate": 0.00018724760892667376,
      "loss": 0.038,
      "step": 250
    },
    {
      "epoch": 0.19944378228049264,
      "grad_norm": 0.7443809509277344,
      "learning_rate": 0.00018719447396386824,
      "loss": 0.5868,
      "step": 251
    },
    {
      "epoch": 0.20023837902264602,
      "grad_norm": 0.5104140043258667,
      "learning_rate": 0.0001871413390010627,
      "loss": 0.4128,
      "step": 252
    },
    {
      "epoch": 0.20103297576479937,
      "grad_norm": 0.3993620276451111,
      "learning_rate": 0.00018708820403825718,
      "loss": 0.3477,
      "step": 253
    },
    {
      "epoch": 0.2018275725069527,
      "grad_norm": 0.3042195439338684,
      "learning_rate": 0.00018703506907545166,
      "loss": 0.3175,
      "step": 254
    },
    {
      "epoch": 0.2026221692491061,
      "grad_norm": 0.2376960664987564,
      "learning_rate": 0.00018698193411264614,
      "loss": 0.2944,
      "step": 255
    },
    {
      "epoch": 0.20341676599125944,
      "grad_norm": 0.20891733467578888,
      "learning_rate": 0.00018692879914984062,
      "loss": 0.2698,
      "step": 256
    },
    {
      "epoch": 0.20421136273341278,
      "grad_norm": 0.28546926379203796,
      "learning_rate": 0.00018687566418703507,
      "loss": 0.2663,
      "step": 257
    },
    {
      "epoch": 0.20500595947556616,
      "grad_norm": 0.20936451852321625,
      "learning_rate": 0.00018682252922422955,
      "loss": 0.238,
      "step": 258
    },
    {
      "epoch": 0.2058005562177195,
      "grad_norm": 0.22359099984169006,
      "learning_rate": 0.00018676939426142403,
      "loss": 0.2139,
      "step": 259
    },
    {
      "epoch": 0.20659515295987285,
      "grad_norm": 0.18875464797019958,
      "learning_rate": 0.00018671625929861849,
      "loss": 0.2149,
      "step": 260
    },
    {
      "epoch": 0.20738974970202623,
      "grad_norm": 0.19195617735385895,
      "learning_rate": 0.00018666312433581297,
      "loss": 0.1757,
      "step": 261
    },
    {
      "epoch": 0.20818434644417957,
      "grad_norm": 0.18069887161254883,
      "learning_rate": 0.00018660998937300745,
      "loss": 0.196,
      "step": 262
    },
    {
      "epoch": 0.20897894318633295,
      "grad_norm": 0.1520906686782837,
      "learning_rate": 0.00018655685441020193,
      "loss": 0.1388,
      "step": 263
    },
    {
      "epoch": 0.2097735399284863,
      "grad_norm": 0.19778677821159363,
      "learning_rate": 0.0001865037194473964,
      "loss": 0.1947,
      "step": 264
    },
    {
      "epoch": 0.21056813667063964,
      "grad_norm": 0.18842343986034393,
      "learning_rate": 0.00018645058448459089,
      "loss": 0.1839,
      "step": 265
    },
    {
      "epoch": 0.21136273341279302,
      "grad_norm": 0.1762600988149643,
      "learning_rate": 0.00018639744952178534,
      "loss": 0.1881,
      "step": 266
    },
    {
      "epoch": 0.21215733015494637,
      "grad_norm": 0.20037174224853516,
      "learning_rate": 0.00018634431455897982,
      "loss": 0.1493,
      "step": 267
    },
    {
      "epoch": 0.2129519268970997,
      "grad_norm": 0.17650973796844482,
      "learning_rate": 0.00018629117959617427,
      "loss": 0.1744,
      "step": 268
    },
    {
      "epoch": 0.2137465236392531,
      "grad_norm": 0.20521490275859833,
      "learning_rate": 0.00018623804463336875,
      "loss": 0.1454,
      "step": 269
    },
    {
      "epoch": 0.21454112038140644,
      "grad_norm": 0.171891450881958,
      "learning_rate": 0.00018618490967056323,
      "loss": 0.1903,
      "step": 270
    },
    {
      "epoch": 0.21533571712355978,
      "grad_norm": 0.19535954296588898,
      "learning_rate": 0.0001861317747077577,
      "loss": 0.1648,
      "step": 271
    },
    {
      "epoch": 0.21613031386571316,
      "grad_norm": 0.1603385955095291,
      "learning_rate": 0.0001860786397449522,
      "loss": 0.1662,
      "step": 272
    },
    {
      "epoch": 0.2169249106078665,
      "grad_norm": 0.1688283085823059,
      "learning_rate": 0.00018602550478214667,
      "loss": 0.1324,
      "step": 273
    },
    {
      "epoch": 0.21771950735001985,
      "grad_norm": 0.1454406976699829,
      "learning_rate": 0.00018597236981934115,
      "loss": 0.1349,
      "step": 274
    },
    {
      "epoch": 0.21851410409217323,
      "grad_norm": 0.14433448016643524,
      "learning_rate": 0.0001859192348565356,
      "loss": 0.1244,
      "step": 275
    },
    {
      "epoch": 0.21930870083432658,
      "grad_norm": 0.16377180814743042,
      "learning_rate": 0.00018586609989373006,
      "loss": 0.146,
      "step": 276
    },
    {
      "epoch": 0.22010329757647992,
      "grad_norm": 0.1509755551815033,
      "learning_rate": 0.00018581296493092454,
      "loss": 0.1381,
      "step": 277
    },
    {
      "epoch": 0.2208978943186333,
      "grad_norm": 0.13850636780261993,
      "learning_rate": 0.00018575982996811902,
      "loss": 0.1141,
      "step": 278
    },
    {
      "epoch": 0.22169249106078665,
      "grad_norm": 0.14569461345672607,
      "learning_rate": 0.0001857066950053135,
      "loss": 0.1132,
      "step": 279
    },
    {
      "epoch": 0.22248708780294002,
      "grad_norm": 0.15754817426204681,
      "learning_rate": 0.00018565356004250798,
      "loss": 0.1083,
      "step": 280
    },
    {
      "epoch": 0.22328168454509337,
      "grad_norm": 0.12774896621704102,
      "learning_rate": 0.00018560042507970246,
      "loss": 0.1194,
      "step": 281
    },
    {
      "epoch": 0.22407628128724671,
      "grad_norm": 0.1569223254919052,
      "learning_rate": 0.00018554729011689694,
      "loss": 0.0931,
      "step": 282
    },
    {
      "epoch": 0.2248708780294001,
      "grad_norm": 0.13270846009254456,
      "learning_rate": 0.00018549415515409142,
      "loss": 0.0869,
      "step": 283
    },
    {
      "epoch": 0.22566547477155344,
      "grad_norm": 0.12781941890716553,
      "learning_rate": 0.00018544102019128587,
      "loss": 0.1003,
      "step": 284
    },
    {
      "epoch": 0.22646007151370678,
      "grad_norm": 0.13108161091804504,
      "learning_rate": 0.00018538788522848033,
      "loss": 0.0987,
      "step": 285
    },
    {
      "epoch": 0.22725466825586016,
      "grad_norm": 0.12517087161540985,
      "learning_rate": 0.0001853347502656748,
      "loss": 0.0843,
      "step": 286
    },
    {
      "epoch": 0.2280492649980135,
      "grad_norm": 0.14053288102149963,
      "learning_rate": 0.0001852816153028693,
      "loss": 0.0876,
      "step": 287
    },
    {
      "epoch": 0.22884386174016685,
      "grad_norm": 0.11192777752876282,
      "learning_rate": 0.00018522848034006377,
      "loss": 0.0775,
      "step": 288
    },
    {
      "epoch": 0.22963845848232023,
      "grad_norm": 0.1255486011505127,
      "learning_rate": 0.00018517534537725825,
      "loss": 0.0932,
      "step": 289
    },
    {
      "epoch": 0.23043305522447358,
      "grad_norm": 0.10283544659614563,
      "learning_rate": 0.00018512221041445273,
      "loss": 0.0526,
      "step": 290
    },
    {
      "epoch": 0.23122765196662692,
      "grad_norm": 0.11816684156656265,
      "learning_rate": 0.0001850690754516472,
      "loss": 0.0875,
      "step": 291
    },
    {
      "epoch": 0.2320222487087803,
      "grad_norm": 0.11222042888402939,
      "learning_rate": 0.0001850159404888417,
      "loss": 0.0731,
      "step": 292
    },
    {
      "epoch": 0.23281684545093365,
      "grad_norm": 0.0812532976269722,
      "learning_rate": 0.00018496280552603614,
      "loss": 0.0406,
      "step": 293
    },
    {
      "epoch": 0.23361144219308702,
      "grad_norm": 0.1140429675579071,
      "learning_rate": 0.0001849096705632306,
      "loss": 0.0521,
      "step": 294
    },
    {
      "epoch": 0.23440603893524037,
      "grad_norm": 0.1127929836511612,
      "learning_rate": 0.00018485653560042507,
      "loss": 0.0669,
      "step": 295
    },
    {
      "epoch": 0.23520063567739372,
      "grad_norm": 0.08382664620876312,
      "learning_rate": 0.00018480340063761956,
      "loss": 0.0525,
      "step": 296
    },
    {
      "epoch": 0.2359952324195471,
      "grad_norm": 0.08195595443248749,
      "learning_rate": 0.00018475026567481404,
      "loss": 0.0415,
      "step": 297
    },
    {
      "epoch": 0.23678982916170044,
      "grad_norm": 0.07257259637117386,
      "learning_rate": 0.00018469713071200852,
      "loss": 0.0403,
      "step": 298
    },
    {
      "epoch": 0.23758442590385379,
      "grad_norm": 0.0934387817978859,
      "learning_rate": 0.000184643995749203,
      "loss": 0.056,
      "step": 299
    },
    {
      "epoch": 0.23837902264600716,
      "grad_norm": 0.08408962190151215,
      "learning_rate": 0.00018459086078639748,
      "loss": 0.049,
      "step": 300
    },
    {
      "epoch": 0.2391736193881605,
      "grad_norm": 0.4774269759654999,
      "learning_rate": 0.00018453772582359193,
      "loss": 0.524,
      "step": 301
    },
    {
      "epoch": 0.23996821613031386,
      "grad_norm": 0.3650152385234833,
      "learning_rate": 0.0001844845908607864,
      "loss": 0.3981,
      "step": 302
    },
    {
      "epoch": 0.24076281287246723,
      "grad_norm": 0.4363173842430115,
      "learning_rate": 0.0001844314558979809,
      "loss": 0.4276,
      "step": 303
    },
    {
      "epoch": 0.24155740961462058,
      "grad_norm": 0.27391302585601807,
      "learning_rate": 0.00018437832093517534,
      "loss": 0.3553,
      "step": 304
    },
    {
      "epoch": 0.24235200635677392,
      "grad_norm": 0.3125152587890625,
      "learning_rate": 0.00018432518597236982,
      "loss": 0.3139,
      "step": 305
    },
    {
      "epoch": 0.2431466030989273,
      "grad_norm": 0.24081191420555115,
      "learning_rate": 0.0001842720510095643,
      "loss": 0.3128,
      "step": 306
    },
    {
      "epoch": 0.24394119984108065,
      "grad_norm": 0.1966840624809265,
      "learning_rate": 0.00018421891604675878,
      "loss": 0.2515,
      "step": 307
    },
    {
      "epoch": 0.24473579658323402,
      "grad_norm": 0.18584159016609192,
      "learning_rate": 0.00018416578108395326,
      "loss": 0.2428,
      "step": 308
    },
    {
      "epoch": 0.24553039332538737,
      "grad_norm": 0.21181358397006989,
      "learning_rate": 0.00018411264612114772,
      "loss": 0.2622,
      "step": 309
    },
    {
      "epoch": 0.24632499006754072,
      "grad_norm": 0.1748988926410675,
      "learning_rate": 0.0001840595111583422,
      "loss": 0.2182,
      "step": 310
    },
    {
      "epoch": 0.2471195868096941,
      "grad_norm": 0.20695795118808746,
      "learning_rate": 0.00018400637619553668,
      "loss": 0.2075,
      "step": 311
    },
    {
      "epoch": 0.24791418355184744,
      "grad_norm": 0.1929444521665573,
      "learning_rate": 0.00018395324123273116,
      "loss": 0.179,
      "step": 312
    },
    {
      "epoch": 0.2487087802940008,
      "grad_norm": 0.19815370440483093,
      "learning_rate": 0.0001839001062699256,
      "loss": 0.2067,
      "step": 313
    },
    {
      "epoch": 0.24950337703615416,
      "grad_norm": 0.16783186793327332,
      "learning_rate": 0.0001838469713071201,
      "loss": 0.2002,
      "step": 314
    },
    {
      "epoch": 0.25029797377830754,
      "grad_norm": 0.2093949019908905,
      "learning_rate": 0.00018379383634431457,
      "loss": 0.2317,
      "step": 315
    },
    {
      "epoch": 0.25109257052046086,
      "grad_norm": 0.17659251391887665,
      "learning_rate": 0.00018374070138150905,
      "loss": 0.1722,
      "step": 316
    },
    {
      "epoch": 0.25188716726261423,
      "grad_norm": 0.16365720331668854,
      "learning_rate": 0.0001836875664187035,
      "loss": 0.1864,
      "step": 317
    },
    {
      "epoch": 0.2526817640047676,
      "grad_norm": 0.18505944311618805,
      "learning_rate": 0.00018363443145589798,
      "loss": 0.1847,
      "step": 318
    },
    {
      "epoch": 0.2534763607469209,
      "grad_norm": 0.18928128480911255,
      "learning_rate": 0.00018358129649309246,
      "loss": 0.1922,
      "step": 319
    },
    {
      "epoch": 0.2542709574890743,
      "grad_norm": 0.18404938280582428,
      "learning_rate": 0.00018352816153028694,
      "loss": 0.156,
      "step": 320
    },
    {
      "epoch": 0.2550655542312277,
      "grad_norm": 0.1524052619934082,
      "learning_rate": 0.00018347502656748142,
      "loss": 0.173,
      "step": 321
    },
    {
      "epoch": 0.255860150973381,
      "grad_norm": 0.15880431234836578,
      "learning_rate": 0.00018342189160467588,
      "loss": 0.1636,
      "step": 322
    },
    {
      "epoch": 0.25665474771553437,
      "grad_norm": 0.17561404407024384,
      "learning_rate": 0.00018336875664187036,
      "loss": 0.1545,
      "step": 323
    },
    {
      "epoch": 0.25744934445768775,
      "grad_norm": 0.20384939014911652,
      "learning_rate": 0.00018331562167906484,
      "loss": 0.162,
      "step": 324
    },
    {
      "epoch": 0.25824394119984106,
      "grad_norm": 0.17336349189281464,
      "learning_rate": 0.00018326248671625932,
      "loss": 0.1413,
      "step": 325
    },
    {
      "epoch": 0.25903853794199444,
      "grad_norm": 0.16647028923034668,
      "learning_rate": 0.00018320935175345377,
      "loss": 0.1673,
      "step": 326
    },
    {
      "epoch": 0.2598331346841478,
      "grad_norm": 0.17103615403175354,
      "learning_rate": 0.00018315621679064825,
      "loss": 0.1292,
      "step": 327
    },
    {
      "epoch": 0.26062773142630113,
      "grad_norm": 0.13630561530590057,
      "learning_rate": 0.00018310308182784273,
      "loss": 0.1179,
      "step": 328
    },
    {
      "epoch": 0.2614223281684545,
      "grad_norm": 0.1453106552362442,
      "learning_rate": 0.0001830499468650372,
      "loss": 0.1195,
      "step": 329
    },
    {
      "epoch": 0.2622169249106079,
      "grad_norm": 0.1364022195339203,
      "learning_rate": 0.0001829968119022317,
      "loss": 0.1318,
      "step": 330
    },
    {
      "epoch": 0.2630115216527612,
      "grad_norm": 0.12560318410396576,
      "learning_rate": 0.00018294367693942614,
      "loss": 0.1189,
      "step": 331
    },
    {
      "epoch": 0.2638061183949146,
      "grad_norm": 0.15900033712387085,
      "learning_rate": 0.00018289054197662063,
      "loss": 0.1495,
      "step": 332
    },
    {
      "epoch": 0.26460071513706795,
      "grad_norm": 0.12530042231082916,
      "learning_rate": 0.0001828374070138151,
      "loss": 0.1131,
      "step": 333
    },
    {
      "epoch": 0.2653953118792213,
      "grad_norm": 0.12570373713970184,
      "learning_rate": 0.00018278427205100956,
      "loss": 0.0941,
      "step": 334
    },
    {
      "epoch": 0.26618990862137465,
      "grad_norm": 0.12103325873613358,
      "learning_rate": 0.00018273113708820404,
      "loss": 0.1125,
      "step": 335
    },
    {
      "epoch": 0.266984505363528,
      "grad_norm": 0.22631710767745972,
      "learning_rate": 0.00018267800212539852,
      "loss": 0.1005,
      "step": 336
    },
    {
      "epoch": 0.26777910210568134,
      "grad_norm": 0.12893043458461761,
      "learning_rate": 0.000182624867162593,
      "loss": 0.0928,
      "step": 337
    },
    {
      "epoch": 0.2685736988478347,
      "grad_norm": 0.10584737360477448,
      "learning_rate": 0.00018257173219978748,
      "loss": 0.1083,
      "step": 338
    },
    {
      "epoch": 0.2693682955899881,
      "grad_norm": 0.11462851613759995,
      "learning_rate": 0.00018251859723698196,
      "loss": 0.0821,
      "step": 339
    },
    {
      "epoch": 0.2701628923321414,
      "grad_norm": 0.1762094497680664,
      "learning_rate": 0.0001824654622741764,
      "loss": 0.0866,
      "step": 340
    },
    {
      "epoch": 0.2709574890742948,
      "grad_norm": 0.1287495344877243,
      "learning_rate": 0.0001824123273113709,
      "loss": 0.0956,
      "step": 341
    },
    {
      "epoch": 0.27175208581644816,
      "grad_norm": 0.09796353429555893,
      "learning_rate": 0.00018235919234856535,
      "loss": 0.0682,
      "step": 342
    },
    {
      "epoch": 0.2725466825586015,
      "grad_norm": 0.09211049228906631,
      "learning_rate": 0.00018230605738575983,
      "loss": 0.0709,
      "step": 343
    },
    {
      "epoch": 0.27334127930075486,
      "grad_norm": 0.07535607367753983,
      "learning_rate": 0.0001822529224229543,
      "loss": 0.0663,
      "step": 344
    },
    {
      "epoch": 0.27413587604290823,
      "grad_norm": 0.07346019893884659,
      "learning_rate": 0.00018219978746014879,
      "loss": 0.0489,
      "step": 345
    },
    {
      "epoch": 0.2749304727850616,
      "grad_norm": 0.08231117576360703,
      "learning_rate": 0.00018214665249734327,
      "loss": 0.0513,
      "step": 346
    },
    {
      "epoch": 0.2757250695272149,
      "grad_norm": 0.07508116960525513,
      "learning_rate": 0.00018209351753453775,
      "loss": 0.0453,
      "step": 347
    },
    {
      "epoch": 0.2765196662693683,
      "grad_norm": 0.08778450638055801,
      "learning_rate": 0.00018204038257173223,
      "loss": 0.0418,
      "step": 348
    },
    {
      "epoch": 0.2773142630115217,
      "grad_norm": 0.08524610847234726,
      "learning_rate": 0.00018198724760892668,
      "loss": 0.0454,
      "step": 349
    },
    {
      "epoch": 0.278108859753675,
      "grad_norm": 0.08434765785932541,
      "learning_rate": 0.00018193411264612113,
      "loss": 0.0412,
      "step": 350
    },
    {
      "epoch": 0.2789034564958284,
      "grad_norm": 0.5407375693321228,
      "learning_rate": 0.0001818809776833156,
      "loss": 0.6257,
      "step": 351
    },
    {
      "epoch": 0.27969805323798175,
      "grad_norm": 0.4241466522216797,
      "learning_rate": 0.0001818278427205101,
      "loss": 0.4843,
      "step": 352
    },
    {
      "epoch": 0.28049264998013507,
      "grad_norm": 0.4140092432498932,
      "learning_rate": 0.00018177470775770457,
      "loss": 0.3838,
      "step": 353
    },
    {
      "epoch": 0.28128724672228844,
      "grad_norm": 0.3156092166900635,
      "learning_rate": 0.00018172157279489905,
      "loss": 0.3705,
      "step": 354
    },
    {
      "epoch": 0.2820818434644418,
      "grad_norm": 0.26371702551841736,
      "learning_rate": 0.00018166843783209353,
      "loss": 0.3146,
      "step": 355
    },
    {
      "epoch": 0.28287644020659514,
      "grad_norm": 0.2153661996126175,
      "learning_rate": 0.00018161530286928801,
      "loss": 0.3199,
      "step": 356
    },
    {
      "epoch": 0.2836710369487485,
      "grad_norm": 0.20497655868530273,
      "learning_rate": 0.0001815621679064825,
      "loss": 0.3051,
      "step": 357
    },
    {
      "epoch": 0.2844656336909019,
      "grad_norm": 0.23229101300239563,
      "learning_rate": 0.00018150903294367695,
      "loss": 0.2773,
      "step": 358
    },
    {
      "epoch": 0.2852602304330552,
      "grad_norm": 0.19682466983795166,
      "learning_rate": 0.00018145589798087143,
      "loss": 0.2325,
      "step": 359
    },
    {
      "epoch": 0.2860548271752086,
      "grad_norm": 0.21088896691799164,
      "learning_rate": 0.00018140276301806588,
      "loss": 0.2387,
      "step": 360
    },
    {
      "epoch": 0.28684942391736196,
      "grad_norm": 0.21169604361057281,
      "learning_rate": 0.00018134962805526036,
      "loss": 0.2579,
      "step": 361
    },
    {
      "epoch": 0.2876440206595153,
      "grad_norm": 0.1782296746969223,
      "learning_rate": 0.00018129649309245484,
      "loss": 0.2114,
      "step": 362
    },
    {
      "epoch": 0.28843861740166865,
      "grad_norm": 0.1722038835287094,
      "learning_rate": 0.00018124335812964932,
      "loss": 0.2269,
      "step": 363
    },
    {
      "epoch": 0.289233214143822,
      "grad_norm": 0.1924397051334381,
      "learning_rate": 0.0001811902231668438,
      "loss": 0.2185,
      "step": 364
    },
    {
      "epoch": 0.29002781088597535,
      "grad_norm": 0.16414694488048553,
      "learning_rate": 0.00018113708820403828,
      "loss": 0.2058,
      "step": 365
    },
    {
      "epoch": 0.2908224076281287,
      "grad_norm": 0.15329936146736145,
      "learning_rate": 0.00018108395324123273,
      "loss": 0.1657,
      "step": 366
    },
    {
      "epoch": 0.2916170043702821,
      "grad_norm": 0.1886078417301178,
      "learning_rate": 0.00018103081827842721,
      "loss": 0.1923,
      "step": 367
    },
    {
      "epoch": 0.2924116011124354,
      "grad_norm": 0.15114136040210724,
      "learning_rate": 0.0001809776833156217,
      "loss": 0.1712,
      "step": 368
    },
    {
      "epoch": 0.2932061978545888,
      "grad_norm": 0.16258707642555237,
      "learning_rate": 0.00018092454835281615,
      "loss": 0.1614,
      "step": 369
    },
    {
      "epoch": 0.29400079459674217,
      "grad_norm": 0.174740269780159,
      "learning_rate": 0.00018087141339001063,
      "loss": 0.1563,
      "step": 370
    },
    {
      "epoch": 0.2947953913388955,
      "grad_norm": 0.16331592202186584,
      "learning_rate": 0.0001808182784272051,
      "loss": 0.1242,
      "step": 371
    },
    {
      "epoch": 0.29558998808104886,
      "grad_norm": 0.14589998126029968,
      "learning_rate": 0.0001807651434643996,
      "loss": 0.1481,
      "step": 372
    },
    {
      "epoch": 0.29638458482320224,
      "grad_norm": 0.17812074720859528,
      "learning_rate": 0.00018071200850159407,
      "loss": 0.1262,
      "step": 373
    },
    {
      "epoch": 0.29717918156535555,
      "grad_norm": 0.16151568293571472,
      "learning_rate": 0.00018065887353878855,
      "loss": 0.1442,
      "step": 374
    },
    {
      "epoch": 0.29797377830750893,
      "grad_norm": 0.19634833931922913,
      "learning_rate": 0.000180605738575983,
      "loss": 0.1232,
      "step": 375
    },
    {
      "epoch": 0.2987683750496623,
      "grad_norm": 0.1378592848777771,
      "learning_rate": 0.00018055260361317748,
      "loss": 0.1157,
      "step": 376
    },
    {
      "epoch": 0.2995629717918157,
      "grad_norm": 0.1596001535654068,
      "learning_rate": 0.00018049946865037196,
      "loss": 0.1253,
      "step": 377
    },
    {
      "epoch": 0.300357568533969,
      "grad_norm": 0.18994049727916718,
      "learning_rate": 0.00018044633368756642,
      "loss": 0.1413,
      "step": 378
    },
    {
      "epoch": 0.3011521652761224,
      "grad_norm": 0.22402173280715942,
      "learning_rate": 0.0001803931987247609,
      "loss": 0.1374,
      "step": 379
    },
    {
      "epoch": 0.30194676201827575,
      "grad_norm": 0.15065738558769226,
      "learning_rate": 0.00018034006376195538,
      "loss": 0.1121,
      "step": 380
    },
    {
      "epoch": 0.30274135876042907,
      "grad_norm": 0.16096651554107666,
      "learning_rate": 0.00018028692879914986,
      "loss": 0.1007,
      "step": 381
    },
    {
      "epoch": 0.30353595550258244,
      "grad_norm": 0.18562498688697815,
      "learning_rate": 0.00018023379383634434,
      "loss": 0.1261,
      "step": 382
    },
    {
      "epoch": 0.3043305522447358,
      "grad_norm": 0.1083584576845169,
      "learning_rate": 0.0001801806588735388,
      "loss": 0.0867,
      "step": 383
    },
    {
      "epoch": 0.30512514898688914,
      "grad_norm": 0.13345786929130554,
      "learning_rate": 0.00018012752391073327,
      "loss": 0.0986,
      "step": 384
    },
    {
      "epoch": 0.3059197457290425,
      "grad_norm": 0.11899159103631973,
      "learning_rate": 0.00018007438894792775,
      "loss": 0.0867,
      "step": 385
    },
    {
      "epoch": 0.3067143424711959,
      "grad_norm": 0.11915700882673264,
      "learning_rate": 0.00018002125398512223,
      "loss": 0.1014,
      "step": 386
    },
    {
      "epoch": 0.3075089392133492,
      "grad_norm": 0.1470143049955368,
      "learning_rate": 0.00017996811902231668,
      "loss": 0.0981,
      "step": 387
    },
    {
      "epoch": 0.3083035359555026,
      "grad_norm": 0.13227562606334686,
      "learning_rate": 0.00017991498405951116,
      "loss": 0.0923,
      "step": 388
    },
    {
      "epoch": 0.30909813269765596,
      "grad_norm": 0.11816904693841934,
      "learning_rate": 0.00017986184909670564,
      "loss": 0.0731,
      "step": 389
    },
    {
      "epoch": 0.3098927294398093,
      "grad_norm": 0.1253548562526703,
      "learning_rate": 0.00017980871413390012,
      "loss": 0.0913,
      "step": 390
    },
    {
      "epoch": 0.31068732618196265,
      "grad_norm": 0.10315605252981186,
      "learning_rate": 0.00017975557917109458,
      "loss": 0.0752,
      "step": 391
    },
    {
      "epoch": 0.31148192292411603,
      "grad_norm": 0.0959479808807373,
      "learning_rate": 0.00017970244420828906,
      "loss": 0.0664,
      "step": 392
    },
    {
      "epoch": 0.31227651966626935,
      "grad_norm": 0.11472196877002716,
      "learning_rate": 0.00017964930924548354,
      "loss": 0.0755,
      "step": 393
    },
    {
      "epoch": 0.3130711164084227,
      "grad_norm": 0.12279083579778671,
      "learning_rate": 0.00017959617428267802,
      "loss": 0.0802,
      "step": 394
    },
    {
      "epoch": 0.3138657131505761,
      "grad_norm": 0.10654245316982269,
      "learning_rate": 0.0001795430393198725,
      "loss": 0.0575,
      "step": 395
    },
    {
      "epoch": 0.3146603098927294,
      "grad_norm": 0.102593332529068,
      "learning_rate": 0.00017948990435706695,
      "loss": 0.0597,
      "step": 396
    },
    {
      "epoch": 0.3154549066348828,
      "grad_norm": 0.09773418307304382,
      "learning_rate": 0.00017943676939426143,
      "loss": 0.0527,
      "step": 397
    },
    {
      "epoch": 0.31624950337703617,
      "grad_norm": 0.13269393146038055,
      "learning_rate": 0.0001793836344314559,
      "loss": 0.0457,
      "step": 398
    },
    {
      "epoch": 0.3170441001191895,
      "grad_norm": 0.10617385804653168,
      "learning_rate": 0.00017933049946865036,
      "loss": 0.0359,
      "step": 399
    },
    {
      "epoch": 0.31783869686134286,
      "grad_norm": 0.10344933718442917,
      "learning_rate": 0.00017927736450584484,
      "loss": 0.038,
      "step": 400
    },
    {
      "epoch": 0.31863329360349624,
      "grad_norm": 0.815968930721283,
      "learning_rate": 0.00017922422954303932,
      "loss": 0.5297,
      "step": 401
    },
    {
      "epoch": 0.31942789034564956,
      "grad_norm": 0.5795943140983582,
      "learning_rate": 0.0001791710945802338,
      "loss": 0.512,
      "step": 402
    },
    {
      "epoch": 0.32022248708780293,
      "grad_norm": 0.5341584086418152,
      "learning_rate": 0.00017911795961742828,
      "loss": 0.4963,
      "step": 403
    },
    {
      "epoch": 0.3210170838299563,
      "grad_norm": 0.33977898955345154,
      "learning_rate": 0.00017906482465462277,
      "loss": 0.3811,
      "step": 404
    },
    {
      "epoch": 0.3218116805721097,
      "grad_norm": 0.2823364734649658,
      "learning_rate": 0.00017901168969181722,
      "loss": 0.31,
      "step": 405
    },
    {
      "epoch": 0.322606277314263,
      "grad_norm": 0.21516966819763184,
      "learning_rate": 0.0001789585547290117,
      "loss": 0.3021,
      "step": 406
    },
    {
      "epoch": 0.3234008740564164,
      "grad_norm": 0.2240457832813263,
      "learning_rate": 0.00017890541976620615,
      "loss": 0.2808,
      "step": 407
    },
    {
      "epoch": 0.32419547079856975,
      "grad_norm": 0.19748429954051971,
      "learning_rate": 0.00017885228480340063,
      "loss": 0.2692,
      "step": 408
    },
    {
      "epoch": 0.32499006754072307,
      "grad_norm": 0.21727398037910461,
      "learning_rate": 0.0001787991498405951,
      "loss": 0.2696,
      "step": 409
    },
    {
      "epoch": 0.32578466428287645,
      "grad_norm": 0.29306739568710327,
      "learning_rate": 0.0001787460148777896,
      "loss": 0.2531,
      "step": 410
    },
    {
      "epoch": 0.3265792610250298,
      "grad_norm": 0.21117408573627472,
      "learning_rate": 0.00017869287991498407,
      "loss": 0.2479,
      "step": 411
    },
    {
      "epoch": 0.32737385776718314,
      "grad_norm": 0.19446948170661926,
      "learning_rate": 0.00017863974495217855,
      "loss": 0.2156,
      "step": 412
    },
    {
      "epoch": 0.3281684545093365,
      "grad_norm": 0.17669881880283356,
      "learning_rate": 0.00017858660998937303,
      "loss": 0.1922,
      "step": 413
    },
    {
      "epoch": 0.3289630512514899,
      "grad_norm": 0.18860073387622833,
      "learning_rate": 0.00017853347502656749,
      "loss": 0.2055,
      "step": 414
    },
    {
      "epoch": 0.3297576479936432,
      "grad_norm": 0.20367181301116943,
      "learning_rate": 0.00017848034006376197,
      "loss": 0.217,
      "step": 415
    },
    {
      "epoch": 0.3305522447357966,
      "grad_norm": 0.20219364762306213,
      "learning_rate": 0.00017842720510095642,
      "loss": 0.22,
      "step": 416
    },
    {
      "epoch": 0.33134684147794996,
      "grad_norm": 0.20035719871520996,
      "learning_rate": 0.0001783740701381509,
      "loss": 0.1911,
      "step": 417
    },
    {
      "epoch": 0.3321414382201033,
      "grad_norm": 0.22803983092308044,
      "learning_rate": 0.00017832093517534538,
      "loss": 0.2466,
      "step": 418
    },
    {
      "epoch": 0.33293603496225666,
      "grad_norm": 0.18323004245758057,
      "learning_rate": 0.00017826780021253986,
      "loss": 0.1696,
      "step": 419
    },
    {
      "epoch": 0.33373063170441003,
      "grad_norm": 0.19410443305969238,
      "learning_rate": 0.00017821466524973434,
      "loss": 0.1858,
      "step": 420
    },
    {
      "epoch": 0.33452522844656335,
      "grad_norm": 0.18000756204128265,
      "learning_rate": 0.00017816153028692882,
      "loss": 0.1522,
      "step": 421
    },
    {
      "epoch": 0.3353198251887167,
      "grad_norm": 0.19147346913814545,
      "learning_rate": 0.0001781083953241233,
      "loss": 0.1403,
      "step": 422
    },
    {
      "epoch": 0.3361144219308701,
      "grad_norm": 0.16993501782417297,
      "learning_rate": 0.00017805526036131775,
      "loss": 0.1501,
      "step": 423
    },
    {
      "epoch": 0.3369090186730234,
      "grad_norm": 0.32250386476516724,
      "learning_rate": 0.00017800212539851223,
      "loss": 0.152,
      "step": 424
    },
    {
      "epoch": 0.3377036154151768,
      "grad_norm": 0.1447669267654419,
      "learning_rate": 0.0001779489904357067,
      "loss": 0.1295,
      "step": 425
    },
    {
      "epoch": 0.33849821215733017,
      "grad_norm": 0.15887509286403656,
      "learning_rate": 0.00017789585547290117,
      "loss": 0.126,
      "step": 426
    },
    {
      "epoch": 0.3392928088994835,
      "grad_norm": 0.20491543412208557,
      "learning_rate": 0.00017784272051009565,
      "loss": 0.1499,
      "step": 427
    },
    {
      "epoch": 0.34008740564163686,
      "grad_norm": 0.14907881617546082,
      "learning_rate": 0.00017778958554729013,
      "loss": 0.1324,
      "step": 428
    },
    {
      "epoch": 0.34088200238379024,
      "grad_norm": 0.23097571730613708,
      "learning_rate": 0.0001777364505844846,
      "loss": 0.1274,
      "step": 429
    },
    {
      "epoch": 0.34167659912594356,
      "grad_norm": 0.14563998579978943,
      "learning_rate": 0.0001776833156216791,
      "loss": 0.1071,
      "step": 430
    },
    {
      "epoch": 0.34247119586809693,
      "grad_norm": 0.14084996283054352,
      "learning_rate": 0.00017763018065887357,
      "loss": 0.1164,
      "step": 431
    },
    {
      "epoch": 0.3432657926102503,
      "grad_norm": 0.15502925217151642,
      "learning_rate": 0.00017757704569606802,
      "loss": 0.116,
      "step": 432
    },
    {
      "epoch": 0.34406038935240363,
      "grad_norm": 0.15625321865081787,
      "learning_rate": 0.0001775239107332625,
      "loss": 0.0876,
      "step": 433
    },
    {
      "epoch": 0.344854986094557,
      "grad_norm": 0.12899446487426758,
      "learning_rate": 0.00017747077577045695,
      "loss": 0.1049,
      "step": 434
    },
    {
      "epoch": 0.3456495828367104,
      "grad_norm": 0.1274585872888565,
      "learning_rate": 0.00017741764080765143,
      "loss": 0.0988,
      "step": 435
    },
    {
      "epoch": 0.34644417957886375,
      "grad_norm": 0.1087879091501236,
      "learning_rate": 0.00017736450584484591,
      "loss": 0.0789,
      "step": 436
    },
    {
      "epoch": 0.3472387763210171,
      "grad_norm": 0.13481460511684418,
      "learning_rate": 0.0001773113708820404,
      "loss": 0.0827,
      "step": 437
    },
    {
      "epoch": 0.34803337306317045,
      "grad_norm": 0.12952278554439545,
      "learning_rate": 0.00017725823591923487,
      "loss": 0.1103,
      "step": 438
    },
    {
      "epoch": 0.3488279698053238,
      "grad_norm": 0.13442721962928772,
      "learning_rate": 0.00017720510095642935,
      "loss": 0.0872,
      "step": 439
    },
    {
      "epoch": 0.34962256654747714,
      "grad_norm": 0.13618211448192596,
      "learning_rate": 0.0001771519659936238,
      "loss": 0.0737,
      "step": 440
    },
    {
      "epoch": 0.3504171632896305,
      "grad_norm": 0.1181938424706459,
      "learning_rate": 0.0001770988310308183,
      "loss": 0.0886,
      "step": 441
    },
    {
      "epoch": 0.3512117600317839,
      "grad_norm": 0.12918734550476074,
      "learning_rate": 0.00017704569606801277,
      "loss": 0.0795,
      "step": 442
    },
    {
      "epoch": 0.3520063567739372,
      "grad_norm": 0.13566656410694122,
      "learning_rate": 0.00017699256110520722,
      "loss": 0.0614,
      "step": 443
    },
    {
      "epoch": 0.3528009535160906,
      "grad_norm": 0.10747001320123672,
      "learning_rate": 0.0001769394261424017,
      "loss": 0.06,
      "step": 444
    },
    {
      "epoch": 0.35359555025824396,
      "grad_norm": 0.10960114747285843,
      "learning_rate": 0.00017688629117959618,
      "loss": 0.0484,
      "step": 445
    },
    {
      "epoch": 0.3543901470003973,
      "grad_norm": 0.1085306704044342,
      "learning_rate": 0.00017683315621679066,
      "loss": 0.0652,
      "step": 446
    },
    {
      "epoch": 0.35518474374255066,
      "grad_norm": 0.09818721562623978,
      "learning_rate": 0.00017678002125398514,
      "loss": 0.0434,
      "step": 447
    },
    {
      "epoch": 0.35597934048470403,
      "grad_norm": 0.11145469546318054,
      "learning_rate": 0.0001767268862911796,
      "loss": 0.0358,
      "step": 448
    },
    {
      "epoch": 0.35677393722685735,
      "grad_norm": 0.09422564506530762,
      "learning_rate": 0.00017667375132837408,
      "loss": 0.0398,
      "step": 449
    },
    {
      "epoch": 0.3575685339690107,
      "grad_norm": 0.09531432390213013,
      "learning_rate": 0.00017662061636556856,
      "loss": 0.0331,
      "step": 450
    },
    {
      "epoch": 0.3583631307111641,
      "grad_norm": 1.0477309226989746,
      "learning_rate": 0.00017656748140276304,
      "loss": 0.5933,
      "step": 451
    },
    {
      "epoch": 0.3591577274533174,
      "grad_norm": 0.6669835448265076,
      "learning_rate": 0.0001765143464399575,
      "loss": 0.5073,
      "step": 452
    },
    {
      "epoch": 0.3599523241954708,
      "grad_norm": 0.5718496441841125,
      "learning_rate": 0.00017646121147715197,
      "loss": 0.4632,
      "step": 453
    },
    {
      "epoch": 0.36074692093762417,
      "grad_norm": 0.38763612508773804,
      "learning_rate": 0.00017640807651434645,
      "loss": 0.4034,
      "step": 454
    },
    {
      "epoch": 0.3615415176797775,
      "grad_norm": 0.3436497747898102,
      "learning_rate": 0.00017635494155154093,
      "loss": 0.3826,
      "step": 455
    },
    {
      "epoch": 0.36233611442193087,
      "grad_norm": 0.2841578423976898,
      "learning_rate": 0.00017630180658873538,
      "loss": 0.2877,
      "step": 456
    },
    {
      "epoch": 0.36313071116408424,
      "grad_norm": 0.19007021188735962,
      "learning_rate": 0.00017624867162592986,
      "loss": 0.2408,
      "step": 457
    },
    {
      "epoch": 0.36392530790623756,
      "grad_norm": 0.20825901627540588,
      "learning_rate": 0.00017619553666312434,
      "loss": 0.26,
      "step": 458
    },
    {
      "epoch": 0.36471990464839094,
      "grad_norm": 0.21573509275913239,
      "learning_rate": 0.00017614240170031882,
      "loss": 0.2335,
      "step": 459
    },
    {
      "epoch": 0.3655145013905443,
      "grad_norm": 0.30814629793167114,
      "learning_rate": 0.0001760892667375133,
      "loss": 0.2577,
      "step": 460
    },
    {
      "epoch": 0.36630909813269763,
      "grad_norm": 0.21933895349502563,
      "learning_rate": 0.00017603613177470776,
      "loss": 0.2347,
      "step": 461
    },
    {
      "epoch": 0.367103694874851,
      "grad_norm": 0.19775919616222382,
      "learning_rate": 0.00017598299681190224,
      "loss": 0.2031,
      "step": 462
    },
    {
      "epoch": 0.3678982916170044,
      "grad_norm": 0.2925421893596649,
      "learning_rate": 0.00017592986184909672,
      "loss": 0.2292,
      "step": 463
    },
    {
      "epoch": 0.3686928883591577,
      "grad_norm": 0.22522221505641937,
      "learning_rate": 0.0001758767268862912,
      "loss": 0.2199,
      "step": 464
    },
    {
      "epoch": 0.3694874851013111,
      "grad_norm": 0.21991795301437378,
      "learning_rate": 0.00017582359192348565,
      "loss": 0.1943,
      "step": 465
    },
    {
      "epoch": 0.37028208184346445,
      "grad_norm": 0.19432100653648376,
      "learning_rate": 0.00017577045696068013,
      "loss": 0.1902,
      "step": 466
    },
    {
      "epoch": 0.3710766785856178,
      "grad_norm": 0.19416193664073944,
      "learning_rate": 0.0001757173219978746,
      "loss": 0.1873,
      "step": 467
    },
    {
      "epoch": 0.37187127532777114,
      "grad_norm": 0.22416891157627106,
      "learning_rate": 0.0001756641870350691,
      "loss": 0.1635,
      "step": 468
    },
    {
      "epoch": 0.3726658720699245,
      "grad_norm": 0.15995021164417267,
      "learning_rate": 0.00017561105207226357,
      "loss": 0.1601,
      "step": 469
    },
    {
      "epoch": 0.3734604688120779,
      "grad_norm": 0.1844729781150818,
      "learning_rate": 0.00017555791710945802,
      "loss": 0.1675,
      "step": 470
    },
    {
      "epoch": 0.3742550655542312,
      "grad_norm": 0.17936724424362183,
      "learning_rate": 0.0001755047821466525,
      "loss": 0.161,
      "step": 471
    },
    {
      "epoch": 0.3750496622963846,
      "grad_norm": 0.2097606360912323,
      "learning_rate": 0.00017545164718384698,
      "loss": 0.1472,
      "step": 472
    },
    {
      "epoch": 0.37584425903853796,
      "grad_norm": 0.18350501358509064,
      "learning_rate": 0.00017539851222104144,
      "loss": 0.1527,
      "step": 473
    },
    {
      "epoch": 0.3766388557806913,
      "grad_norm": 0.16713862121105194,
      "learning_rate": 0.00017534537725823592,
      "loss": 0.1599,
      "step": 474
    },
    {
      "epoch": 0.37743345252284466,
      "grad_norm": 0.19787098467350006,
      "learning_rate": 0.0001752922422954304,
      "loss": 0.1341,
      "step": 475
    },
    {
      "epoch": 0.37822804926499803,
      "grad_norm": 0.23306138813495636,
      "learning_rate": 0.00017523910733262488,
      "loss": 0.1548,
      "step": 476
    },
    {
      "epoch": 0.37902264600715135,
      "grad_norm": 0.172861248254776,
      "learning_rate": 0.00017518597236981936,
      "loss": 0.1209,
      "step": 477
    },
    {
      "epoch": 0.37981724274930473,
      "grad_norm": 0.18460042774677277,
      "learning_rate": 0.00017513283740701384,
      "loss": 0.1088,
      "step": 478
    },
    {
      "epoch": 0.3806118394914581,
      "grad_norm": 0.14146049320697784,
      "learning_rate": 0.0001750797024442083,
      "loss": 0.1257,
      "step": 479
    },
    {
      "epoch": 0.3814064362336114,
      "grad_norm": 0.16980722546577454,
      "learning_rate": 0.00017502656748140277,
      "loss": 0.1331,
      "step": 480
    },
    {
      "epoch": 0.3822010329757648,
      "grad_norm": 0.14929276704788208,
      "learning_rate": 0.00017497343251859722,
      "loss": 0.1172,
      "step": 481
    },
    {
      "epoch": 0.3829956297179182,
      "grad_norm": 0.14789730310440063,
      "learning_rate": 0.0001749202975557917,
      "loss": 0.1067,
      "step": 482
    },
    {
      "epoch": 0.3837902264600715,
      "grad_norm": 2.624328136444092,
      "learning_rate": 0.00017486716259298619,
      "loss": 0.0885,
      "step": 483
    },
    {
      "epoch": 0.38458482320222487,
      "grad_norm": 16.18034553527832,
      "learning_rate": 0.00017481402763018067,
      "loss": 0.5988,
      "step": 484
    },
    {
      "epoch": 0.38537941994437824,
      "grad_norm": 1.0832313299179077,
      "learning_rate": 0.00017476089266737515,
      "loss": 0.1451,
      "step": 485
    },
    {
      "epoch": 0.38617401668653156,
      "grad_norm": 0.3220563232898712,
      "learning_rate": 0.00017470775770456963,
      "loss": 0.1114,
      "step": 486
    },
    {
      "epoch": 0.38696861342868494,
      "grad_norm": 0.11379662156105042,
      "learning_rate": 0.0001746546227417641,
      "loss": 0.095,
      "step": 487
    },
    {
      "epoch": 0.3877632101708383,
      "grad_norm": 0.1331019550561905,
      "learning_rate": 0.00017460148777895856,
      "loss": 0.0681,
      "step": 488
    },
    {
      "epoch": 0.38855780691299163,
      "grad_norm": 0.12626010179519653,
      "learning_rate": 0.00017454835281615304,
      "loss": 0.083,
      "step": 489
    },
    {
      "epoch": 0.389352403655145,
      "grad_norm": 0.13573628664016724,
      "learning_rate": 0.0001744952178533475,
      "loss": 0.0999,
      "step": 490
    },
    {
      "epoch": 0.3901470003972984,
      "grad_norm": 0.17958004772663116,
      "learning_rate": 0.00017444208289054197,
      "loss": 0.0647,
      "step": 491
    },
    {
      "epoch": 0.3909415971394517,
      "grad_norm": 0.12049554288387299,
      "learning_rate": 0.00017438894792773645,
      "loss": 0.0723,
      "step": 492
    },
    {
      "epoch": 0.3917361938816051,
      "grad_norm": 0.1240827888250351,
      "learning_rate": 0.00017433581296493093,
      "loss": 0.0601,
      "step": 493
    },
    {
      "epoch": 0.39253079062375845,
      "grad_norm": 0.10330583155155182,
      "learning_rate": 0.0001742826780021254,
      "loss": 0.0454,
      "step": 494
    },
    {
      "epoch": 0.3933253873659118,
      "grad_norm": 0.10036259144544601,
      "learning_rate": 0.0001742295430393199,
      "loss": 0.0665,
      "step": 495
    },
    {
      "epoch": 0.39411998410806515,
      "grad_norm": 0.11036942899227142,
      "learning_rate": 0.00017417640807651437,
      "loss": 0.0612,
      "step": 496
    },
    {
      "epoch": 0.3949145808502185,
      "grad_norm": 0.13097025454044342,
      "learning_rate": 0.00017412327311370883,
      "loss": 0.0447,
      "step": 497
    },
    {
      "epoch": 0.3957091775923719,
      "grad_norm": 0.1235135868191719,
      "learning_rate": 0.0001740701381509033,
      "loss": 0.0434,
      "step": 498
    },
    {
      "epoch": 0.3965037743345252,
      "grad_norm": 0.10748361051082611,
      "learning_rate": 0.00017401700318809776,
      "loss": 0.042,
      "step": 499
    },
    {
      "epoch": 0.3972983710766786,
      "grad_norm": 0.08436639606952667,
      "learning_rate": 0.00017396386822529224,
      "loss": 0.0389,
      "step": 500
    },
    {
      "epoch": 0.39809296781883197,
      "grad_norm": 1.3532744646072388,
      "learning_rate": 0.00017391073326248672,
      "loss": 0.6865,
      "step": 501
    },
    {
      "epoch": 0.3988875645609853,
      "grad_norm": 0.8535931706428528,
      "learning_rate": 0.0001738575982996812,
      "loss": 0.4783,
      "step": 502
    },
    {
      "epoch": 0.39968216130313866,
      "grad_norm": 0.4678332805633545,
      "learning_rate": 0.00017380446333687568,
      "loss": 0.4309,
      "step": 503
    },
    {
      "epoch": 0.40047675804529204,
      "grad_norm": 0.3833351135253906,
      "learning_rate": 0.00017375132837407016,
      "loss": 0.4289,
      "step": 504
    },
    {
      "epoch": 0.40127135478744536,
      "grad_norm": 0.2639719545841217,
      "learning_rate": 0.00017369819341126464,
      "loss": 0.3328,
      "step": 505
    },
    {
      "epoch": 0.40206595152959873,
      "grad_norm": 0.24227659404277802,
      "learning_rate": 0.0001736450584484591,
      "loss": 0.2857,
      "step": 506
    },
    {
      "epoch": 0.4028605482717521,
      "grad_norm": 0.2591593563556671,
      "learning_rate": 0.00017359192348565357,
      "loss": 0.3045,
      "step": 507
    },
    {
      "epoch": 0.4036551450139054,
      "grad_norm": 0.24281808733940125,
      "learning_rate": 0.00017353878852284803,
      "loss": 0.3178,
      "step": 508
    },
    {
      "epoch": 0.4044497417560588,
      "grad_norm": 0.27179622650146484,
      "learning_rate": 0.0001734856535600425,
      "loss": 0.2896,
      "step": 509
    },
    {
      "epoch": 0.4052443384982122,
      "grad_norm": 0.2061675488948822,
      "learning_rate": 0.000173432518597237,
      "loss": 0.2501,
      "step": 510
    },
    {
      "epoch": 0.4060389352403655,
      "grad_norm": 0.21467508375644684,
      "learning_rate": 0.00017337938363443147,
      "loss": 0.216,
      "step": 511
    },
    {
      "epoch": 0.40683353198251887,
      "grad_norm": 0.24157379567623138,
      "learning_rate": 0.00017332624867162595,
      "loss": 0.2379,
      "step": 512
    },
    {
      "epoch": 0.40762812872467225,
      "grad_norm": 0.19514483213424683,
      "learning_rate": 0.00017327311370882043,
      "loss": 0.186,
      "step": 513
    },
    {
      "epoch": 0.40842272546682556,
      "grad_norm": 0.19463635981082916,
      "learning_rate": 0.00017321997874601488,
      "loss": 0.2124,
      "step": 514
    },
    {
      "epoch": 0.40921732220897894,
      "grad_norm": 0.20250986516475677,
      "learning_rate": 0.00017316684378320936,
      "loss": 0.2299,
      "step": 515
    },
    {
      "epoch": 0.4100119189511323,
      "grad_norm": 0.2274000644683838,
      "learning_rate": 0.00017311370882040384,
      "loss": 0.223,
      "step": 516
    },
    {
      "epoch": 0.41080651569328563,
      "grad_norm": 0.21220611035823822,
      "learning_rate": 0.0001730605738575983,
      "loss": 0.1906,
      "step": 517
    },
    {
      "epoch": 0.411601112435439,
      "grad_norm": 0.16713078320026398,
      "learning_rate": 0.00017300743889479277,
      "loss": 0.167,
      "step": 518
    },
    {
      "epoch": 0.4123957091775924,
      "grad_norm": 0.16015617549419403,
      "learning_rate": 0.00017295430393198726,
      "loss": 0.1873,
      "step": 519
    },
    {
      "epoch": 0.4131903059197457,
      "grad_norm": 0.17407336831092834,
      "learning_rate": 0.00017290116896918174,
      "loss": 0.1765,
      "step": 520
    },
    {
      "epoch": 0.4139849026618991,
      "grad_norm": 0.18234583735466003,
      "learning_rate": 0.00017284803400637622,
      "loss": 0.1597,
      "step": 521
    },
    {
      "epoch": 0.41477949940405245,
      "grad_norm": 0.12813718616962433,
      "learning_rate": 0.00017279489904357067,
      "loss": 0.1334,
      "step": 522
    },
    {
      "epoch": 0.4155740961462058,
      "grad_norm": 0.15167921781539917,
      "learning_rate": 0.00017274176408076515,
      "loss": 0.1368,
      "step": 523
    },
    {
      "epoch": 0.41636869288835915,
      "grad_norm": 0.19684261083602905,
      "learning_rate": 0.00017268862911795963,
      "loss": 0.131,
      "step": 524
    },
    {
      "epoch": 0.4171632896305125,
      "grad_norm": 0.1527683585882187,
      "learning_rate": 0.0001726354941551541,
      "loss": 0.1338,
      "step": 525
    },
    {
      "epoch": 0.4179578863726659,
      "grad_norm": 0.16256847977638245,
      "learning_rate": 0.00017258235919234856,
      "loss": 0.1209,
      "step": 526
    },
    {
      "epoch": 0.4187524831148192,
      "grad_norm": 0.1611175239086151,
      "learning_rate": 0.00017252922422954304,
      "loss": 0.1342,
      "step": 527
    },
    {
      "epoch": 0.4195470798569726,
      "grad_norm": 0.15386508405208588,
      "learning_rate": 0.00017247608926673752,
      "loss": 0.1349,
      "step": 528
    },
    {
      "epoch": 0.42034167659912597,
      "grad_norm": 0.13807763159275055,
      "learning_rate": 0.000172422954303932,
      "loss": 0.1045,
      "step": 529
    },
    {
      "epoch": 0.4211362733412793,
      "grad_norm": 0.15460006892681122,
      "learning_rate": 0.00017236981934112646,
      "loss": 0.1167,
      "step": 530
    },
    {
      "epoch": 0.42193087008343266,
      "grad_norm": 0.16303877532482147,
      "learning_rate": 0.00017231668437832094,
      "loss": 0.144,
      "step": 531
    },
    {
      "epoch": 0.42272546682558604,
      "grad_norm": 0.11480847746133804,
      "learning_rate": 0.00017226354941551542,
      "loss": 0.09,
      "step": 532
    },
    {
      "epoch": 0.42352006356773936,
      "grad_norm": 0.1374879777431488,
      "learning_rate": 0.0001722104144527099,
      "loss": 0.1146,
      "step": 533
    },
    {
      "epoch": 0.42431466030989273,
      "grad_norm": 0.15954264998435974,
      "learning_rate": 0.00017215727948990438,
      "loss": 0.1043,
      "step": 534
    },
    {
      "epoch": 0.4251092570520461,
      "grad_norm": 0.11786545813083649,
      "learning_rate": 0.00017210414452709883,
      "loss": 0.1048,
      "step": 535
    },
    {
      "epoch": 0.4259038537941994,
      "grad_norm": 0.13134117424488068,
      "learning_rate": 0.0001720510095642933,
      "loss": 0.104,
      "step": 536
    },
    {
      "epoch": 0.4266984505363528,
      "grad_norm": 0.11975903809070587,
      "learning_rate": 0.0001719978746014878,
      "loss": 0.0904,
      "step": 537
    },
    {
      "epoch": 0.4274930472785062,
      "grad_norm": 0.11612091958522797,
      "learning_rate": 0.00017194473963868224,
      "loss": 0.0733,
      "step": 538
    },
    {
      "epoch": 0.4282876440206595,
      "grad_norm": 0.10892501473426819,
      "learning_rate": 0.00017189160467587672,
      "loss": 0.0782,
      "step": 539
    },
    {
      "epoch": 0.42908224076281287,
      "grad_norm": 0.11306760460138321,
      "learning_rate": 0.0001718384697130712,
      "loss": 0.062,
      "step": 540
    },
    {
      "epoch": 0.42987683750496625,
      "grad_norm": 0.14252065122127533,
      "learning_rate": 0.00017178533475026568,
      "loss": 0.0675,
      "step": 541
    },
    {
      "epoch": 0.43067143424711957,
      "grad_norm": 0.10595011711120605,
      "learning_rate": 0.00017173219978746016,
      "loss": 0.0756,
      "step": 542
    },
    {
      "epoch": 0.43146603098927294,
      "grad_norm": 0.12251737713813782,
      "learning_rate": 0.00017167906482465464,
      "loss": 0.05,
      "step": 543
    },
    {
      "epoch": 0.4322606277314263,
      "grad_norm": 0.11248825490474701,
      "learning_rate": 0.0001716259298618491,
      "loss": 0.0419,
      "step": 544
    },
    {
      "epoch": 0.43305522447357964,
      "grad_norm": 0.09291782230138779,
      "learning_rate": 0.00017157279489904358,
      "loss": 0.0525,
      "step": 545
    },
    {
      "epoch": 0.433849821215733,
      "grad_norm": 0.0952376201748848,
      "learning_rate": 0.00017151965993623803,
      "loss": 0.0483,
      "step": 546
    },
    {
      "epoch": 0.4346444179578864,
      "grad_norm": 0.09954146295785904,
      "learning_rate": 0.0001714665249734325,
      "loss": 0.0538,
      "step": 547
    },
    {
      "epoch": 0.4354390147000397,
      "grad_norm": 0.09918416291475296,
      "learning_rate": 0.000171413390010627,
      "loss": 0.0406,
      "step": 548
    },
    {
      "epoch": 0.4362336114421931,
      "grad_norm": 0.09304721653461456,
      "learning_rate": 0.00017136025504782147,
      "loss": 0.0334,
      "step": 549
    },
    {
      "epoch": 0.43702820818434646,
      "grad_norm": 0.11751978099346161,
      "learning_rate": 0.00017130712008501595,
      "loss": 0.0422,
      "step": 550
    },
    {
      "epoch": 0.4378228049264998,
      "grad_norm": 0.6405899524688721,
      "learning_rate": 0.00017125398512221043,
      "loss": 0.6003,
      "step": 551
    },
    {
      "epoch": 0.43861740166865315,
      "grad_norm": 0.5206952691078186,
      "learning_rate": 0.0001712008501594049,
      "loss": 0.5092,
      "step": 552
    },
    {
      "epoch": 0.4394119984108065,
      "grad_norm": 0.4875452518463135,
      "learning_rate": 0.0001711477151965994,
      "loss": 0.4131,
      "step": 553
    },
    {
      "epoch": 0.44020659515295985,
      "grad_norm": 0.29319000244140625,
      "learning_rate": 0.00017109458023379384,
      "loss": 0.4218,
      "step": 554
    },
    {
      "epoch": 0.4410011918951132,
      "grad_norm": 0.2839665710926056,
      "learning_rate": 0.0001710414452709883,
      "loss": 0.357,
      "step": 555
    },
    {
      "epoch": 0.4417957886372666,
      "grad_norm": 0.29354360699653625,
      "learning_rate": 0.00017098831030818278,
      "loss": 0.3146,
      "step": 556
    },
    {
      "epoch": 0.44259038537941997,
      "grad_norm": 0.2391386777162552,
      "learning_rate": 0.00017093517534537726,
      "loss": 0.3074,
      "step": 557
    },
    {
      "epoch": 0.4433849821215733,
      "grad_norm": 0.2562192678451538,
      "learning_rate": 0.00017088204038257174,
      "loss": 0.3362,
      "step": 558
    },
    {
      "epoch": 0.44417957886372667,
      "grad_norm": 0.23815485835075378,
      "learning_rate": 0.00017082890541976622,
      "loss": 0.2741,
      "step": 559
    },
    {
      "epoch": 0.44497417560588004,
      "grad_norm": 0.22830252349376678,
      "learning_rate": 0.0001707757704569607,
      "loss": 0.2739,
      "step": 560
    },
    {
      "epoch": 0.44576877234803336,
      "grad_norm": 0.24359333515167236,
      "learning_rate": 0.00017072263549415518,
      "loss": 0.277,
      "step": 561
    },
    {
      "epoch": 0.44656336909018673,
      "grad_norm": 0.24038057029247284,
      "learning_rate": 0.00017066950053134966,
      "loss": 0.2617,
      "step": 562
    },
    {
      "epoch": 0.4473579658323401,
      "grad_norm": 0.2074430137872696,
      "learning_rate": 0.0001706163655685441,
      "loss": 0.2167,
      "step": 563
    },
    {
      "epoch": 0.44815256257449343,
      "grad_norm": 0.17728093266487122,
      "learning_rate": 0.00017056323060573857,
      "loss": 0.2097,
      "step": 564
    },
    {
      "epoch": 0.4489471593166468,
      "grad_norm": 0.21102969348430634,
      "learning_rate": 0.00017051009564293305,
      "loss": 0.2182,
      "step": 565
    },
    {
      "epoch": 0.4497417560588002,
      "grad_norm": 0.20054851472377777,
      "learning_rate": 0.00017045696068012753,
      "loss": 0.2052,
      "step": 566
    },
    {
      "epoch": 0.4505363528009535,
      "grad_norm": 0.21039541065692902,
      "learning_rate": 0.000170403825717322,
      "loss": 0.2092,
      "step": 567
    },
    {
      "epoch": 0.4513309495431069,
      "grad_norm": 0.19313572347164154,
      "learning_rate": 0.00017035069075451649,
      "loss": 0.2008,
      "step": 568
    },
    {
      "epoch": 0.45212554628526025,
      "grad_norm": 0.18476566672325134,
      "learning_rate": 0.00017029755579171097,
      "loss": 0.1973,
      "step": 569
    },
    {
      "epoch": 0.45292014302741357,
      "grad_norm": 0.17767760157585144,
      "learning_rate": 0.00017024442082890545,
      "loss": 0.1688,
      "step": 570
    },
    {
      "epoch": 0.45371473976956694,
      "grad_norm": 0.16385503113269806,
      "learning_rate": 0.0001701912858660999,
      "loss": 0.1476,
      "step": 571
    },
    {
      "epoch": 0.4545093365117203,
      "grad_norm": 0.1573036164045334,
      "learning_rate": 0.00017013815090329438,
      "loss": 0.1403,
      "step": 572
    },
    {
      "epoch": 0.45530393325387364,
      "grad_norm": 0.14095774292945862,
      "learning_rate": 0.00017008501594048883,
      "loss": 0.1451,
      "step": 573
    },
    {
      "epoch": 0.456098529996027,
      "grad_norm": 0.15940800309181213,
      "learning_rate": 0.0001700318809776833,
      "loss": 0.1597,
      "step": 574
    },
    {
      "epoch": 0.4568931267381804,
      "grad_norm": 0.18398946523666382,
      "learning_rate": 0.0001699787460148778,
      "loss": 0.1677,
      "step": 575
    },
    {
      "epoch": 0.4576877234803337,
      "grad_norm": 0.17532724142074585,
      "learning_rate": 0.00016992561105207227,
      "loss": 0.1526,
      "step": 576
    },
    {
      "epoch": 0.4584823202224871,
      "grad_norm": 0.13531874120235443,
      "learning_rate": 0.00016987247608926675,
      "loss": 0.1347,
      "step": 577
    },
    {
      "epoch": 0.45927691696464046,
      "grad_norm": 0.14403687417507172,
      "learning_rate": 0.00016981934112646123,
      "loss": 0.1355,
      "step": 578
    },
    {
      "epoch": 0.4600715137067938,
      "grad_norm": 0.1270918846130371,
      "learning_rate": 0.0001697662061636557,
      "loss": 0.1322,
      "step": 579
    },
    {
      "epoch": 0.46086611044894715,
      "grad_norm": 0.14452433586120605,
      "learning_rate": 0.00016971307120085017,
      "loss": 0.1227,
      "step": 580
    },
    {
      "epoch": 0.46166070719110053,
      "grad_norm": 0.1376820057630539,
      "learning_rate": 0.00016965993623804465,
      "loss": 0.122,
      "step": 581
    },
    {
      "epoch": 0.46245530393325385,
      "grad_norm": 0.10571511089801788,
      "learning_rate": 0.0001696068012752391,
      "loss": 0.0859,
      "step": 582
    },
    {
      "epoch": 0.4632499006754072,
      "grad_norm": 0.12337704747915268,
      "learning_rate": 0.00016955366631243358,
      "loss": 0.1025,
      "step": 583
    },
    {
      "epoch": 0.4640444974175606,
      "grad_norm": 0.12889760732650757,
      "learning_rate": 0.00016950053134962806,
      "loss": 0.1081,
      "step": 584
    },
    {
      "epoch": 0.464839094159714,
      "grad_norm": 0.10969044268131256,
      "learning_rate": 0.00016944739638682254,
      "loss": 0.0828,
      "step": 585
    },
    {
      "epoch": 0.4656336909018673,
      "grad_norm": 0.12754599750041962,
      "learning_rate": 0.00016939426142401702,
      "loss": 0.0798,
      "step": 586
    },
    {
      "epoch": 0.46642828764402067,
      "grad_norm": 0.10851863771677017,
      "learning_rate": 0.00016934112646121147,
      "loss": 0.0943,
      "step": 587
    },
    {
      "epoch": 0.46722288438617404,
      "grad_norm": 0.10078829526901245,
      "learning_rate": 0.00016928799149840595,
      "loss": 0.0733,
      "step": 588
    },
    {
      "epoch": 0.46801748112832736,
      "grad_norm": 0.12145230174064636,
      "learning_rate": 0.00016923485653560043,
      "loss": 0.0854,
      "step": 589
    },
    {
      "epoch": 0.46881207787048074,
      "grad_norm": 0.10914859920740128,
      "learning_rate": 0.00016918172157279491,
      "loss": 0.0778,
      "step": 590
    },
    {
      "epoch": 0.4696066746126341,
      "grad_norm": 0.10615339130163193,
      "learning_rate": 0.00016912858660998937,
      "loss": 0.0791,
      "step": 591
    },
    {
      "epoch": 0.47040127135478743,
      "grad_norm": 0.11796209961175919,
      "learning_rate": 0.00016907545164718385,
      "loss": 0.0729,
      "step": 592
    },
    {
      "epoch": 0.4711958680969408,
      "grad_norm": 0.10080892592668533,
      "learning_rate": 0.00016902231668437833,
      "loss": 0.0812,
      "step": 593
    },
    {
      "epoch": 0.4719904648390942,
      "grad_norm": 0.07477190345525742,
      "learning_rate": 0.0001689691817215728,
      "loss": 0.0478,
      "step": 594
    },
    {
      "epoch": 0.4727850615812475,
      "grad_norm": 0.08991693705320358,
      "learning_rate": 0.0001689160467587673,
      "loss": 0.0629,
      "step": 595
    },
    {
      "epoch": 0.4735796583234009,
      "grad_norm": 0.08759844303131104,
      "learning_rate": 0.00016886291179596174,
      "loss": 0.0395,
      "step": 596
    },
    {
      "epoch": 0.47437425506555425,
      "grad_norm": 0.08860986679792404,
      "learning_rate": 0.00016880977683315622,
      "loss": 0.0398,
      "step": 597
    },
    {
      "epoch": 0.47516885180770757,
      "grad_norm": 0.08822234719991684,
      "learning_rate": 0.0001687566418703507,
      "loss": 0.0336,
      "step": 598
    },
    {
      "epoch": 0.47596344854986095,
      "grad_norm": 0.08599933236837387,
      "learning_rate": 0.00016870350690754518,
      "loss": 0.037,
      "step": 599
    },
    {
      "epoch": 0.4767580452920143,
      "grad_norm": 0.08145996928215027,
      "learning_rate": 0.00016865037194473964,
      "loss": 0.032,
      "step": 600
    },
    {
      "epoch": 0.47755264203416764,
      "grad_norm": 0.6958491206169128,
      "learning_rate": 0.00016859723698193412,
      "loss": 0.7426,
      "step": 601
    },
    {
      "epoch": 0.478347238776321,
      "grad_norm": 0.5394800901412964,
      "learning_rate": 0.0001685441020191286,
      "loss": 0.4991,
      "step": 602
    },
    {
      "epoch": 0.4791418355184744,
      "grad_norm": 0.4228609502315521,
      "learning_rate": 0.00016849096705632308,
      "loss": 0.4661,
      "step": 603
    },
    {
      "epoch": 0.4799364322606277,
      "grad_norm": 0.3216509222984314,
      "learning_rate": 0.00016843783209351753,
      "loss": 0.3719,
      "step": 604
    },
    {
      "epoch": 0.4807310290027811,
      "grad_norm": 0.25649672746658325,
      "learning_rate": 0.000168384697130712,
      "loss": 0.3248,
      "step": 605
    },
    {
      "epoch": 0.48152562574493446,
      "grad_norm": 0.22266343235969543,
      "learning_rate": 0.0001683315621679065,
      "loss": 0.2933,
      "step": 606
    },
    {
      "epoch": 0.4823202224870878,
      "grad_norm": 0.27043452858924866,
      "learning_rate": 0.00016827842720510097,
      "loss": 0.2962,
      "step": 607
    },
    {
      "epoch": 0.48311481922924115,
      "grad_norm": 0.20372124016284943,
      "learning_rate": 0.00016822529224229545,
      "loss": 0.2698,
      "step": 608
    },
    {
      "epoch": 0.48390941597139453,
      "grad_norm": 0.21176624298095703,
      "learning_rate": 0.00016817215727948993,
      "loss": 0.2722,
      "step": 609
    },
    {
      "epoch": 0.48470401271354785,
      "grad_norm": 0.20498968660831451,
      "learning_rate": 0.00016811902231668438,
      "loss": 0.2362,
      "step": 610
    },
    {
      "epoch": 0.4854986094557012,
      "grad_norm": 0.26487547159194946,
      "learning_rate": 0.00016806588735387886,
      "loss": 0.2383,
      "step": 611
    },
    {
      "epoch": 0.4862932061978546,
      "grad_norm": 0.18695175647735596,
      "learning_rate": 0.00016801275239107332,
      "loss": 0.2038,
      "step": 612
    },
    {
      "epoch": 0.4870878029400079,
      "grad_norm": 0.19320477545261383,
      "learning_rate": 0.0001679596174282678,
      "loss": 0.2098,
      "step": 613
    },
    {
      "epoch": 0.4878823996821613,
      "grad_norm": 0.19279474020004272,
      "learning_rate": 0.00016790648246546228,
      "loss": 0.1873,
      "step": 614
    },
    {
      "epoch": 0.48867699642431467,
      "grad_norm": 0.16420583426952362,
      "learning_rate": 0.00016785334750265676,
      "loss": 0.1888,
      "step": 615
    },
    {
      "epoch": 0.48947159316646804,
      "grad_norm": 0.18118548393249512,
      "learning_rate": 0.00016780021253985124,
      "loss": 0.1962,
      "step": 616
    },
    {
      "epoch": 0.49026618990862136,
      "grad_norm": 0.1687295138835907,
      "learning_rate": 0.00016774707757704572,
      "loss": 0.1623,
      "step": 617
    },
    {
      "epoch": 0.49106078665077474,
      "grad_norm": 0.19524388015270233,
      "learning_rate": 0.0001676939426142402,
      "loss": 0.1553,
      "step": 618
    },
    {
      "epoch": 0.4918553833929281,
      "grad_norm": 0.1852264702320099,
      "learning_rate": 0.00016764080765143465,
      "loss": 0.1568,
      "step": 619
    },
    {
      "epoch": 0.49264998013508143,
      "grad_norm": 0.16435974836349487,
      "learning_rate": 0.0001675876726886291,
      "loss": 0.1701,
      "step": 620
    },
    {
      "epoch": 0.4934445768772348,
      "grad_norm": 0.19954049587249756,
      "learning_rate": 0.00016753453772582358,
      "loss": 0.1509,
      "step": 621
    },
    {
      "epoch": 0.4942391736193882,
      "grad_norm": 0.16141052544116974,
      "learning_rate": 0.00016748140276301806,
      "loss": 0.1539,
      "step": 622
    },
    {
      "epoch": 0.4950337703615415,
      "grad_norm": 0.16733233630657196,
      "learning_rate": 0.00016742826780021254,
      "loss": 0.1313,
      "step": 623
    },
    {
      "epoch": 0.4958283671036949,
      "grad_norm": 0.141399547457695,
      "learning_rate": 0.00016737513283740702,
      "loss": 0.1253,
      "step": 624
    },
    {
      "epoch": 0.49662296384584825,
      "grad_norm": 0.2049967348575592,
      "learning_rate": 0.0001673219978746015,
      "loss": 0.1458,
      "step": 625
    },
    {
      "epoch": 0.4974175605880016,
      "grad_norm": 0.1681317538022995,
      "learning_rate": 0.00016726886291179598,
      "loss": 0.122,
      "step": 626
    },
    {
      "epoch": 0.49821215733015495,
      "grad_norm": 0.16735979914665222,
      "learning_rate": 0.00016721572794899047,
      "loss": 0.1244,
      "step": 627
    },
    {
      "epoch": 0.4990067540723083,
      "grad_norm": 0.21398334205150604,
      "learning_rate": 0.00016716259298618492,
      "loss": 0.1404,
      "step": 628
    },
    {
      "epoch": 0.49980135081446164,
      "grad_norm": 0.12683236598968506,
      "learning_rate": 0.00016710945802337937,
      "loss": 0.1078,
      "step": 629
    },
    {
      "epoch": 0.5005959475566151,
      "grad_norm": 0.12924201786518097,
      "learning_rate": 0.00016705632306057385,
      "loss": 0.0995,
      "step": 630
    },
    {
      "epoch": 0.5013905442987684,
      "grad_norm": 0.12943007051944733,
      "learning_rate": 0.00016700318809776833,
      "loss": 0.1131,
      "step": 631
    },
    {
      "epoch": 0.5021851410409217,
      "grad_norm": 0.1379767805337906,
      "learning_rate": 0.0001669500531349628,
      "loss": 0.0896,
      "step": 632
    },
    {
      "epoch": 0.5029797377830751,
      "grad_norm": 0.1383536458015442,
      "learning_rate": 0.0001668969181721573,
      "loss": 0.0935,
      "step": 633
    },
    {
      "epoch": 0.5037743345252285,
      "grad_norm": 0.11201071739196777,
      "learning_rate": 0.00016684378320935177,
      "loss": 0.09,
      "step": 634
    },
    {
      "epoch": 0.5045689312673818,
      "grad_norm": 0.13013210892677307,
      "learning_rate": 0.00016679064824654625,
      "loss": 0.0977,
      "step": 635
    },
    {
      "epoch": 0.5053635280095352,
      "grad_norm": 0.1170118898153305,
      "learning_rate": 0.0001667375132837407,
      "loss": 0.0714,
      "step": 636
    },
    {
      "epoch": 0.5061581247516885,
      "grad_norm": 0.10926445573568344,
      "learning_rate": 0.00016668437832093519,
      "loss": 0.0813,
      "step": 637
    },
    {
      "epoch": 0.5069527214938419,
      "grad_norm": 0.11709357053041458,
      "learning_rate": 0.00016663124335812964,
      "loss": 0.0665,
      "step": 638
    },
    {
      "epoch": 0.5077473182359953,
      "grad_norm": 0.11044281721115112,
      "learning_rate": 0.00016657810839532412,
      "loss": 0.0818,
      "step": 639
    },
    {
      "epoch": 0.5085419149781486,
      "grad_norm": 0.10964597761631012,
      "learning_rate": 0.0001665249734325186,
      "loss": 0.089,
      "step": 640
    },
    {
      "epoch": 0.5093365117203019,
      "grad_norm": 0.10428038239479065,
      "learning_rate": 0.00016647183846971308,
      "loss": 0.0539,
      "step": 641
    },
    {
      "epoch": 0.5101311084624554,
      "grad_norm": 0.10011354833841324,
      "learning_rate": 0.00016641870350690756,
      "loss": 0.0565,
      "step": 642
    },
    {
      "epoch": 0.5109257052046087,
      "grad_norm": 0.09789957106113434,
      "learning_rate": 0.00016636556854410204,
      "loss": 0.0534,
      "step": 643
    },
    {
      "epoch": 0.511720301946762,
      "grad_norm": 0.15499535202980042,
      "learning_rate": 0.00016631243358129652,
      "loss": 0.0609,
      "step": 644
    },
    {
      "epoch": 0.5125148986889154,
      "grad_norm": 0.0898820161819458,
      "learning_rate": 0.00016625929861849097,
      "loss": 0.0505,
      "step": 645
    },
    {
      "epoch": 0.5133094954310687,
      "grad_norm": 0.09114237874746323,
      "learning_rate": 0.00016620616365568545,
      "loss": 0.0509,
      "step": 646
    },
    {
      "epoch": 0.5141040921732221,
      "grad_norm": 0.08712676167488098,
      "learning_rate": 0.0001661530286928799,
      "loss": 0.0435,
      "step": 647
    },
    {
      "epoch": 0.5148986889153755,
      "grad_norm": 0.08626683801412582,
      "learning_rate": 0.0001660998937300744,
      "loss": 0.0386,
      "step": 648
    },
    {
      "epoch": 0.5156932856575288,
      "grad_norm": 0.08220765739679337,
      "learning_rate": 0.00016604675876726887,
      "loss": 0.0358,
      "step": 649
    },
    {
      "epoch": 0.5164878823996821,
      "grad_norm": 0.0968245342373848,
      "learning_rate": 0.00016599362380446335,
      "loss": 0.0421,
      "step": 650
    },
    {
      "epoch": 0.5172824791418356,
      "grad_norm": 0.6951093673706055,
      "learning_rate": 0.00016594048884165783,
      "loss": 0.5908,
      "step": 651
    },
    {
      "epoch": 0.5180770758839889,
      "grad_norm": 0.7204357981681824,
      "learning_rate": 0.0001658873538788523,
      "loss": 0.4778,
      "step": 652
    },
    {
      "epoch": 0.5188716726261422,
      "grad_norm": 0.5625940561294556,
      "learning_rate": 0.00016583421891604676,
      "loss": 0.4331,
      "step": 653
    },
    {
      "epoch": 0.5196662693682956,
      "grad_norm": 0.3845457136631012,
      "learning_rate": 0.00016578108395324124,
      "loss": 0.4383,
      "step": 654
    },
    {
      "epoch": 0.520460866110449,
      "grad_norm": 0.31672534346580505,
      "learning_rate": 0.00016572794899043572,
      "loss": 0.3628,
      "step": 655
    },
    {
      "epoch": 0.5212554628526023,
      "grad_norm": 0.2632070779800415,
      "learning_rate": 0.0001656748140276302,
      "loss": 0.2884,
      "step": 656
    },
    {
      "epoch": 0.5220500595947557,
      "grad_norm": 0.1956564038991928,
      "learning_rate": 0.00016562167906482465,
      "loss": 0.2295,
      "step": 657
    },
    {
      "epoch": 0.522844656336909,
      "grad_norm": 0.20164911448955536,
      "learning_rate": 0.00016556854410201913,
      "loss": 0.2922,
      "step": 658
    },
    {
      "epoch": 0.5236392530790623,
      "grad_norm": 0.210745170712471,
      "learning_rate": 0.00016551540913921361,
      "loss": 0.2527,
      "step": 659
    },
    {
      "epoch": 0.5244338498212158,
      "grad_norm": 0.23250262439250946,
      "learning_rate": 0.0001654622741764081,
      "loss": 0.2292,
      "step": 660
    },
    {
      "epoch": 0.5252284465633691,
      "grad_norm": 0.2170725017786026,
      "learning_rate": 0.00016540913921360255,
      "loss": 0.2539,
      "step": 661
    },
    {
      "epoch": 0.5260230433055224,
      "grad_norm": 0.2121613621711731,
      "learning_rate": 0.00016535600425079703,
      "loss": 0.2289,
      "step": 662
    },
    {
      "epoch": 0.5268176400476758,
      "grad_norm": 0.2281489372253418,
      "learning_rate": 0.0001653028692879915,
      "loss": 0.2548,
      "step": 663
    },
    {
      "epoch": 0.5276122367898292,
      "grad_norm": 0.16878920793533325,
      "learning_rate": 0.000165249734325186,
      "loss": 0.1884,
      "step": 664
    },
    {
      "epoch": 0.5284068335319825,
      "grad_norm": 0.19336383044719696,
      "learning_rate": 0.00016519659936238047,
      "loss": 0.1892,
      "step": 665
    },
    {
      "epoch": 0.5292014302741359,
      "grad_norm": 0.18649530410766602,
      "learning_rate": 0.00016514346439957492,
      "loss": 0.1982,
      "step": 666
    },
    {
      "epoch": 0.5299960270162892,
      "grad_norm": 0.17531649768352509,
      "learning_rate": 0.0001650903294367694,
      "loss": 0.1833,
      "step": 667
    },
    {
      "epoch": 0.5307906237584425,
      "grad_norm": 0.19203226268291473,
      "learning_rate": 0.00016503719447396388,
      "loss": 0.1822,
      "step": 668
    },
    {
      "epoch": 0.531585220500596,
      "grad_norm": 0.20249062776565552,
      "learning_rate": 0.00016498405951115834,
      "loss": 0.1652,
      "step": 669
    },
    {
      "epoch": 0.5323798172427493,
      "grad_norm": 0.1944902241230011,
      "learning_rate": 0.00016493092454835282,
      "loss": 0.1774,
      "step": 670
    },
    {
      "epoch": 0.5331744139849026,
      "grad_norm": 0.22583326697349548,
      "learning_rate": 0.0001648777895855473,
      "loss": 0.1456,
      "step": 671
    },
    {
      "epoch": 0.533969010727056,
      "grad_norm": 0.17457987368106842,
      "learning_rate": 0.00016482465462274178,
      "loss": 0.1584,
      "step": 672
    },
    {
      "epoch": 0.5347636074692094,
      "grad_norm": 0.18834908306598663,
      "learning_rate": 0.00016477151965993626,
      "loss": 0.1716,
      "step": 673
    },
    {
      "epoch": 0.5355582042113627,
      "grad_norm": 0.1352584958076477,
      "learning_rate": 0.00016471838469713074,
      "loss": 0.1285,
      "step": 674
    },
    {
      "epoch": 0.5363528009535161,
      "grad_norm": 0.14262254536151886,
      "learning_rate": 0.0001646652497343252,
      "loss": 0.1206,
      "step": 675
    },
    {
      "epoch": 0.5371473976956694,
      "grad_norm": 0.15499314665794373,
      "learning_rate": 0.00016461211477151967,
      "loss": 0.141,
      "step": 676
    },
    {
      "epoch": 0.5379419944378228,
      "grad_norm": 0.15070171654224396,
      "learning_rate": 0.00016455897980871412,
      "loss": 0.1502,
      "step": 677
    },
    {
      "epoch": 0.5387365911799762,
      "grad_norm": 0.159584641456604,
      "learning_rate": 0.0001645058448459086,
      "loss": 0.1287,
      "step": 678
    },
    {
      "epoch": 0.5395311879221295,
      "grad_norm": 0.13231301307678223,
      "learning_rate": 0.00016445270988310308,
      "loss": 0.1283,
      "step": 679
    },
    {
      "epoch": 0.5403257846642828,
      "grad_norm": 0.127310112118721,
      "learning_rate": 0.00016439957492029756,
      "loss": 0.1215,
      "step": 680
    },
    {
      "epoch": 0.5411203814064363,
      "grad_norm": 0.1596185714006424,
      "learning_rate": 0.00016434643995749204,
      "loss": 0.1423,
      "step": 681
    },
    {
      "epoch": 0.5419149781485896,
      "grad_norm": 0.11697930842638016,
      "learning_rate": 0.00016429330499468652,
      "loss": 0.0917,
      "step": 682
    },
    {
      "epoch": 0.5427095748907429,
      "grad_norm": 0.12343541532754898,
      "learning_rate": 0.000164240170031881,
      "loss": 0.1197,
      "step": 683
    },
    {
      "epoch": 0.5435041716328963,
      "grad_norm": 0.14299806952476501,
      "learning_rate": 0.00016418703506907546,
      "loss": 0.1206,
      "step": 684
    },
    {
      "epoch": 0.5442987683750496,
      "grad_norm": 0.11287374794483185,
      "learning_rate": 0.00016413390010626994,
      "loss": 0.0966,
      "step": 685
    },
    {
      "epoch": 0.545093365117203,
      "grad_norm": 0.1287161409854889,
      "learning_rate": 0.0001640807651434644,
      "loss": 0.0962,
      "step": 686
    },
    {
      "epoch": 0.5458879618593564,
      "grad_norm": 0.14539708197116852,
      "learning_rate": 0.00016402763018065887,
      "loss": 0.1205,
      "step": 687
    },
    {
      "epoch": 0.5466825586015097,
      "grad_norm": 0.12033561617136002,
      "learning_rate": 0.00016397449521785335,
      "loss": 0.0887,
      "step": 688
    },
    {
      "epoch": 0.547477155343663,
      "grad_norm": 0.10007776319980621,
      "learning_rate": 0.00016392136025504783,
      "loss": 0.0768,
      "step": 689
    },
    {
      "epoch": 0.5482717520858165,
      "grad_norm": 0.10888214409351349,
      "learning_rate": 0.0001638682252922423,
      "loss": 0.0806,
      "step": 690
    },
    {
      "epoch": 0.5490663488279698,
      "grad_norm": 0.09900055080652237,
      "learning_rate": 0.0001638150903294368,
      "loss": 0.0847,
      "step": 691
    },
    {
      "epoch": 0.5498609455701232,
      "grad_norm": 0.10102885216474533,
      "learning_rate": 0.00016376195536663127,
      "loss": 0.0815,
      "step": 692
    },
    {
      "epoch": 0.5506555423122765,
      "grad_norm": 0.09336532652378082,
      "learning_rate": 0.00016370882040382572,
      "loss": 0.0601,
      "step": 693
    },
    {
      "epoch": 0.5514501390544299,
      "grad_norm": 0.09683185070753098,
      "learning_rate": 0.00016365568544102018,
      "loss": 0.0495,
      "step": 694
    },
    {
      "epoch": 0.5522447357965833,
      "grad_norm": 0.09547777473926544,
      "learning_rate": 0.00016360255047821466,
      "loss": 0.0536,
      "step": 695
    },
    {
      "epoch": 0.5530393325387366,
      "grad_norm": 0.07931177318096161,
      "learning_rate": 0.00016354941551540914,
      "loss": 0.0632,
      "step": 696
    },
    {
      "epoch": 0.5538339292808899,
      "grad_norm": 0.07589127868413925,
      "learning_rate": 0.00016349628055260362,
      "loss": 0.0386,
      "step": 697
    },
    {
      "epoch": 0.5546285260230434,
      "grad_norm": 0.10413055121898651,
      "learning_rate": 0.0001634431455897981,
      "loss": 0.044,
      "step": 698
    },
    {
      "epoch": 0.5554231227651967,
      "grad_norm": 0.09753414988517761,
      "learning_rate": 0.00016339001062699258,
      "loss": 0.0373,
      "step": 699
    },
    {
      "epoch": 0.55621771950735,
      "grad_norm": 0.08821544796228409,
      "learning_rate": 0.00016333687566418706,
      "loss": 0.0395,
      "step": 700
    },
    {
      "epoch": 0.5570123162495034,
      "grad_norm": 1.132065773010254,
      "learning_rate": 0.00016328374070138154,
      "loss": 0.7612,
      "step": 701
    },
    {
      "epoch": 0.5578069129916567,
      "grad_norm": 0.6405693292617798,
      "learning_rate": 0.000163230605738576,
      "loss": 0.5343,
      "step": 702
    },
    {
      "epoch": 0.5586015097338101,
      "grad_norm": 0.3990688621997833,
      "learning_rate": 0.00016317747077577044,
      "loss": 0.3777,
      "step": 703
    },
    {
      "epoch": 0.5593961064759635,
      "grad_norm": 0.3034796714782715,
      "learning_rate": 0.00016312433581296492,
      "loss": 0.3349,
      "step": 704
    },
    {
      "epoch": 0.5601907032181168,
      "grad_norm": 0.33816733956336975,
      "learning_rate": 0.0001630712008501594,
      "loss": 0.3643,
      "step": 705
    },
    {
      "epoch": 0.5609852999602701,
      "grad_norm": 0.24680352210998535,
      "learning_rate": 0.00016301806588735389,
      "loss": 0.3043,
      "step": 706
    },
    {
      "epoch": 0.5617798967024236,
      "grad_norm": 0.246956005692482,
      "learning_rate": 0.00016296493092454837,
      "loss": 0.3314,
      "step": 707
    },
    {
      "epoch": 0.5625744934445769,
      "grad_norm": 0.22934018075466156,
      "learning_rate": 0.00016291179596174285,
      "loss": 0.2829,
      "step": 708
    },
    {
      "epoch": 0.5633690901867302,
      "grad_norm": 0.22942501306533813,
      "learning_rate": 0.00016285866099893733,
      "loss": 0.2778,
      "step": 709
    },
    {
      "epoch": 0.5641636869288836,
      "grad_norm": 0.2229616492986679,
      "learning_rate": 0.00016280552603613178,
      "loss": 0.2583,
      "step": 710
    },
    {
      "epoch": 0.564958283671037,
      "grad_norm": 0.2174660712480545,
      "learning_rate": 0.00016275239107332626,
      "loss": 0.2101,
      "step": 711
    },
    {
      "epoch": 0.5657528804131903,
      "grad_norm": 0.24063381552696228,
      "learning_rate": 0.00016269925611052074,
      "loss": 0.2696,
      "step": 712
    },
    {
      "epoch": 0.5665474771553437,
      "grad_norm": 0.4026258885860443,
      "learning_rate": 0.0001626461211477152,
      "loss": 0.2265,
      "step": 713
    },
    {
      "epoch": 0.567342073897497,
      "grad_norm": 1.2688955068588257,
      "learning_rate": 0.00016259298618490967,
      "loss": 0.2426,
      "step": 714
    },
    {
      "epoch": 0.5681366706396503,
      "grad_norm": 0.19637733697891235,
      "learning_rate": 0.00016253985122210415,
      "loss": 0.1727,
      "step": 715
    },
    {
      "epoch": 0.5689312673818038,
      "grad_norm": 2.4659271240234375,
      "learning_rate": 0.00016248671625929863,
      "loss": 0.3198,
      "step": 716
    },
    {
      "epoch": 0.5697258641239571,
      "grad_norm": 0.2315731793642044,
      "learning_rate": 0.0001624335812964931,
      "loss": 0.1954,
      "step": 717
    },
    {
      "epoch": 0.5705204608661104,
      "grad_norm": 0.2035675048828125,
      "learning_rate": 0.00016238044633368757,
      "loss": 0.1589,
      "step": 718
    },
    {
      "epoch": 0.5713150576082638,
      "grad_norm": 0.1927490085363388,
      "learning_rate": 0.00016232731137088205,
      "loss": 0.1496,
      "step": 719
    },
    {
      "epoch": 0.5721096543504172,
      "grad_norm": 0.18121927976608276,
      "learning_rate": 0.00016227417640807653,
      "loss": 0.1826,
      "step": 720
    },
    {
      "epoch": 0.5729042510925705,
      "grad_norm": 0.2265416979789734,
      "learning_rate": 0.000162221041445271,
      "loss": 0.1663,
      "step": 721
    },
    {
      "epoch": 0.5736988478347239,
      "grad_norm": 0.17162835597991943,
      "learning_rate": 0.00016216790648246546,
      "loss": 0.1604,
      "step": 722
    },
    {
      "epoch": 0.5744934445768772,
      "grad_norm": 0.20011308789253235,
      "learning_rate": 0.00016211477151965994,
      "loss": 0.1611,
      "step": 723
    },
    {
      "epoch": 0.5752880413190306,
      "grad_norm": 0.1709226816892624,
      "learning_rate": 0.00016206163655685442,
      "loss": 0.1107,
      "step": 724
    },
    {
      "epoch": 0.576082638061184,
      "grad_norm": 0.18157874047756195,
      "learning_rate": 0.0001620085015940489,
      "loss": 0.1592,
      "step": 725
    },
    {
      "epoch": 0.5768772348033373,
      "grad_norm": 0.20633453130722046,
      "learning_rate": 0.00016195536663124335,
      "loss": 0.1764,
      "step": 726
    },
    {
      "epoch": 0.5776718315454906,
      "grad_norm": 0.16789625585079193,
      "learning_rate": 0.00016190223166843783,
      "loss": 0.1423,
      "step": 727
    },
    {
      "epoch": 0.578466428287644,
      "grad_norm": 0.15606899559497833,
      "learning_rate": 0.00016184909670563231,
      "loss": 0.1297,
      "step": 728
    },
    {
      "epoch": 0.5792610250297974,
      "grad_norm": 0.19828598201274872,
      "learning_rate": 0.0001617959617428268,
      "loss": 0.1231,
      "step": 729
    },
    {
      "epoch": 0.5800556217719507,
      "grad_norm": 0.15548956394195557,
      "learning_rate": 0.00016174282678002127,
      "loss": 0.1267,
      "step": 730
    },
    {
      "epoch": 0.5808502185141041,
      "grad_norm": 0.20172038674354553,
      "learning_rate": 0.00016168969181721573,
      "loss": 0.1211,
      "step": 731
    },
    {
      "epoch": 0.5816448152562574,
      "grad_norm": 0.16032472252845764,
      "learning_rate": 0.0001616365568544102,
      "loss": 0.1211,
      "step": 732
    },
    {
      "epoch": 0.5824394119984108,
      "grad_norm": 0.18731114268302917,
      "learning_rate": 0.0001615834218916047,
      "loss": 0.1132,
      "step": 733
    },
    {
      "epoch": 0.5832340087405642,
      "grad_norm": 0.20096363127231598,
      "learning_rate": 0.00016153028692879917,
      "loss": 0.1185,
      "step": 734
    },
    {
      "epoch": 0.5840286054827175,
      "grad_norm": 0.13719375431537628,
      "learning_rate": 0.00016147715196599362,
      "loss": 0.1198,
      "step": 735
    },
    {
      "epoch": 0.5848232022248708,
      "grad_norm": 0.1686636060476303,
      "learning_rate": 0.0001614240170031881,
      "loss": 0.1139,
      "step": 736
    },
    {
      "epoch": 0.5856177989670243,
      "grad_norm": 0.14287561178207397,
      "learning_rate": 0.00016137088204038258,
      "loss": 0.0855,
      "step": 737
    },
    {
      "epoch": 0.5864123957091776,
      "grad_norm": 0.14618562161922455,
      "learning_rate": 0.00016131774707757706,
      "loss": 0.0845,
      "step": 738
    },
    {
      "epoch": 0.5872069924513309,
      "grad_norm": 0.1274416297674179,
      "learning_rate": 0.00016126461211477154,
      "loss": 0.0931,
      "step": 739
    },
    {
      "epoch": 0.5880015891934843,
      "grad_norm": 0.12459716945886612,
      "learning_rate": 0.000161211477151966,
      "loss": 0.0907,
      "step": 740
    },
    {
      "epoch": 0.5887961859356377,
      "grad_norm": 0.1482650488615036,
      "learning_rate": 0.00016115834218916048,
      "loss": 0.0864,
      "step": 741
    },
    {
      "epoch": 0.589590782677791,
      "grad_norm": 0.1332787722349167,
      "learning_rate": 0.00016110520722635496,
      "loss": 0.0722,
      "step": 742
    },
    {
      "epoch": 0.5903853794199444,
      "grad_norm": 0.16148334741592407,
      "learning_rate": 0.0001610520722635494,
      "loss": 0.0779,
      "step": 743
    },
    {
      "epoch": 0.5911799761620977,
      "grad_norm": 0.11318366229534149,
      "learning_rate": 0.0001609989373007439,
      "loss": 0.0672,
      "step": 744
    },
    {
      "epoch": 0.591974572904251,
      "grad_norm": 0.16228853166103363,
      "learning_rate": 0.00016094580233793837,
      "loss": 0.0499,
      "step": 745
    },
    {
      "epoch": 0.5927691696464045,
      "grad_norm": 0.1373501718044281,
      "learning_rate": 0.00016089266737513285,
      "loss": 0.0584,
      "step": 746
    },
    {
      "epoch": 0.5935637663885578,
      "grad_norm": 0.10946714133024216,
      "learning_rate": 0.00016083953241232733,
      "loss": 0.0437,
      "step": 747
    },
    {
      "epoch": 0.5943583631307111,
      "grad_norm": 0.10562935471534729,
      "learning_rate": 0.0001607863974495218,
      "loss": 0.0384,
      "step": 748
    },
    {
      "epoch": 0.5951529598728645,
      "grad_norm": 0.10019563883543015,
      "learning_rate": 0.00016073326248671626,
      "loss": 0.0395,
      "step": 749
    },
    {
      "epoch": 0.5959475566150179,
      "grad_norm": 0.1085941269993782,
      "learning_rate": 0.00016068012752391074,
      "loss": 0.0492,
      "step": 750
    },
    {
      "epoch": 0.5967421533571713,
      "grad_norm": 1.1121731996536255,
      "learning_rate": 0.0001606269925611052,
      "loss": 0.5234,
      "step": 751
    },
    {
      "epoch": 0.5975367500993246,
      "grad_norm": 0.5160964727401733,
      "learning_rate": 0.00016057385759829968,
      "loss": 0.5035,
      "step": 752
    },
    {
      "epoch": 0.5983313468414779,
      "grad_norm": 0.4228474795818329,
      "learning_rate": 0.00016052072263549416,
      "loss": 0.4175,
      "step": 753
    },
    {
      "epoch": 0.5991259435836314,
      "grad_norm": 0.41054975986480713,
      "learning_rate": 0.00016046758767268864,
      "loss": 0.3674,
      "step": 754
    },
    {
      "epoch": 0.5999205403257847,
      "grad_norm": 0.3304517865180969,
      "learning_rate": 0.00016041445270988312,
      "loss": 0.3742,
      "step": 755
    },
    {
      "epoch": 0.5999205403257847,
      "eval_loss": 0.15635450184345245,
      "eval_runtime": 300.8581,
      "eval_samples_per_second": 3.171,
      "eval_steps_per_second": 1.057,
      "step": 755
    },
    {
      "epoch": 0.600715137067938,
      "grad_norm": 0.2887241542339325,
      "learning_rate": 0.0001603613177470776,
      "loss": 0.3677,
      "step": 756
    },
    {
      "epoch": 0.6015097338100914,
      "grad_norm": 0.33603569865226746,
      "learning_rate": 0.00016030818278427208,
      "loss": 0.3095,
      "step": 757
    },
    {
      "epoch": 0.6023043305522447,
      "grad_norm": 0.33944064378738403,
      "learning_rate": 0.00016025504782146653,
      "loss": 0.275,
      "step": 758
    },
    {
      "epoch": 0.6030989272943981,
      "grad_norm": 0.31103232502937317,
      "learning_rate": 0.00016020191285866098,
      "loss": 0.2625,
      "step": 759
    },
    {
      "epoch": 0.6038935240365515,
      "grad_norm": 0.2857193946838379,
      "learning_rate": 0.00016014877789585546,
      "loss": 0.2309,
      "step": 760
    },
    {
      "epoch": 0.6046881207787048,
      "grad_norm": 0.252677857875824,
      "learning_rate": 0.00016009564293304994,
      "loss": 0.231,
      "step": 761
    },
    {
      "epoch": 0.6054827175208581,
      "grad_norm": 0.2156064808368683,
      "learning_rate": 0.00016004250797024442,
      "loss": 0.2222,
      "step": 762
    },
    {
      "epoch": 0.6062773142630116,
      "grad_norm": 0.22114630043506622,
      "learning_rate": 0.0001599893730074389,
      "loss": 0.2331,
      "step": 763
    },
    {
      "epoch": 0.6070719110051649,
      "grad_norm": 0.171585351228714,
      "learning_rate": 0.00015993623804463338,
      "loss": 0.1904,
      "step": 764
    },
    {
      "epoch": 0.6078665077473182,
      "grad_norm": 0.21495848894119263,
      "learning_rate": 0.00015988310308182786,
      "loss": 0.212,
      "step": 765
    },
    {
      "epoch": 0.6086611044894716,
      "grad_norm": 0.2126864194869995,
      "learning_rate": 0.00015982996811902234,
      "loss": 0.2236,
      "step": 766
    },
    {
      "epoch": 0.609455701231625,
      "grad_norm": 0.18560358881950378,
      "learning_rate": 0.0001597768331562168,
      "loss": 0.2098,
      "step": 767
    },
    {
      "epoch": 0.6102502979737783,
      "grad_norm": 0.16898883879184723,
      "learning_rate": 0.00015972369819341128,
      "loss": 0.1695,
      "step": 768
    },
    {
      "epoch": 0.6110448947159317,
      "grad_norm": 0.23021505773067474,
      "learning_rate": 0.00015967056323060573,
      "loss": 0.166,
      "step": 769
    },
    {
      "epoch": 0.611839491458085,
      "grad_norm": 0.20647498965263367,
      "learning_rate": 0.0001596174282678002,
      "loss": 0.1702,
      "step": 770
    },
    {
      "epoch": 0.6126340882002383,
      "grad_norm": 0.1607869565486908,
      "learning_rate": 0.0001595642933049947,
      "loss": 0.1568,
      "step": 771
    },
    {
      "epoch": 0.6134286849423918,
      "grad_norm": 0.1519125998020172,
      "learning_rate": 0.00015951115834218917,
      "loss": 0.1451,
      "step": 772
    },
    {
      "epoch": 0.6142232816845451,
      "grad_norm": 0.14512525498867035,
      "learning_rate": 0.00015945802337938365,
      "loss": 0.1428,
      "step": 773
    },
    {
      "epoch": 0.6150178784266984,
      "grad_norm": 0.16227658092975616,
      "learning_rate": 0.00015940488841657813,
      "loss": 0.1573,
      "step": 774
    },
    {
      "epoch": 0.6158124751688518,
      "grad_norm": 0.17023614048957825,
      "learning_rate": 0.0001593517534537726,
      "loss": 0.1523,
      "step": 775
    },
    {
      "epoch": 0.6166070719110052,
      "grad_norm": 0.1583867371082306,
      "learning_rate": 0.00015929861849096706,
      "loss": 0.1242,
      "step": 776
    },
    {
      "epoch": 0.6174016686531585,
      "grad_norm": 0.1608862429857254,
      "learning_rate": 0.00015924548352816155,
      "loss": 0.1215,
      "step": 777
    },
    {
      "epoch": 0.6181962653953119,
      "grad_norm": 0.1336755007505417,
      "learning_rate": 0.000159192348565356,
      "loss": 0.1222,
      "step": 778
    },
    {
      "epoch": 0.6189908621374652,
      "grad_norm": 0.19663363695144653,
      "learning_rate": 0.00015913921360255048,
      "loss": 0.1375,
      "step": 779
    },
    {
      "epoch": 0.6197854588796186,
      "grad_norm": 0.15700852870941162,
      "learning_rate": 0.00015908607863974496,
      "loss": 0.1188,
      "step": 780
    },
    {
      "epoch": 0.620580055621772,
      "grad_norm": 0.14438439905643463,
      "learning_rate": 0.00015903294367693944,
      "loss": 0.1041,
      "step": 781
    },
    {
      "epoch": 0.6213746523639253,
      "grad_norm": 0.1315022110939026,
      "learning_rate": 0.00015897980871413392,
      "loss": 0.1029,
      "step": 782
    },
    {
      "epoch": 0.6221692491060786,
      "grad_norm": 0.14895732700824738,
      "learning_rate": 0.0001589266737513284,
      "loss": 0.1002,
      "step": 783
    },
    {
      "epoch": 0.6229638458482321,
      "grad_norm": 0.13309700787067413,
      "learning_rate": 0.00015887353878852285,
      "loss": 0.1048,
      "step": 784
    },
    {
      "epoch": 0.6237584425903854,
      "grad_norm": 0.1379583775997162,
      "learning_rate": 0.00015882040382571733,
      "loss": 0.1027,
      "step": 785
    },
    {
      "epoch": 0.6245530393325387,
      "grad_norm": 0.12613476812839508,
      "learning_rate": 0.0001587672688629118,
      "loss": 0.11,
      "step": 786
    },
    {
      "epoch": 0.6253476360746921,
      "grad_norm": 0.17210915684700012,
      "learning_rate": 0.00015871413390010627,
      "loss": 0.116,
      "step": 787
    },
    {
      "epoch": 0.6261422328168454,
      "grad_norm": 0.16118450462818146,
      "learning_rate": 0.00015866099893730075,
      "loss": 0.1233,
      "step": 788
    },
    {
      "epoch": 0.6269368295589988,
      "grad_norm": 0.1353677213191986,
      "learning_rate": 0.00015860786397449523,
      "loss": 0.0964,
      "step": 789
    },
    {
      "epoch": 0.6277314263011522,
      "grad_norm": 0.10656167566776276,
      "learning_rate": 0.0001585547290116897,
      "loss": 0.0832,
      "step": 790
    },
    {
      "epoch": 0.6285260230433055,
      "grad_norm": 0.1553148776292801,
      "learning_rate": 0.00015850159404888419,
      "loss": 0.0836,
      "step": 791
    },
    {
      "epoch": 0.6293206197854588,
      "grad_norm": 0.09940006583929062,
      "learning_rate": 0.00015844845908607864,
      "loss": 0.0694,
      "step": 792
    },
    {
      "epoch": 0.6301152165276123,
      "grad_norm": 0.11613394320011139,
      "learning_rate": 0.00015839532412327312,
      "loss": 0.0857,
      "step": 793
    },
    {
      "epoch": 0.6309098132697656,
      "grad_norm": 0.10293091088533401,
      "learning_rate": 0.0001583421891604676,
      "loss": 0.0745,
      "step": 794
    },
    {
      "epoch": 0.6317044100119189,
      "grad_norm": 0.08698353171348572,
      "learning_rate": 0.00015828905419766208,
      "loss": 0.0569,
      "step": 795
    },
    {
      "epoch": 0.6324990067540723,
      "grad_norm": 0.09978461265563965,
      "learning_rate": 0.00015823591923485653,
      "loss": 0.0568,
      "step": 796
    },
    {
      "epoch": 0.6332936034962257,
      "grad_norm": 0.088238924741745,
      "learning_rate": 0.000158182784272051,
      "loss": 0.0517,
      "step": 797
    },
    {
      "epoch": 0.634088200238379,
      "grad_norm": 0.10042648017406464,
      "learning_rate": 0.0001581296493092455,
      "loss": 0.0524,
      "step": 798
    },
    {
      "epoch": 0.6348827969805324,
      "grad_norm": 0.11305853724479675,
      "learning_rate": 0.00015807651434643997,
      "loss": 0.0381,
      "step": 799
    },
    {
      "epoch": 0.6356773937226857,
      "grad_norm": 0.09357500821352005,
      "learning_rate": 0.00015802337938363443,
      "loss": 0.0486,
      "step": 800
    },
    {
      "epoch": 0.636471990464839,
      "grad_norm": 0.541445255279541,
      "learning_rate": 0.0001579702444208289,
      "loss": 0.571,
      "step": 801
    },
    {
      "epoch": 0.6372665872069925,
      "grad_norm": 0.4609028995037079,
      "learning_rate": 0.0001579171094580234,
      "loss": 0.4654,
      "step": 802
    },
    {
      "epoch": 0.6380611839491458,
      "grad_norm": 0.48764342069625854,
      "learning_rate": 0.00015786397449521787,
      "loss": 0.4822,
      "step": 803
    },
    {
      "epoch": 0.6388557806912991,
      "grad_norm": 0.3877433240413666,
      "learning_rate": 0.00015781083953241235,
      "loss": 0.3934,
      "step": 804
    },
    {
      "epoch": 0.6396503774334525,
      "grad_norm": 2.7081058025360107,
      "learning_rate": 0.0001577577045696068,
      "loss": 0.4499,
      "step": 805
    },
    {
      "epoch": 0.6404449741756059,
      "grad_norm": 1.2951513528823853,
      "learning_rate": 0.00015770456960680128,
      "loss": 0.3071,
      "step": 806
    },
    {
      "epoch": 0.6412395709177592,
      "grad_norm": 0.32757440209388733,
      "learning_rate": 0.00015765143464399576,
      "loss": 0.3173,
      "step": 807
    },
    {
      "epoch": 0.6420341676599126,
      "grad_norm": 0.2470272034406662,
      "learning_rate": 0.00015759829968119021,
      "loss": 0.2644,
      "step": 808
    },
    {
      "epoch": 0.6428287644020659,
      "grad_norm": 0.20689138770103455,
      "learning_rate": 0.0001575451647183847,
      "loss": 0.2475,
      "step": 809
    },
    {
      "epoch": 0.6436233611442194,
      "grad_norm": 0.2183491438627243,
      "learning_rate": 0.00015749202975557917,
      "loss": 0.2497,
      "step": 810
    },
    {
      "epoch": 0.6444179578863727,
      "grad_norm": 0.203469917178154,
      "learning_rate": 0.00015743889479277365,
      "loss": 0.217,
      "step": 811
    },
    {
      "epoch": 0.645212554628526,
      "grad_norm": 0.19532045722007751,
      "learning_rate": 0.00015738575982996813,
      "loss": 0.209,
      "step": 812
    },
    {
      "epoch": 0.6460071513706794,
      "grad_norm": 0.22475062310695648,
      "learning_rate": 0.00015733262486716262,
      "loss": 0.1791,
      "step": 813
    },
    {
      "epoch": 0.6468017481128328,
      "grad_norm": 0.20370744168758392,
      "learning_rate": 0.00015727948990435707,
      "loss": 0.2238,
      "step": 814
    },
    {
      "epoch": 0.6475963448549861,
      "grad_norm": 0.1931508630514145,
      "learning_rate": 0.00015722635494155155,
      "loss": 0.2168,
      "step": 815
    },
    {
      "epoch": 0.6483909415971395,
      "grad_norm": 0.17058776319026947,
      "learning_rate": 0.000157173219978746,
      "loss": 0.183,
      "step": 816
    },
    {
      "epoch": 0.6491855383392928,
      "grad_norm": 0.19255024194717407,
      "learning_rate": 0.00015712008501594048,
      "loss": 0.2077,
      "step": 817
    },
    {
      "epoch": 0.6499801350814461,
      "grad_norm": 0.15157672762870789,
      "learning_rate": 0.00015706695005313496,
      "loss": 0.1548,
      "step": 818
    },
    {
      "epoch": 0.6507747318235996,
      "grad_norm": 0.19671405851840973,
      "learning_rate": 0.00015701381509032944,
      "loss": 0.1796,
      "step": 819
    },
    {
      "epoch": 0.6515693285657529,
      "grad_norm": 0.14942291378974915,
      "learning_rate": 0.00015696068012752392,
      "loss": 0.1498,
      "step": 820
    },
    {
      "epoch": 0.6523639253079062,
      "grad_norm": 0.14758694171905518,
      "learning_rate": 0.0001569075451647184,
      "loss": 0.153,
      "step": 821
    },
    {
      "epoch": 0.6531585220500596,
      "grad_norm": 0.16902419924736023,
      "learning_rate": 0.00015685441020191288,
      "loss": 0.1462,
      "step": 822
    },
    {
      "epoch": 0.653953118792213,
      "grad_norm": 0.14098156988620758,
      "learning_rate": 0.00015680127523910734,
      "loss": 0.1592,
      "step": 823
    },
    {
      "epoch": 0.6547477155343663,
      "grad_norm": 0.16769036650657654,
      "learning_rate": 0.00015674814027630182,
      "loss": 0.1471,
      "step": 824
    },
    {
      "epoch": 0.6555423122765197,
      "grad_norm": 0.15487892925739288,
      "learning_rate": 0.00015669500531349627,
      "loss": 0.1533,
      "step": 825
    },
    {
      "epoch": 0.656336909018673,
      "grad_norm": 0.12373887747526169,
      "learning_rate": 0.00015664187035069075,
      "loss": 0.1354,
      "step": 826
    },
    {
      "epoch": 0.6571315057608264,
      "grad_norm": 0.12332677096128464,
      "learning_rate": 0.00015658873538788523,
      "loss": 0.1183,
      "step": 827
    },
    {
      "epoch": 0.6579261025029798,
      "grad_norm": 0.14650800824165344,
      "learning_rate": 0.0001565356004250797,
      "loss": 0.1317,
      "step": 828
    },
    {
      "epoch": 0.6587206992451331,
      "grad_norm": 0.1608908474445343,
      "learning_rate": 0.0001564824654622742,
      "loss": 0.1177,
      "step": 829
    },
    {
      "epoch": 0.6595152959872864,
      "grad_norm": 0.14412282407283783,
      "learning_rate": 0.00015642933049946867,
      "loss": 0.1043,
      "step": 830
    },
    {
      "epoch": 0.6603098927294399,
      "grad_norm": 0.10838835686445236,
      "learning_rate": 0.00015637619553666315,
      "loss": 0.0997,
      "step": 831
    },
    {
      "epoch": 0.6611044894715932,
      "grad_norm": 0.14359134435653687,
      "learning_rate": 0.0001563230605738576,
      "loss": 0.1201,
      "step": 832
    },
    {
      "epoch": 0.6618990862137465,
      "grad_norm": 0.09975313395261765,
      "learning_rate": 0.00015626992561105208,
      "loss": 0.0842,
      "step": 833
    },
    {
      "epoch": 0.6626936829558999,
      "grad_norm": 0.10912968218326569,
      "learning_rate": 0.00015621679064824654,
      "loss": 0.0827,
      "step": 834
    },
    {
      "epoch": 0.6634882796980532,
      "grad_norm": 0.1177326887845993,
      "learning_rate": 0.00015616365568544102,
      "loss": 0.1012,
      "step": 835
    },
    {
      "epoch": 0.6642828764402066,
      "grad_norm": 0.10228876769542694,
      "learning_rate": 0.0001561105207226355,
      "loss": 0.0938,
      "step": 836
    },
    {
      "epoch": 0.66507747318236,
      "grad_norm": 0.10407134145498276,
      "learning_rate": 0.00015605738575982998,
      "loss": 0.081,
      "step": 837
    },
    {
      "epoch": 0.6658720699245133,
      "grad_norm": 0.09947404265403748,
      "learning_rate": 0.00015600425079702446,
      "loss": 0.0771,
      "step": 838
    },
    {
      "epoch": 0.6666666666666666,
      "grad_norm": 0.10491498559713364,
      "learning_rate": 0.00015595111583421894,
      "loss": 0.0741,
      "step": 839
    },
    {
      "epoch": 0.6674612634088201,
      "grad_norm": 0.09212974458932877,
      "learning_rate": 0.00015589798087141342,
      "loss": 0.0735,
      "step": 840
    },
    {
      "epoch": 0.6682558601509734,
      "grad_norm": 0.1042516678571701,
      "learning_rate": 0.00015584484590860787,
      "loss": 0.0751,
      "step": 841
    },
    {
      "epoch": 0.6690504568931267,
      "grad_norm": 0.1004156842827797,
      "learning_rate": 0.00015579171094580235,
      "loss": 0.0701,
      "step": 842
    },
    {
      "epoch": 0.6698450536352801,
      "grad_norm": 0.10128264129161835,
      "learning_rate": 0.0001557385759829968,
      "loss": 0.0561,
      "step": 843
    },
    {
      "epoch": 0.6706396503774334,
      "grad_norm": 0.07632110267877579,
      "learning_rate": 0.00015568544102019128,
      "loss": 0.0581,
      "step": 844
    },
    {
      "epoch": 0.6714342471195868,
      "grad_norm": 0.07227075099945068,
      "learning_rate": 0.00015563230605738576,
      "loss": 0.0522,
      "step": 845
    },
    {
      "epoch": 0.6722288438617402,
      "grad_norm": 0.0899520218372345,
      "learning_rate": 0.00015557917109458024,
      "loss": 0.0529,
      "step": 846
    },
    {
      "epoch": 0.6730234406038935,
      "grad_norm": 0.09612081199884415,
      "learning_rate": 0.00015552603613177472,
      "loss": 0.0465,
      "step": 847
    },
    {
      "epoch": 0.6738180373460468,
      "grad_norm": 0.09274063259363174,
      "learning_rate": 0.0001554729011689692,
      "loss": 0.0469,
      "step": 848
    },
    {
      "epoch": 0.6746126340882003,
      "grad_norm": 0.08494765311479568,
      "learning_rate": 0.00015541976620616366,
      "loss": 0.0384,
      "step": 849
    },
    {
      "epoch": 0.6754072308303536,
      "grad_norm": 0.07244168221950531,
      "learning_rate": 0.00015536663124335814,
      "loss": 0.036,
      "step": 850
    },
    {
      "epoch": 0.6762018275725069,
      "grad_norm": 0.6010459065437317,
      "learning_rate": 0.00015531349628055262,
      "loss": 0.6212,
      "step": 851
    },
    {
      "epoch": 0.6769964243146603,
      "grad_norm": 0.5257092118263245,
      "learning_rate": 0.00015526036131774707,
      "loss": 0.5556,
      "step": 852
    },
    {
      "epoch": 0.6777910210568137,
      "grad_norm": 0.3934745788574219,
      "learning_rate": 0.00015520722635494155,
      "loss": 0.4329,
      "step": 853
    },
    {
      "epoch": 0.678585617798967,
      "grad_norm": 0.2981342077255249,
      "learning_rate": 0.00015515409139213603,
      "loss": 0.3525,
      "step": 854
    },
    {
      "epoch": 0.6793802145411204,
      "grad_norm": 0.281978577375412,
      "learning_rate": 0.0001551009564293305,
      "loss": 0.3726,
      "step": 855
    },
    {
      "epoch": 0.6801748112832737,
      "grad_norm": 0.24806897342205048,
      "learning_rate": 0.000155047821466525,
      "loss": 0.3183,
      "step": 856
    },
    {
      "epoch": 0.680969408025427,
      "grad_norm": 0.21533361077308655,
      "learning_rate": 0.00015499468650371945,
      "loss": 0.3042,
      "step": 857
    },
    {
      "epoch": 0.6817640047675805,
      "grad_norm": 0.20488591492176056,
      "learning_rate": 0.00015494155154091393,
      "loss": 0.3059,
      "step": 858
    },
    {
      "epoch": 0.6825586015097338,
      "grad_norm": 0.18938808143138885,
      "learning_rate": 0.0001548884165781084,
      "loss": 0.2499,
      "step": 859
    },
    {
      "epoch": 0.6833531982518871,
      "grad_norm": 0.1756967455148697,
      "learning_rate": 0.00015483528161530289,
      "loss": 0.241,
      "step": 860
    },
    {
      "epoch": 0.6841477949940405,
      "grad_norm": 0.17692844569683075,
      "learning_rate": 0.00015478214665249734,
      "loss": 0.2335,
      "step": 861
    },
    {
      "epoch": 0.6849423917361939,
      "grad_norm": 0.1890135109424591,
      "learning_rate": 0.00015472901168969182,
      "loss": 0.2274,
      "step": 862
    },
    {
      "epoch": 0.6857369884783472,
      "grad_norm": 0.20004349946975708,
      "learning_rate": 0.0001546758767268863,
      "loss": 0.222,
      "step": 863
    },
    {
      "epoch": 0.6865315852205006,
      "grad_norm": 0.21096085011959076,
      "learning_rate": 0.00015462274176408078,
      "loss": 0.2513,
      "step": 864
    },
    {
      "epoch": 0.6873261819626539,
      "grad_norm": 0.2102903574705124,
      "learning_rate": 0.00015456960680127526,
      "loss": 0.2501,
      "step": 865
    },
    {
      "epoch": 0.6881207787048073,
      "grad_norm": 0.18359464406967163,
      "learning_rate": 0.0001545164718384697,
      "loss": 0.2014,
      "step": 866
    },
    {
      "epoch": 0.6889153754469607,
      "grad_norm": 0.17571285367012024,
      "learning_rate": 0.0001544633368756642,
      "loss": 0.194,
      "step": 867
    },
    {
      "epoch": 0.689709972189114,
      "grad_norm": 0.18363459408283234,
      "learning_rate": 0.00015441020191285867,
      "loss": 0.2043,
      "step": 868
    },
    {
      "epoch": 0.6905045689312674,
      "grad_norm": 0.14756089448928833,
      "learning_rate": 0.00015435706695005315,
      "loss": 0.1839,
      "step": 869
    },
    {
      "epoch": 0.6912991656734208,
      "grad_norm": 0.1605052351951599,
      "learning_rate": 0.0001543039319872476,
      "loss": 0.1465,
      "step": 870
    },
    {
      "epoch": 0.6920937624155741,
      "grad_norm": 0.17290230095386505,
      "learning_rate": 0.0001542507970244421,
      "loss": 0.1636,
      "step": 871
    },
    {
      "epoch": 0.6928883591577275,
      "grad_norm": 0.14696426689624786,
      "learning_rate": 0.00015419766206163657,
      "loss": 0.1592,
      "step": 872
    },
    {
      "epoch": 0.6936829558998808,
      "grad_norm": 0.15093308687210083,
      "learning_rate": 0.00015414452709883105,
      "loss": 0.1526,
      "step": 873
    },
    {
      "epoch": 0.6944775526420341,
      "grad_norm": 0.13490872085094452,
      "learning_rate": 0.0001540913921360255,
      "loss": 0.1288,
      "step": 874
    },
    {
      "epoch": 0.6952721493841876,
      "grad_norm": 0.17242932319641113,
      "learning_rate": 0.00015403825717321998,
      "loss": 0.1434,
      "step": 875
    },
    {
      "epoch": 0.6960667461263409,
      "grad_norm": 0.13226795196533203,
      "learning_rate": 0.00015398512221041446,
      "loss": 0.1451,
      "step": 876
    },
    {
      "epoch": 0.6968613428684942,
      "grad_norm": 0.1453486829996109,
      "learning_rate": 0.00015393198724760894,
      "loss": 0.1367,
      "step": 877
    },
    {
      "epoch": 0.6976559396106476,
      "grad_norm": 0.14889761805534363,
      "learning_rate": 0.00015387885228480342,
      "loss": 0.1316,
      "step": 878
    },
    {
      "epoch": 0.698450536352801,
      "grad_norm": 0.1764766126871109,
      "learning_rate": 0.00015382571732199787,
      "loss": 0.1497,
      "step": 879
    },
    {
      "epoch": 0.6992451330949543,
      "grad_norm": 0.14940160512924194,
      "learning_rate": 0.00015377258235919235,
      "loss": 0.1196,
      "step": 880
    },
    {
      "epoch": 0.7000397298371077,
      "grad_norm": 0.12179233133792877,
      "learning_rate": 0.00015371944739638683,
      "loss": 0.0999,
      "step": 881
    },
    {
      "epoch": 0.700834326579261,
      "grad_norm": 0.11307493597269058,
      "learning_rate": 0.0001536663124335813,
      "loss": 0.1109,
      "step": 882
    },
    {
      "epoch": 0.7016289233214144,
      "grad_norm": 0.129949152469635,
      "learning_rate": 0.00015361317747077577,
      "loss": 0.1031,
      "step": 883
    },
    {
      "epoch": 0.7024235200635678,
      "grad_norm": 0.12910374999046326,
      "learning_rate": 0.00015356004250797025,
      "loss": 0.1106,
      "step": 884
    },
    {
      "epoch": 0.7032181168057211,
      "grad_norm": 0.1278323084115982,
      "learning_rate": 0.00015350690754516473,
      "loss": 0.0879,
      "step": 885
    },
    {
      "epoch": 0.7040127135478744,
      "grad_norm": 0.11691456288099289,
      "learning_rate": 0.0001534537725823592,
      "loss": 0.105,
      "step": 886
    },
    {
      "epoch": 0.7048073102900279,
      "grad_norm": 0.11593248695135117,
      "learning_rate": 0.0001534006376195537,
      "loss": 0.0912,
      "step": 887
    },
    {
      "epoch": 0.7056019070321812,
      "grad_norm": 0.11432858556509018,
      "learning_rate": 0.00015334750265674814,
      "loss": 0.0993,
      "step": 888
    },
    {
      "epoch": 0.7063965037743345,
      "grad_norm": 0.1033988669514656,
      "learning_rate": 0.00015329436769394262,
      "loss": 0.0773,
      "step": 889
    },
    {
      "epoch": 0.7071911005164879,
      "grad_norm": 0.1578824669122696,
      "learning_rate": 0.00015324123273113707,
      "loss": 0.0793,
      "step": 890
    },
    {
      "epoch": 0.7079856972586412,
      "grad_norm": 0.10812303423881531,
      "learning_rate": 0.00015318809776833155,
      "loss": 0.0893,
      "step": 891
    },
    {
      "epoch": 0.7087802940007946,
      "grad_norm": 0.10565468668937683,
      "learning_rate": 0.00015313496280552604,
      "loss": 0.0821,
      "step": 892
    },
    {
      "epoch": 0.709574890742948,
      "grad_norm": 0.09359944611787796,
      "learning_rate": 0.00015308182784272052,
      "loss": 0.0641,
      "step": 893
    },
    {
      "epoch": 0.7103694874851013,
      "grad_norm": 0.08907123655080795,
      "learning_rate": 0.000153028692879915,
      "loss": 0.0406,
      "step": 894
    },
    {
      "epoch": 0.7111640842272546,
      "grad_norm": 0.0890696793794632,
      "learning_rate": 0.00015297555791710948,
      "loss": 0.0527,
      "step": 895
    },
    {
      "epoch": 0.7119586809694081,
      "grad_norm": 0.09487970918416977,
      "learning_rate": 0.00015292242295430396,
      "loss": 0.0638,
      "step": 896
    },
    {
      "epoch": 0.7127532777115614,
      "grad_norm": 0.07490167766809464,
      "learning_rate": 0.0001528692879914984,
      "loss": 0.0524,
      "step": 897
    },
    {
      "epoch": 0.7135478744537147,
      "grad_norm": 0.09016551822423935,
      "learning_rate": 0.0001528161530286929,
      "loss": 0.0422,
      "step": 898
    },
    {
      "epoch": 0.7143424711958681,
      "grad_norm": 0.10610288381576538,
      "learning_rate": 0.00015276301806588734,
      "loss": 0.0388,
      "step": 899
    },
    {
      "epoch": 0.7151370679380215,
      "grad_norm": 0.10187870264053345,
      "learning_rate": 0.00015270988310308182,
      "loss": 0.0395,
      "step": 900
    },
    {
      "epoch": 0.7159316646801748,
      "grad_norm": 0.5524362325668335,
      "learning_rate": 0.0001526567481402763,
      "loss": 0.6549,
      "step": 901
    },
    {
      "epoch": 0.7167262614223282,
      "grad_norm": 0.44480687379837036,
      "learning_rate": 0.00015260361317747078,
      "loss": 0.5276,
      "step": 902
    },
    {
      "epoch": 0.7175208581644815,
      "grad_norm": 0.3485763967037201,
      "learning_rate": 0.00015255047821466526,
      "loss": 0.3978,
      "step": 903
    },
    {
      "epoch": 0.7183154549066348,
      "grad_norm": 0.31222444772720337,
      "learning_rate": 0.00015249734325185974,
      "loss": 0.3519,
      "step": 904
    },
    {
      "epoch": 0.7191100516487883,
      "grad_norm": 0.3030798137187958,
      "learning_rate": 0.00015244420828905422,
      "loss": 0.3438,
      "step": 905
    },
    {
      "epoch": 0.7199046483909416,
      "grad_norm": 0.28741419315338135,
      "learning_rate": 0.00015239107332624868,
      "loss": 0.2776,
      "step": 906
    },
    {
      "epoch": 0.7206992451330949,
      "grad_norm": 0.25087860226631165,
      "learning_rate": 0.00015233793836344316,
      "loss": 0.3094,
      "step": 907
    },
    {
      "epoch": 0.7214938418752483,
      "grad_norm": 0.23296049237251282,
      "learning_rate": 0.0001522848034006376,
      "loss": 0.3148,
      "step": 908
    },
    {
      "epoch": 0.7222884386174017,
      "grad_norm": 0.22973895072937012,
      "learning_rate": 0.0001522316684378321,
      "loss": 0.278,
      "step": 909
    },
    {
      "epoch": 0.723083035359555,
      "grad_norm": 0.25666916370391846,
      "learning_rate": 0.00015217853347502657,
      "loss": 0.2893,
      "step": 910
    },
    {
      "epoch": 0.7238776321017084,
      "grad_norm": 0.18476302921772003,
      "learning_rate": 0.00015212539851222105,
      "loss": 0.2144,
      "step": 911
    },
    {
      "epoch": 0.7246722288438617,
      "grad_norm": 0.17033450305461884,
      "learning_rate": 0.00015207226354941553,
      "loss": 0.2178,
      "step": 912
    },
    {
      "epoch": 0.725466825586015,
      "grad_norm": 0.22406944632530212,
      "learning_rate": 0.00015201912858661,
      "loss": 0.2232,
      "step": 913
    },
    {
      "epoch": 0.7262614223281685,
      "grad_norm": 0.1855459064245224,
      "learning_rate": 0.0001519659936238045,
      "loss": 0.2238,
      "step": 914
    },
    {
      "epoch": 0.7270560190703218,
      "grad_norm": 0.17534932494163513,
      "learning_rate": 0.00015191285866099894,
      "loss": 0.1829,
      "step": 915
    },
    {
      "epoch": 0.7278506158124751,
      "grad_norm": 0.17830877006053925,
      "learning_rate": 0.00015185972369819342,
      "loss": 0.1911,
      "step": 916
    },
    {
      "epoch": 0.7286452125546286,
      "grad_norm": 0.18996073305606842,
      "learning_rate": 0.00015180658873538788,
      "loss": 0.2083,
      "step": 917
    },
    {
      "epoch": 0.7294398092967819,
      "grad_norm": 0.16163833439350128,
      "learning_rate": 0.00015175345377258236,
      "loss": 0.1688,
      "step": 918
    },
    {
      "epoch": 0.7302344060389352,
      "grad_norm": 0.18759693205356598,
      "learning_rate": 0.00015170031880977684,
      "loss": 0.1892,
      "step": 919
    },
    {
      "epoch": 0.7310290027810886,
      "grad_norm": 0.1666620522737503,
      "learning_rate": 0.00015164718384697132,
      "loss": 0.2028,
      "step": 920
    },
    {
      "epoch": 0.7318235995232419,
      "grad_norm": 0.15551327168941498,
      "learning_rate": 0.0001515940488841658,
      "loss": 0.149,
      "step": 921
    },
    {
      "epoch": 0.7326181962653953,
      "grad_norm": 0.13187117874622345,
      "learning_rate": 0.00015154091392136028,
      "loss": 0.1631,
      "step": 922
    },
    {
      "epoch": 0.7334127930075487,
      "grad_norm": 0.15222927927970886,
      "learning_rate": 0.00015148777895855473,
      "loss": 0.1646,
      "step": 923
    },
    {
      "epoch": 0.734207389749702,
      "grad_norm": 0.15013282001018524,
      "learning_rate": 0.0001514346439957492,
      "loss": 0.1451,
      "step": 924
    },
    {
      "epoch": 0.7350019864918553,
      "grad_norm": 0.15980419516563416,
      "learning_rate": 0.0001513815090329437,
      "loss": 0.145,
      "step": 925
    },
    {
      "epoch": 0.7357965832340088,
      "grad_norm": 0.13976669311523438,
      "learning_rate": 0.00015132837407013814,
      "loss": 0.1323,
      "step": 926
    },
    {
      "epoch": 0.7365911799761621,
      "grad_norm": 0.18615901470184326,
      "learning_rate": 0.00015127523910733262,
      "loss": 0.1249,
      "step": 927
    },
    {
      "epoch": 0.7373857767183154,
      "grad_norm": 0.12407401204109192,
      "learning_rate": 0.0001512221041445271,
      "loss": 0.1229,
      "step": 928
    },
    {
      "epoch": 0.7381803734604688,
      "grad_norm": 0.12538042664527893,
      "learning_rate": 0.00015116896918172159,
      "loss": 0.1241,
      "step": 929
    },
    {
      "epoch": 0.7389749702026222,
      "grad_norm": 0.12717090547084808,
      "learning_rate": 0.00015111583421891607,
      "loss": 0.1328,
      "step": 930
    },
    {
      "epoch": 0.7397695669447756,
      "grad_norm": 0.12394813448190689,
      "learning_rate": 0.00015106269925611052,
      "loss": 0.1178,
      "step": 931
    },
    {
      "epoch": 0.7405641636869289,
      "grad_norm": 0.14310665428638458,
      "learning_rate": 0.000151009564293305,
      "loss": 0.1102,
      "step": 932
    },
    {
      "epoch": 0.7413587604290822,
      "grad_norm": 0.11430836468935013,
      "learning_rate": 0.00015095642933049948,
      "loss": 0.1067,
      "step": 933
    },
    {
      "epoch": 0.7421533571712357,
      "grad_norm": 0.10837774723768234,
      "learning_rate": 0.00015090329436769396,
      "loss": 0.114,
      "step": 934
    },
    {
      "epoch": 0.742947953913389,
      "grad_norm": 0.13309143483638763,
      "learning_rate": 0.0001508501594048884,
      "loss": 0.0963,
      "step": 935
    },
    {
      "epoch": 0.7437425506555423,
      "grad_norm": 0.1250973790884018,
      "learning_rate": 0.0001507970244420829,
      "loss": 0.0984,
      "step": 936
    },
    {
      "epoch": 0.7445371473976957,
      "grad_norm": 0.09105899930000305,
      "learning_rate": 0.00015074388947927737,
      "loss": 0.0866,
      "step": 937
    },
    {
      "epoch": 0.745331744139849,
      "grad_norm": 0.12140386551618576,
      "learning_rate": 0.00015069075451647185,
      "loss": 0.0848,
      "step": 938
    },
    {
      "epoch": 0.7461263408820024,
      "grad_norm": 0.11533164232969284,
      "learning_rate": 0.0001506376195536663,
      "loss": 0.086,
      "step": 939
    },
    {
      "epoch": 0.7469209376241558,
      "grad_norm": 0.12977193295955658,
      "learning_rate": 0.00015058448459086079,
      "loss": 0.0817,
      "step": 940
    },
    {
      "epoch": 0.7477155343663091,
      "grad_norm": 0.11200359463691711,
      "learning_rate": 0.00015053134962805527,
      "loss": 0.0737,
      "step": 941
    },
    {
      "epoch": 0.7485101311084624,
      "grad_norm": 0.11996280401945114,
      "learning_rate": 0.00015047821466524975,
      "loss": 0.0775,
      "step": 942
    },
    {
      "epoch": 0.7493047278506159,
      "grad_norm": 0.09982285648584366,
      "learning_rate": 0.00015042507970244423,
      "loss": 0.0632,
      "step": 943
    },
    {
      "epoch": 0.7500993245927692,
      "grad_norm": 0.1250828206539154,
      "learning_rate": 0.00015037194473963868,
      "loss": 0.0766,
      "step": 944
    },
    {
      "epoch": 0.7508939213349225,
      "grad_norm": 0.09339133650064468,
      "learning_rate": 0.00015031880977683316,
      "loss": 0.0527,
      "step": 945
    },
    {
      "epoch": 0.7516885180770759,
      "grad_norm": 0.1033630222082138,
      "learning_rate": 0.00015026567481402764,
      "loss": 0.0608,
      "step": 946
    },
    {
      "epoch": 0.7524831148192292,
      "grad_norm": 0.07617101073265076,
      "learning_rate": 0.0001502125398512221,
      "loss": 0.0425,
      "step": 947
    },
    {
      "epoch": 0.7532777115613826,
      "grad_norm": 0.08297976106405258,
      "learning_rate": 0.00015015940488841657,
      "loss": 0.0374,
      "step": 948
    },
    {
      "epoch": 0.754072308303536,
      "grad_norm": 0.08770835399627686,
      "learning_rate": 0.00015010626992561105,
      "loss": 0.0401,
      "step": 949
    },
    {
      "epoch": 0.7548669050456893,
      "grad_norm": 0.08744286000728607,
      "learning_rate": 0.00015005313496280553,
      "loss": 0.041,
      "step": 950
    },
    {
      "epoch": 0.7556615017878426,
      "grad_norm": 1.1166263818740845,
      "learning_rate": 0.00015000000000000001,
      "loss": 0.8082,
      "step": 951
    },
    {
      "epoch": 0.7564560985299961,
      "grad_norm": 0.6174643039703369,
      "learning_rate": 0.0001499468650371945,
      "loss": 0.5286,
      "step": 952
    },
    {
      "epoch": 0.7572506952721494,
      "grad_norm": 0.4276581406593323,
      "learning_rate": 0.00014989373007438895,
      "loss": 0.4409,
      "step": 953
    },
    {
      "epoch": 0.7580452920143027,
      "grad_norm": 0.3548150062561035,
      "learning_rate": 0.00014984059511158343,
      "loss": 0.3916,
      "step": 954
    },
    {
      "epoch": 0.7588398887564561,
      "grad_norm": 0.262378454208374,
      "learning_rate": 0.0001497874601487779,
      "loss": 0.3464,
      "step": 955
    },
    {
      "epoch": 0.7596344854986095,
      "grad_norm": 0.22821268439292908,
      "learning_rate": 0.00014973432518597236,
      "loss": 0.3627,
      "step": 956
    },
    {
      "epoch": 0.7604290822407628,
      "grad_norm": 0.21509510278701782,
      "learning_rate": 0.00014968119022316684,
      "loss": 0.2743,
      "step": 957
    },
    {
      "epoch": 0.7612236789829162,
      "grad_norm": 0.22652535140514374,
      "learning_rate": 0.00014962805526036132,
      "loss": 0.2851,
      "step": 958
    },
    {
      "epoch": 0.7620182757250695,
      "grad_norm": 0.21353915333747864,
      "learning_rate": 0.0001495749202975558,
      "loss": 0.256,
      "step": 959
    },
    {
      "epoch": 0.7628128724672228,
      "grad_norm": 0.20576009154319763,
      "learning_rate": 0.00014952178533475028,
      "loss": 0.266,
      "step": 960
    },
    {
      "epoch": 0.7636074692093763,
      "grad_norm": 0.22500286996364594,
      "learning_rate": 0.00014946865037194476,
      "loss": 0.2472,
      "step": 961
    },
    {
      "epoch": 0.7644020659515296,
      "grad_norm": 0.1894240379333496,
      "learning_rate": 0.00014941551540913924,
      "loss": 0.2137,
      "step": 962
    },
    {
      "epoch": 0.7651966626936829,
      "grad_norm": 0.21805627644062042,
      "learning_rate": 0.0001493623804463337,
      "loss": 0.2256,
      "step": 963
    },
    {
      "epoch": 0.7659912594358363,
      "grad_norm": 0.1821945160627365,
      "learning_rate": 0.00014930924548352815,
      "loss": 0.1857,
      "step": 964
    },
    {
      "epoch": 0.7667858561779897,
      "grad_norm": 0.1960521787405014,
      "learning_rate": 0.00014925611052072263,
      "loss": 0.2085,
      "step": 965
    },
    {
      "epoch": 0.767580452920143,
      "grad_norm": 0.18112990260124207,
      "learning_rate": 0.0001492029755579171,
      "loss": 0.1661,
      "step": 966
    },
    {
      "epoch": 0.7683750496622964,
      "grad_norm": 0.18633733689785004,
      "learning_rate": 0.0001491498405951116,
      "loss": 0.201,
      "step": 967
    },
    {
      "epoch": 0.7691696464044497,
      "grad_norm": 0.1491515338420868,
      "learning_rate": 0.00014909670563230607,
      "loss": 0.151,
      "step": 968
    },
    {
      "epoch": 0.7699642431466031,
      "grad_norm": 0.17171186208724976,
      "learning_rate": 0.00014904357066950055,
      "loss": 0.1425,
      "step": 969
    },
    {
      "epoch": 0.7707588398887565,
      "grad_norm": 0.20572105050086975,
      "learning_rate": 0.00014899043570669503,
      "loss": 0.1675,
      "step": 970
    },
    {
      "epoch": 0.7715534366309098,
      "grad_norm": 0.16931883990764618,
      "learning_rate": 0.0001489373007438895,
      "loss": 0.1773,
      "step": 971
    },
    {
      "epoch": 0.7723480333730631,
      "grad_norm": 0.15122970938682556,
      "learning_rate": 0.00014888416578108396,
      "loss": 0.1387,
      "step": 972
    },
    {
      "epoch": 0.7731426301152166,
      "grad_norm": 0.14790543913841248,
      "learning_rate": 0.00014883103081827842,
      "loss": 0.1327,
      "step": 973
    },
    {
      "epoch": 0.7739372268573699,
      "grad_norm": 0.15891239047050476,
      "learning_rate": 0.0001487778958554729,
      "loss": 0.1631,
      "step": 974
    },
    {
      "epoch": 0.7747318235995232,
      "grad_norm": 0.1503559947013855,
      "learning_rate": 0.00014872476089266738,
      "loss": 0.1287,
      "step": 975
    },
    {
      "epoch": 0.7755264203416766,
      "grad_norm": 0.13630451261997223,
      "learning_rate": 0.00014867162592986186,
      "loss": 0.1273,
      "step": 976
    },
    {
      "epoch": 0.77632101708383,
      "grad_norm": 0.15950947999954224,
      "learning_rate": 0.00014861849096705634,
      "loss": 0.1259,
      "step": 977
    },
    {
      "epoch": 0.7771156138259833,
      "grad_norm": 0.1692427545785904,
      "learning_rate": 0.00014856535600425082,
      "loss": 0.1383,
      "step": 978
    },
    {
      "epoch": 0.7779102105681367,
      "grad_norm": 0.17961415648460388,
      "learning_rate": 0.0001485122210414453,
      "loss": 0.1135,
      "step": 979
    },
    {
      "epoch": 0.77870480731029,
      "grad_norm": 0.18368515372276306,
      "learning_rate": 0.00014845908607863975,
      "loss": 0.1225,
      "step": 980
    },
    {
      "epoch": 0.7794994040524433,
      "grad_norm": 0.14315643906593323,
      "learning_rate": 0.00014840595111583423,
      "loss": 0.115,
      "step": 981
    },
    {
      "epoch": 0.7802940007945968,
      "grad_norm": 0.12215398252010345,
      "learning_rate": 0.00014835281615302868,
      "loss": 0.0922,
      "step": 982
    },
    {
      "epoch": 0.7810885975367501,
      "grad_norm": 0.14005695283412933,
      "learning_rate": 0.00014829968119022316,
      "loss": 0.1104,
      "step": 983
    },
    {
      "epoch": 0.7818831942789034,
      "grad_norm": 0.13215531408786774,
      "learning_rate": 0.00014824654622741764,
      "loss": 0.1129,
      "step": 984
    },
    {
      "epoch": 0.7826777910210568,
      "grad_norm": 0.14320342242717743,
      "learning_rate": 0.00014819341126461212,
      "loss": 0.1195,
      "step": 985
    },
    {
      "epoch": 0.7834723877632102,
      "grad_norm": 0.11862819641828537,
      "learning_rate": 0.0001481402763018066,
      "loss": 0.09,
      "step": 986
    },
    {
      "epoch": 0.7842669845053635,
      "grad_norm": 0.14149117469787598,
      "learning_rate": 0.00014808714133900108,
      "loss": 0.0917,
      "step": 987
    },
    {
      "epoch": 0.7850615812475169,
      "grad_norm": 0.1347786784172058,
      "learning_rate": 0.00014803400637619554,
      "loss": 0.0923,
      "step": 988
    },
    {
      "epoch": 0.7858561779896702,
      "grad_norm": 0.1400180608034134,
      "learning_rate": 0.00014798087141339002,
      "loss": 0.0774,
      "step": 989
    },
    {
      "epoch": 0.7866507747318237,
      "grad_norm": 0.12244634330272675,
      "learning_rate": 0.0001479277364505845,
      "loss": 0.0861,
      "step": 990
    },
    {
      "epoch": 0.787445371473977,
      "grad_norm": 0.14861556887626648,
      "learning_rate": 0.00014787460148777895,
      "loss": 0.0809,
      "step": 991
    },
    {
      "epoch": 0.7882399682161303,
      "grad_norm": 0.1157178208231926,
      "learning_rate": 0.00014782146652497343,
      "loss": 0.0702,
      "step": 992
    },
    {
      "epoch": 0.7890345649582837,
      "grad_norm": 0.11569633334875107,
      "learning_rate": 0.0001477683315621679,
      "loss": 0.0866,
      "step": 993
    },
    {
      "epoch": 0.789829161700437,
      "grad_norm": 0.10847916454076767,
      "learning_rate": 0.0001477151965993624,
      "loss": 0.0821,
      "step": 994
    },
    {
      "epoch": 0.7906237584425904,
      "grad_norm": 0.09860499203205109,
      "learning_rate": 0.00014766206163655687,
      "loss": 0.058,
      "step": 995
    },
    {
      "epoch": 0.7914183551847438,
      "grad_norm": 0.14462539553642273,
      "learning_rate": 0.00014760892667375135,
      "loss": 0.053,
      "step": 996
    },
    {
      "epoch": 0.7922129519268971,
      "grad_norm": 0.09537909924983978,
      "learning_rate": 0.0001475557917109458,
      "loss": 0.047,
      "step": 997
    },
    {
      "epoch": 0.7930075486690504,
      "grad_norm": 0.10279659181833267,
      "learning_rate": 0.00014750265674814028,
      "loss": 0.0408,
      "step": 998
    },
    {
      "epoch": 0.7938021454112039,
      "grad_norm": 0.09692683815956116,
      "learning_rate": 0.00014744952178533476,
      "loss": 0.0323,
      "step": 999
    },
    {
      "epoch": 0.7945967421533572,
      "grad_norm": 0.09459809958934784,
      "learning_rate": 0.00014739638682252922,
      "loss": 0.0445,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 3774,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.198252198705562e+16,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
