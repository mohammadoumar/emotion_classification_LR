{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.3972983710766786,
  "eval_steps": 755,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0007945967421533572,
      "grad_norm": 5.095927715301514,
      "learning_rate": 2e-05,
      "loss": 3.0773,
      "step": 1
    },
    {
      "epoch": 0.0015891934843067143,
      "grad_norm": 5.295345783233643,
      "learning_rate": 4e-05,
      "loss": 3.0638,
      "step": 2
    },
    {
      "epoch": 0.0023837902264600714,
      "grad_norm": 3.9243922233581543,
      "learning_rate": 6e-05,
      "loss": 2.9447,
      "step": 3
    },
    {
      "epoch": 0.0031783869686134287,
      "grad_norm": 2.9748141765594482,
      "learning_rate": 8e-05,
      "loss": 2.7782,
      "step": 4
    },
    {
      "epoch": 0.003972983710766786,
      "grad_norm": 6.790021896362305,
      "learning_rate": 0.0001,
      "loss": 2.4546,
      "step": 5
    },
    {
      "epoch": 0.004767580452920143,
      "grad_norm": 2.016477584838867,
      "learning_rate": 0.00012,
      "loss": 2.2744,
      "step": 6
    },
    {
      "epoch": 0.0055621771950735005,
      "grad_norm": 1.9537750482559204,
      "learning_rate": 0.00014,
      "loss": 2.0014,
      "step": 7
    },
    {
      "epoch": 0.006356773937226857,
      "grad_norm": 1.7805707454681396,
      "learning_rate": 0.00016,
      "loss": 1.7197,
      "step": 8
    },
    {
      "epoch": 0.007151370679380214,
      "grad_norm": 1.8606624603271484,
      "learning_rate": 0.00018,
      "loss": 1.3944,
      "step": 9
    },
    {
      "epoch": 0.007945967421533572,
      "grad_norm": 5.094567775726318,
      "learning_rate": 0.0002,
      "loss": 1.068,
      "step": 10
    },
    {
      "epoch": 0.00874056416368693,
      "grad_norm": 7.3542938232421875,
      "learning_rate": 0.0001999468650371945,
      "loss": 0.8414,
      "step": 11
    },
    {
      "epoch": 0.009535160905840286,
      "grad_norm": 2.5358312129974365,
      "learning_rate": 0.00019989373007438897,
      "loss": 0.575,
      "step": 12
    },
    {
      "epoch": 0.010329757647993643,
      "grad_norm": 1.625028133392334,
      "learning_rate": 0.00019984059511158345,
      "loss": 0.4307,
      "step": 13
    },
    {
      "epoch": 0.011124354390147001,
      "grad_norm": 1.3451693058013916,
      "learning_rate": 0.0001997874601487779,
      "loss": 0.3319,
      "step": 14
    },
    {
      "epoch": 0.011918951132300357,
      "grad_norm": 1.885640025138855,
      "learning_rate": 0.00019973432518597238,
      "loss": 0.3196,
      "step": 15
    },
    {
      "epoch": 0.012713547874453715,
      "grad_norm": 1.2925968170166016,
      "learning_rate": 0.00019968119022316684,
      "loss": 0.2892,
      "step": 16
    },
    {
      "epoch": 0.013508144616607072,
      "grad_norm": 0.6818913221359253,
      "learning_rate": 0.00019962805526036132,
      "loss": 0.2906,
      "step": 17
    },
    {
      "epoch": 0.014302741358760428,
      "grad_norm": 1.793684482574463,
      "learning_rate": 0.0001995749202975558,
      "loss": 0.2529,
      "step": 18
    },
    {
      "epoch": 0.015097338100913786,
      "grad_norm": 1.2905668020248413,
      "learning_rate": 0.00019952178533475028,
      "loss": 0.2545,
      "step": 19
    },
    {
      "epoch": 0.015891934843067144,
      "grad_norm": 3.4983420372009277,
      "learning_rate": 0.00019946865037194476,
      "loss": 0.2479,
      "step": 20
    },
    {
      "epoch": 0.0166865315852205,
      "grad_norm": 1.3703911304473877,
      "learning_rate": 0.00019941551540913924,
      "loss": 0.2466,
      "step": 21
    },
    {
      "epoch": 0.01748112832737386,
      "grad_norm": 1.2794137001037598,
      "learning_rate": 0.00019936238044633372,
      "loss": 0.2425,
      "step": 22
    },
    {
      "epoch": 0.018275725069527213,
      "grad_norm": 4.779952049255371,
      "learning_rate": 0.00019930924548352817,
      "loss": 0.1825,
      "step": 23
    },
    {
      "epoch": 0.01907032181168057,
      "grad_norm": 6.491456985473633,
      "learning_rate": 0.00019925611052072265,
      "loss": 0.2387,
      "step": 24
    },
    {
      "epoch": 0.01986491855383393,
      "grad_norm": 5.676915645599365,
      "learning_rate": 0.0001992029755579171,
      "loss": 0.2204,
      "step": 25
    },
    {
      "epoch": 0.020659515295987287,
      "grad_norm": 0.3911113440990448,
      "learning_rate": 0.00019914984059511158,
      "loss": 0.1705,
      "step": 26
    },
    {
      "epoch": 0.021454112038140644,
      "grad_norm": 0.50523841381073,
      "learning_rate": 0.00019909670563230606,
      "loss": 0.1899,
      "step": 27
    },
    {
      "epoch": 0.022248708780294002,
      "grad_norm": 0.5957397222518921,
      "learning_rate": 0.00019904357066950054,
      "loss": 0.1861,
      "step": 28
    },
    {
      "epoch": 0.023043305522447356,
      "grad_norm": 0.775301992893219,
      "learning_rate": 0.00019899043570669502,
      "loss": 0.1574,
      "step": 29
    },
    {
      "epoch": 0.023837902264600714,
      "grad_norm": 1.0150798559188843,
      "learning_rate": 0.0001989373007438895,
      "loss": 0.1264,
      "step": 30
    },
    {
      "epoch": 0.02463249900675407,
      "grad_norm": 0.7563843131065369,
      "learning_rate": 0.00019888416578108396,
      "loss": 0.1429,
      "step": 31
    },
    {
      "epoch": 0.02542709574890743,
      "grad_norm": 0.4484422504901886,
      "learning_rate": 0.00019883103081827844,
      "loss": 0.1387,
      "step": 32
    },
    {
      "epoch": 0.026221692491060787,
      "grad_norm": 0.37793880701065063,
      "learning_rate": 0.00019877789585547292,
      "loss": 0.0917,
      "step": 33
    },
    {
      "epoch": 0.027016289233214145,
      "grad_norm": 0.6019443273544312,
      "learning_rate": 0.00019872476089266737,
      "loss": 0.123,
      "step": 34
    },
    {
      "epoch": 0.027810885975367503,
      "grad_norm": 0.48734718561172485,
      "learning_rate": 0.00019867162592986185,
      "loss": 0.131,
      "step": 35
    },
    {
      "epoch": 0.028605482717520857,
      "grad_norm": 0.5307793617248535,
      "learning_rate": 0.00019861849096705633,
      "loss": 0.0855,
      "step": 36
    },
    {
      "epoch": 0.029400079459674214,
      "grad_norm": 0.577855110168457,
      "learning_rate": 0.0001985653560042508,
      "loss": 0.082,
      "step": 37
    },
    {
      "epoch": 0.030194676201827572,
      "grad_norm": 0.37780269980430603,
      "learning_rate": 0.0001985122210414453,
      "loss": 0.0856,
      "step": 38
    },
    {
      "epoch": 0.03098927294398093,
      "grad_norm": 0.20342932641506195,
      "learning_rate": 0.00019845908607863975,
      "loss": 0.0824,
      "step": 39
    },
    {
      "epoch": 0.03178386968613429,
      "grad_norm": 0.36485666036605835,
      "learning_rate": 0.00019840595111583423,
      "loss": 0.0866,
      "step": 40
    },
    {
      "epoch": 0.03257846642828764,
      "grad_norm": 0.38292914628982544,
      "learning_rate": 0.0001983528161530287,
      "loss": 0.0813,
      "step": 41
    },
    {
      "epoch": 0.033373063170441,
      "grad_norm": 0.42815956473350525,
      "learning_rate": 0.00019829968119022319,
      "loss": 0.0777,
      "step": 42
    },
    {
      "epoch": 0.03416765991259436,
      "grad_norm": 0.42552876472473145,
      "learning_rate": 0.00019824654622741764,
      "loss": 0.0506,
      "step": 43
    },
    {
      "epoch": 0.03496225665474772,
      "grad_norm": 0.47190940380096436,
      "learning_rate": 0.00019819341126461212,
      "loss": 0.0624,
      "step": 44
    },
    {
      "epoch": 0.03575685339690107,
      "grad_norm": 0.2830035090446472,
      "learning_rate": 0.0001981402763018066,
      "loss": 0.0616,
      "step": 45
    },
    {
      "epoch": 0.03655145013905443,
      "grad_norm": 0.5423195362091064,
      "learning_rate": 0.00019808714133900108,
      "loss": 0.0614,
      "step": 46
    },
    {
      "epoch": 0.03734604688120779,
      "grad_norm": 0.20404629409313202,
      "learning_rate": 0.00019803400637619553,
      "loss": 0.0504,
      "step": 47
    },
    {
      "epoch": 0.03814064362336114,
      "grad_norm": 0.2916088402271271,
      "learning_rate": 0.00019798087141339,
      "loss": 0.0394,
      "step": 48
    },
    {
      "epoch": 0.038935240365514504,
      "grad_norm": 6.089454650878906,
      "learning_rate": 0.0001979277364505845,
      "loss": 0.0444,
      "step": 49
    },
    {
      "epoch": 0.03972983710766786,
      "grad_norm": 0.31506067514419556,
      "learning_rate": 0.00019787460148777897,
      "loss": 0.035,
      "step": 50
    },
    {
      "epoch": 0.04052443384982122,
      "grad_norm": 1.4184181690216064,
      "learning_rate": 0.00019782146652497345,
      "loss": 0.749,
      "step": 51
    },
    {
      "epoch": 0.04131903059197457,
      "grad_norm": 2.142108201980591,
      "learning_rate": 0.0001977683315621679,
      "loss": 0.5594,
      "step": 52
    },
    {
      "epoch": 0.04211362733412793,
      "grad_norm": 0.6221466064453125,
      "learning_rate": 0.0001977151965993624,
      "loss": 0.494,
      "step": 53
    },
    {
      "epoch": 0.04290822407628129,
      "grad_norm": 0.39055222272872925,
      "learning_rate": 0.00019766206163655687,
      "loss": 0.3899,
      "step": 54
    },
    {
      "epoch": 0.04370282081843464,
      "grad_norm": 0.4236755669116974,
      "learning_rate": 0.00019760892667375135,
      "loss": 0.3525,
      "step": 55
    },
    {
      "epoch": 0.044497417560588004,
      "grad_norm": 0.3338366746902466,
      "learning_rate": 0.0001975557917109458,
      "loss": 0.3568,
      "step": 56
    },
    {
      "epoch": 0.04529201430274136,
      "grad_norm": 0.34272903203964233,
      "learning_rate": 0.00019750265674814028,
      "loss": 0.3519,
      "step": 57
    },
    {
      "epoch": 0.04608661104489471,
      "grad_norm": 0.28583958745002747,
      "learning_rate": 0.00019744952178533476,
      "loss": 0.2993,
      "step": 58
    },
    {
      "epoch": 0.046881207787048074,
      "grad_norm": 0.3079226315021515,
      "learning_rate": 0.00019739638682252924,
      "loss": 0.3023,
      "step": 59
    },
    {
      "epoch": 0.04767580452920143,
      "grad_norm": 0.2857295572757721,
      "learning_rate": 0.00019734325185972372,
      "loss": 0.2599,
      "step": 60
    },
    {
      "epoch": 0.04847040127135479,
      "grad_norm": 0.26531514525413513,
      "learning_rate": 0.00019729011689691817,
      "loss": 0.2562,
      "step": 61
    },
    {
      "epoch": 0.04926499801350814,
      "grad_norm": 0.2522497773170471,
      "learning_rate": 0.00019723698193411265,
      "loss": 0.2805,
      "step": 62
    },
    {
      "epoch": 0.050059594755661505,
      "grad_norm": 0.19372712075710297,
      "learning_rate": 0.00019718384697130713,
      "loss": 0.2578,
      "step": 63
    },
    {
      "epoch": 0.05085419149781486,
      "grad_norm": 0.2371576875448227,
      "learning_rate": 0.0001971307120085016,
      "loss": 0.2443,
      "step": 64
    },
    {
      "epoch": 0.05164878823996821,
      "grad_norm": 0.2858528792858124,
      "learning_rate": 0.00019707757704569607,
      "loss": 0.2271,
      "step": 65
    },
    {
      "epoch": 0.052443384982121574,
      "grad_norm": 0.23870187997817993,
      "learning_rate": 0.00019702444208289055,
      "loss": 0.1795,
      "step": 66
    },
    {
      "epoch": 0.05323798172427493,
      "grad_norm": 0.22088764607906342,
      "learning_rate": 0.00019697130712008503,
      "loss": 0.2156,
      "step": 67
    },
    {
      "epoch": 0.05403257846642829,
      "grad_norm": 0.22069445252418518,
      "learning_rate": 0.0001969181721572795,
      "loss": 0.1977,
      "step": 68
    },
    {
      "epoch": 0.054827175208581644,
      "grad_norm": 0.2257574200630188,
      "learning_rate": 0.000196865037194474,
      "loss": 0.1556,
      "step": 69
    },
    {
      "epoch": 0.055621771950735005,
      "grad_norm": 0.24251490831375122,
      "learning_rate": 0.00019681190223166844,
      "loss": 0.1942,
      "step": 70
    },
    {
      "epoch": 0.05641636869288836,
      "grad_norm": 0.22877848148345947,
      "learning_rate": 0.00019675876726886292,
      "loss": 0.2078,
      "step": 71
    },
    {
      "epoch": 0.057210965435041714,
      "grad_norm": 0.2661331593990326,
      "learning_rate": 0.00019670563230605737,
      "loss": 0.1829,
      "step": 72
    },
    {
      "epoch": 0.058005562177195075,
      "grad_norm": 0.18543265759944916,
      "learning_rate": 0.00019665249734325185,
      "loss": 0.1766,
      "step": 73
    },
    {
      "epoch": 0.05880015891934843,
      "grad_norm": 0.19438567757606506,
      "learning_rate": 0.00019659936238044634,
      "loss": 0.1803,
      "step": 74
    },
    {
      "epoch": 0.05959475566150179,
      "grad_norm": 0.20390024781227112,
      "learning_rate": 0.00019654622741764082,
      "loss": 0.1298,
      "step": 75
    },
    {
      "epoch": 0.060389352403655144,
      "grad_norm": 0.18488165736198425,
      "learning_rate": 0.0001964930924548353,
      "loss": 0.1342,
      "step": 76
    },
    {
      "epoch": 0.061183949145808506,
      "grad_norm": 0.29963284730911255,
      "learning_rate": 0.00019643995749202978,
      "loss": 0.1634,
      "step": 77
    },
    {
      "epoch": 0.06197854588796186,
      "grad_norm": 0.1895529329776764,
      "learning_rate": 0.00019638682252922426,
      "loss": 0.1159,
      "step": 78
    },
    {
      "epoch": 0.06277314263011521,
      "grad_norm": 0.1705477386713028,
      "learning_rate": 0.0001963336875664187,
      "loss": 0.126,
      "step": 79
    },
    {
      "epoch": 0.06356773937226858,
      "grad_norm": 0.20234054327011108,
      "learning_rate": 0.0001962805526036132,
      "loss": 0.109,
      "step": 80
    },
    {
      "epoch": 0.06436233611442194,
      "grad_norm": 0.16916969418525696,
      "learning_rate": 0.00019622741764080764,
      "loss": 0.1424,
      "step": 81
    },
    {
      "epoch": 0.06515693285657528,
      "grad_norm": 0.17861494421958923,
      "learning_rate": 0.00019617428267800212,
      "loss": 0.1154,
      "step": 82
    },
    {
      "epoch": 0.06595152959872864,
      "grad_norm": 0.20245608687400818,
      "learning_rate": 0.0001961211477151966,
      "loss": 0.1337,
      "step": 83
    },
    {
      "epoch": 0.066746126340882,
      "grad_norm": 0.15971405804157257,
      "learning_rate": 0.00019606801275239108,
      "loss": 0.127,
      "step": 84
    },
    {
      "epoch": 0.06754072308303535,
      "grad_norm": 0.1884482502937317,
      "learning_rate": 0.00019601487778958556,
      "loss": 0.1215,
      "step": 85
    },
    {
      "epoch": 0.06833531982518871,
      "grad_norm": 1.080165147781372,
      "learning_rate": 0.00019596174282678004,
      "loss": 0.1195,
      "step": 86
    },
    {
      "epoch": 0.06912991656734208,
      "grad_norm": 0.1586466282606125,
      "learning_rate": 0.00019590860786397452,
      "loss": 0.0987,
      "step": 87
    },
    {
      "epoch": 0.06992451330949544,
      "grad_norm": 0.19644121825695038,
      "learning_rate": 0.00019585547290116898,
      "loss": 0.0735,
      "step": 88
    },
    {
      "epoch": 0.07071911005164878,
      "grad_norm": 0.15086694061756134,
      "learning_rate": 0.00019580233793836346,
      "loss": 0.0821,
      "step": 89
    },
    {
      "epoch": 0.07151370679380215,
      "grad_norm": 0.17575614154338837,
      "learning_rate": 0.0001957492029755579,
      "loss": 0.0847,
      "step": 90
    },
    {
      "epoch": 0.0723083035359555,
      "grad_norm": 0.15817910432815552,
      "learning_rate": 0.0001956960680127524,
      "loss": 0.0775,
      "step": 91
    },
    {
      "epoch": 0.07310290027810885,
      "grad_norm": 0.28526571393013,
      "learning_rate": 0.00019564293304994687,
      "loss": 0.0745,
      "step": 92
    },
    {
      "epoch": 0.07389749702026222,
      "grad_norm": 0.1360766887664795,
      "learning_rate": 0.00019558979808714135,
      "loss": 0.0637,
      "step": 93
    },
    {
      "epoch": 0.07469209376241558,
      "grad_norm": 0.13167662918567657,
      "learning_rate": 0.00019553666312433583,
      "loss": 0.0592,
      "step": 94
    },
    {
      "epoch": 0.07548669050456894,
      "grad_norm": 0.14108988642692566,
      "learning_rate": 0.0001954835281615303,
      "loss": 0.0381,
      "step": 95
    },
    {
      "epoch": 0.07628128724672228,
      "grad_norm": 0.09228210151195526,
      "learning_rate": 0.00019543039319872476,
      "loss": 0.0453,
      "step": 96
    },
    {
      "epoch": 0.07707588398887565,
      "grad_norm": 0.11658669263124466,
      "learning_rate": 0.00019537725823591924,
      "loss": 0.0418,
      "step": 97
    },
    {
      "epoch": 0.07787048073102901,
      "grad_norm": 0.13790568709373474,
      "learning_rate": 0.00019532412327311372,
      "loss": 0.0307,
      "step": 98
    },
    {
      "epoch": 0.07866507747318235,
      "grad_norm": 0.12323784083127975,
      "learning_rate": 0.00019527098831030818,
      "loss": 0.0473,
      "step": 99
    },
    {
      "epoch": 0.07945967421533572,
      "grad_norm": 0.10029814392328262,
      "learning_rate": 0.00019521785334750266,
      "loss": 0.033,
      "step": 100
    },
    {
      "epoch": 0.08025427095748908,
      "grad_norm": 2.2295618057250977,
      "learning_rate": 0.00019516471838469714,
      "loss": 0.8273,
      "step": 101
    },
    {
      "epoch": 0.08104886769964244,
      "grad_norm": 0.9544523358345032,
      "learning_rate": 0.00019511158342189162,
      "loss": 0.5899,
      "step": 102
    },
    {
      "epoch": 0.08184346444179579,
      "grad_norm": 0.7177934646606445,
      "learning_rate": 0.0001950584484590861,
      "loss": 0.4572,
      "step": 103
    },
    {
      "epoch": 0.08263806118394915,
      "grad_norm": 0.38069862127304077,
      "learning_rate": 0.00019500531349628058,
      "loss": 0.3659,
      "step": 104
    },
    {
      "epoch": 0.08343265792610251,
      "grad_norm": 0.3473935127258301,
      "learning_rate": 0.00019495217853347503,
      "loss": 0.3858,
      "step": 105
    },
    {
      "epoch": 0.08422725466825585,
      "grad_norm": 0.36638519167900085,
      "learning_rate": 0.0001948990435706695,
      "loss": 0.3261,
      "step": 106
    },
    {
      "epoch": 0.08502185141040922,
      "grad_norm": 0.5606485605239868,
      "learning_rate": 0.000194845908607864,
      "loss": 0.3292,
      "step": 107
    },
    {
      "epoch": 0.08581644815256258,
      "grad_norm": 2.4229931831359863,
      "learning_rate": 0.00019479277364505844,
      "loss": 0.419,
      "step": 108
    },
    {
      "epoch": 0.08661104489471594,
      "grad_norm": 5.446245193481445,
      "learning_rate": 0.00019473963868225293,
      "loss": 0.3847,
      "step": 109
    },
    {
      "epoch": 0.08740564163686929,
      "grad_norm": 0.5252211689949036,
      "learning_rate": 0.0001946865037194474,
      "loss": 0.3849,
      "step": 110
    },
    {
      "epoch": 0.08820023837902265,
      "grad_norm": 0.539202868938446,
      "learning_rate": 0.00019463336875664189,
      "loss": 0.309,
      "step": 111
    },
    {
      "epoch": 0.08899483512117601,
      "grad_norm": 0.27690789103507996,
      "learning_rate": 0.00019458023379383637,
      "loss": 0.2729,
      "step": 112
    },
    {
      "epoch": 0.08978943186332936,
      "grad_norm": 0.27345505356788635,
      "learning_rate": 0.00019452709883103082,
      "loss": 0.2915,
      "step": 113
    },
    {
      "epoch": 0.09058402860548272,
      "grad_norm": 0.30040210485458374,
      "learning_rate": 0.0001944739638682253,
      "loss": 0.2996,
      "step": 114
    },
    {
      "epoch": 0.09137862534763608,
      "grad_norm": 0.31928932666778564,
      "learning_rate": 0.00019442082890541978,
      "loss": 0.2556,
      "step": 115
    },
    {
      "epoch": 0.09217322208978942,
      "grad_norm": 0.2683221995830536,
      "learning_rate": 0.00019436769394261426,
      "loss": 0.2559,
      "step": 116
    },
    {
      "epoch": 0.09296781883194279,
      "grad_norm": 0.27670955657958984,
      "learning_rate": 0.0001943145589798087,
      "loss": 0.2535,
      "step": 117
    },
    {
      "epoch": 0.09376241557409615,
      "grad_norm": 0.2962208390235901,
      "learning_rate": 0.0001942614240170032,
      "loss": 0.2281,
      "step": 118
    },
    {
      "epoch": 0.09455701231624951,
      "grad_norm": 0.24815694987773895,
      "learning_rate": 0.00019420828905419767,
      "loss": 0.2095,
      "step": 119
    },
    {
      "epoch": 0.09535160905840286,
      "grad_norm": 0.3147117793560028,
      "learning_rate": 0.00019415515409139215,
      "loss": 0.2255,
      "step": 120
    },
    {
      "epoch": 0.09614620580055622,
      "grad_norm": 0.30625513195991516,
      "learning_rate": 0.0001941020191285866,
      "loss": 0.1762,
      "step": 121
    },
    {
      "epoch": 0.09694080254270958,
      "grad_norm": 0.38549360632896423,
      "learning_rate": 0.00019404888416578109,
      "loss": 0.182,
      "step": 122
    },
    {
      "epoch": 0.09773539928486293,
      "grad_norm": 0.34338653087615967,
      "learning_rate": 0.00019399574920297557,
      "loss": 0.1489,
      "step": 123
    },
    {
      "epoch": 0.09852999602701629,
      "grad_norm": 0.3558342456817627,
      "learning_rate": 0.00019394261424017005,
      "loss": 0.1786,
      "step": 124
    },
    {
      "epoch": 0.09932459276916965,
      "grad_norm": 0.39403101801872253,
      "learning_rate": 0.00019388947927736453,
      "loss": 0.1454,
      "step": 125
    },
    {
      "epoch": 0.10011918951132301,
      "grad_norm": 0.45242002606391907,
      "learning_rate": 0.00019383634431455898,
      "loss": 0.1497,
      "step": 126
    },
    {
      "epoch": 0.10091378625347636,
      "grad_norm": 0.66505366563797,
      "learning_rate": 0.00019378320935175346,
      "loss": 0.1607,
      "step": 127
    },
    {
      "epoch": 0.10170838299562972,
      "grad_norm": 1.2089029550552368,
      "learning_rate": 0.00019373007438894794,
      "loss": 0.1418,
      "step": 128
    },
    {
      "epoch": 0.10250297973778308,
      "grad_norm": 0.8559134602546692,
      "learning_rate": 0.0001936769394261424,
      "loss": 0.1274,
      "step": 129
    },
    {
      "epoch": 0.10329757647993643,
      "grad_norm": 0.29022493958473206,
      "learning_rate": 0.00019362380446333687,
      "loss": 0.0959,
      "step": 130
    },
    {
      "epoch": 0.10409217322208979,
      "grad_norm": 0.15817007422447205,
      "learning_rate": 0.00019357066950053135,
      "loss": 0.1159,
      "step": 131
    },
    {
      "epoch": 0.10488676996424315,
      "grad_norm": 0.178268700838089,
      "learning_rate": 0.00019351753453772583,
      "loss": 0.1063,
      "step": 132
    },
    {
      "epoch": 0.10568136670639651,
      "grad_norm": 0.19003534317016602,
      "learning_rate": 0.00019346439957492031,
      "loss": 0.136,
      "step": 133
    },
    {
      "epoch": 0.10647596344854986,
      "grad_norm": 0.1463114619255066,
      "learning_rate": 0.0001934112646121148,
      "loss": 0.1085,
      "step": 134
    },
    {
      "epoch": 0.10727056019070322,
      "grad_norm": 0.15220893919467926,
      "learning_rate": 0.00019335812964930927,
      "loss": 0.0957,
      "step": 135
    },
    {
      "epoch": 0.10806515693285658,
      "grad_norm": 0.13502717018127441,
      "learning_rate": 0.00019330499468650373,
      "loss": 0.0958,
      "step": 136
    },
    {
      "epoch": 0.10885975367500993,
      "grad_norm": 0.14319714903831482,
      "learning_rate": 0.00019325185972369818,
      "loss": 0.1062,
      "step": 137
    },
    {
      "epoch": 0.10965435041716329,
      "grad_norm": 0.15498748421669006,
      "learning_rate": 0.00019319872476089266,
      "loss": 0.0713,
      "step": 138
    },
    {
      "epoch": 0.11044894715931665,
      "grad_norm": 0.1616978794336319,
      "learning_rate": 0.00019314558979808714,
      "loss": 0.103,
      "step": 139
    },
    {
      "epoch": 0.11124354390147001,
      "grad_norm": 0.13654562830924988,
      "learning_rate": 0.00019309245483528162,
      "loss": 0.0786,
      "step": 140
    },
    {
      "epoch": 0.11203814064362336,
      "grad_norm": 0.1615074723958969,
      "learning_rate": 0.0001930393198724761,
      "loss": 0.075,
      "step": 141
    },
    {
      "epoch": 0.11283273738577672,
      "grad_norm": 0.10743191093206406,
      "learning_rate": 0.00019298618490967058,
      "loss": 0.0641,
      "step": 142
    },
    {
      "epoch": 0.11362733412793008,
      "grad_norm": 0.14171114563941956,
      "learning_rate": 0.00019293304994686506,
      "loss": 0.0518,
      "step": 143
    },
    {
      "epoch": 0.11442193087008343,
      "grad_norm": 0.11164332926273346,
      "learning_rate": 0.00019287991498405954,
      "loss": 0.048,
      "step": 144
    },
    {
      "epoch": 0.11521652761223679,
      "grad_norm": 0.10373976826667786,
      "learning_rate": 0.000192826780021254,
      "loss": 0.0583,
      "step": 145
    },
    {
      "epoch": 0.11601112435439015,
      "grad_norm": 0.13332654535770416,
      "learning_rate": 0.00019277364505844845,
      "loss": 0.0367,
      "step": 146
    },
    {
      "epoch": 0.11680572109654351,
      "grad_norm": 0.09105151146650314,
      "learning_rate": 0.00019272051009564293,
      "loss": 0.0506,
      "step": 147
    },
    {
      "epoch": 0.11760031783869686,
      "grad_norm": 0.07898186892271042,
      "learning_rate": 0.0001926673751328374,
      "loss": 0.0367,
      "step": 148
    },
    {
      "epoch": 0.11839491458085022,
      "grad_norm": 0.15004244446754456,
      "learning_rate": 0.0001926142401700319,
      "loss": 0.0397,
      "step": 149
    },
    {
      "epoch": 0.11918951132300358,
      "grad_norm": 0.10065368562936783,
      "learning_rate": 0.00019256110520722637,
      "loss": 0.0252,
      "step": 150
    },
    {
      "epoch": 0.11998410806515693,
      "grad_norm": 2.053464651107788,
      "learning_rate": 0.00019250797024442085,
      "loss": 0.911,
      "step": 151
    },
    {
      "epoch": 0.12077870480731029,
      "grad_norm": 1.0877612829208374,
      "learning_rate": 0.00019245483528161533,
      "loss": 0.5193,
      "step": 152
    },
    {
      "epoch": 0.12157330154946365,
      "grad_norm": 0.5766066908836365,
      "learning_rate": 0.0001924017003188098,
      "loss": 0.4419,
      "step": 153
    },
    {
      "epoch": 0.12236789829161701,
      "grad_norm": 0.3879833221435547,
      "learning_rate": 0.00019234856535600426,
      "loss": 0.3987,
      "step": 154
    },
    {
      "epoch": 0.12316249503377036,
      "grad_norm": 0.3370971083641052,
      "learning_rate": 0.00019229543039319872,
      "loss": 0.3516,
      "step": 155
    },
    {
      "epoch": 0.12395709177592372,
      "grad_norm": 0.3980254530906677,
      "learning_rate": 0.0001922422954303932,
      "loss": 0.3753,
      "step": 156
    },
    {
      "epoch": 0.12475168851807708,
      "grad_norm": 0.3528785705566406,
      "learning_rate": 0.00019218916046758768,
      "loss": 0.3268,
      "step": 157
    },
    {
      "epoch": 0.12554628526023043,
      "grad_norm": 0.3944130837917328,
      "learning_rate": 0.00019213602550478216,
      "loss": 0.3272,
      "step": 158
    },
    {
      "epoch": 0.1263408820023838,
      "grad_norm": 0.25333133339881897,
      "learning_rate": 0.00019208289054197664,
      "loss": 0.2303,
      "step": 159
    },
    {
      "epoch": 0.12713547874453715,
      "grad_norm": 0.3143738806247711,
      "learning_rate": 0.00019202975557917112,
      "loss": 0.3348,
      "step": 160
    },
    {
      "epoch": 0.1279300754866905,
      "grad_norm": 0.25687018036842346,
      "learning_rate": 0.0001919766206163656,
      "loss": 0.3177,
      "step": 161
    },
    {
      "epoch": 0.12872467222884387,
      "grad_norm": 0.27663454413414,
      "learning_rate": 0.00019192348565356005,
      "loss": 0.2664,
      "step": 162
    },
    {
      "epoch": 0.12951926897099722,
      "grad_norm": 0.26116472482681274,
      "learning_rate": 0.00019187035069075453,
      "loss": 0.2391,
      "step": 163
    },
    {
      "epoch": 0.13031386571315057,
      "grad_norm": 0.2535942792892456,
      "learning_rate": 0.00019181721572794898,
      "loss": 0.2354,
      "step": 164
    },
    {
      "epoch": 0.13110846245530394,
      "grad_norm": 0.30917888879776,
      "learning_rate": 0.00019176408076514346,
      "loss": 0.2492,
      "step": 165
    },
    {
      "epoch": 0.1319030591974573,
      "grad_norm": 0.22282648086547852,
      "learning_rate": 0.00019171094580233794,
      "loss": 0.1903,
      "step": 166
    },
    {
      "epoch": 0.13269765593961064,
      "grad_norm": 0.21345986425876617,
      "learning_rate": 0.00019165781083953242,
      "loss": 0.2048,
      "step": 167
    },
    {
      "epoch": 0.133492252681764,
      "grad_norm": 0.20483341813087463,
      "learning_rate": 0.0001916046758767269,
      "loss": 0.1784,
      "step": 168
    },
    {
      "epoch": 0.13428684942391736,
      "grad_norm": 0.23842015862464905,
      "learning_rate": 0.00019155154091392138,
      "loss": 0.1894,
      "step": 169
    },
    {
      "epoch": 0.1350814461660707,
      "grad_norm": 0.19351354241371155,
      "learning_rate": 0.00019149840595111584,
      "loss": 0.1733,
      "step": 170
    },
    {
      "epoch": 0.13587604290822408,
      "grad_norm": 0.1911739706993103,
      "learning_rate": 0.00019144527098831032,
      "loss": 0.1664,
      "step": 171
    },
    {
      "epoch": 0.13667063965037743,
      "grad_norm": 0.18692897260189056,
      "learning_rate": 0.0001913921360255048,
      "loss": 0.1592,
      "step": 172
    },
    {
      "epoch": 0.1374652363925308,
      "grad_norm": 0.19956745207309723,
      "learning_rate": 0.00019133900106269925,
      "loss": 0.1689,
      "step": 173
    },
    {
      "epoch": 0.13825983313468415,
      "grad_norm": 0.20633526146411896,
      "learning_rate": 0.00019128586609989373,
      "loss": 0.1447,
      "step": 174
    },
    {
      "epoch": 0.1390544298768375,
      "grad_norm": 0.1701001673936844,
      "learning_rate": 0.0001912327311370882,
      "loss": 0.1432,
      "step": 175
    },
    {
      "epoch": 0.13984902661899087,
      "grad_norm": 0.217815563082695,
      "learning_rate": 0.0001911795961742827,
      "loss": 0.1469,
      "step": 176
    },
    {
      "epoch": 0.14064362336114422,
      "grad_norm": 0.16553503274917603,
      "learning_rate": 0.00019112646121147717,
      "loss": 0.1273,
      "step": 177
    },
    {
      "epoch": 0.14143822010329757,
      "grad_norm": 0.1608160138130188,
      "learning_rate": 0.00019107332624867162,
      "loss": 0.107,
      "step": 178
    },
    {
      "epoch": 0.14223281684545094,
      "grad_norm": 0.15976440906524658,
      "learning_rate": 0.0001910201912858661,
      "loss": 0.1038,
      "step": 179
    },
    {
      "epoch": 0.1430274135876043,
      "grad_norm": 0.14395356178283691,
      "learning_rate": 0.00019096705632306058,
      "loss": 0.1176,
      "step": 180
    },
    {
      "epoch": 0.14382201032975764,
      "grad_norm": 0.1603294312953949,
      "learning_rate": 0.00019091392136025507,
      "loss": 0.1308,
      "step": 181
    },
    {
      "epoch": 0.144616607071911,
      "grad_norm": 0.1494060456752777,
      "learning_rate": 0.00019086078639744952,
      "loss": 0.1047,
      "step": 182
    },
    {
      "epoch": 0.14541120381406436,
      "grad_norm": 0.14104509353637695,
      "learning_rate": 0.000190807651434644,
      "loss": 0.1106,
      "step": 183
    },
    {
      "epoch": 0.1462058005562177,
      "grad_norm": 0.13810361921787262,
      "learning_rate": 0.00019075451647183848,
      "loss": 0.1022,
      "step": 184
    },
    {
      "epoch": 0.14700039729837108,
      "grad_norm": 0.1312577873468399,
      "learning_rate": 0.00019070138150903296,
      "loss": 0.0748,
      "step": 185
    },
    {
      "epoch": 0.14779499404052443,
      "grad_norm": 0.12264079600572586,
      "learning_rate": 0.0001906482465462274,
      "loss": 0.0969,
      "step": 186
    },
    {
      "epoch": 0.14858959078267778,
      "grad_norm": 0.1455862671136856,
      "learning_rate": 0.0001905951115834219,
      "loss": 0.1209,
      "step": 187
    },
    {
      "epoch": 0.14938418752483115,
      "grad_norm": 0.17013278603553772,
      "learning_rate": 0.00019054197662061637,
      "loss": 0.0815,
      "step": 188
    },
    {
      "epoch": 0.1501787842669845,
      "grad_norm": 0.12392491102218628,
      "learning_rate": 0.00019048884165781085,
      "loss": 0.0988,
      "step": 189
    },
    {
      "epoch": 0.15097338100913787,
      "grad_norm": 0.1342221349477768,
      "learning_rate": 0.00019043570669500533,
      "loss": 0.0677,
      "step": 190
    },
    {
      "epoch": 0.15176797775129122,
      "grad_norm": 0.12016434967517853,
      "learning_rate": 0.0001903825717321998,
      "loss": 0.0629,
      "step": 191
    },
    {
      "epoch": 0.15256257449344457,
      "grad_norm": 0.14949846267700195,
      "learning_rate": 0.00019032943676939427,
      "loss": 0.0847,
      "step": 192
    },
    {
      "epoch": 0.15335717123559794,
      "grad_norm": 0.11531606316566467,
      "learning_rate": 0.00019027630180658875,
      "loss": 0.0965,
      "step": 193
    },
    {
      "epoch": 0.1541517679777513,
      "grad_norm": 0.09808109700679779,
      "learning_rate": 0.00019022316684378323,
      "loss": 0.0548,
      "step": 194
    },
    {
      "epoch": 0.15494636471990464,
      "grad_norm": 0.090995192527771,
      "learning_rate": 0.00019017003188097768,
      "loss": 0.0583,
      "step": 195
    },
    {
      "epoch": 0.15574096146205801,
      "grad_norm": 0.10583622008562088,
      "learning_rate": 0.00019011689691817216,
      "loss": 0.0479,
      "step": 196
    },
    {
      "epoch": 0.15653555820421136,
      "grad_norm": 0.09739190340042114,
      "learning_rate": 0.00019006376195536664,
      "loss": 0.0562,
      "step": 197
    },
    {
      "epoch": 0.1573301549463647,
      "grad_norm": 0.0883338451385498,
      "learning_rate": 0.00019001062699256112,
      "loss": 0.0369,
      "step": 198
    },
    {
      "epoch": 0.15812475168851808,
      "grad_norm": 0.0873837023973465,
      "learning_rate": 0.0001899574920297556,
      "loss": 0.0472,
      "step": 199
    },
    {
      "epoch": 0.15891934843067143,
      "grad_norm": 0.10254475474357605,
      "learning_rate": 0.00018990435706695008,
      "loss": 0.045,
      "step": 200
    },
    {
      "epoch": 0.15971394517282478,
      "grad_norm": 1.2343448400497437,
      "learning_rate": 0.00018985122210414453,
      "loss": 0.7572,
      "step": 201
    },
    {
      "epoch": 0.16050854191497815,
      "grad_norm": 0.6580986380577087,
      "learning_rate": 0.000189798087141339,
      "loss": 0.4785,
      "step": 202
    },
    {
      "epoch": 0.1613031386571315,
      "grad_norm": 0.5708999633789062,
      "learning_rate": 0.00018974495217853347,
      "loss": 0.4374,
      "step": 203
    },
    {
      "epoch": 0.16209773539928488,
      "grad_norm": 0.4224390387535095,
      "learning_rate": 0.00018969181721572795,
      "loss": 0.3299,
      "step": 204
    },
    {
      "epoch": 0.16289233214143822,
      "grad_norm": 0.2796633541584015,
      "learning_rate": 0.00018963868225292243,
      "loss": 0.3331,
      "step": 205
    },
    {
      "epoch": 0.16368692888359157,
      "grad_norm": 0.323624849319458,
      "learning_rate": 0.0001895855472901169,
      "loss": 0.2753,
      "step": 206
    },
    {
      "epoch": 0.16448152562574495,
      "grad_norm": 0.31121471524238586,
      "learning_rate": 0.0001895324123273114,
      "loss": 0.2937,
      "step": 207
    },
    {
      "epoch": 0.1652761223678983,
      "grad_norm": 0.320325642824173,
      "learning_rate": 0.00018947927736450587,
      "loss": 0.3051,
      "step": 208
    },
    {
      "epoch": 0.16607071911005164,
      "grad_norm": 0.2650983929634094,
      "learning_rate": 0.00018942614240170035,
      "loss": 0.233,
      "step": 209
    },
    {
      "epoch": 0.16686531585220502,
      "grad_norm": 0.2619933485984802,
      "learning_rate": 0.0001893730074388948,
      "loss": 0.268,
      "step": 210
    },
    {
      "epoch": 0.16765991259435836,
      "grad_norm": 0.2706146538257599,
      "learning_rate": 0.00018931987247608925,
      "loss": 0.2425,
      "step": 211
    },
    {
      "epoch": 0.1684545093365117,
      "grad_norm": 0.26196661591529846,
      "learning_rate": 0.00018926673751328373,
      "loss": 0.2045,
      "step": 212
    },
    {
      "epoch": 0.16924910607866508,
      "grad_norm": 0.2148316353559494,
      "learning_rate": 0.00018921360255047821,
      "loss": 0.2268,
      "step": 213
    },
    {
      "epoch": 0.17004370282081843,
      "grad_norm": 0.24098920822143555,
      "learning_rate": 0.0001891604675876727,
      "loss": 0.2187,
      "step": 214
    },
    {
      "epoch": 0.17083829956297178,
      "grad_norm": 0.21333810687065125,
      "learning_rate": 0.00018910733262486717,
      "loss": 0.2266,
      "step": 215
    },
    {
      "epoch": 0.17163289630512515,
      "grad_norm": 0.20518247783184052,
      "learning_rate": 0.00018905419766206165,
      "loss": 0.2121,
      "step": 216
    },
    {
      "epoch": 0.1724274930472785,
      "grad_norm": 0.20147433876991272,
      "learning_rate": 0.00018900106269925614,
      "loss": 0.178,
      "step": 217
    },
    {
      "epoch": 0.17322208978943188,
      "grad_norm": 0.19207751750946045,
      "learning_rate": 0.00018894792773645062,
      "loss": 0.1983,
      "step": 218
    },
    {
      "epoch": 0.17401668653158522,
      "grad_norm": 0.18413332104682922,
      "learning_rate": 0.00018889479277364507,
      "loss": 0.1786,
      "step": 219
    },
    {
      "epoch": 0.17481128327373857,
      "grad_norm": 0.1892576813697815,
      "learning_rate": 0.00018884165781083952,
      "loss": 0.15,
      "step": 220
    },
    {
      "epoch": 0.17560588001589195,
      "grad_norm": 0.3022839426994324,
      "learning_rate": 0.000188788522848034,
      "loss": 0.1626,
      "step": 221
    },
    {
      "epoch": 0.1764004767580453,
      "grad_norm": 0.1650765836238861,
      "learning_rate": 0.00018873538788522848,
      "loss": 0.1732,
      "step": 222
    },
    {
      "epoch": 0.17719507350019864,
      "grad_norm": 0.1676028072834015,
      "learning_rate": 0.00018868225292242296,
      "loss": 0.1559,
      "step": 223
    },
    {
      "epoch": 0.17798967024235202,
      "grad_norm": 0.1494373381137848,
      "learning_rate": 0.00018862911795961744,
      "loss": 0.1356,
      "step": 224
    },
    {
      "epoch": 0.17878426698450536,
      "grad_norm": 0.1572408378124237,
      "learning_rate": 0.00018857598299681192,
      "loss": 0.1467,
      "step": 225
    },
    {
      "epoch": 0.1795788637266587,
      "grad_norm": 0.18835245072841644,
      "learning_rate": 0.0001885228480340064,
      "loss": 0.1344,
      "step": 226
    },
    {
      "epoch": 0.18037346046881209,
      "grad_norm": 0.19205854833126068,
      "learning_rate": 0.00018846971307120086,
      "loss": 0.1515,
      "step": 227
    },
    {
      "epoch": 0.18116805721096543,
      "grad_norm": 0.1536608785390854,
      "learning_rate": 0.00018841657810839534,
      "loss": 0.1243,
      "step": 228
    },
    {
      "epoch": 0.18196265395311878,
      "grad_norm": 0.16713586449623108,
      "learning_rate": 0.0001883634431455898,
      "loss": 0.1416,
      "step": 229
    },
    {
      "epoch": 0.18275725069527216,
      "grad_norm": 0.13304764032363892,
      "learning_rate": 0.00018831030818278427,
      "loss": 0.1065,
      "step": 230
    },
    {
      "epoch": 0.1835518474374255,
      "grad_norm": 0.14029183983802795,
      "learning_rate": 0.00018825717321997875,
      "loss": 0.105,
      "step": 231
    },
    {
      "epoch": 0.18434644417957885,
      "grad_norm": 0.14151990413665771,
      "learning_rate": 0.00018820403825717323,
      "loss": 0.1163,
      "step": 232
    },
    {
      "epoch": 0.18514104092173223,
      "grad_norm": 0.13942307233810425,
      "learning_rate": 0.0001881509032943677,
      "loss": 0.0947,
      "step": 233
    },
    {
      "epoch": 0.18593563766388557,
      "grad_norm": 0.12254980951547623,
      "learning_rate": 0.0001880977683315622,
      "loss": 0.0821,
      "step": 234
    },
    {
      "epoch": 0.18673023440603895,
      "grad_norm": 0.13873480260372162,
      "learning_rate": 0.00018804463336875667,
      "loss": 0.0944,
      "step": 235
    },
    {
      "epoch": 0.1875248311481923,
      "grad_norm": 0.1375495195388794,
      "learning_rate": 0.00018799149840595112,
      "loss": 0.0952,
      "step": 236
    },
    {
      "epoch": 0.18831942789034564,
      "grad_norm": 0.12424521148204803,
      "learning_rate": 0.0001879383634431456,
      "loss": 0.0786,
      "step": 237
    },
    {
      "epoch": 0.18911402463249902,
      "grad_norm": 0.18482917547225952,
      "learning_rate": 0.00018788522848034006,
      "loss": 0.0885,
      "step": 238
    },
    {
      "epoch": 0.18990862137465236,
      "grad_norm": 0.10911871492862701,
      "learning_rate": 0.00018783209351753454,
      "loss": 0.0759,
      "step": 239
    },
    {
      "epoch": 0.1907032181168057,
      "grad_norm": 0.11924850195646286,
      "learning_rate": 0.00018777895855472902,
      "loss": 0.0847,
      "step": 240
    },
    {
      "epoch": 0.1914978148589591,
      "grad_norm": 0.10834884643554688,
      "learning_rate": 0.0001877258235919235,
      "loss": 0.0754,
      "step": 241
    },
    {
      "epoch": 0.19229241160111243,
      "grad_norm": 0.09732711315155029,
      "learning_rate": 0.00018767268862911798,
      "loss": 0.0827,
      "step": 242
    },
    {
      "epoch": 0.19308700834326578,
      "grad_norm": 0.09349105507135391,
      "learning_rate": 0.00018761955366631246,
      "loss": 0.0579,
      "step": 243
    },
    {
      "epoch": 0.19388160508541916,
      "grad_norm": 0.11625517904758453,
      "learning_rate": 0.0001875664187035069,
      "loss": 0.0691,
      "step": 244
    },
    {
      "epoch": 0.1946762018275725,
      "grad_norm": 0.10142417252063751,
      "learning_rate": 0.0001875132837407014,
      "loss": 0.0634,
      "step": 245
    },
    {
      "epoch": 0.19547079856972585,
      "grad_norm": 0.11022219806909561,
      "learning_rate": 0.00018746014877789587,
      "loss": 0.0497,
      "step": 246
    },
    {
      "epoch": 0.19626539531187923,
      "grad_norm": 0.07341694831848145,
      "learning_rate": 0.00018740701381509035,
      "loss": 0.0478,
      "step": 247
    },
    {
      "epoch": 0.19705999205403257,
      "grad_norm": 0.08137212693691254,
      "learning_rate": 0.0001873538788522848,
      "loss": 0.0432,
      "step": 248
    },
    {
      "epoch": 0.19785458879618595,
      "grad_norm": 0.09138885140419006,
      "learning_rate": 0.00018730074388947928,
      "loss": 0.0424,
      "step": 249
    },
    {
      "epoch": 0.1986491855383393,
      "grad_norm": 0.09371628612279892,
      "learning_rate": 0.00018724760892667376,
      "loss": 0.038,
      "step": 250
    },
    {
      "epoch": 0.19944378228049264,
      "grad_norm": 0.7443809509277344,
      "learning_rate": 0.00018719447396386824,
      "loss": 0.5868,
      "step": 251
    },
    {
      "epoch": 0.20023837902264602,
      "grad_norm": 0.5104140043258667,
      "learning_rate": 0.0001871413390010627,
      "loss": 0.4128,
      "step": 252
    },
    {
      "epoch": 0.20103297576479937,
      "grad_norm": 0.3993620276451111,
      "learning_rate": 0.00018708820403825718,
      "loss": 0.3477,
      "step": 253
    },
    {
      "epoch": 0.2018275725069527,
      "grad_norm": 0.3042195439338684,
      "learning_rate": 0.00018703506907545166,
      "loss": 0.3175,
      "step": 254
    },
    {
      "epoch": 0.2026221692491061,
      "grad_norm": 0.2376960664987564,
      "learning_rate": 0.00018698193411264614,
      "loss": 0.2944,
      "step": 255
    },
    {
      "epoch": 0.20341676599125944,
      "grad_norm": 0.20891733467578888,
      "learning_rate": 0.00018692879914984062,
      "loss": 0.2698,
      "step": 256
    },
    {
      "epoch": 0.20421136273341278,
      "grad_norm": 0.28546926379203796,
      "learning_rate": 0.00018687566418703507,
      "loss": 0.2663,
      "step": 257
    },
    {
      "epoch": 0.20500595947556616,
      "grad_norm": 0.20936451852321625,
      "learning_rate": 0.00018682252922422955,
      "loss": 0.238,
      "step": 258
    },
    {
      "epoch": 0.2058005562177195,
      "grad_norm": 0.22359099984169006,
      "learning_rate": 0.00018676939426142403,
      "loss": 0.2139,
      "step": 259
    },
    {
      "epoch": 0.20659515295987285,
      "grad_norm": 0.18875464797019958,
      "learning_rate": 0.00018671625929861849,
      "loss": 0.2149,
      "step": 260
    },
    {
      "epoch": 0.20738974970202623,
      "grad_norm": 0.19195617735385895,
      "learning_rate": 0.00018666312433581297,
      "loss": 0.1757,
      "step": 261
    },
    {
      "epoch": 0.20818434644417957,
      "grad_norm": 0.18069887161254883,
      "learning_rate": 0.00018660998937300745,
      "loss": 0.196,
      "step": 262
    },
    {
      "epoch": 0.20897894318633295,
      "grad_norm": 0.1520906686782837,
      "learning_rate": 0.00018655685441020193,
      "loss": 0.1388,
      "step": 263
    },
    {
      "epoch": 0.2097735399284863,
      "grad_norm": 0.19778677821159363,
      "learning_rate": 0.0001865037194473964,
      "loss": 0.1947,
      "step": 264
    },
    {
      "epoch": 0.21056813667063964,
      "grad_norm": 0.18842343986034393,
      "learning_rate": 0.00018645058448459089,
      "loss": 0.1839,
      "step": 265
    },
    {
      "epoch": 0.21136273341279302,
      "grad_norm": 0.1762600988149643,
      "learning_rate": 0.00018639744952178534,
      "loss": 0.1881,
      "step": 266
    },
    {
      "epoch": 0.21215733015494637,
      "grad_norm": 0.20037174224853516,
      "learning_rate": 0.00018634431455897982,
      "loss": 0.1493,
      "step": 267
    },
    {
      "epoch": 0.2129519268970997,
      "grad_norm": 0.17650973796844482,
      "learning_rate": 0.00018629117959617427,
      "loss": 0.1744,
      "step": 268
    },
    {
      "epoch": 0.2137465236392531,
      "grad_norm": 0.20521490275859833,
      "learning_rate": 0.00018623804463336875,
      "loss": 0.1454,
      "step": 269
    },
    {
      "epoch": 0.21454112038140644,
      "grad_norm": 0.171891450881958,
      "learning_rate": 0.00018618490967056323,
      "loss": 0.1903,
      "step": 270
    },
    {
      "epoch": 0.21533571712355978,
      "grad_norm": 0.19535954296588898,
      "learning_rate": 0.0001861317747077577,
      "loss": 0.1648,
      "step": 271
    },
    {
      "epoch": 0.21613031386571316,
      "grad_norm": 0.1603385955095291,
      "learning_rate": 0.0001860786397449522,
      "loss": 0.1662,
      "step": 272
    },
    {
      "epoch": 0.2169249106078665,
      "grad_norm": 0.1688283085823059,
      "learning_rate": 0.00018602550478214667,
      "loss": 0.1324,
      "step": 273
    },
    {
      "epoch": 0.21771950735001985,
      "grad_norm": 0.1454406976699829,
      "learning_rate": 0.00018597236981934115,
      "loss": 0.1349,
      "step": 274
    },
    {
      "epoch": 0.21851410409217323,
      "grad_norm": 0.14433448016643524,
      "learning_rate": 0.0001859192348565356,
      "loss": 0.1244,
      "step": 275
    },
    {
      "epoch": 0.21930870083432658,
      "grad_norm": 0.16377180814743042,
      "learning_rate": 0.00018586609989373006,
      "loss": 0.146,
      "step": 276
    },
    {
      "epoch": 0.22010329757647992,
      "grad_norm": 0.1509755551815033,
      "learning_rate": 0.00018581296493092454,
      "loss": 0.1381,
      "step": 277
    },
    {
      "epoch": 0.2208978943186333,
      "grad_norm": 0.13850636780261993,
      "learning_rate": 0.00018575982996811902,
      "loss": 0.1141,
      "step": 278
    },
    {
      "epoch": 0.22169249106078665,
      "grad_norm": 0.14569461345672607,
      "learning_rate": 0.0001857066950053135,
      "loss": 0.1132,
      "step": 279
    },
    {
      "epoch": 0.22248708780294002,
      "grad_norm": 0.15754817426204681,
      "learning_rate": 0.00018565356004250798,
      "loss": 0.1083,
      "step": 280
    },
    {
      "epoch": 0.22328168454509337,
      "grad_norm": 0.12774896621704102,
      "learning_rate": 0.00018560042507970246,
      "loss": 0.1194,
      "step": 281
    },
    {
      "epoch": 0.22407628128724671,
      "grad_norm": 0.1569223254919052,
      "learning_rate": 0.00018554729011689694,
      "loss": 0.0931,
      "step": 282
    },
    {
      "epoch": 0.2248708780294001,
      "grad_norm": 0.13270846009254456,
      "learning_rate": 0.00018549415515409142,
      "loss": 0.0869,
      "step": 283
    },
    {
      "epoch": 0.22566547477155344,
      "grad_norm": 0.12781941890716553,
      "learning_rate": 0.00018544102019128587,
      "loss": 0.1003,
      "step": 284
    },
    {
      "epoch": 0.22646007151370678,
      "grad_norm": 0.13108161091804504,
      "learning_rate": 0.00018538788522848033,
      "loss": 0.0987,
      "step": 285
    },
    {
      "epoch": 0.22725466825586016,
      "grad_norm": 0.12517087161540985,
      "learning_rate": 0.0001853347502656748,
      "loss": 0.0843,
      "step": 286
    },
    {
      "epoch": 0.2280492649980135,
      "grad_norm": 0.14053288102149963,
      "learning_rate": 0.0001852816153028693,
      "loss": 0.0876,
      "step": 287
    },
    {
      "epoch": 0.22884386174016685,
      "grad_norm": 0.11192777752876282,
      "learning_rate": 0.00018522848034006377,
      "loss": 0.0775,
      "step": 288
    },
    {
      "epoch": 0.22963845848232023,
      "grad_norm": 0.1255486011505127,
      "learning_rate": 0.00018517534537725825,
      "loss": 0.0932,
      "step": 289
    },
    {
      "epoch": 0.23043305522447358,
      "grad_norm": 0.10283544659614563,
      "learning_rate": 0.00018512221041445273,
      "loss": 0.0526,
      "step": 290
    },
    {
      "epoch": 0.23122765196662692,
      "grad_norm": 0.11816684156656265,
      "learning_rate": 0.0001850690754516472,
      "loss": 0.0875,
      "step": 291
    },
    {
      "epoch": 0.2320222487087803,
      "grad_norm": 0.11222042888402939,
      "learning_rate": 0.0001850159404888417,
      "loss": 0.0731,
      "step": 292
    },
    {
      "epoch": 0.23281684545093365,
      "grad_norm": 0.0812532976269722,
      "learning_rate": 0.00018496280552603614,
      "loss": 0.0406,
      "step": 293
    },
    {
      "epoch": 0.23361144219308702,
      "grad_norm": 0.1140429675579071,
      "learning_rate": 0.0001849096705632306,
      "loss": 0.0521,
      "step": 294
    },
    {
      "epoch": 0.23440603893524037,
      "grad_norm": 0.1127929836511612,
      "learning_rate": 0.00018485653560042507,
      "loss": 0.0669,
      "step": 295
    },
    {
      "epoch": 0.23520063567739372,
      "grad_norm": 0.08382664620876312,
      "learning_rate": 0.00018480340063761956,
      "loss": 0.0525,
      "step": 296
    },
    {
      "epoch": 0.2359952324195471,
      "grad_norm": 0.08195595443248749,
      "learning_rate": 0.00018475026567481404,
      "loss": 0.0415,
      "step": 297
    },
    {
      "epoch": 0.23678982916170044,
      "grad_norm": 0.07257259637117386,
      "learning_rate": 0.00018469713071200852,
      "loss": 0.0403,
      "step": 298
    },
    {
      "epoch": 0.23758442590385379,
      "grad_norm": 0.0934387817978859,
      "learning_rate": 0.000184643995749203,
      "loss": 0.056,
      "step": 299
    },
    {
      "epoch": 0.23837902264600716,
      "grad_norm": 0.08408962190151215,
      "learning_rate": 0.00018459086078639748,
      "loss": 0.049,
      "step": 300
    },
    {
      "epoch": 0.2391736193881605,
      "grad_norm": 0.4774269759654999,
      "learning_rate": 0.00018453772582359193,
      "loss": 0.524,
      "step": 301
    },
    {
      "epoch": 0.23996821613031386,
      "grad_norm": 0.3650152385234833,
      "learning_rate": 0.0001844845908607864,
      "loss": 0.3981,
      "step": 302
    },
    {
      "epoch": 0.24076281287246723,
      "grad_norm": 0.4363173842430115,
      "learning_rate": 0.0001844314558979809,
      "loss": 0.4276,
      "step": 303
    },
    {
      "epoch": 0.24155740961462058,
      "grad_norm": 0.27391302585601807,
      "learning_rate": 0.00018437832093517534,
      "loss": 0.3553,
      "step": 304
    },
    {
      "epoch": 0.24235200635677392,
      "grad_norm": 0.3125152587890625,
      "learning_rate": 0.00018432518597236982,
      "loss": 0.3139,
      "step": 305
    },
    {
      "epoch": 0.2431466030989273,
      "grad_norm": 0.24081191420555115,
      "learning_rate": 0.0001842720510095643,
      "loss": 0.3128,
      "step": 306
    },
    {
      "epoch": 0.24394119984108065,
      "grad_norm": 0.1966840624809265,
      "learning_rate": 0.00018421891604675878,
      "loss": 0.2515,
      "step": 307
    },
    {
      "epoch": 0.24473579658323402,
      "grad_norm": 0.18584159016609192,
      "learning_rate": 0.00018416578108395326,
      "loss": 0.2428,
      "step": 308
    },
    {
      "epoch": 0.24553039332538737,
      "grad_norm": 0.21181358397006989,
      "learning_rate": 0.00018411264612114772,
      "loss": 0.2622,
      "step": 309
    },
    {
      "epoch": 0.24632499006754072,
      "grad_norm": 0.1748988926410675,
      "learning_rate": 0.0001840595111583422,
      "loss": 0.2182,
      "step": 310
    },
    {
      "epoch": 0.2471195868096941,
      "grad_norm": 0.20695795118808746,
      "learning_rate": 0.00018400637619553668,
      "loss": 0.2075,
      "step": 311
    },
    {
      "epoch": 0.24791418355184744,
      "grad_norm": 0.1929444521665573,
      "learning_rate": 0.00018395324123273116,
      "loss": 0.179,
      "step": 312
    },
    {
      "epoch": 0.2487087802940008,
      "grad_norm": 0.19815370440483093,
      "learning_rate": 0.0001839001062699256,
      "loss": 0.2067,
      "step": 313
    },
    {
      "epoch": 0.24950337703615416,
      "grad_norm": 0.16783186793327332,
      "learning_rate": 0.0001838469713071201,
      "loss": 0.2002,
      "step": 314
    },
    {
      "epoch": 0.25029797377830754,
      "grad_norm": 0.2093949019908905,
      "learning_rate": 0.00018379383634431457,
      "loss": 0.2317,
      "step": 315
    },
    {
      "epoch": 0.25109257052046086,
      "grad_norm": 0.17659251391887665,
      "learning_rate": 0.00018374070138150905,
      "loss": 0.1722,
      "step": 316
    },
    {
      "epoch": 0.25188716726261423,
      "grad_norm": 0.16365720331668854,
      "learning_rate": 0.0001836875664187035,
      "loss": 0.1864,
      "step": 317
    },
    {
      "epoch": 0.2526817640047676,
      "grad_norm": 0.18505944311618805,
      "learning_rate": 0.00018363443145589798,
      "loss": 0.1847,
      "step": 318
    },
    {
      "epoch": 0.2534763607469209,
      "grad_norm": 0.18928128480911255,
      "learning_rate": 0.00018358129649309246,
      "loss": 0.1922,
      "step": 319
    },
    {
      "epoch": 0.2542709574890743,
      "grad_norm": 0.18404938280582428,
      "learning_rate": 0.00018352816153028694,
      "loss": 0.156,
      "step": 320
    },
    {
      "epoch": 0.2550655542312277,
      "grad_norm": 0.1524052619934082,
      "learning_rate": 0.00018347502656748142,
      "loss": 0.173,
      "step": 321
    },
    {
      "epoch": 0.255860150973381,
      "grad_norm": 0.15880431234836578,
      "learning_rate": 0.00018342189160467588,
      "loss": 0.1636,
      "step": 322
    },
    {
      "epoch": 0.25665474771553437,
      "grad_norm": 0.17561404407024384,
      "learning_rate": 0.00018336875664187036,
      "loss": 0.1545,
      "step": 323
    },
    {
      "epoch": 0.25744934445768775,
      "grad_norm": 0.20384939014911652,
      "learning_rate": 0.00018331562167906484,
      "loss": 0.162,
      "step": 324
    },
    {
      "epoch": 0.25824394119984106,
      "grad_norm": 0.17336349189281464,
      "learning_rate": 0.00018326248671625932,
      "loss": 0.1413,
      "step": 325
    },
    {
      "epoch": 0.25903853794199444,
      "grad_norm": 0.16647028923034668,
      "learning_rate": 0.00018320935175345377,
      "loss": 0.1673,
      "step": 326
    },
    {
      "epoch": 0.2598331346841478,
      "grad_norm": 0.17103615403175354,
      "learning_rate": 0.00018315621679064825,
      "loss": 0.1292,
      "step": 327
    },
    {
      "epoch": 0.26062773142630113,
      "grad_norm": 0.13630561530590057,
      "learning_rate": 0.00018310308182784273,
      "loss": 0.1179,
      "step": 328
    },
    {
      "epoch": 0.2614223281684545,
      "grad_norm": 0.1453106552362442,
      "learning_rate": 0.0001830499468650372,
      "loss": 0.1195,
      "step": 329
    },
    {
      "epoch": 0.2622169249106079,
      "grad_norm": 0.1364022195339203,
      "learning_rate": 0.0001829968119022317,
      "loss": 0.1318,
      "step": 330
    },
    {
      "epoch": 0.2630115216527612,
      "grad_norm": 0.12560318410396576,
      "learning_rate": 0.00018294367693942614,
      "loss": 0.1189,
      "step": 331
    },
    {
      "epoch": 0.2638061183949146,
      "grad_norm": 0.15900033712387085,
      "learning_rate": 0.00018289054197662063,
      "loss": 0.1495,
      "step": 332
    },
    {
      "epoch": 0.26460071513706795,
      "grad_norm": 0.12530042231082916,
      "learning_rate": 0.0001828374070138151,
      "loss": 0.1131,
      "step": 333
    },
    {
      "epoch": 0.2653953118792213,
      "grad_norm": 0.12570373713970184,
      "learning_rate": 0.00018278427205100956,
      "loss": 0.0941,
      "step": 334
    },
    {
      "epoch": 0.26618990862137465,
      "grad_norm": 0.12103325873613358,
      "learning_rate": 0.00018273113708820404,
      "loss": 0.1125,
      "step": 335
    },
    {
      "epoch": 0.266984505363528,
      "grad_norm": 0.22631710767745972,
      "learning_rate": 0.00018267800212539852,
      "loss": 0.1005,
      "step": 336
    },
    {
      "epoch": 0.26777910210568134,
      "grad_norm": 0.12893043458461761,
      "learning_rate": 0.000182624867162593,
      "loss": 0.0928,
      "step": 337
    },
    {
      "epoch": 0.2685736988478347,
      "grad_norm": 0.10584737360477448,
      "learning_rate": 0.00018257173219978748,
      "loss": 0.1083,
      "step": 338
    },
    {
      "epoch": 0.2693682955899881,
      "grad_norm": 0.11462851613759995,
      "learning_rate": 0.00018251859723698196,
      "loss": 0.0821,
      "step": 339
    },
    {
      "epoch": 0.2701628923321414,
      "grad_norm": 0.1762094497680664,
      "learning_rate": 0.0001824654622741764,
      "loss": 0.0866,
      "step": 340
    },
    {
      "epoch": 0.2709574890742948,
      "grad_norm": 0.1287495344877243,
      "learning_rate": 0.0001824123273113709,
      "loss": 0.0956,
      "step": 341
    },
    {
      "epoch": 0.27175208581644816,
      "grad_norm": 0.09796353429555893,
      "learning_rate": 0.00018235919234856535,
      "loss": 0.0682,
      "step": 342
    },
    {
      "epoch": 0.2725466825586015,
      "grad_norm": 0.09211049228906631,
      "learning_rate": 0.00018230605738575983,
      "loss": 0.0709,
      "step": 343
    },
    {
      "epoch": 0.27334127930075486,
      "grad_norm": 0.07535607367753983,
      "learning_rate": 0.0001822529224229543,
      "loss": 0.0663,
      "step": 344
    },
    {
      "epoch": 0.27413587604290823,
      "grad_norm": 0.07346019893884659,
      "learning_rate": 0.00018219978746014879,
      "loss": 0.0489,
      "step": 345
    },
    {
      "epoch": 0.2749304727850616,
      "grad_norm": 0.08231117576360703,
      "learning_rate": 0.00018214665249734327,
      "loss": 0.0513,
      "step": 346
    },
    {
      "epoch": 0.2757250695272149,
      "grad_norm": 0.07508116960525513,
      "learning_rate": 0.00018209351753453775,
      "loss": 0.0453,
      "step": 347
    },
    {
      "epoch": 0.2765196662693683,
      "grad_norm": 0.08778450638055801,
      "learning_rate": 0.00018204038257173223,
      "loss": 0.0418,
      "step": 348
    },
    {
      "epoch": 0.2773142630115217,
      "grad_norm": 0.08524610847234726,
      "learning_rate": 0.00018198724760892668,
      "loss": 0.0454,
      "step": 349
    },
    {
      "epoch": 0.278108859753675,
      "grad_norm": 0.08434765785932541,
      "learning_rate": 0.00018193411264612113,
      "loss": 0.0412,
      "step": 350
    },
    {
      "epoch": 0.2789034564958284,
      "grad_norm": 0.5407375693321228,
      "learning_rate": 0.0001818809776833156,
      "loss": 0.6257,
      "step": 351
    },
    {
      "epoch": 0.27969805323798175,
      "grad_norm": 0.4241466522216797,
      "learning_rate": 0.0001818278427205101,
      "loss": 0.4843,
      "step": 352
    },
    {
      "epoch": 0.28049264998013507,
      "grad_norm": 0.4140092432498932,
      "learning_rate": 0.00018177470775770457,
      "loss": 0.3838,
      "step": 353
    },
    {
      "epoch": 0.28128724672228844,
      "grad_norm": 0.3156092166900635,
      "learning_rate": 0.00018172157279489905,
      "loss": 0.3705,
      "step": 354
    },
    {
      "epoch": 0.2820818434644418,
      "grad_norm": 0.26371702551841736,
      "learning_rate": 0.00018166843783209353,
      "loss": 0.3146,
      "step": 355
    },
    {
      "epoch": 0.28287644020659514,
      "grad_norm": 0.2153661996126175,
      "learning_rate": 0.00018161530286928801,
      "loss": 0.3199,
      "step": 356
    },
    {
      "epoch": 0.2836710369487485,
      "grad_norm": 0.20497655868530273,
      "learning_rate": 0.0001815621679064825,
      "loss": 0.3051,
      "step": 357
    },
    {
      "epoch": 0.2844656336909019,
      "grad_norm": 0.23229101300239563,
      "learning_rate": 0.00018150903294367695,
      "loss": 0.2773,
      "step": 358
    },
    {
      "epoch": 0.2852602304330552,
      "grad_norm": 0.19682466983795166,
      "learning_rate": 0.00018145589798087143,
      "loss": 0.2325,
      "step": 359
    },
    {
      "epoch": 0.2860548271752086,
      "grad_norm": 0.21088896691799164,
      "learning_rate": 0.00018140276301806588,
      "loss": 0.2387,
      "step": 360
    },
    {
      "epoch": 0.28684942391736196,
      "grad_norm": 0.21169604361057281,
      "learning_rate": 0.00018134962805526036,
      "loss": 0.2579,
      "step": 361
    },
    {
      "epoch": 0.2876440206595153,
      "grad_norm": 0.1782296746969223,
      "learning_rate": 0.00018129649309245484,
      "loss": 0.2114,
      "step": 362
    },
    {
      "epoch": 0.28843861740166865,
      "grad_norm": 0.1722038835287094,
      "learning_rate": 0.00018124335812964932,
      "loss": 0.2269,
      "step": 363
    },
    {
      "epoch": 0.289233214143822,
      "grad_norm": 0.1924397051334381,
      "learning_rate": 0.0001811902231668438,
      "loss": 0.2185,
      "step": 364
    },
    {
      "epoch": 0.29002781088597535,
      "grad_norm": 0.16414694488048553,
      "learning_rate": 0.00018113708820403828,
      "loss": 0.2058,
      "step": 365
    },
    {
      "epoch": 0.2908224076281287,
      "grad_norm": 0.15329936146736145,
      "learning_rate": 0.00018108395324123273,
      "loss": 0.1657,
      "step": 366
    },
    {
      "epoch": 0.2916170043702821,
      "grad_norm": 0.1886078417301178,
      "learning_rate": 0.00018103081827842721,
      "loss": 0.1923,
      "step": 367
    },
    {
      "epoch": 0.2924116011124354,
      "grad_norm": 0.15114136040210724,
      "learning_rate": 0.0001809776833156217,
      "loss": 0.1712,
      "step": 368
    },
    {
      "epoch": 0.2932061978545888,
      "grad_norm": 0.16258707642555237,
      "learning_rate": 0.00018092454835281615,
      "loss": 0.1614,
      "step": 369
    },
    {
      "epoch": 0.29400079459674217,
      "grad_norm": 0.174740269780159,
      "learning_rate": 0.00018087141339001063,
      "loss": 0.1563,
      "step": 370
    },
    {
      "epoch": 0.2947953913388955,
      "grad_norm": 0.16331592202186584,
      "learning_rate": 0.0001808182784272051,
      "loss": 0.1242,
      "step": 371
    },
    {
      "epoch": 0.29558998808104886,
      "grad_norm": 0.14589998126029968,
      "learning_rate": 0.0001807651434643996,
      "loss": 0.1481,
      "step": 372
    },
    {
      "epoch": 0.29638458482320224,
      "grad_norm": 0.17812074720859528,
      "learning_rate": 0.00018071200850159407,
      "loss": 0.1262,
      "step": 373
    },
    {
      "epoch": 0.29717918156535555,
      "grad_norm": 0.16151568293571472,
      "learning_rate": 0.00018065887353878855,
      "loss": 0.1442,
      "step": 374
    },
    {
      "epoch": 0.29797377830750893,
      "grad_norm": 0.19634833931922913,
      "learning_rate": 0.000180605738575983,
      "loss": 0.1232,
      "step": 375
    },
    {
      "epoch": 0.2987683750496623,
      "grad_norm": 0.1378592848777771,
      "learning_rate": 0.00018055260361317748,
      "loss": 0.1157,
      "step": 376
    },
    {
      "epoch": 0.2995629717918157,
      "grad_norm": 0.1596001535654068,
      "learning_rate": 0.00018049946865037196,
      "loss": 0.1253,
      "step": 377
    },
    {
      "epoch": 0.300357568533969,
      "grad_norm": 0.18994049727916718,
      "learning_rate": 0.00018044633368756642,
      "loss": 0.1413,
      "step": 378
    },
    {
      "epoch": 0.3011521652761224,
      "grad_norm": 0.22402173280715942,
      "learning_rate": 0.0001803931987247609,
      "loss": 0.1374,
      "step": 379
    },
    {
      "epoch": 0.30194676201827575,
      "grad_norm": 0.15065738558769226,
      "learning_rate": 0.00018034006376195538,
      "loss": 0.1121,
      "step": 380
    },
    {
      "epoch": 0.30274135876042907,
      "grad_norm": 0.16096651554107666,
      "learning_rate": 0.00018028692879914986,
      "loss": 0.1007,
      "step": 381
    },
    {
      "epoch": 0.30353595550258244,
      "grad_norm": 0.18562498688697815,
      "learning_rate": 0.00018023379383634434,
      "loss": 0.1261,
      "step": 382
    },
    {
      "epoch": 0.3043305522447358,
      "grad_norm": 0.1083584576845169,
      "learning_rate": 0.0001801806588735388,
      "loss": 0.0867,
      "step": 383
    },
    {
      "epoch": 0.30512514898688914,
      "grad_norm": 0.13345786929130554,
      "learning_rate": 0.00018012752391073327,
      "loss": 0.0986,
      "step": 384
    },
    {
      "epoch": 0.3059197457290425,
      "grad_norm": 0.11899159103631973,
      "learning_rate": 0.00018007438894792775,
      "loss": 0.0867,
      "step": 385
    },
    {
      "epoch": 0.3067143424711959,
      "grad_norm": 0.11915700882673264,
      "learning_rate": 0.00018002125398512223,
      "loss": 0.1014,
      "step": 386
    },
    {
      "epoch": 0.3075089392133492,
      "grad_norm": 0.1470143049955368,
      "learning_rate": 0.00017996811902231668,
      "loss": 0.0981,
      "step": 387
    },
    {
      "epoch": 0.3083035359555026,
      "grad_norm": 0.13227562606334686,
      "learning_rate": 0.00017991498405951116,
      "loss": 0.0923,
      "step": 388
    },
    {
      "epoch": 0.30909813269765596,
      "grad_norm": 0.11816904693841934,
      "learning_rate": 0.00017986184909670564,
      "loss": 0.0731,
      "step": 389
    },
    {
      "epoch": 0.3098927294398093,
      "grad_norm": 0.1253548562526703,
      "learning_rate": 0.00017980871413390012,
      "loss": 0.0913,
      "step": 390
    },
    {
      "epoch": 0.31068732618196265,
      "grad_norm": 0.10315605252981186,
      "learning_rate": 0.00017975557917109458,
      "loss": 0.0752,
      "step": 391
    },
    {
      "epoch": 0.31148192292411603,
      "grad_norm": 0.0959479808807373,
      "learning_rate": 0.00017970244420828906,
      "loss": 0.0664,
      "step": 392
    },
    {
      "epoch": 0.31227651966626935,
      "grad_norm": 0.11472196877002716,
      "learning_rate": 0.00017964930924548354,
      "loss": 0.0755,
      "step": 393
    },
    {
      "epoch": 0.3130711164084227,
      "grad_norm": 0.12279083579778671,
      "learning_rate": 0.00017959617428267802,
      "loss": 0.0802,
      "step": 394
    },
    {
      "epoch": 0.3138657131505761,
      "grad_norm": 0.10654245316982269,
      "learning_rate": 0.0001795430393198725,
      "loss": 0.0575,
      "step": 395
    },
    {
      "epoch": 0.3146603098927294,
      "grad_norm": 0.102593332529068,
      "learning_rate": 0.00017948990435706695,
      "loss": 0.0597,
      "step": 396
    },
    {
      "epoch": 0.3154549066348828,
      "grad_norm": 0.09773418307304382,
      "learning_rate": 0.00017943676939426143,
      "loss": 0.0527,
      "step": 397
    },
    {
      "epoch": 0.31624950337703617,
      "grad_norm": 0.13269393146038055,
      "learning_rate": 0.0001793836344314559,
      "loss": 0.0457,
      "step": 398
    },
    {
      "epoch": 0.3170441001191895,
      "grad_norm": 0.10617385804653168,
      "learning_rate": 0.00017933049946865036,
      "loss": 0.0359,
      "step": 399
    },
    {
      "epoch": 0.31783869686134286,
      "grad_norm": 0.10344933718442917,
      "learning_rate": 0.00017927736450584484,
      "loss": 0.038,
      "step": 400
    },
    {
      "epoch": 0.31863329360349624,
      "grad_norm": 0.815968930721283,
      "learning_rate": 0.00017922422954303932,
      "loss": 0.5297,
      "step": 401
    },
    {
      "epoch": 0.31942789034564956,
      "grad_norm": 0.5795943140983582,
      "learning_rate": 0.0001791710945802338,
      "loss": 0.512,
      "step": 402
    },
    {
      "epoch": 0.32022248708780293,
      "grad_norm": 0.5341584086418152,
      "learning_rate": 0.00017911795961742828,
      "loss": 0.4963,
      "step": 403
    },
    {
      "epoch": 0.3210170838299563,
      "grad_norm": 0.33977898955345154,
      "learning_rate": 0.00017906482465462277,
      "loss": 0.3811,
      "step": 404
    },
    {
      "epoch": 0.3218116805721097,
      "grad_norm": 0.2823364734649658,
      "learning_rate": 0.00017901168969181722,
      "loss": 0.31,
      "step": 405
    },
    {
      "epoch": 0.322606277314263,
      "grad_norm": 0.21516966819763184,
      "learning_rate": 0.0001789585547290117,
      "loss": 0.3021,
      "step": 406
    },
    {
      "epoch": 0.3234008740564164,
      "grad_norm": 0.2240457832813263,
      "learning_rate": 0.00017890541976620615,
      "loss": 0.2808,
      "step": 407
    },
    {
      "epoch": 0.32419547079856975,
      "grad_norm": 0.19748429954051971,
      "learning_rate": 0.00017885228480340063,
      "loss": 0.2692,
      "step": 408
    },
    {
      "epoch": 0.32499006754072307,
      "grad_norm": 0.21727398037910461,
      "learning_rate": 0.0001787991498405951,
      "loss": 0.2696,
      "step": 409
    },
    {
      "epoch": 0.32578466428287645,
      "grad_norm": 0.29306739568710327,
      "learning_rate": 0.0001787460148777896,
      "loss": 0.2531,
      "step": 410
    },
    {
      "epoch": 0.3265792610250298,
      "grad_norm": 0.21117408573627472,
      "learning_rate": 0.00017869287991498407,
      "loss": 0.2479,
      "step": 411
    },
    {
      "epoch": 0.32737385776718314,
      "grad_norm": 0.19446948170661926,
      "learning_rate": 0.00017863974495217855,
      "loss": 0.2156,
      "step": 412
    },
    {
      "epoch": 0.3281684545093365,
      "grad_norm": 0.17669881880283356,
      "learning_rate": 0.00017858660998937303,
      "loss": 0.1922,
      "step": 413
    },
    {
      "epoch": 0.3289630512514899,
      "grad_norm": 0.18860073387622833,
      "learning_rate": 0.00017853347502656749,
      "loss": 0.2055,
      "step": 414
    },
    {
      "epoch": 0.3297576479936432,
      "grad_norm": 0.20367181301116943,
      "learning_rate": 0.00017848034006376197,
      "loss": 0.217,
      "step": 415
    },
    {
      "epoch": 0.3305522447357966,
      "grad_norm": 0.20219364762306213,
      "learning_rate": 0.00017842720510095642,
      "loss": 0.22,
      "step": 416
    },
    {
      "epoch": 0.33134684147794996,
      "grad_norm": 0.20035719871520996,
      "learning_rate": 0.0001783740701381509,
      "loss": 0.1911,
      "step": 417
    },
    {
      "epoch": 0.3321414382201033,
      "grad_norm": 0.22803983092308044,
      "learning_rate": 0.00017832093517534538,
      "loss": 0.2466,
      "step": 418
    },
    {
      "epoch": 0.33293603496225666,
      "grad_norm": 0.18323004245758057,
      "learning_rate": 0.00017826780021253986,
      "loss": 0.1696,
      "step": 419
    },
    {
      "epoch": 0.33373063170441003,
      "grad_norm": 0.19410443305969238,
      "learning_rate": 0.00017821466524973434,
      "loss": 0.1858,
      "step": 420
    },
    {
      "epoch": 0.33452522844656335,
      "grad_norm": 0.18000756204128265,
      "learning_rate": 0.00017816153028692882,
      "loss": 0.1522,
      "step": 421
    },
    {
      "epoch": 0.3353198251887167,
      "grad_norm": 0.19147346913814545,
      "learning_rate": 0.0001781083953241233,
      "loss": 0.1403,
      "step": 422
    },
    {
      "epoch": 0.3361144219308701,
      "grad_norm": 0.16993501782417297,
      "learning_rate": 0.00017805526036131775,
      "loss": 0.1501,
      "step": 423
    },
    {
      "epoch": 0.3369090186730234,
      "grad_norm": 0.32250386476516724,
      "learning_rate": 0.00017800212539851223,
      "loss": 0.152,
      "step": 424
    },
    {
      "epoch": 0.3377036154151768,
      "grad_norm": 0.1447669267654419,
      "learning_rate": 0.0001779489904357067,
      "loss": 0.1295,
      "step": 425
    },
    {
      "epoch": 0.33849821215733017,
      "grad_norm": 0.15887509286403656,
      "learning_rate": 0.00017789585547290117,
      "loss": 0.126,
      "step": 426
    },
    {
      "epoch": 0.3392928088994835,
      "grad_norm": 0.20491543412208557,
      "learning_rate": 0.00017784272051009565,
      "loss": 0.1499,
      "step": 427
    },
    {
      "epoch": 0.34008740564163686,
      "grad_norm": 0.14907881617546082,
      "learning_rate": 0.00017778958554729013,
      "loss": 0.1324,
      "step": 428
    },
    {
      "epoch": 0.34088200238379024,
      "grad_norm": 0.23097571730613708,
      "learning_rate": 0.0001777364505844846,
      "loss": 0.1274,
      "step": 429
    },
    {
      "epoch": 0.34167659912594356,
      "grad_norm": 0.14563998579978943,
      "learning_rate": 0.0001776833156216791,
      "loss": 0.1071,
      "step": 430
    },
    {
      "epoch": 0.34247119586809693,
      "grad_norm": 0.14084996283054352,
      "learning_rate": 0.00017763018065887357,
      "loss": 0.1164,
      "step": 431
    },
    {
      "epoch": 0.3432657926102503,
      "grad_norm": 0.15502925217151642,
      "learning_rate": 0.00017757704569606802,
      "loss": 0.116,
      "step": 432
    },
    {
      "epoch": 0.34406038935240363,
      "grad_norm": 0.15625321865081787,
      "learning_rate": 0.0001775239107332625,
      "loss": 0.0876,
      "step": 433
    },
    {
      "epoch": 0.344854986094557,
      "grad_norm": 0.12899446487426758,
      "learning_rate": 0.00017747077577045695,
      "loss": 0.1049,
      "step": 434
    },
    {
      "epoch": 0.3456495828367104,
      "grad_norm": 0.1274585872888565,
      "learning_rate": 0.00017741764080765143,
      "loss": 0.0988,
      "step": 435
    },
    {
      "epoch": 0.34644417957886375,
      "grad_norm": 0.1087879091501236,
      "learning_rate": 0.00017736450584484591,
      "loss": 0.0789,
      "step": 436
    },
    {
      "epoch": 0.3472387763210171,
      "grad_norm": 0.13481460511684418,
      "learning_rate": 0.0001773113708820404,
      "loss": 0.0827,
      "step": 437
    },
    {
      "epoch": 0.34803337306317045,
      "grad_norm": 0.12952278554439545,
      "learning_rate": 0.00017725823591923487,
      "loss": 0.1103,
      "step": 438
    },
    {
      "epoch": 0.3488279698053238,
      "grad_norm": 0.13442721962928772,
      "learning_rate": 0.00017720510095642935,
      "loss": 0.0872,
      "step": 439
    },
    {
      "epoch": 0.34962256654747714,
      "grad_norm": 0.13618211448192596,
      "learning_rate": 0.0001771519659936238,
      "loss": 0.0737,
      "step": 440
    },
    {
      "epoch": 0.3504171632896305,
      "grad_norm": 0.1181938424706459,
      "learning_rate": 0.0001770988310308183,
      "loss": 0.0886,
      "step": 441
    },
    {
      "epoch": 0.3512117600317839,
      "grad_norm": 0.12918734550476074,
      "learning_rate": 0.00017704569606801277,
      "loss": 0.0795,
      "step": 442
    },
    {
      "epoch": 0.3520063567739372,
      "grad_norm": 0.13566656410694122,
      "learning_rate": 0.00017699256110520722,
      "loss": 0.0614,
      "step": 443
    },
    {
      "epoch": 0.3528009535160906,
      "grad_norm": 0.10747001320123672,
      "learning_rate": 0.0001769394261424017,
      "loss": 0.06,
      "step": 444
    },
    {
      "epoch": 0.35359555025824396,
      "grad_norm": 0.10960114747285843,
      "learning_rate": 0.00017688629117959618,
      "loss": 0.0484,
      "step": 445
    },
    {
      "epoch": 0.3543901470003973,
      "grad_norm": 0.1085306704044342,
      "learning_rate": 0.00017683315621679066,
      "loss": 0.0652,
      "step": 446
    },
    {
      "epoch": 0.35518474374255066,
      "grad_norm": 0.09818721562623978,
      "learning_rate": 0.00017678002125398514,
      "loss": 0.0434,
      "step": 447
    },
    {
      "epoch": 0.35597934048470403,
      "grad_norm": 0.11145469546318054,
      "learning_rate": 0.0001767268862911796,
      "loss": 0.0358,
      "step": 448
    },
    {
      "epoch": 0.35677393722685735,
      "grad_norm": 0.09422564506530762,
      "learning_rate": 0.00017667375132837408,
      "loss": 0.0398,
      "step": 449
    },
    {
      "epoch": 0.3575685339690107,
      "grad_norm": 0.09531432390213013,
      "learning_rate": 0.00017662061636556856,
      "loss": 0.0331,
      "step": 450
    },
    {
      "epoch": 0.3583631307111641,
      "grad_norm": 1.0477309226989746,
      "learning_rate": 0.00017656748140276304,
      "loss": 0.5933,
      "step": 451
    },
    {
      "epoch": 0.3591577274533174,
      "grad_norm": 0.6669835448265076,
      "learning_rate": 0.0001765143464399575,
      "loss": 0.5073,
      "step": 452
    },
    {
      "epoch": 0.3599523241954708,
      "grad_norm": 0.5718496441841125,
      "learning_rate": 0.00017646121147715197,
      "loss": 0.4632,
      "step": 453
    },
    {
      "epoch": 0.36074692093762417,
      "grad_norm": 0.38763612508773804,
      "learning_rate": 0.00017640807651434645,
      "loss": 0.4034,
      "step": 454
    },
    {
      "epoch": 0.3615415176797775,
      "grad_norm": 0.3436497747898102,
      "learning_rate": 0.00017635494155154093,
      "loss": 0.3826,
      "step": 455
    },
    {
      "epoch": 0.36233611442193087,
      "grad_norm": 0.2841578423976898,
      "learning_rate": 0.00017630180658873538,
      "loss": 0.2877,
      "step": 456
    },
    {
      "epoch": 0.36313071116408424,
      "grad_norm": 0.19007021188735962,
      "learning_rate": 0.00017624867162592986,
      "loss": 0.2408,
      "step": 457
    },
    {
      "epoch": 0.36392530790623756,
      "grad_norm": 0.20825901627540588,
      "learning_rate": 0.00017619553666312434,
      "loss": 0.26,
      "step": 458
    },
    {
      "epoch": 0.36471990464839094,
      "grad_norm": 0.21573509275913239,
      "learning_rate": 0.00017614240170031882,
      "loss": 0.2335,
      "step": 459
    },
    {
      "epoch": 0.3655145013905443,
      "grad_norm": 0.30814629793167114,
      "learning_rate": 0.0001760892667375133,
      "loss": 0.2577,
      "step": 460
    },
    {
      "epoch": 0.36630909813269763,
      "grad_norm": 0.21933895349502563,
      "learning_rate": 0.00017603613177470776,
      "loss": 0.2347,
      "step": 461
    },
    {
      "epoch": 0.367103694874851,
      "grad_norm": 0.19775919616222382,
      "learning_rate": 0.00017598299681190224,
      "loss": 0.2031,
      "step": 462
    },
    {
      "epoch": 0.3678982916170044,
      "grad_norm": 0.2925421893596649,
      "learning_rate": 0.00017592986184909672,
      "loss": 0.2292,
      "step": 463
    },
    {
      "epoch": 0.3686928883591577,
      "grad_norm": 0.22522221505641937,
      "learning_rate": 0.0001758767268862912,
      "loss": 0.2199,
      "step": 464
    },
    {
      "epoch": 0.3694874851013111,
      "grad_norm": 0.21991795301437378,
      "learning_rate": 0.00017582359192348565,
      "loss": 0.1943,
      "step": 465
    },
    {
      "epoch": 0.37028208184346445,
      "grad_norm": 0.19432100653648376,
      "learning_rate": 0.00017577045696068013,
      "loss": 0.1902,
      "step": 466
    },
    {
      "epoch": 0.3710766785856178,
      "grad_norm": 0.19416193664073944,
      "learning_rate": 0.0001757173219978746,
      "loss": 0.1873,
      "step": 467
    },
    {
      "epoch": 0.37187127532777114,
      "grad_norm": 0.22416891157627106,
      "learning_rate": 0.0001756641870350691,
      "loss": 0.1635,
      "step": 468
    },
    {
      "epoch": 0.3726658720699245,
      "grad_norm": 0.15995021164417267,
      "learning_rate": 0.00017561105207226357,
      "loss": 0.1601,
      "step": 469
    },
    {
      "epoch": 0.3734604688120779,
      "grad_norm": 0.1844729781150818,
      "learning_rate": 0.00017555791710945802,
      "loss": 0.1675,
      "step": 470
    },
    {
      "epoch": 0.3742550655542312,
      "grad_norm": 0.17936724424362183,
      "learning_rate": 0.0001755047821466525,
      "loss": 0.161,
      "step": 471
    },
    {
      "epoch": 0.3750496622963846,
      "grad_norm": 0.2097606360912323,
      "learning_rate": 0.00017545164718384698,
      "loss": 0.1472,
      "step": 472
    },
    {
      "epoch": 0.37584425903853796,
      "grad_norm": 0.18350501358509064,
      "learning_rate": 0.00017539851222104144,
      "loss": 0.1527,
      "step": 473
    },
    {
      "epoch": 0.3766388557806913,
      "grad_norm": 0.16713862121105194,
      "learning_rate": 0.00017534537725823592,
      "loss": 0.1599,
      "step": 474
    },
    {
      "epoch": 0.37743345252284466,
      "grad_norm": 0.19787098467350006,
      "learning_rate": 0.0001752922422954304,
      "loss": 0.1341,
      "step": 475
    },
    {
      "epoch": 0.37822804926499803,
      "grad_norm": 0.23306138813495636,
      "learning_rate": 0.00017523910733262488,
      "loss": 0.1548,
      "step": 476
    },
    {
      "epoch": 0.37902264600715135,
      "grad_norm": 0.172861248254776,
      "learning_rate": 0.00017518597236981936,
      "loss": 0.1209,
      "step": 477
    },
    {
      "epoch": 0.37981724274930473,
      "grad_norm": 0.18460042774677277,
      "learning_rate": 0.00017513283740701384,
      "loss": 0.1088,
      "step": 478
    },
    {
      "epoch": 0.3806118394914581,
      "grad_norm": 0.14146049320697784,
      "learning_rate": 0.0001750797024442083,
      "loss": 0.1257,
      "step": 479
    },
    {
      "epoch": 0.3814064362336114,
      "grad_norm": 0.16980722546577454,
      "learning_rate": 0.00017502656748140277,
      "loss": 0.1331,
      "step": 480
    },
    {
      "epoch": 0.3822010329757648,
      "grad_norm": 0.14929276704788208,
      "learning_rate": 0.00017497343251859722,
      "loss": 0.1172,
      "step": 481
    },
    {
      "epoch": 0.3829956297179182,
      "grad_norm": 0.14789730310440063,
      "learning_rate": 0.0001749202975557917,
      "loss": 0.1067,
      "step": 482
    },
    {
      "epoch": 0.3837902264600715,
      "grad_norm": 2.624328136444092,
      "learning_rate": 0.00017486716259298619,
      "loss": 0.0885,
      "step": 483
    },
    {
      "epoch": 0.38458482320222487,
      "grad_norm": 16.18034553527832,
      "learning_rate": 0.00017481402763018067,
      "loss": 0.5988,
      "step": 484
    },
    {
      "epoch": 0.38537941994437824,
      "grad_norm": 1.0832313299179077,
      "learning_rate": 0.00017476089266737515,
      "loss": 0.1451,
      "step": 485
    },
    {
      "epoch": 0.38617401668653156,
      "grad_norm": 0.3220563232898712,
      "learning_rate": 0.00017470775770456963,
      "loss": 0.1114,
      "step": 486
    },
    {
      "epoch": 0.38696861342868494,
      "grad_norm": 0.11379662156105042,
      "learning_rate": 0.0001746546227417641,
      "loss": 0.095,
      "step": 487
    },
    {
      "epoch": 0.3877632101708383,
      "grad_norm": 0.1331019550561905,
      "learning_rate": 0.00017460148777895856,
      "loss": 0.0681,
      "step": 488
    },
    {
      "epoch": 0.38855780691299163,
      "grad_norm": 0.12626010179519653,
      "learning_rate": 0.00017454835281615304,
      "loss": 0.083,
      "step": 489
    },
    {
      "epoch": 0.389352403655145,
      "grad_norm": 0.13573628664016724,
      "learning_rate": 0.0001744952178533475,
      "loss": 0.0999,
      "step": 490
    },
    {
      "epoch": 0.3901470003972984,
      "grad_norm": 0.17958004772663116,
      "learning_rate": 0.00017444208289054197,
      "loss": 0.0647,
      "step": 491
    },
    {
      "epoch": 0.3909415971394517,
      "grad_norm": 0.12049554288387299,
      "learning_rate": 0.00017438894792773645,
      "loss": 0.0723,
      "step": 492
    },
    {
      "epoch": 0.3917361938816051,
      "grad_norm": 0.1240827888250351,
      "learning_rate": 0.00017433581296493093,
      "loss": 0.0601,
      "step": 493
    },
    {
      "epoch": 0.39253079062375845,
      "grad_norm": 0.10330583155155182,
      "learning_rate": 0.0001742826780021254,
      "loss": 0.0454,
      "step": 494
    },
    {
      "epoch": 0.3933253873659118,
      "grad_norm": 0.10036259144544601,
      "learning_rate": 0.0001742295430393199,
      "loss": 0.0665,
      "step": 495
    },
    {
      "epoch": 0.39411998410806515,
      "grad_norm": 0.11036942899227142,
      "learning_rate": 0.00017417640807651437,
      "loss": 0.0612,
      "step": 496
    },
    {
      "epoch": 0.3949145808502185,
      "grad_norm": 0.13097025454044342,
      "learning_rate": 0.00017412327311370883,
      "loss": 0.0447,
      "step": 497
    },
    {
      "epoch": 0.3957091775923719,
      "grad_norm": 0.1235135868191719,
      "learning_rate": 0.0001740701381509033,
      "loss": 0.0434,
      "step": 498
    },
    {
      "epoch": 0.3965037743345252,
      "grad_norm": 0.10748361051082611,
      "learning_rate": 0.00017401700318809776,
      "loss": 0.042,
      "step": 499
    },
    {
      "epoch": 0.3972983710766786,
      "grad_norm": 0.08436639606952667,
      "learning_rate": 0.00017396386822529224,
      "loss": 0.0389,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 3774,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.095743589572608e+16,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
