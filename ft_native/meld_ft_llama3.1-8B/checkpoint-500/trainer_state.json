{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.6,
  "eval_steps": 188,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0032,
      "grad_norm": 0.2473185956478119,
      "learning_rate": 2e-05,
      "loss": 0.3716,
      "step": 1
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.19704203307628632,
      "learning_rate": 4e-05,
      "loss": 0.3007,
      "step": 2
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.1681884378194809,
      "learning_rate": 6e-05,
      "loss": 0.2768,
      "step": 3
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.15028852224349976,
      "learning_rate": 8e-05,
      "loss": 0.2785,
      "step": 4
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.15018749237060547,
      "learning_rate": 0.0001,
      "loss": 0.2633,
      "step": 5
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.13611064851284027,
      "learning_rate": 0.00012,
      "loss": 0.2394,
      "step": 6
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.12946367263793945,
      "learning_rate": 0.00014,
      "loss": 0.2104,
      "step": 7
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.12480983138084412,
      "learning_rate": 0.00016,
      "loss": 0.2104,
      "step": 8
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.11112841218709946,
      "learning_rate": 0.00018,
      "loss": 0.1959,
      "step": 9
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.12642277777194977,
      "learning_rate": 0.0002,
      "loss": 0.2286,
      "step": 10
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.09982245415449142,
      "learning_rate": 0.00019978401727861774,
      "loss": 0.1681,
      "step": 11
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.11517184227705002,
      "learning_rate": 0.00019956803455723542,
      "loss": 0.1859,
      "step": 12
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.11242034286260605,
      "learning_rate": 0.00019935205183585315,
      "loss": 0.1924,
      "step": 13
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.10396683216094971,
      "learning_rate": 0.00019913606911447086,
      "loss": 0.1672,
      "step": 14
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.10824700444936752,
      "learning_rate": 0.00019892008639308856,
      "loss": 0.1782,
      "step": 15
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.10267966240644455,
      "learning_rate": 0.00019870410367170627,
      "loss": 0.1754,
      "step": 16
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.10092981904745102,
      "learning_rate": 0.00019848812095032398,
      "loss": 0.1474,
      "step": 17
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.09654837846755981,
      "learning_rate": 0.00019827213822894168,
      "loss": 0.1776,
      "step": 18
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.09118690341711044,
      "learning_rate": 0.00019805615550755941,
      "loss": 0.1518,
      "step": 19
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.1038360595703125,
      "learning_rate": 0.00019784017278617712,
      "loss": 0.1451,
      "step": 20
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.09426870942115784,
      "learning_rate": 0.00019762419006479483,
      "loss": 0.1423,
      "step": 21
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.11023803800344467,
      "learning_rate": 0.00019740820734341253,
      "loss": 0.1413,
      "step": 22
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.10804465413093567,
      "learning_rate": 0.00019719222462203024,
      "loss": 0.1305,
      "step": 23
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.09621129930019379,
      "learning_rate": 0.00019697624190064797,
      "loss": 0.1369,
      "step": 24
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.11130337417125702,
      "learning_rate": 0.00019676025917926565,
      "loss": 0.1437,
      "step": 25
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.07736033201217651,
      "learning_rate": 0.00019654427645788338,
      "loss": 0.1207,
      "step": 26
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.10844156891107559,
      "learning_rate": 0.0001963282937365011,
      "loss": 0.1199,
      "step": 27
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.09079986065626144,
      "learning_rate": 0.0001961123110151188,
      "loss": 0.1192,
      "step": 28
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.06796689331531525,
      "learning_rate": 0.00019589632829373652,
      "loss": 0.102,
      "step": 29
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.08792231231927872,
      "learning_rate": 0.00019568034557235423,
      "loss": 0.1109,
      "step": 30
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.08039865642786026,
      "learning_rate": 0.00019546436285097194,
      "loss": 0.0948,
      "step": 31
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.08922789990901947,
      "learning_rate": 0.00019524838012958964,
      "loss": 0.0934,
      "step": 32
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.08575041592121124,
      "learning_rate": 0.00019503239740820735,
      "loss": 0.0972,
      "step": 33
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.061780139803886414,
      "learning_rate": 0.00019481641468682505,
      "loss": 0.0939,
      "step": 34
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.07323917746543884,
      "learning_rate": 0.00019460043196544279,
      "loss": 0.0837,
      "step": 35
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.09208838641643524,
      "learning_rate": 0.00019438444924406046,
      "loss": 0.0859,
      "step": 36
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.09746457636356354,
      "learning_rate": 0.0001941684665226782,
      "loss": 0.0861,
      "step": 37
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.08083730190992355,
      "learning_rate": 0.0001939524838012959,
      "loss": 0.0774,
      "step": 38
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.07755818217992783,
      "learning_rate": 0.0001937365010799136,
      "loss": 0.0784,
      "step": 39
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.07258199155330658,
      "learning_rate": 0.00019352051835853134,
      "loss": 0.0693,
      "step": 40
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.06922148913145065,
      "learning_rate": 0.00019330453563714902,
      "loss": 0.0719,
      "step": 41
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.08534162491559982,
      "learning_rate": 0.00019308855291576675,
      "loss": 0.0614,
      "step": 42
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.070091612637043,
      "learning_rate": 0.00019287257019438446,
      "loss": 0.0584,
      "step": 43
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.08243141323328018,
      "learning_rate": 0.00019265658747300216,
      "loss": 0.0528,
      "step": 44
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.05600489303469658,
      "learning_rate": 0.0001924406047516199,
      "loss": 0.0526,
      "step": 45
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.06003555282950401,
      "learning_rate": 0.0001922246220302376,
      "loss": 0.0438,
      "step": 46
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.07310241460800171,
      "learning_rate": 0.0001920086393088553,
      "loss": 0.0386,
      "step": 47
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.10228697955608368,
      "learning_rate": 0.000191792656587473,
      "loss": 0.035,
      "step": 48
    },
    {
      "epoch": 0.1568,
      "grad_norm": 1.4498757123947144,
      "learning_rate": 0.00019157667386609072,
      "loss": 0.0353,
      "step": 49
    },
    {
      "epoch": 0.16,
      "grad_norm": 5.631339073181152,
      "learning_rate": 0.00019136069114470842,
      "loss": 0.2535,
      "step": 50
    },
    {
      "epoch": 0.1632,
      "grad_norm": 3.974756956100464,
      "learning_rate": 0.00019114470842332616,
      "loss": 0.6719,
      "step": 51
    },
    {
      "epoch": 0.1664,
      "grad_norm": 1.5104315280914307,
      "learning_rate": 0.00019092872570194384,
      "loss": 0.4445,
      "step": 52
    },
    {
      "epoch": 0.1696,
      "grad_norm": 3.19195818901062,
      "learning_rate": 0.00019071274298056157,
      "loss": 0.3947,
      "step": 53
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.6743426322937012,
      "learning_rate": 0.00019049676025917927,
      "loss": 0.3238,
      "step": 54
    },
    {
      "epoch": 0.176,
      "grad_norm": 2.4868826866149902,
      "learning_rate": 0.00019028077753779698,
      "loss": 0.3254,
      "step": 55
    },
    {
      "epoch": 0.1792,
      "grad_norm": 1.392816185951233,
      "learning_rate": 0.0001900647948164147,
      "loss": 0.2989,
      "step": 56
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.22468656301498413,
      "learning_rate": 0.0001898488120950324,
      "loss": 0.283,
      "step": 57
    },
    {
      "epoch": 0.1856,
      "grad_norm": 1.8186620473861694,
      "learning_rate": 0.00018963282937365012,
      "loss": 0.307,
      "step": 58
    },
    {
      "epoch": 0.1888,
      "grad_norm": 2.909487724304199,
      "learning_rate": 0.00018941684665226783,
      "loss": 0.279,
      "step": 59
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.9266829490661621,
      "learning_rate": 0.00018920086393088553,
      "loss": 0.2515,
      "step": 60
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.1794801950454712,
      "learning_rate": 0.00018898488120950324,
      "loss": 0.2286,
      "step": 61
    },
    {
      "epoch": 0.1984,
      "grad_norm": 1.20906400680542,
      "learning_rate": 0.00018876889848812097,
      "loss": 0.2258,
      "step": 62
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.12893147766590118,
      "learning_rate": 0.00018855291576673868,
      "loss": 0.2198,
      "step": 63
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.1524917334318161,
      "learning_rate": 0.00018833693304535638,
      "loss": 0.201,
      "step": 64
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.5775185227394104,
      "learning_rate": 0.0001881209503239741,
      "loss": 0.2012,
      "step": 65
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.15016920864582062,
      "learning_rate": 0.0001879049676025918,
      "loss": 0.1906,
      "step": 66
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.11539745330810547,
      "learning_rate": 0.00018768898488120953,
      "loss": 0.1837,
      "step": 67
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.10456038266420364,
      "learning_rate": 0.0001874730021598272,
      "loss": 0.1868,
      "step": 68
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.09295433014631271,
      "learning_rate": 0.00018725701943844494,
      "loss": 0.1645,
      "step": 69
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.09027903527021408,
      "learning_rate": 0.00018704103671706265,
      "loss": 0.1605,
      "step": 70
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.09788890928030014,
      "learning_rate": 0.00018682505399568035,
      "loss": 0.163,
      "step": 71
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.095504030585289,
      "learning_rate": 0.00018660907127429808,
      "loss": 0.1469,
      "step": 72
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.10050319135189056,
      "learning_rate": 0.00018639308855291576,
      "loss": 0.1511,
      "step": 73
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.09039634466171265,
      "learning_rate": 0.0001861771058315335,
      "loss": 0.1298,
      "step": 74
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.10619691759347916,
      "learning_rate": 0.0001859611231101512,
      "loss": 0.1396,
      "step": 75
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.08168186992406845,
      "learning_rate": 0.0001857451403887689,
      "loss": 0.1289,
      "step": 76
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.0853278636932373,
      "learning_rate": 0.0001855291576673866,
      "loss": 0.121,
      "step": 77
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.08455134928226471,
      "learning_rate": 0.00018531317494600432,
      "loss": 0.1272,
      "step": 78
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.09596329182386398,
      "learning_rate": 0.00018509719222462202,
      "loss": 0.1106,
      "step": 79
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.07441899925470352,
      "learning_rate": 0.00018488120950323976,
      "loss": 0.1017,
      "step": 80
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.07067946344614029,
      "learning_rate": 0.00018466522678185746,
      "loss": 0.1099,
      "step": 81
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.07373460382223129,
      "learning_rate": 0.00018444924406047517,
      "loss": 0.1051,
      "step": 82
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.08122047036886215,
      "learning_rate": 0.0001842332613390929,
      "loss": 0.1055,
      "step": 83
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.06536902487277985,
      "learning_rate": 0.00018401727861771058,
      "loss": 0.0974,
      "step": 84
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.06507558375597,
      "learning_rate": 0.0001838012958963283,
      "loss": 0.1017,
      "step": 85
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.06138524413108826,
      "learning_rate": 0.00018358531317494602,
      "loss": 0.0799,
      "step": 86
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.0597354881465435,
      "learning_rate": 0.00018336933045356372,
      "loss": 0.0841,
      "step": 87
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.06616238504648209,
      "learning_rate": 0.00018315334773218143,
      "loss": 0.0799,
      "step": 88
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.07355864346027374,
      "learning_rate": 0.00018293736501079913,
      "loss": 0.0757,
      "step": 89
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.059434302151203156,
      "learning_rate": 0.00018272138228941687,
      "loss": 0.0801,
      "step": 90
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.05746084451675415,
      "learning_rate": 0.00018250539956803457,
      "loss": 0.065,
      "step": 91
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.06596305966377258,
      "learning_rate": 0.00018228941684665228,
      "loss": 0.0729,
      "step": 92
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.06921606510877609,
      "learning_rate": 0.00018207343412526998,
      "loss": 0.0639,
      "step": 93
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.06688746809959412,
      "learning_rate": 0.0001818574514038877,
      "loss": 0.0545,
      "step": 94
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.06817151606082916,
      "learning_rate": 0.0001816414686825054,
      "loss": 0.0505,
      "step": 95
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.060189444571733475,
      "learning_rate": 0.00018142548596112313,
      "loss": 0.0459,
      "step": 96
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.09709952026605606,
      "learning_rate": 0.0001812095032397408,
      "loss": 0.0409,
      "step": 97
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.09310758858919144,
      "learning_rate": 0.00018099352051835854,
      "loss": 0.0357,
      "step": 98
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.05933742597699165,
      "learning_rate": 0.00018077753779697627,
      "loss": 0.0327,
      "step": 99
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.06387529522180557,
      "learning_rate": 0.00018056155507559395,
      "loss": 0.0342,
      "step": 100
    },
    {
      "epoch": 0.3232,
      "grad_norm": 1.8403183221817017,
      "learning_rate": 0.00018034557235421168,
      "loss": 0.5741,
      "step": 101
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.8151398301124573,
      "learning_rate": 0.0001801295896328294,
      "loss": 0.4198,
      "step": 102
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.3821924030780792,
      "learning_rate": 0.0001799136069114471,
      "loss": 0.3347,
      "step": 103
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.18419210612773895,
      "learning_rate": 0.0001796976241900648,
      "loss": 0.2783,
      "step": 104
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.1795380711555481,
      "learning_rate": 0.0001794816414686825,
      "loss": 0.2823,
      "step": 105
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.2535008192062378,
      "learning_rate": 0.0001792656587473002,
      "loss": 0.2786,
      "step": 106
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.21778462827205658,
      "learning_rate": 0.00017904967602591794,
      "loss": 0.2487,
      "step": 107
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.19404543936252594,
      "learning_rate": 0.00017883369330453565,
      "loss": 0.251,
      "step": 108
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.167414128780365,
      "learning_rate": 0.00017861771058315335,
      "loss": 0.2482,
      "step": 109
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.12931066751480103,
      "learning_rate": 0.00017840172786177106,
      "loss": 0.2219,
      "step": 110
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.1537570059299469,
      "learning_rate": 0.00017818574514038877,
      "loss": 0.2158,
      "step": 111
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.15754641592502594,
      "learning_rate": 0.0001779697624190065,
      "loss": 0.2014,
      "step": 112
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.14788292348384857,
      "learning_rate": 0.00017775377969762418,
      "loss": 0.2147,
      "step": 113
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.15218223631381989,
      "learning_rate": 0.0001775377969762419,
      "loss": 0.186,
      "step": 114
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.13024581968784332,
      "learning_rate": 0.00017732181425485964,
      "loss": 0.1876,
      "step": 115
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.10386557877063751,
      "learning_rate": 0.00017710583153347732,
      "loss": 0.1745,
      "step": 116
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.09104938805103302,
      "learning_rate": 0.00017688984881209505,
      "loss": 0.1737,
      "step": 117
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.11098423600196838,
      "learning_rate": 0.00017667386609071276,
      "loss": 0.1704,
      "step": 118
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.1334501951932907,
      "learning_rate": 0.00017645788336933046,
      "loss": 0.1493,
      "step": 119
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.1114867702126503,
      "learning_rate": 0.00017624190064794817,
      "loss": 0.1448,
      "step": 120
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.11684813350439072,
      "learning_rate": 0.00017602591792656588,
      "loss": 0.1403,
      "step": 121
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.12127739936113358,
      "learning_rate": 0.00017580993520518358,
      "loss": 0.1396,
      "step": 122
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.09601961076259613,
      "learning_rate": 0.00017559395248380131,
      "loss": 0.1511,
      "step": 123
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.0876307561993599,
      "learning_rate": 0.00017537796976241902,
      "loss": 0.1253,
      "step": 124
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.07560454308986664,
      "learning_rate": 0.00017516198704103673,
      "loss": 0.1288,
      "step": 125
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.08914800733327866,
      "learning_rate": 0.00017494600431965443,
      "loss": 0.1262,
      "step": 126
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.07434625923633575,
      "learning_rate": 0.00017473002159827214,
      "loss": 0.1216,
      "step": 127
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.08822549134492874,
      "learning_rate": 0.00017451403887688987,
      "loss": 0.1177,
      "step": 128
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.07859709113836288,
      "learning_rate": 0.00017429805615550755,
      "loss": 0.1161,
      "step": 129
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.08584029227495193,
      "learning_rate": 0.00017408207343412528,
      "loss": 0.1225,
      "step": 130
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.0720861479640007,
      "learning_rate": 0.000173866090712743,
      "loss": 0.1061,
      "step": 131
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.0756726935505867,
      "learning_rate": 0.0001736501079913607,
      "loss": 0.1032,
      "step": 132
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.07314030826091766,
      "learning_rate": 0.00017343412526997842,
      "loss": 0.1041,
      "step": 133
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.06986541301012039,
      "learning_rate": 0.00017321814254859613,
      "loss": 0.093,
      "step": 134
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.08515734225511551,
      "learning_rate": 0.00017300215982721384,
      "loss": 0.0877,
      "step": 135
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.06462622433900833,
      "learning_rate": 0.00017278617710583154,
      "loss": 0.0913,
      "step": 136
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.08543089032173157,
      "learning_rate": 0.00017257019438444925,
      "loss": 0.0833,
      "step": 137
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.0615629181265831,
      "learning_rate": 0.00017235421166306695,
      "loss": 0.0747,
      "step": 138
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.06748052686452866,
      "learning_rate": 0.00017213822894168469,
      "loss": 0.0732,
      "step": 139
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.07206647843122482,
      "learning_rate": 0.00017192224622030236,
      "loss": 0.0668,
      "step": 140
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.05431283637881279,
      "learning_rate": 0.0001717062634989201,
      "loss": 0.0635,
      "step": 141
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.0703502893447876,
      "learning_rate": 0.0001714902807775378,
      "loss": 0.068,
      "step": 142
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.06487338989973068,
      "learning_rate": 0.0001712742980561555,
      "loss": 0.0632,
      "step": 143
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.07282714545726776,
      "learning_rate": 0.00017105831533477324,
      "loss": 0.048,
      "step": 144
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.07322162389755249,
      "learning_rate": 0.00017084233261339092,
      "loss": 0.0498,
      "step": 145
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.06104642525315285,
      "learning_rate": 0.00017062634989200865,
      "loss": 0.0458,
      "step": 146
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.12298593670129776,
      "learning_rate": 0.00017041036717062636,
      "loss": 0.0412,
      "step": 147
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.09498516470193863,
      "learning_rate": 0.00017019438444924406,
      "loss": 0.036,
      "step": 148
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.07625502347946167,
      "learning_rate": 0.00016997840172786177,
      "loss": 0.0344,
      "step": 149
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.06744737923145294,
      "learning_rate": 0.0001697624190064795,
      "loss": 0.0401,
      "step": 150
    },
    {
      "epoch": 0.4832,
      "grad_norm": 2.582364082336426,
      "learning_rate": 0.0001695464362850972,
      "loss": 0.6235,
      "step": 151
    },
    {
      "epoch": 0.4864,
      "grad_norm": 1.1677980422973633,
      "learning_rate": 0.0001693304535637149,
      "loss": 0.4514,
      "step": 152
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.5365772843360901,
      "learning_rate": 0.00016911447084233262,
      "loss": 0.3403,
      "step": 153
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.2059655338525772,
      "learning_rate": 0.00016889848812095032,
      "loss": 0.2937,
      "step": 154
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.14388377964496613,
      "learning_rate": 0.00016868250539956806,
      "loss": 0.2697,
      "step": 155
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.30698126554489136,
      "learning_rate": 0.00016846652267818574,
      "loss": 0.2642,
      "step": 156
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.22217732667922974,
      "learning_rate": 0.00016825053995680347,
      "loss": 0.247,
      "step": 157
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.23607200384140015,
      "learning_rate": 0.00016803455723542117,
      "loss": 0.2527,
      "step": 158
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.184879332780838,
      "learning_rate": 0.00016781857451403888,
      "loss": 0.2492,
      "step": 159
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.16862154006958008,
      "learning_rate": 0.0001676025917926566,
      "loss": 0.2328,
      "step": 160
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.16483110189437866,
      "learning_rate": 0.0001673866090712743,
      "loss": 0.2149,
      "step": 161
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.16620157659053802,
      "learning_rate": 0.00016717062634989202,
      "loss": 0.2288,
      "step": 162
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.17595551908016205,
      "learning_rate": 0.00016695464362850973,
      "loss": 0.1984,
      "step": 163
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.11563430726528168,
      "learning_rate": 0.00016673866090712743,
      "loss": 0.1843,
      "step": 164
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.10846944898366928,
      "learning_rate": 0.00016652267818574514,
      "loss": 0.167,
      "step": 165
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.1084708422422409,
      "learning_rate": 0.00016630669546436287,
      "loss": 0.1781,
      "step": 166
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.0964418351650238,
      "learning_rate": 0.00016609071274298055,
      "loss": 0.169,
      "step": 167
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.09275882691144943,
      "learning_rate": 0.00016587473002159828,
      "loss": 0.1631,
      "step": 168
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.08877965062856674,
      "learning_rate": 0.000165658747300216,
      "loss": 0.1597,
      "step": 169
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.10385245829820633,
      "learning_rate": 0.0001654427645788337,
      "loss": 0.1621,
      "step": 170
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.09570787847042084,
      "learning_rate": 0.00016522678185745143,
      "loss": 0.1553,
      "step": 171
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.09157850593328476,
      "learning_rate": 0.0001650107991360691,
      "loss": 0.1397,
      "step": 172
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.09800048172473907,
      "learning_rate": 0.00016479481641468684,
      "loss": 0.1498,
      "step": 173
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.10191880166530609,
      "learning_rate": 0.00016457883369330455,
      "loss": 0.1337,
      "step": 174
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.08677957952022552,
      "learning_rate": 0.00016436285097192225,
      "loss": 0.1373,
      "step": 175
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.11023259162902832,
      "learning_rate": 0.00016414686825053998,
      "loss": 0.1378,
      "step": 176
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.08356870710849762,
      "learning_rate": 0.00016393088552915766,
      "loss": 0.1143,
      "step": 177
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.30474942922592163,
      "learning_rate": 0.0001637149028077754,
      "loss": 0.1226,
      "step": 178
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.11169648170471191,
      "learning_rate": 0.0001634989200863931,
      "loss": 0.1229,
      "step": 179
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.12769091129302979,
      "learning_rate": 0.0001632829373650108,
      "loss": 0.1226,
      "step": 180
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.07679124176502228,
      "learning_rate": 0.0001630669546436285,
      "loss": 0.1041,
      "step": 181
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.0922919362783432,
      "learning_rate": 0.00016285097192224624,
      "loss": 0.1163,
      "step": 182
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.09230438619852066,
      "learning_rate": 0.00016263498920086392,
      "loss": 0.1106,
      "step": 183
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.08038726449012756,
      "learning_rate": 0.00016241900647948166,
      "loss": 0.0965,
      "step": 184
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.09995442628860474,
      "learning_rate": 0.00016220302375809936,
      "loss": 0.0914,
      "step": 185
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.07371248304843903,
      "learning_rate": 0.00016198704103671707,
      "loss": 0.0814,
      "step": 186
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.08044587075710297,
      "learning_rate": 0.0001617710583153348,
      "loss": 0.0794,
      "step": 187
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.06729457527399063,
      "learning_rate": 0.00016155507559395248,
      "loss": 0.0789,
      "step": 188
    },
    {
      "epoch": 0.6016,
      "eval_loss": 0.15358756482601166,
      "eval_runtime": 100.1446,
      "eval_samples_per_second": 11.074,
      "eval_steps_per_second": 0.699,
      "step": 188
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.10994323343038559,
      "learning_rate": 0.0001613390928725702,
      "loss": 0.0799,
      "step": 189
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.08133586496114731,
      "learning_rate": 0.00016112311015118792,
      "loss": 0.0783,
      "step": 190
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.07946256548166275,
      "learning_rate": 0.00016090712742980562,
      "loss": 0.0802,
      "step": 191
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.0951666310429573,
      "learning_rate": 0.00016069114470842333,
      "loss": 0.0714,
      "step": 192
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.06924145668745041,
      "learning_rate": 0.00016047516198704103,
      "loss": 0.0648,
      "step": 193
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.07558314502239227,
      "learning_rate": 0.00016025917926565877,
      "loss": 0.0525,
      "step": 194
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.07862892746925354,
      "learning_rate": 0.00016004319654427647,
      "loss": 0.0456,
      "step": 195
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.07655524462461472,
      "learning_rate": 0.00015982721382289418,
      "loss": 0.0516,
      "step": 196
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.11946225166320801,
      "learning_rate": 0.00015961123110151188,
      "loss": 0.0426,
      "step": 197
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.13277865946292877,
      "learning_rate": 0.00015939524838012962,
      "loss": 0.0378,
      "step": 198
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.07876647263765335,
      "learning_rate": 0.0001591792656587473,
      "loss": 0.031,
      "step": 199
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.08112156391143799,
      "learning_rate": 0.00015896328293736503,
      "loss": 0.0456,
      "step": 200
    },
    {
      "epoch": 0.6432,
      "grad_norm": 1.5295963287353516,
      "learning_rate": 0.00015874730021598273,
      "loss": 0.4905,
      "step": 201
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.8346420526504517,
      "learning_rate": 0.00015853131749460044,
      "loss": 0.4233,
      "step": 202
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.4731906056404114,
      "learning_rate": 0.00015831533477321817,
      "loss": 0.3467,
      "step": 203
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.25568652153015137,
      "learning_rate": 0.00015809935205183585,
      "loss": 0.3207,
      "step": 204
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.16455748677253723,
      "learning_rate": 0.00015788336933045358,
      "loss": 0.2663,
      "step": 205
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.21264716982841492,
      "learning_rate": 0.0001576673866090713,
      "loss": 0.2759,
      "step": 206
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.22452408075332642,
      "learning_rate": 0.000157451403887689,
      "loss": 0.2639,
      "step": 207
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.25361788272857666,
      "learning_rate": 0.0001572354211663067,
      "loss": 0.2481,
      "step": 208
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.1893545240163803,
      "learning_rate": 0.0001570194384449244,
      "loss": 0.2265,
      "step": 209
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.18016454577445984,
      "learning_rate": 0.0001568034557235421,
      "loss": 0.2215,
      "step": 210
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.14851114153862,
      "learning_rate": 0.00015658747300215984,
      "loss": 0.2237,
      "step": 211
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.14893892407417297,
      "learning_rate": 0.00015637149028077755,
      "loss": 0.2033,
      "step": 212
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.1302412748336792,
      "learning_rate": 0.00015615550755939525,
      "loss": 0.1924,
      "step": 213
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.122309610247612,
      "learning_rate": 0.00015593952483801296,
      "loss": 0.195,
      "step": 214
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.11879032850265503,
      "learning_rate": 0.00015572354211663067,
      "loss": 0.1765,
      "step": 215
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.0962909534573555,
      "learning_rate": 0.0001555075593952484,
      "loss": 0.1713,
      "step": 216
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.09997446089982986,
      "learning_rate": 0.00015529157667386608,
      "loss": 0.1647,
      "step": 217
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.08514430373907089,
      "learning_rate": 0.0001550755939524838,
      "loss": 0.1639,
      "step": 218
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.09083016216754913,
      "learning_rate": 0.00015485961123110152,
      "loss": 0.1544,
      "step": 219
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.07983364164829254,
      "learning_rate": 0.00015464362850971922,
      "loss": 0.1484,
      "step": 220
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.0770895928144455,
      "learning_rate": 0.00015442764578833695,
      "loss": 0.1308,
      "step": 221
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.09385045617818832,
      "learning_rate": 0.00015421166306695466,
      "loss": 0.147,
      "step": 222
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.09331627190113068,
      "learning_rate": 0.00015399568034557237,
      "loss": 0.1234,
      "step": 223
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.11082423478364944,
      "learning_rate": 0.00015377969762419007,
      "loss": 0.1421,
      "step": 224
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.10347947478294373,
      "learning_rate": 0.00015356371490280778,
      "loss": 0.1327,
      "step": 225
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.08602286875247955,
      "learning_rate": 0.00015334773218142548,
      "loss": 0.1158,
      "step": 226
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.0751379057765007,
      "learning_rate": 0.00015313174946004321,
      "loss": 0.1114,
      "step": 227
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.08718612790107727,
      "learning_rate": 0.0001529157667386609,
      "loss": 0.1259,
      "step": 228
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.09808294475078583,
      "learning_rate": 0.00015269978401727863,
      "loss": 0.1086,
      "step": 229
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.08569410443305969,
      "learning_rate": 0.00015248380129589633,
      "loss": 0.1132,
      "step": 230
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.07641404867172241,
      "learning_rate": 0.00015226781857451404,
      "loss": 0.098,
      "step": 231
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.07934756577014923,
      "learning_rate": 0.00015205183585313177,
      "loss": 0.0977,
      "step": 232
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.0751657709479332,
      "learning_rate": 0.00015183585313174945,
      "loss": 0.1039,
      "step": 233
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.07843207567930222,
      "learning_rate": 0.00015161987041036718,
      "loss": 0.0844,
      "step": 234
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.08769173175096512,
      "learning_rate": 0.0001514038876889849,
      "loss": 0.0866,
      "step": 235
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.0678262934088707,
      "learning_rate": 0.0001511879049676026,
      "loss": 0.0872,
      "step": 236
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.07057864964008331,
      "learning_rate": 0.0001509719222462203,
      "loss": 0.0758,
      "step": 237
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.08254123479127884,
      "learning_rate": 0.00015075593952483803,
      "loss": 0.0752,
      "step": 238
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.09618472307920456,
      "learning_rate": 0.00015053995680345574,
      "loss": 0.083,
      "step": 239
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.07441812008619308,
      "learning_rate": 0.00015032397408207344,
      "loss": 0.0675,
      "step": 240
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.0733884796500206,
      "learning_rate": 0.00015010799136069115,
      "loss": 0.0701,
      "step": 241
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.0727105364203453,
      "learning_rate": 0.00014989200863930885,
      "loss": 0.0684,
      "step": 242
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.08703511208295822,
      "learning_rate": 0.00014967602591792659,
      "loss": 0.0711,
      "step": 243
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.0790431946516037,
      "learning_rate": 0.00014946004319654426,
      "loss": 0.0562,
      "step": 244
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.07802541553974152,
      "learning_rate": 0.000149244060475162,
      "loss": 0.049,
      "step": 245
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.08394356817007065,
      "learning_rate": 0.0001490280777537797,
      "loss": 0.0476,
      "step": 246
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.08037304133176804,
      "learning_rate": 0.0001488120950323974,
      "loss": 0.0472,
      "step": 247
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.10269023478031158,
      "learning_rate": 0.00014859611231101514,
      "loss": 0.0422,
      "step": 248
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.07242658734321594,
      "learning_rate": 0.00014838012958963282,
      "loss": 0.0394,
      "step": 249
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.07327985763549805,
      "learning_rate": 0.00014816414686825055,
      "loss": 0.0414,
      "step": 250
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.674603283405304,
      "learning_rate": 0.00014794816414686826,
      "loss": 0.4243,
      "step": 251
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.5341923236846924,
      "learning_rate": 0.00014773218142548596,
      "loss": 0.3671,
      "step": 252
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.39013245701789856,
      "learning_rate": 0.00014751619870410367,
      "loss": 0.3289,
      "step": 253
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.24735485017299652,
      "learning_rate": 0.0001473002159827214,
      "loss": 0.3106,
      "step": 254
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.16006790101528168,
      "learning_rate": 0.0001470842332613391,
      "loss": 0.2906,
      "step": 255
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.23042429983615875,
      "learning_rate": 0.0001468682505399568,
      "loss": 0.2613,
      "step": 256
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.32497379183769226,
      "learning_rate": 0.00014665226781857452,
      "loss": 0.2567,
      "step": 257
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.30977165699005127,
      "learning_rate": 0.00014643628509719222,
      "loss": 0.2615,
      "step": 258
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.22327324748039246,
      "learning_rate": 0.00014622030237580996,
      "loss": 0.2311,
      "step": 259
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.16415902972221375,
      "learning_rate": 0.00014600431965442764,
      "loss": 0.2293,
      "step": 260
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.1401928812265396,
      "learning_rate": 0.00014578833693304537,
      "loss": 0.2123,
      "step": 261
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.13532733917236328,
      "learning_rate": 0.00014557235421166307,
      "loss": 0.2154,
      "step": 262
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.11128929257392883,
      "learning_rate": 0.00014535637149028078,
      "loss": 0.2037,
      "step": 263
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.11990294605493546,
      "learning_rate": 0.0001451403887688985,
      "loss": 0.1933,
      "step": 264
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.10572594404220581,
      "learning_rate": 0.0001449244060475162,
      "loss": 0.1852,
      "step": 265
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.09436079859733582,
      "learning_rate": 0.00014470842332613392,
      "loss": 0.1781,
      "step": 266
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.10206999629735947,
      "learning_rate": 0.00014449244060475163,
      "loss": 0.1661,
      "step": 267
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.09557899832725525,
      "learning_rate": 0.00014427645788336934,
      "loss": 0.1872,
      "step": 268
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.08524663001298904,
      "learning_rate": 0.00014406047516198704,
      "loss": 0.1535,
      "step": 269
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.08678577840328217,
      "learning_rate": 0.00014384449244060477,
      "loss": 0.1437,
      "step": 270
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.09264212101697922,
      "learning_rate": 0.00014362850971922245,
      "loss": 0.1435,
      "step": 271
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.07844077050685883,
      "learning_rate": 0.00014341252699784018,
      "loss": 0.1366,
      "step": 272
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.08219824731349945,
      "learning_rate": 0.0001431965442764579,
      "loss": 0.1274,
      "step": 273
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.07055019587278366,
      "learning_rate": 0.0001429805615550756,
      "loss": 0.1333,
      "step": 274
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.0853000283241272,
      "learning_rate": 0.00014276457883369333,
      "loss": 0.1297,
      "step": 275
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.0726621225476265,
      "learning_rate": 0.000142548596112311,
      "loss": 0.1198,
      "step": 276
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.06854601204395294,
      "learning_rate": 0.00014233261339092874,
      "loss": 0.1209,
      "step": 277
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.06103048473596573,
      "learning_rate": 0.00014211663066954645,
      "loss": 0.1115,
      "step": 278
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.07323264330625534,
      "learning_rate": 0.00014190064794816415,
      "loss": 0.1142,
      "step": 279
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.07760905474424362,
      "learning_rate": 0.00014168466522678186,
      "loss": 0.1021,
      "step": 280
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.07650589197874069,
      "learning_rate": 0.00014146868250539956,
      "loss": 0.1161,
      "step": 281
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.07565004378557205,
      "learning_rate": 0.0001412526997840173,
      "loss": 0.1016,
      "step": 282
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.07086750864982605,
      "learning_rate": 0.000141036717062635,
      "loss": 0.0966,
      "step": 283
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.07491260766983032,
      "learning_rate": 0.0001408207343412527,
      "loss": 0.0973,
      "step": 284
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.11182582378387451,
      "learning_rate": 0.0001406047516198704,
      "loss": 0.1014,
      "step": 285
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.06034325808286667,
      "learning_rate": 0.00014038876889848814,
      "loss": 0.0908,
      "step": 286
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.06185375899076462,
      "learning_rate": 0.00014017278617710582,
      "loss": 0.0921,
      "step": 287
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.10146602243185043,
      "learning_rate": 0.00013995680345572356,
      "loss": 0.0785,
      "step": 288
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.08139444142580032,
      "learning_rate": 0.00013974082073434126,
      "loss": 0.0734,
      "step": 289
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.05981762707233429,
      "learning_rate": 0.00013952483801295897,
      "loss": 0.0745,
      "step": 290
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.06275299936532974,
      "learning_rate": 0.0001393088552915767,
      "loss": 0.0625,
      "step": 291
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.05989274010062218,
      "learning_rate": 0.00013909287257019438,
      "loss": 0.0654,
      "step": 292
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.06842871755361557,
      "learning_rate": 0.0001388768898488121,
      "loss": 0.0628,
      "step": 293
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.06952448189258575,
      "learning_rate": 0.00013866090712742982,
      "loss": 0.0563,
      "step": 294
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.06065935641527176,
      "learning_rate": 0.00013844492440604752,
      "loss": 0.0528,
      "step": 295
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.061113715171813965,
      "learning_rate": 0.00013822894168466523,
      "loss": 0.0421,
      "step": 296
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.10677459836006165,
      "learning_rate": 0.00013801295896328293,
      "loss": 0.0394,
      "step": 297
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.08022179454565048,
      "learning_rate": 0.00013779697624190064,
      "loss": 0.041,
      "step": 298
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.06484784930944443,
      "learning_rate": 0.00013758099352051837,
      "loss": 0.0333,
      "step": 299
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.06363236904144287,
      "learning_rate": 0.00013736501079913608,
      "loss": 0.0427,
      "step": 300
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.44433721899986267,
      "learning_rate": 0.00013714902807775378,
      "loss": 0.3584,
      "step": 301
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.24111196398735046,
      "learning_rate": 0.00013693304535637152,
      "loss": 0.2799,
      "step": 302
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.17339332401752472,
      "learning_rate": 0.0001367170626349892,
      "loss": 0.2343,
      "step": 303
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.1310667097568512,
      "learning_rate": 0.00013650107991360693,
      "loss": 0.1868,
      "step": 304
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.10640361905097961,
      "learning_rate": 0.00013628509719222463,
      "loss": 0.1664,
      "step": 305
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.08672438561916351,
      "learning_rate": 0.00013606911447084234,
      "loss": 0.1479,
      "step": 306
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.08859781920909882,
      "learning_rate": 0.00013585313174946004,
      "loss": 0.1079,
      "step": 307
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.07510612905025482,
      "learning_rate": 0.00013563714902807775,
      "loss": 0.1101,
      "step": 308
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.09146241098642349,
      "learning_rate": 0.00013542116630669548,
      "loss": 0.0811,
      "step": 309
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.09475333243608475,
      "learning_rate": 0.0001352051835853132,
      "loss": 0.0721,
      "step": 310
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.130778968334198,
      "learning_rate": 0.0001349892008639309,
      "loss": 0.0635,
      "step": 311
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.14115048944950104,
      "learning_rate": 0.0001347732181425486,
      "loss": 0.0497,
      "step": 312
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.11997000873088837,
      "learning_rate": 0.0001345572354211663,
      "loss": 0.2256,
      "step": 313
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.14707410335540771,
      "learning_rate": 0.000134341252699784,
      "loss": 0.3419,
      "step": 314
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.17011718451976776,
      "learning_rate": 0.00013412526997840174,
      "loss": 0.2989,
      "step": 315
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.17345085740089417,
      "learning_rate": 0.00013390928725701945,
      "loss": 0.272,
      "step": 316
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.14271925389766693,
      "learning_rate": 0.00013369330453563715,
      "loss": 0.258,
      "step": 317
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.12236033380031586,
      "learning_rate": 0.0001334773218142549,
      "loss": 0.2353,
      "step": 318
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.11290924996137619,
      "learning_rate": 0.00013326133909287257,
      "loss": 0.2235,
      "step": 319
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.11244039982557297,
      "learning_rate": 0.0001330453563714903,
      "loss": 0.2153,
      "step": 320
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.10569936782121658,
      "learning_rate": 0.000132829373650108,
      "loss": 0.2356,
      "step": 321
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.14320407807826996,
      "learning_rate": 0.0001326133909287257,
      "loss": 0.2225,
      "step": 322
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.11067403107881546,
      "learning_rate": 0.00013239740820734342,
      "loss": 0.2071,
      "step": 323
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.12213852256536484,
      "learning_rate": 0.00013218142548596112,
      "loss": 0.1933,
      "step": 324
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.12561529874801636,
      "learning_rate": 0.00013196544276457885,
      "loss": 0.1827,
      "step": 325
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.14336295425891876,
      "learning_rate": 0.00013174946004319656,
      "loss": 0.186,
      "step": 326
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.12256599962711334,
      "learning_rate": 0.00013153347732181427,
      "loss": 0.1731,
      "step": 327
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.1231326013803482,
      "learning_rate": 0.00013131749460043197,
      "loss": 0.1803,
      "step": 328
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.12226280570030212,
      "learning_rate": 0.00013110151187904968,
      "loss": 0.1708,
      "step": 329
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.10937453806400299,
      "learning_rate": 0.00013088552915766738,
      "loss": 0.1727,
      "step": 330
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.09929589182138443,
      "learning_rate": 0.00013066954643628511,
      "loss": 0.1452,
      "step": 331
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.1063147708773613,
      "learning_rate": 0.0001304535637149028,
      "loss": 0.1462,
      "step": 332
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.12070031464099884,
      "learning_rate": 0.00013023758099352053,
      "loss": 0.1282,
      "step": 333
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.10257162898778915,
      "learning_rate": 0.00013002159827213826,
      "loss": 0.1244,
      "step": 334
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.08255746960639954,
      "learning_rate": 0.00012980561555075594,
      "loss": 0.1375,
      "step": 335
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.08366654813289642,
      "learning_rate": 0.00012958963282937367,
      "loss": 0.1286,
      "step": 336
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.09608781337738037,
      "learning_rate": 0.00012937365010799138,
      "loss": 0.1296,
      "step": 337
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.10134371370077133,
      "learning_rate": 0.00012915766738660908,
      "loss": 0.1292,
      "step": 338
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.07989580929279327,
      "learning_rate": 0.0001289416846652268,
      "loss": 0.1026,
      "step": 339
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.0782737210392952,
      "learning_rate": 0.0001287257019438445,
      "loss": 0.1125,
      "step": 340
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.07798503339290619,
      "learning_rate": 0.0001285097192224622,
      "loss": 0.1091,
      "step": 341
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.08558062463998795,
      "learning_rate": 0.00012829373650107993,
      "loss": 0.1068,
      "step": 342
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.09030447155237198,
      "learning_rate": 0.00012807775377969764,
      "loss": 0.1019,
      "step": 343
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.07005513459444046,
      "learning_rate": 0.00012786177105831534,
      "loss": 0.0889,
      "step": 344
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.08939553797245026,
      "learning_rate": 0.00012764578833693305,
      "loss": 0.1134,
      "step": 345
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.08030259609222412,
      "learning_rate": 0.00012742980561555075,
      "loss": 0.085,
      "step": 346
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.06897532194852829,
      "learning_rate": 0.00012721382289416849,
      "loss": 0.0772,
      "step": 347
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.08519865572452545,
      "learning_rate": 0.00012699784017278616,
      "loss": 0.0873,
      "step": 348
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.0698448196053505,
      "learning_rate": 0.0001267818574514039,
      "loss": 0.0809,
      "step": 349
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.06411396712064743,
      "learning_rate": 0.0001265658747300216,
      "loss": 0.0748,
      "step": 350
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.0637090727686882,
      "learning_rate": 0.0001263498920086393,
      "loss": 0.0731,
      "step": 351
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.08060378581285477,
      "learning_rate": 0.00012613390928725704,
      "loss": 0.0784,
      "step": 352
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.06315179169178009,
      "learning_rate": 0.00012591792656587472,
      "loss": 0.0656,
      "step": 353
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.061039481312036514,
      "learning_rate": 0.00012570194384449245,
      "loss": 0.0639,
      "step": 354
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.06964940577745438,
      "learning_rate": 0.00012548596112311016,
      "loss": 0.0692,
      "step": 355
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.06637610495090485,
      "learning_rate": 0.00012526997840172786,
      "loss": 0.0498,
      "step": 356
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.06137174740433693,
      "learning_rate": 0.00012505399568034557,
      "loss": 0.0497,
      "step": 357
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.05875043570995331,
      "learning_rate": 0.0001248380129589633,
      "loss": 0.051,
      "step": 358
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.05981156975030899,
      "learning_rate": 0.00012462203023758098,
      "loss": 0.0481,
      "step": 359
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.07167306542396545,
      "learning_rate": 0.0001244060475161987,
      "loss": 0.0365,
      "step": 360
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.07272736728191376,
      "learning_rate": 0.00012419006479481642,
      "loss": 0.0392,
      "step": 361
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.06320641934871674,
      "learning_rate": 0.00012397408207343412,
      "loss": 0.0378,
      "step": 362
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.19552962481975555,
      "learning_rate": 0.00012375809935205186,
      "loss": 0.2453,
      "step": 363
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.2649109363555908,
      "learning_rate": 0.00012354211663066954,
      "loss": 0.3521,
      "step": 364
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.22942207753658295,
      "learning_rate": 0.00012332613390928727,
      "loss": 0.3139,
      "step": 365
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.20003998279571533,
      "learning_rate": 0.00012311015118790497,
      "loss": 0.2872,
      "step": 366
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.6745431423187256,
      "learning_rate": 0.00012289416846652268,
      "loss": 0.2839,
      "step": 367
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.17137108743190765,
      "learning_rate": 0.00012267818574514039,
      "loss": 0.2364,
      "step": 368
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.1637425720691681,
      "learning_rate": 0.0001224622030237581,
      "loss": 0.2488,
      "step": 369
    },
    {
      "epoch": 1.184,
      "grad_norm": 1.1395171880722046,
      "learning_rate": 0.00012224622030237582,
      "loss": 0.2203,
      "step": 370
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.12725816667079926,
      "learning_rate": 0.00012203023758099353,
      "loss": 0.2194,
      "step": 371
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.10857982933521271,
      "learning_rate": 0.00012181425485961125,
      "loss": 0.1918,
      "step": 372
    },
    {
      "epoch": 1.1936,
      "grad_norm": 1.2869576215744019,
      "learning_rate": 0.00012159827213822894,
      "loss": 0.2199,
      "step": 373
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.29680728912353516,
      "learning_rate": 0.00012138228941684666,
      "loss": 0.1891,
      "step": 374
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.18445530533790588,
      "learning_rate": 0.00012116630669546437,
      "loss": 0.1877,
      "step": 375
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.13325351476669312,
      "learning_rate": 0.00012095032397408208,
      "loss": 0.1798,
      "step": 376
    },
    {
      "epoch": 1.2032,
      "eval_loss": 0.1591818630695343,
      "eval_runtime": 98.4691,
      "eval_samples_per_second": 11.262,
      "eval_steps_per_second": 0.711,
      "step": 376
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.6245388984680176,
      "learning_rate": 0.0001207343412526998,
      "loss": 0.1741,
      "step": 377
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.140795037150383,
      "learning_rate": 0.0001205183585313175,
      "loss": 0.1678,
      "step": 378
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.14888934791088104,
      "learning_rate": 0.00012030237580993522,
      "loss": 0.1667,
      "step": 379
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.8047127723693848,
      "learning_rate": 0.00012008639308855292,
      "loss": 0.1602,
      "step": 380
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.11481031775474548,
      "learning_rate": 0.00011987041036717064,
      "loss": 0.1573,
      "step": 381
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.11270076036453247,
      "learning_rate": 0.00011965442764578833,
      "loss": 0.1526,
      "step": 382
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.09626296162605286,
      "learning_rate": 0.00011943844492440605,
      "loss": 0.139,
      "step": 383
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.10542811453342438,
      "learning_rate": 0.00011922246220302376,
      "loss": 0.1417,
      "step": 384
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.8815963268280029,
      "learning_rate": 0.00011900647948164148,
      "loss": 0.129,
      "step": 385
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.09686513990163803,
      "learning_rate": 0.0001187904967602592,
      "loss": 0.1351,
      "step": 386
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.0849158987402916,
      "learning_rate": 0.00011857451403887689,
      "loss": 0.1189,
      "step": 387
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.09099497646093369,
      "learning_rate": 0.00011835853131749462,
      "loss": 0.1284,
      "step": 388
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.09169624000787735,
      "learning_rate": 0.00011814254859611231,
      "loss": 0.1294,
      "step": 389
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.09578943997621536,
      "learning_rate": 0.00011792656587473003,
      "loss": 0.1111,
      "step": 390
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.10189872980117798,
      "learning_rate": 0.00011771058315334774,
      "loss": 0.1197,
      "step": 391
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.09034481644630432,
      "learning_rate": 0.00011749460043196546,
      "loss": 0.1052,
      "step": 392
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.08263595402240753,
      "learning_rate": 0.00011727861771058315,
      "loss": 0.1057,
      "step": 393
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.07547786086797714,
      "learning_rate": 0.00011706263498920087,
      "loss": 0.0993,
      "step": 394
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.0667828842997551,
      "learning_rate": 0.00011684665226781859,
      "loss": 0.0927,
      "step": 395
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.06701038032770157,
      "learning_rate": 0.00011663066954643629,
      "loss": 0.0996,
      "step": 396
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.07439185678958893,
      "learning_rate": 0.00011641468682505401,
      "loss": 0.0956,
      "step": 397
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.06494053453207016,
      "learning_rate": 0.0001161987041036717,
      "loss": 0.0887,
      "step": 398
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.06355660408735275,
      "learning_rate": 0.00011598272138228942,
      "loss": 0.0867,
      "step": 399
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.05834250897169113,
      "learning_rate": 0.00011576673866090713,
      "loss": 0.0772,
      "step": 400
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.06987501680850983,
      "learning_rate": 0.00011555075593952485,
      "loss": 0.077,
      "step": 401
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.05944366008043289,
      "learning_rate": 0.00011533477321814254,
      "loss": 0.0758,
      "step": 402
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.06509342044591904,
      "learning_rate": 0.00011511879049676026,
      "loss": 0.0589,
      "step": 403
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.059185851365327835,
      "learning_rate": 0.00011490280777537799,
      "loss": 0.0611,
      "step": 404
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.06579429656267166,
      "learning_rate": 0.00011468682505399568,
      "loss": 0.061,
      "step": 405
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.053226109594106674,
      "learning_rate": 0.0001144708423326134,
      "loss": 0.0524,
      "step": 406
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.05422969162464142,
      "learning_rate": 0.00011425485961123111,
      "loss": 0.0463,
      "step": 407
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.060504183173179626,
      "learning_rate": 0.00011403887688984883,
      "loss": 0.049,
      "step": 408
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.05598820373415947,
      "learning_rate": 0.00011382289416846652,
      "loss": 0.0452,
      "step": 409
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.08275706321001053,
      "learning_rate": 0.00011360691144708424,
      "loss": 0.0379,
      "step": 410
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.0849904716014862,
      "learning_rate": 0.00011339092872570194,
      "loss": 0.0382,
      "step": 411
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.0567975752055645,
      "learning_rate": 0.00011317494600431966,
      "loss": 0.0359,
      "step": 412
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.1660701036453247,
      "learning_rate": 0.00011295896328293738,
      "loss": 0.211,
      "step": 413
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.2668968737125397,
      "learning_rate": 0.00011274298056155507,
      "loss": 0.3317,
      "step": 414
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.25007253885269165,
      "learning_rate": 0.0001125269978401728,
      "loss": 0.3164,
      "step": 415
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.22479476034641266,
      "learning_rate": 0.0001123110151187905,
      "loss": 0.3017,
      "step": 416
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.1871844083070755,
      "learning_rate": 0.00011209503239740822,
      "loss": 0.2792,
      "step": 417
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.1820467710494995,
      "learning_rate": 0.00011187904967602591,
      "loss": 0.2575,
      "step": 418
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.15836143493652344,
      "learning_rate": 0.00011166306695464363,
      "loss": 0.2606,
      "step": 419
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.14027643203735352,
      "learning_rate": 0.00011144708423326134,
      "loss": 0.2164,
      "step": 420
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.12318983674049377,
      "learning_rate": 0.00011123110151187905,
      "loss": 0.2269,
      "step": 421
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.1222982257604599,
      "learning_rate": 0.00011101511879049677,
      "loss": 0.1992,
      "step": 422
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.12370932847261429,
      "learning_rate": 0.00011079913606911448,
      "loss": 0.2037,
      "step": 423
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.11547400057315826,
      "learning_rate": 0.0001105831533477322,
      "loss": 0.1875,
      "step": 424
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.11140944808721542,
      "learning_rate": 0.00011036717062634989,
      "loss": 0.1833,
      "step": 425
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.11044490337371826,
      "learning_rate": 0.00011015118790496761,
      "loss": 0.1833,
      "step": 426
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.10642653703689575,
      "learning_rate": 0.00010993520518358532,
      "loss": 0.1732,
      "step": 427
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.10137318074703217,
      "learning_rate": 0.00010971922246220303,
      "loss": 0.1598,
      "step": 428
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.10386919975280762,
      "learning_rate": 0.00010950323974082073,
      "loss": 0.1527,
      "step": 429
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.0981709286570549,
      "learning_rate": 0.00010928725701943845,
      "loss": 0.1408,
      "step": 430
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.1260475367307663,
      "learning_rate": 0.00010907127429805617,
      "loss": 0.1515,
      "step": 431
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.10942622274160385,
      "learning_rate": 0.00010885529157667387,
      "loss": 0.1428,
      "step": 432
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.10764030367136002,
      "learning_rate": 0.00010863930885529159,
      "loss": 0.1423,
      "step": 433
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.10711009800434113,
      "learning_rate": 0.00010842332613390928,
      "loss": 0.1313,
      "step": 434
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.10206203907728195,
      "learning_rate": 0.000108207343412527,
      "loss": 0.1319,
      "step": 435
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.11249946057796478,
      "learning_rate": 0.00010799136069114471,
      "loss": 0.1306,
      "step": 436
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.09348681569099426,
      "learning_rate": 0.00010777537796976243,
      "loss": 0.1213,
      "step": 437
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.0990813672542572,
      "learning_rate": 0.00010755939524838012,
      "loss": 0.119,
      "step": 438
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.0866088792681694,
      "learning_rate": 0.00010734341252699785,
      "loss": 0.116,
      "step": 439
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.094899021089077,
      "learning_rate": 0.00010712742980561557,
      "loss": 0.1166,
      "step": 440
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.08335098624229431,
      "learning_rate": 0.00010691144708423326,
      "loss": 0.1023,
      "step": 441
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.07794898003339767,
      "learning_rate": 0.00010669546436285098,
      "loss": 0.1031,
      "step": 442
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.08483266830444336,
      "learning_rate": 0.00010647948164146869,
      "loss": 0.1027,
      "step": 443
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.08544091880321503,
      "learning_rate": 0.0001062634989200864,
      "loss": 0.0945,
      "step": 444
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.08047832548618317,
      "learning_rate": 0.0001060475161987041,
      "loss": 0.0979,
      "step": 445
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.07661319524049759,
      "learning_rate": 0.00010583153347732182,
      "loss": 0.0926,
      "step": 446
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.06809470057487488,
      "learning_rate": 0.00010561555075593954,
      "loss": 0.0869,
      "step": 447
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.07238508015871048,
      "learning_rate": 0.00010539956803455724,
      "loss": 0.0895,
      "step": 448
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.07382765412330627,
      "learning_rate": 0.00010518358531317496,
      "loss": 0.0842,
      "step": 449
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.07234997302293777,
      "learning_rate": 0.00010496760259179265,
      "loss": 0.0845,
      "step": 450
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.0634097009897232,
      "learning_rate": 0.00010475161987041037,
      "loss": 0.0706,
      "step": 451
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.07205670326948166,
      "learning_rate": 0.00010453563714902808,
      "loss": 0.0689,
      "step": 452
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.07272996008396149,
      "learning_rate": 0.0001043196544276458,
      "loss": 0.0643,
      "step": 453
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.07861189544200897,
      "learning_rate": 0.00010410367170626349,
      "loss": 0.0714,
      "step": 454
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.06273604184389114,
      "learning_rate": 0.00010388768898488121,
      "loss": 0.0603,
      "step": 455
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.07177518308162689,
      "learning_rate": 0.00010367170626349894,
      "loss": 0.0638,
      "step": 456
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.06667538732290268,
      "learning_rate": 0.00010345572354211663,
      "loss": 0.0502,
      "step": 457
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.07561179250478745,
      "learning_rate": 0.00010323974082073435,
      "loss": 0.0519,
      "step": 458
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.08505142480134964,
      "learning_rate": 0.00010302375809935206,
      "loss": 0.0539,
      "step": 459
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.1028430238366127,
      "learning_rate": 0.00010280777537796978,
      "loss": 0.0394,
      "step": 460
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.07230327278375626,
      "learning_rate": 0.00010259179265658747,
      "loss": 0.0365,
      "step": 461
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.06497851014137268,
      "learning_rate": 0.00010237580993520519,
      "loss": 0.0332,
      "step": 462
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.24923889338970184,
      "learning_rate": 0.0001021598272138229,
      "loss": 0.2434,
      "step": 463
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.39703354239463806,
      "learning_rate": 0.00010194384449244061,
      "loss": 0.3476,
      "step": 464
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.3577578663825989,
      "learning_rate": 0.00010172786177105833,
      "loss": 0.3116,
      "step": 465
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.27886849641799927,
      "learning_rate": 0.00010151187904967602,
      "loss": 0.2956,
      "step": 466
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.2198692262172699,
      "learning_rate": 0.00010129589632829374,
      "loss": 0.2692,
      "step": 467
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.1740591824054718,
      "learning_rate": 0.00010107991360691145,
      "loss": 0.2519,
      "step": 468
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.15478554368019104,
      "learning_rate": 0.00010086393088552917,
      "loss": 0.2413,
      "step": 469
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.13286903500556946,
      "learning_rate": 0.00010064794816414686,
      "loss": 0.2235,
      "step": 470
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.1512717306613922,
      "learning_rate": 0.00010043196544276458,
      "loss": 0.2263,
      "step": 471
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.13422799110412598,
      "learning_rate": 0.00010021598272138229,
      "loss": 0.21,
      "step": 472
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.12860913574695587,
      "learning_rate": 0.0001,
      "loss": 0.207,
      "step": 473
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.1341039389371872,
      "learning_rate": 9.978401727861771e-05,
      "loss": 0.2048,
      "step": 474
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.14098693430423737,
      "learning_rate": 9.956803455723543e-05,
      "loss": 0.1876,
      "step": 475
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.1215980276465416,
      "learning_rate": 9.935205183585314e-05,
      "loss": 0.1781,
      "step": 476
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.1203489676117897,
      "learning_rate": 9.913606911447084e-05,
      "loss": 0.1746,
      "step": 477
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.1114560142159462,
      "learning_rate": 9.892008639308856e-05,
      "loss": 0.1712,
      "step": 478
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.11476129293441772,
      "learning_rate": 9.870410367170627e-05,
      "loss": 0.1703,
      "step": 479
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.11486128717660904,
      "learning_rate": 9.848812095032398e-05,
      "loss": 0.1586,
      "step": 480
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.1113382875919342,
      "learning_rate": 9.827213822894169e-05,
      "loss": 0.1634,
      "step": 481
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.11009319126605988,
      "learning_rate": 9.80561555075594e-05,
      "loss": 0.1523,
      "step": 482
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.11553367227315903,
      "learning_rate": 9.784017278617712e-05,
      "loss": 0.154,
      "step": 483
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.09562712907791138,
      "learning_rate": 9.762419006479482e-05,
      "loss": 0.1311,
      "step": 484
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.10335250943899155,
      "learning_rate": 9.740820734341253e-05,
      "loss": 0.1437,
      "step": 485
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.10609379410743713,
      "learning_rate": 9.719222462203023e-05,
      "loss": 0.1347,
      "step": 486
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.09954191744327545,
      "learning_rate": 9.697624190064795e-05,
      "loss": 0.1284,
      "step": 487
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.10272558778524399,
      "learning_rate": 9.676025917926567e-05,
      "loss": 0.1263,
      "step": 488
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.10160176455974579,
      "learning_rate": 9.654427645788338e-05,
      "loss": 0.1191,
      "step": 489
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.08763245493173599,
      "learning_rate": 9.632829373650108e-05,
      "loss": 0.1142,
      "step": 490
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.09454362094402313,
      "learning_rate": 9.61123110151188e-05,
      "loss": 0.1093,
      "step": 491
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.09002625197172165,
      "learning_rate": 9.58963282937365e-05,
      "loss": 0.1061,
      "step": 492
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.08534427732229233,
      "learning_rate": 9.568034557235421e-05,
      "loss": 0.103,
      "step": 493
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.08607471734285355,
      "learning_rate": 9.546436285097192e-05,
      "loss": 0.1115,
      "step": 494
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.08401267975568771,
      "learning_rate": 9.524838012958964e-05,
      "loss": 0.0964,
      "step": 495
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.07432910799980164,
      "learning_rate": 9.503239740820736e-05,
      "loss": 0.09,
      "step": 496
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.08111179620027542,
      "learning_rate": 9.481641468682506e-05,
      "loss": 0.0856,
      "step": 497
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.08575079590082169,
      "learning_rate": 9.460043196544277e-05,
      "loss": 0.0897,
      "step": 498
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.07825019210577011,
      "learning_rate": 9.438444924406049e-05,
      "loss": 0.0808,
      "step": 499
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.08180250227451324,
      "learning_rate": 9.416846652267819e-05,
      "loss": 0.0745,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 936,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.9264167507551846e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
