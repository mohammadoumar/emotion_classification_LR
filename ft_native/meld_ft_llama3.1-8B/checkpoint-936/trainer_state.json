{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.9952,
  "eval_steps": 188,
  "global_step": 936,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0032,
      "grad_norm": 0.2473185956478119,
      "learning_rate": 2e-05,
      "loss": 0.3716,
      "step": 1
    },
    {
      "epoch": 0.0064,
      "grad_norm": 0.19704203307628632,
      "learning_rate": 4e-05,
      "loss": 0.3007,
      "step": 2
    },
    {
      "epoch": 0.0096,
      "grad_norm": 0.1681884378194809,
      "learning_rate": 6e-05,
      "loss": 0.2768,
      "step": 3
    },
    {
      "epoch": 0.0128,
      "grad_norm": 0.15028852224349976,
      "learning_rate": 8e-05,
      "loss": 0.2785,
      "step": 4
    },
    {
      "epoch": 0.016,
      "grad_norm": 0.15018749237060547,
      "learning_rate": 0.0001,
      "loss": 0.2633,
      "step": 5
    },
    {
      "epoch": 0.0192,
      "grad_norm": 0.13611064851284027,
      "learning_rate": 0.00012,
      "loss": 0.2394,
      "step": 6
    },
    {
      "epoch": 0.0224,
      "grad_norm": 0.12946367263793945,
      "learning_rate": 0.00014,
      "loss": 0.2104,
      "step": 7
    },
    {
      "epoch": 0.0256,
      "grad_norm": 0.12480983138084412,
      "learning_rate": 0.00016,
      "loss": 0.2104,
      "step": 8
    },
    {
      "epoch": 0.0288,
      "grad_norm": 0.11112841218709946,
      "learning_rate": 0.00018,
      "loss": 0.1959,
      "step": 9
    },
    {
      "epoch": 0.032,
      "grad_norm": 0.12642277777194977,
      "learning_rate": 0.0002,
      "loss": 0.2286,
      "step": 10
    },
    {
      "epoch": 0.0352,
      "grad_norm": 0.09982245415449142,
      "learning_rate": 0.00019978401727861774,
      "loss": 0.1681,
      "step": 11
    },
    {
      "epoch": 0.0384,
      "grad_norm": 0.11517184227705002,
      "learning_rate": 0.00019956803455723542,
      "loss": 0.1859,
      "step": 12
    },
    {
      "epoch": 0.0416,
      "grad_norm": 0.11242034286260605,
      "learning_rate": 0.00019935205183585315,
      "loss": 0.1924,
      "step": 13
    },
    {
      "epoch": 0.0448,
      "grad_norm": 0.10396683216094971,
      "learning_rate": 0.00019913606911447086,
      "loss": 0.1672,
      "step": 14
    },
    {
      "epoch": 0.048,
      "grad_norm": 0.10824700444936752,
      "learning_rate": 0.00019892008639308856,
      "loss": 0.1782,
      "step": 15
    },
    {
      "epoch": 0.0512,
      "grad_norm": 0.10267966240644455,
      "learning_rate": 0.00019870410367170627,
      "loss": 0.1754,
      "step": 16
    },
    {
      "epoch": 0.0544,
      "grad_norm": 0.10092981904745102,
      "learning_rate": 0.00019848812095032398,
      "loss": 0.1474,
      "step": 17
    },
    {
      "epoch": 0.0576,
      "grad_norm": 0.09654837846755981,
      "learning_rate": 0.00019827213822894168,
      "loss": 0.1776,
      "step": 18
    },
    {
      "epoch": 0.0608,
      "grad_norm": 0.09118690341711044,
      "learning_rate": 0.00019805615550755941,
      "loss": 0.1518,
      "step": 19
    },
    {
      "epoch": 0.064,
      "grad_norm": 0.1038360595703125,
      "learning_rate": 0.00019784017278617712,
      "loss": 0.1451,
      "step": 20
    },
    {
      "epoch": 0.0672,
      "grad_norm": 0.09426870942115784,
      "learning_rate": 0.00019762419006479483,
      "loss": 0.1423,
      "step": 21
    },
    {
      "epoch": 0.0704,
      "grad_norm": 0.11023803800344467,
      "learning_rate": 0.00019740820734341253,
      "loss": 0.1413,
      "step": 22
    },
    {
      "epoch": 0.0736,
      "grad_norm": 0.10804465413093567,
      "learning_rate": 0.00019719222462203024,
      "loss": 0.1305,
      "step": 23
    },
    {
      "epoch": 0.0768,
      "grad_norm": 0.09621129930019379,
      "learning_rate": 0.00019697624190064797,
      "loss": 0.1369,
      "step": 24
    },
    {
      "epoch": 0.08,
      "grad_norm": 0.11130337417125702,
      "learning_rate": 0.00019676025917926565,
      "loss": 0.1437,
      "step": 25
    },
    {
      "epoch": 0.0832,
      "grad_norm": 0.07736033201217651,
      "learning_rate": 0.00019654427645788338,
      "loss": 0.1207,
      "step": 26
    },
    {
      "epoch": 0.0864,
      "grad_norm": 0.10844156891107559,
      "learning_rate": 0.0001963282937365011,
      "loss": 0.1199,
      "step": 27
    },
    {
      "epoch": 0.0896,
      "grad_norm": 0.09079986065626144,
      "learning_rate": 0.0001961123110151188,
      "loss": 0.1192,
      "step": 28
    },
    {
      "epoch": 0.0928,
      "grad_norm": 0.06796689331531525,
      "learning_rate": 0.00019589632829373652,
      "loss": 0.102,
      "step": 29
    },
    {
      "epoch": 0.096,
      "grad_norm": 0.08792231231927872,
      "learning_rate": 0.00019568034557235423,
      "loss": 0.1109,
      "step": 30
    },
    {
      "epoch": 0.0992,
      "grad_norm": 0.08039865642786026,
      "learning_rate": 0.00019546436285097194,
      "loss": 0.0948,
      "step": 31
    },
    {
      "epoch": 0.1024,
      "grad_norm": 0.08922789990901947,
      "learning_rate": 0.00019524838012958964,
      "loss": 0.0934,
      "step": 32
    },
    {
      "epoch": 0.1056,
      "grad_norm": 0.08575041592121124,
      "learning_rate": 0.00019503239740820735,
      "loss": 0.0972,
      "step": 33
    },
    {
      "epoch": 0.1088,
      "grad_norm": 0.061780139803886414,
      "learning_rate": 0.00019481641468682505,
      "loss": 0.0939,
      "step": 34
    },
    {
      "epoch": 0.112,
      "grad_norm": 0.07323917746543884,
      "learning_rate": 0.00019460043196544279,
      "loss": 0.0837,
      "step": 35
    },
    {
      "epoch": 0.1152,
      "grad_norm": 0.09208838641643524,
      "learning_rate": 0.00019438444924406046,
      "loss": 0.0859,
      "step": 36
    },
    {
      "epoch": 0.1184,
      "grad_norm": 0.09746457636356354,
      "learning_rate": 0.0001941684665226782,
      "loss": 0.0861,
      "step": 37
    },
    {
      "epoch": 0.1216,
      "grad_norm": 0.08083730190992355,
      "learning_rate": 0.0001939524838012959,
      "loss": 0.0774,
      "step": 38
    },
    {
      "epoch": 0.1248,
      "grad_norm": 0.07755818217992783,
      "learning_rate": 0.0001937365010799136,
      "loss": 0.0784,
      "step": 39
    },
    {
      "epoch": 0.128,
      "grad_norm": 0.07258199155330658,
      "learning_rate": 0.00019352051835853134,
      "loss": 0.0693,
      "step": 40
    },
    {
      "epoch": 0.1312,
      "grad_norm": 0.06922148913145065,
      "learning_rate": 0.00019330453563714902,
      "loss": 0.0719,
      "step": 41
    },
    {
      "epoch": 0.1344,
      "grad_norm": 0.08534162491559982,
      "learning_rate": 0.00019308855291576675,
      "loss": 0.0614,
      "step": 42
    },
    {
      "epoch": 0.1376,
      "grad_norm": 0.070091612637043,
      "learning_rate": 0.00019287257019438446,
      "loss": 0.0584,
      "step": 43
    },
    {
      "epoch": 0.1408,
      "grad_norm": 0.08243141323328018,
      "learning_rate": 0.00019265658747300216,
      "loss": 0.0528,
      "step": 44
    },
    {
      "epoch": 0.144,
      "grad_norm": 0.05600489303469658,
      "learning_rate": 0.0001924406047516199,
      "loss": 0.0526,
      "step": 45
    },
    {
      "epoch": 0.1472,
      "grad_norm": 0.06003555282950401,
      "learning_rate": 0.0001922246220302376,
      "loss": 0.0438,
      "step": 46
    },
    {
      "epoch": 0.1504,
      "grad_norm": 0.07310241460800171,
      "learning_rate": 0.0001920086393088553,
      "loss": 0.0386,
      "step": 47
    },
    {
      "epoch": 0.1536,
      "grad_norm": 0.10228697955608368,
      "learning_rate": 0.000191792656587473,
      "loss": 0.035,
      "step": 48
    },
    {
      "epoch": 0.1568,
      "grad_norm": 1.4498757123947144,
      "learning_rate": 0.00019157667386609072,
      "loss": 0.0353,
      "step": 49
    },
    {
      "epoch": 0.16,
      "grad_norm": 5.631339073181152,
      "learning_rate": 0.00019136069114470842,
      "loss": 0.2535,
      "step": 50
    },
    {
      "epoch": 0.1632,
      "grad_norm": 3.974756956100464,
      "learning_rate": 0.00019114470842332616,
      "loss": 0.6719,
      "step": 51
    },
    {
      "epoch": 0.1664,
      "grad_norm": 1.5104315280914307,
      "learning_rate": 0.00019092872570194384,
      "loss": 0.4445,
      "step": 52
    },
    {
      "epoch": 0.1696,
      "grad_norm": 3.19195818901062,
      "learning_rate": 0.00019071274298056157,
      "loss": 0.3947,
      "step": 53
    },
    {
      "epoch": 0.1728,
      "grad_norm": 0.6743426322937012,
      "learning_rate": 0.00019049676025917927,
      "loss": 0.3238,
      "step": 54
    },
    {
      "epoch": 0.176,
      "grad_norm": 2.4868826866149902,
      "learning_rate": 0.00019028077753779698,
      "loss": 0.3254,
      "step": 55
    },
    {
      "epoch": 0.1792,
      "grad_norm": 1.392816185951233,
      "learning_rate": 0.0001900647948164147,
      "loss": 0.2989,
      "step": 56
    },
    {
      "epoch": 0.1824,
      "grad_norm": 0.22468656301498413,
      "learning_rate": 0.0001898488120950324,
      "loss": 0.283,
      "step": 57
    },
    {
      "epoch": 0.1856,
      "grad_norm": 1.8186620473861694,
      "learning_rate": 0.00018963282937365012,
      "loss": 0.307,
      "step": 58
    },
    {
      "epoch": 0.1888,
      "grad_norm": 2.909487724304199,
      "learning_rate": 0.00018941684665226783,
      "loss": 0.279,
      "step": 59
    },
    {
      "epoch": 0.192,
      "grad_norm": 0.9266829490661621,
      "learning_rate": 0.00018920086393088553,
      "loss": 0.2515,
      "step": 60
    },
    {
      "epoch": 0.1952,
      "grad_norm": 0.1794801950454712,
      "learning_rate": 0.00018898488120950324,
      "loss": 0.2286,
      "step": 61
    },
    {
      "epoch": 0.1984,
      "grad_norm": 1.20906400680542,
      "learning_rate": 0.00018876889848812097,
      "loss": 0.2258,
      "step": 62
    },
    {
      "epoch": 0.2016,
      "grad_norm": 0.12893147766590118,
      "learning_rate": 0.00018855291576673868,
      "loss": 0.2198,
      "step": 63
    },
    {
      "epoch": 0.2048,
      "grad_norm": 0.1524917334318161,
      "learning_rate": 0.00018833693304535638,
      "loss": 0.201,
      "step": 64
    },
    {
      "epoch": 0.208,
      "grad_norm": 0.5775185227394104,
      "learning_rate": 0.0001881209503239741,
      "loss": 0.2012,
      "step": 65
    },
    {
      "epoch": 0.2112,
      "grad_norm": 0.15016920864582062,
      "learning_rate": 0.0001879049676025918,
      "loss": 0.1906,
      "step": 66
    },
    {
      "epoch": 0.2144,
      "grad_norm": 0.11539745330810547,
      "learning_rate": 0.00018768898488120953,
      "loss": 0.1837,
      "step": 67
    },
    {
      "epoch": 0.2176,
      "grad_norm": 0.10456038266420364,
      "learning_rate": 0.0001874730021598272,
      "loss": 0.1868,
      "step": 68
    },
    {
      "epoch": 0.2208,
      "grad_norm": 0.09295433014631271,
      "learning_rate": 0.00018725701943844494,
      "loss": 0.1645,
      "step": 69
    },
    {
      "epoch": 0.224,
      "grad_norm": 0.09027903527021408,
      "learning_rate": 0.00018704103671706265,
      "loss": 0.1605,
      "step": 70
    },
    {
      "epoch": 0.2272,
      "grad_norm": 0.09788890928030014,
      "learning_rate": 0.00018682505399568035,
      "loss": 0.163,
      "step": 71
    },
    {
      "epoch": 0.2304,
      "grad_norm": 0.095504030585289,
      "learning_rate": 0.00018660907127429808,
      "loss": 0.1469,
      "step": 72
    },
    {
      "epoch": 0.2336,
      "grad_norm": 0.10050319135189056,
      "learning_rate": 0.00018639308855291576,
      "loss": 0.1511,
      "step": 73
    },
    {
      "epoch": 0.2368,
      "grad_norm": 0.09039634466171265,
      "learning_rate": 0.0001861771058315335,
      "loss": 0.1298,
      "step": 74
    },
    {
      "epoch": 0.24,
      "grad_norm": 0.10619691759347916,
      "learning_rate": 0.0001859611231101512,
      "loss": 0.1396,
      "step": 75
    },
    {
      "epoch": 0.2432,
      "grad_norm": 0.08168186992406845,
      "learning_rate": 0.0001857451403887689,
      "loss": 0.1289,
      "step": 76
    },
    {
      "epoch": 0.2464,
      "grad_norm": 0.0853278636932373,
      "learning_rate": 0.0001855291576673866,
      "loss": 0.121,
      "step": 77
    },
    {
      "epoch": 0.2496,
      "grad_norm": 0.08455134928226471,
      "learning_rate": 0.00018531317494600432,
      "loss": 0.1272,
      "step": 78
    },
    {
      "epoch": 0.2528,
      "grad_norm": 0.09596329182386398,
      "learning_rate": 0.00018509719222462202,
      "loss": 0.1106,
      "step": 79
    },
    {
      "epoch": 0.256,
      "grad_norm": 0.07441899925470352,
      "learning_rate": 0.00018488120950323976,
      "loss": 0.1017,
      "step": 80
    },
    {
      "epoch": 0.2592,
      "grad_norm": 0.07067946344614029,
      "learning_rate": 0.00018466522678185746,
      "loss": 0.1099,
      "step": 81
    },
    {
      "epoch": 0.2624,
      "grad_norm": 0.07373460382223129,
      "learning_rate": 0.00018444924406047517,
      "loss": 0.1051,
      "step": 82
    },
    {
      "epoch": 0.2656,
      "grad_norm": 0.08122047036886215,
      "learning_rate": 0.0001842332613390929,
      "loss": 0.1055,
      "step": 83
    },
    {
      "epoch": 0.2688,
      "grad_norm": 0.06536902487277985,
      "learning_rate": 0.00018401727861771058,
      "loss": 0.0974,
      "step": 84
    },
    {
      "epoch": 0.272,
      "grad_norm": 0.06507558375597,
      "learning_rate": 0.0001838012958963283,
      "loss": 0.1017,
      "step": 85
    },
    {
      "epoch": 0.2752,
      "grad_norm": 0.06138524413108826,
      "learning_rate": 0.00018358531317494602,
      "loss": 0.0799,
      "step": 86
    },
    {
      "epoch": 0.2784,
      "grad_norm": 0.0597354881465435,
      "learning_rate": 0.00018336933045356372,
      "loss": 0.0841,
      "step": 87
    },
    {
      "epoch": 0.2816,
      "grad_norm": 0.06616238504648209,
      "learning_rate": 0.00018315334773218143,
      "loss": 0.0799,
      "step": 88
    },
    {
      "epoch": 0.2848,
      "grad_norm": 0.07355864346027374,
      "learning_rate": 0.00018293736501079913,
      "loss": 0.0757,
      "step": 89
    },
    {
      "epoch": 0.288,
      "grad_norm": 0.059434302151203156,
      "learning_rate": 0.00018272138228941687,
      "loss": 0.0801,
      "step": 90
    },
    {
      "epoch": 0.2912,
      "grad_norm": 0.05746084451675415,
      "learning_rate": 0.00018250539956803457,
      "loss": 0.065,
      "step": 91
    },
    {
      "epoch": 0.2944,
      "grad_norm": 0.06596305966377258,
      "learning_rate": 0.00018228941684665228,
      "loss": 0.0729,
      "step": 92
    },
    {
      "epoch": 0.2976,
      "grad_norm": 0.06921606510877609,
      "learning_rate": 0.00018207343412526998,
      "loss": 0.0639,
      "step": 93
    },
    {
      "epoch": 0.3008,
      "grad_norm": 0.06688746809959412,
      "learning_rate": 0.0001818574514038877,
      "loss": 0.0545,
      "step": 94
    },
    {
      "epoch": 0.304,
      "grad_norm": 0.06817151606082916,
      "learning_rate": 0.0001816414686825054,
      "loss": 0.0505,
      "step": 95
    },
    {
      "epoch": 0.3072,
      "grad_norm": 0.060189444571733475,
      "learning_rate": 0.00018142548596112313,
      "loss": 0.0459,
      "step": 96
    },
    {
      "epoch": 0.3104,
      "grad_norm": 0.09709952026605606,
      "learning_rate": 0.0001812095032397408,
      "loss": 0.0409,
      "step": 97
    },
    {
      "epoch": 0.3136,
      "grad_norm": 0.09310758858919144,
      "learning_rate": 0.00018099352051835854,
      "loss": 0.0357,
      "step": 98
    },
    {
      "epoch": 0.3168,
      "grad_norm": 0.05933742597699165,
      "learning_rate": 0.00018077753779697627,
      "loss": 0.0327,
      "step": 99
    },
    {
      "epoch": 0.32,
      "grad_norm": 0.06387529522180557,
      "learning_rate": 0.00018056155507559395,
      "loss": 0.0342,
      "step": 100
    },
    {
      "epoch": 0.3232,
      "grad_norm": 1.8403183221817017,
      "learning_rate": 0.00018034557235421168,
      "loss": 0.5741,
      "step": 101
    },
    {
      "epoch": 0.3264,
      "grad_norm": 0.8151398301124573,
      "learning_rate": 0.0001801295896328294,
      "loss": 0.4198,
      "step": 102
    },
    {
      "epoch": 0.3296,
      "grad_norm": 0.3821924030780792,
      "learning_rate": 0.0001799136069114471,
      "loss": 0.3347,
      "step": 103
    },
    {
      "epoch": 0.3328,
      "grad_norm": 0.18419210612773895,
      "learning_rate": 0.0001796976241900648,
      "loss": 0.2783,
      "step": 104
    },
    {
      "epoch": 0.336,
      "grad_norm": 0.1795380711555481,
      "learning_rate": 0.0001794816414686825,
      "loss": 0.2823,
      "step": 105
    },
    {
      "epoch": 0.3392,
      "grad_norm": 0.2535008192062378,
      "learning_rate": 0.0001792656587473002,
      "loss": 0.2786,
      "step": 106
    },
    {
      "epoch": 0.3424,
      "grad_norm": 0.21778462827205658,
      "learning_rate": 0.00017904967602591794,
      "loss": 0.2487,
      "step": 107
    },
    {
      "epoch": 0.3456,
      "grad_norm": 0.19404543936252594,
      "learning_rate": 0.00017883369330453565,
      "loss": 0.251,
      "step": 108
    },
    {
      "epoch": 0.3488,
      "grad_norm": 0.167414128780365,
      "learning_rate": 0.00017861771058315335,
      "loss": 0.2482,
      "step": 109
    },
    {
      "epoch": 0.352,
      "grad_norm": 0.12931066751480103,
      "learning_rate": 0.00017840172786177106,
      "loss": 0.2219,
      "step": 110
    },
    {
      "epoch": 0.3552,
      "grad_norm": 0.1537570059299469,
      "learning_rate": 0.00017818574514038877,
      "loss": 0.2158,
      "step": 111
    },
    {
      "epoch": 0.3584,
      "grad_norm": 0.15754641592502594,
      "learning_rate": 0.0001779697624190065,
      "loss": 0.2014,
      "step": 112
    },
    {
      "epoch": 0.3616,
      "grad_norm": 0.14788292348384857,
      "learning_rate": 0.00017775377969762418,
      "loss": 0.2147,
      "step": 113
    },
    {
      "epoch": 0.3648,
      "grad_norm": 0.15218223631381989,
      "learning_rate": 0.0001775377969762419,
      "loss": 0.186,
      "step": 114
    },
    {
      "epoch": 0.368,
      "grad_norm": 0.13024581968784332,
      "learning_rate": 0.00017732181425485964,
      "loss": 0.1876,
      "step": 115
    },
    {
      "epoch": 0.3712,
      "grad_norm": 0.10386557877063751,
      "learning_rate": 0.00017710583153347732,
      "loss": 0.1745,
      "step": 116
    },
    {
      "epoch": 0.3744,
      "grad_norm": 0.09104938805103302,
      "learning_rate": 0.00017688984881209505,
      "loss": 0.1737,
      "step": 117
    },
    {
      "epoch": 0.3776,
      "grad_norm": 0.11098423600196838,
      "learning_rate": 0.00017667386609071276,
      "loss": 0.1704,
      "step": 118
    },
    {
      "epoch": 0.3808,
      "grad_norm": 0.1334501951932907,
      "learning_rate": 0.00017645788336933046,
      "loss": 0.1493,
      "step": 119
    },
    {
      "epoch": 0.384,
      "grad_norm": 0.1114867702126503,
      "learning_rate": 0.00017624190064794817,
      "loss": 0.1448,
      "step": 120
    },
    {
      "epoch": 0.3872,
      "grad_norm": 0.11684813350439072,
      "learning_rate": 0.00017602591792656588,
      "loss": 0.1403,
      "step": 121
    },
    {
      "epoch": 0.3904,
      "grad_norm": 0.12127739936113358,
      "learning_rate": 0.00017580993520518358,
      "loss": 0.1396,
      "step": 122
    },
    {
      "epoch": 0.3936,
      "grad_norm": 0.09601961076259613,
      "learning_rate": 0.00017559395248380131,
      "loss": 0.1511,
      "step": 123
    },
    {
      "epoch": 0.3968,
      "grad_norm": 0.0876307561993599,
      "learning_rate": 0.00017537796976241902,
      "loss": 0.1253,
      "step": 124
    },
    {
      "epoch": 0.4,
      "grad_norm": 0.07560454308986664,
      "learning_rate": 0.00017516198704103673,
      "loss": 0.1288,
      "step": 125
    },
    {
      "epoch": 0.4032,
      "grad_norm": 0.08914800733327866,
      "learning_rate": 0.00017494600431965443,
      "loss": 0.1262,
      "step": 126
    },
    {
      "epoch": 0.4064,
      "grad_norm": 0.07434625923633575,
      "learning_rate": 0.00017473002159827214,
      "loss": 0.1216,
      "step": 127
    },
    {
      "epoch": 0.4096,
      "grad_norm": 0.08822549134492874,
      "learning_rate": 0.00017451403887688987,
      "loss": 0.1177,
      "step": 128
    },
    {
      "epoch": 0.4128,
      "grad_norm": 0.07859709113836288,
      "learning_rate": 0.00017429805615550755,
      "loss": 0.1161,
      "step": 129
    },
    {
      "epoch": 0.416,
      "grad_norm": 0.08584029227495193,
      "learning_rate": 0.00017408207343412528,
      "loss": 0.1225,
      "step": 130
    },
    {
      "epoch": 0.4192,
      "grad_norm": 0.0720861479640007,
      "learning_rate": 0.000173866090712743,
      "loss": 0.1061,
      "step": 131
    },
    {
      "epoch": 0.4224,
      "grad_norm": 0.0756726935505867,
      "learning_rate": 0.0001736501079913607,
      "loss": 0.1032,
      "step": 132
    },
    {
      "epoch": 0.4256,
      "grad_norm": 0.07314030826091766,
      "learning_rate": 0.00017343412526997842,
      "loss": 0.1041,
      "step": 133
    },
    {
      "epoch": 0.4288,
      "grad_norm": 0.06986541301012039,
      "learning_rate": 0.00017321814254859613,
      "loss": 0.093,
      "step": 134
    },
    {
      "epoch": 0.432,
      "grad_norm": 0.08515734225511551,
      "learning_rate": 0.00017300215982721384,
      "loss": 0.0877,
      "step": 135
    },
    {
      "epoch": 0.4352,
      "grad_norm": 0.06462622433900833,
      "learning_rate": 0.00017278617710583154,
      "loss": 0.0913,
      "step": 136
    },
    {
      "epoch": 0.4384,
      "grad_norm": 0.08543089032173157,
      "learning_rate": 0.00017257019438444925,
      "loss": 0.0833,
      "step": 137
    },
    {
      "epoch": 0.4416,
      "grad_norm": 0.0615629181265831,
      "learning_rate": 0.00017235421166306695,
      "loss": 0.0747,
      "step": 138
    },
    {
      "epoch": 0.4448,
      "grad_norm": 0.06748052686452866,
      "learning_rate": 0.00017213822894168469,
      "loss": 0.0732,
      "step": 139
    },
    {
      "epoch": 0.448,
      "grad_norm": 0.07206647843122482,
      "learning_rate": 0.00017192224622030236,
      "loss": 0.0668,
      "step": 140
    },
    {
      "epoch": 0.4512,
      "grad_norm": 0.05431283637881279,
      "learning_rate": 0.0001717062634989201,
      "loss": 0.0635,
      "step": 141
    },
    {
      "epoch": 0.4544,
      "grad_norm": 0.0703502893447876,
      "learning_rate": 0.0001714902807775378,
      "loss": 0.068,
      "step": 142
    },
    {
      "epoch": 0.4576,
      "grad_norm": 0.06487338989973068,
      "learning_rate": 0.0001712742980561555,
      "loss": 0.0632,
      "step": 143
    },
    {
      "epoch": 0.4608,
      "grad_norm": 0.07282714545726776,
      "learning_rate": 0.00017105831533477324,
      "loss": 0.048,
      "step": 144
    },
    {
      "epoch": 0.464,
      "grad_norm": 0.07322162389755249,
      "learning_rate": 0.00017084233261339092,
      "loss": 0.0498,
      "step": 145
    },
    {
      "epoch": 0.4672,
      "grad_norm": 0.06104642525315285,
      "learning_rate": 0.00017062634989200865,
      "loss": 0.0458,
      "step": 146
    },
    {
      "epoch": 0.4704,
      "grad_norm": 0.12298593670129776,
      "learning_rate": 0.00017041036717062636,
      "loss": 0.0412,
      "step": 147
    },
    {
      "epoch": 0.4736,
      "grad_norm": 0.09498516470193863,
      "learning_rate": 0.00017019438444924406,
      "loss": 0.036,
      "step": 148
    },
    {
      "epoch": 0.4768,
      "grad_norm": 0.07625502347946167,
      "learning_rate": 0.00016997840172786177,
      "loss": 0.0344,
      "step": 149
    },
    {
      "epoch": 0.48,
      "grad_norm": 0.06744737923145294,
      "learning_rate": 0.0001697624190064795,
      "loss": 0.0401,
      "step": 150
    },
    {
      "epoch": 0.4832,
      "grad_norm": 2.582364082336426,
      "learning_rate": 0.0001695464362850972,
      "loss": 0.6235,
      "step": 151
    },
    {
      "epoch": 0.4864,
      "grad_norm": 1.1677980422973633,
      "learning_rate": 0.0001693304535637149,
      "loss": 0.4514,
      "step": 152
    },
    {
      "epoch": 0.4896,
      "grad_norm": 0.5365772843360901,
      "learning_rate": 0.00016911447084233262,
      "loss": 0.3403,
      "step": 153
    },
    {
      "epoch": 0.4928,
      "grad_norm": 0.2059655338525772,
      "learning_rate": 0.00016889848812095032,
      "loss": 0.2937,
      "step": 154
    },
    {
      "epoch": 0.496,
      "grad_norm": 0.14388377964496613,
      "learning_rate": 0.00016868250539956806,
      "loss": 0.2697,
      "step": 155
    },
    {
      "epoch": 0.4992,
      "grad_norm": 0.30698126554489136,
      "learning_rate": 0.00016846652267818574,
      "loss": 0.2642,
      "step": 156
    },
    {
      "epoch": 0.5024,
      "grad_norm": 0.22217732667922974,
      "learning_rate": 0.00016825053995680347,
      "loss": 0.247,
      "step": 157
    },
    {
      "epoch": 0.5056,
      "grad_norm": 0.23607200384140015,
      "learning_rate": 0.00016803455723542117,
      "loss": 0.2527,
      "step": 158
    },
    {
      "epoch": 0.5088,
      "grad_norm": 0.184879332780838,
      "learning_rate": 0.00016781857451403888,
      "loss": 0.2492,
      "step": 159
    },
    {
      "epoch": 0.512,
      "grad_norm": 0.16862154006958008,
      "learning_rate": 0.0001676025917926566,
      "loss": 0.2328,
      "step": 160
    },
    {
      "epoch": 0.5152,
      "grad_norm": 0.16483110189437866,
      "learning_rate": 0.0001673866090712743,
      "loss": 0.2149,
      "step": 161
    },
    {
      "epoch": 0.5184,
      "grad_norm": 0.16620157659053802,
      "learning_rate": 0.00016717062634989202,
      "loss": 0.2288,
      "step": 162
    },
    {
      "epoch": 0.5216,
      "grad_norm": 0.17595551908016205,
      "learning_rate": 0.00016695464362850973,
      "loss": 0.1984,
      "step": 163
    },
    {
      "epoch": 0.5248,
      "grad_norm": 0.11563430726528168,
      "learning_rate": 0.00016673866090712743,
      "loss": 0.1843,
      "step": 164
    },
    {
      "epoch": 0.528,
      "grad_norm": 0.10846944898366928,
      "learning_rate": 0.00016652267818574514,
      "loss": 0.167,
      "step": 165
    },
    {
      "epoch": 0.5312,
      "grad_norm": 0.1084708422422409,
      "learning_rate": 0.00016630669546436287,
      "loss": 0.1781,
      "step": 166
    },
    {
      "epoch": 0.5344,
      "grad_norm": 0.0964418351650238,
      "learning_rate": 0.00016609071274298055,
      "loss": 0.169,
      "step": 167
    },
    {
      "epoch": 0.5376,
      "grad_norm": 0.09275882691144943,
      "learning_rate": 0.00016587473002159828,
      "loss": 0.1631,
      "step": 168
    },
    {
      "epoch": 0.5408,
      "grad_norm": 0.08877965062856674,
      "learning_rate": 0.000165658747300216,
      "loss": 0.1597,
      "step": 169
    },
    {
      "epoch": 0.544,
      "grad_norm": 0.10385245829820633,
      "learning_rate": 0.0001654427645788337,
      "loss": 0.1621,
      "step": 170
    },
    {
      "epoch": 0.5472,
      "grad_norm": 0.09570787847042084,
      "learning_rate": 0.00016522678185745143,
      "loss": 0.1553,
      "step": 171
    },
    {
      "epoch": 0.5504,
      "grad_norm": 0.09157850593328476,
      "learning_rate": 0.0001650107991360691,
      "loss": 0.1397,
      "step": 172
    },
    {
      "epoch": 0.5536,
      "grad_norm": 0.09800048172473907,
      "learning_rate": 0.00016479481641468684,
      "loss": 0.1498,
      "step": 173
    },
    {
      "epoch": 0.5568,
      "grad_norm": 0.10191880166530609,
      "learning_rate": 0.00016457883369330455,
      "loss": 0.1337,
      "step": 174
    },
    {
      "epoch": 0.56,
      "grad_norm": 0.08677957952022552,
      "learning_rate": 0.00016436285097192225,
      "loss": 0.1373,
      "step": 175
    },
    {
      "epoch": 0.5632,
      "grad_norm": 0.11023259162902832,
      "learning_rate": 0.00016414686825053998,
      "loss": 0.1378,
      "step": 176
    },
    {
      "epoch": 0.5664,
      "grad_norm": 0.08356870710849762,
      "learning_rate": 0.00016393088552915766,
      "loss": 0.1143,
      "step": 177
    },
    {
      "epoch": 0.5696,
      "grad_norm": 0.30474942922592163,
      "learning_rate": 0.0001637149028077754,
      "loss": 0.1226,
      "step": 178
    },
    {
      "epoch": 0.5728,
      "grad_norm": 0.11169648170471191,
      "learning_rate": 0.0001634989200863931,
      "loss": 0.1229,
      "step": 179
    },
    {
      "epoch": 0.576,
      "grad_norm": 0.12769091129302979,
      "learning_rate": 0.0001632829373650108,
      "loss": 0.1226,
      "step": 180
    },
    {
      "epoch": 0.5792,
      "grad_norm": 0.07679124176502228,
      "learning_rate": 0.0001630669546436285,
      "loss": 0.1041,
      "step": 181
    },
    {
      "epoch": 0.5824,
      "grad_norm": 0.0922919362783432,
      "learning_rate": 0.00016285097192224624,
      "loss": 0.1163,
      "step": 182
    },
    {
      "epoch": 0.5856,
      "grad_norm": 0.09230438619852066,
      "learning_rate": 0.00016263498920086392,
      "loss": 0.1106,
      "step": 183
    },
    {
      "epoch": 0.5888,
      "grad_norm": 0.08038726449012756,
      "learning_rate": 0.00016241900647948166,
      "loss": 0.0965,
      "step": 184
    },
    {
      "epoch": 0.592,
      "grad_norm": 0.09995442628860474,
      "learning_rate": 0.00016220302375809936,
      "loss": 0.0914,
      "step": 185
    },
    {
      "epoch": 0.5952,
      "grad_norm": 0.07371248304843903,
      "learning_rate": 0.00016198704103671707,
      "loss": 0.0814,
      "step": 186
    },
    {
      "epoch": 0.5984,
      "grad_norm": 0.08044587075710297,
      "learning_rate": 0.0001617710583153348,
      "loss": 0.0794,
      "step": 187
    },
    {
      "epoch": 0.6016,
      "grad_norm": 0.06729457527399063,
      "learning_rate": 0.00016155507559395248,
      "loss": 0.0789,
      "step": 188
    },
    {
      "epoch": 0.6016,
      "eval_loss": 0.15358756482601166,
      "eval_runtime": 100.1446,
      "eval_samples_per_second": 11.074,
      "eval_steps_per_second": 0.699,
      "step": 188
    },
    {
      "epoch": 0.6048,
      "grad_norm": 0.10994323343038559,
      "learning_rate": 0.0001613390928725702,
      "loss": 0.0799,
      "step": 189
    },
    {
      "epoch": 0.608,
      "grad_norm": 0.08133586496114731,
      "learning_rate": 0.00016112311015118792,
      "loss": 0.0783,
      "step": 190
    },
    {
      "epoch": 0.6112,
      "grad_norm": 0.07946256548166275,
      "learning_rate": 0.00016090712742980562,
      "loss": 0.0802,
      "step": 191
    },
    {
      "epoch": 0.6144,
      "grad_norm": 0.0951666310429573,
      "learning_rate": 0.00016069114470842333,
      "loss": 0.0714,
      "step": 192
    },
    {
      "epoch": 0.6176,
      "grad_norm": 0.06924145668745041,
      "learning_rate": 0.00016047516198704103,
      "loss": 0.0648,
      "step": 193
    },
    {
      "epoch": 0.6208,
      "grad_norm": 0.07558314502239227,
      "learning_rate": 0.00016025917926565877,
      "loss": 0.0525,
      "step": 194
    },
    {
      "epoch": 0.624,
      "grad_norm": 0.07862892746925354,
      "learning_rate": 0.00016004319654427647,
      "loss": 0.0456,
      "step": 195
    },
    {
      "epoch": 0.6272,
      "grad_norm": 0.07655524462461472,
      "learning_rate": 0.00015982721382289418,
      "loss": 0.0516,
      "step": 196
    },
    {
      "epoch": 0.6304,
      "grad_norm": 0.11946225166320801,
      "learning_rate": 0.00015961123110151188,
      "loss": 0.0426,
      "step": 197
    },
    {
      "epoch": 0.6336,
      "grad_norm": 0.13277865946292877,
      "learning_rate": 0.00015939524838012962,
      "loss": 0.0378,
      "step": 198
    },
    {
      "epoch": 0.6368,
      "grad_norm": 0.07876647263765335,
      "learning_rate": 0.0001591792656587473,
      "loss": 0.031,
      "step": 199
    },
    {
      "epoch": 0.64,
      "grad_norm": 0.08112156391143799,
      "learning_rate": 0.00015896328293736503,
      "loss": 0.0456,
      "step": 200
    },
    {
      "epoch": 0.6432,
      "grad_norm": 1.5295963287353516,
      "learning_rate": 0.00015874730021598273,
      "loss": 0.4905,
      "step": 201
    },
    {
      "epoch": 0.6464,
      "grad_norm": 0.8346420526504517,
      "learning_rate": 0.00015853131749460044,
      "loss": 0.4233,
      "step": 202
    },
    {
      "epoch": 0.6496,
      "grad_norm": 0.4731906056404114,
      "learning_rate": 0.00015831533477321817,
      "loss": 0.3467,
      "step": 203
    },
    {
      "epoch": 0.6528,
      "grad_norm": 0.25568652153015137,
      "learning_rate": 0.00015809935205183585,
      "loss": 0.3207,
      "step": 204
    },
    {
      "epoch": 0.656,
      "grad_norm": 0.16455748677253723,
      "learning_rate": 0.00015788336933045358,
      "loss": 0.2663,
      "step": 205
    },
    {
      "epoch": 0.6592,
      "grad_norm": 0.21264716982841492,
      "learning_rate": 0.0001576673866090713,
      "loss": 0.2759,
      "step": 206
    },
    {
      "epoch": 0.6624,
      "grad_norm": 0.22452408075332642,
      "learning_rate": 0.000157451403887689,
      "loss": 0.2639,
      "step": 207
    },
    {
      "epoch": 0.6656,
      "grad_norm": 0.25361788272857666,
      "learning_rate": 0.0001572354211663067,
      "loss": 0.2481,
      "step": 208
    },
    {
      "epoch": 0.6688,
      "grad_norm": 0.1893545240163803,
      "learning_rate": 0.0001570194384449244,
      "loss": 0.2265,
      "step": 209
    },
    {
      "epoch": 0.672,
      "grad_norm": 0.18016454577445984,
      "learning_rate": 0.0001568034557235421,
      "loss": 0.2215,
      "step": 210
    },
    {
      "epoch": 0.6752,
      "grad_norm": 0.14851114153862,
      "learning_rate": 0.00015658747300215984,
      "loss": 0.2237,
      "step": 211
    },
    {
      "epoch": 0.6784,
      "grad_norm": 0.14893892407417297,
      "learning_rate": 0.00015637149028077755,
      "loss": 0.2033,
      "step": 212
    },
    {
      "epoch": 0.6816,
      "grad_norm": 0.1302412748336792,
      "learning_rate": 0.00015615550755939525,
      "loss": 0.1924,
      "step": 213
    },
    {
      "epoch": 0.6848,
      "grad_norm": 0.122309610247612,
      "learning_rate": 0.00015593952483801296,
      "loss": 0.195,
      "step": 214
    },
    {
      "epoch": 0.688,
      "grad_norm": 0.11879032850265503,
      "learning_rate": 0.00015572354211663067,
      "loss": 0.1765,
      "step": 215
    },
    {
      "epoch": 0.6912,
      "grad_norm": 0.0962909534573555,
      "learning_rate": 0.0001555075593952484,
      "loss": 0.1713,
      "step": 216
    },
    {
      "epoch": 0.6944,
      "grad_norm": 0.09997446089982986,
      "learning_rate": 0.00015529157667386608,
      "loss": 0.1647,
      "step": 217
    },
    {
      "epoch": 0.6976,
      "grad_norm": 0.08514430373907089,
      "learning_rate": 0.0001550755939524838,
      "loss": 0.1639,
      "step": 218
    },
    {
      "epoch": 0.7008,
      "grad_norm": 0.09083016216754913,
      "learning_rate": 0.00015485961123110152,
      "loss": 0.1544,
      "step": 219
    },
    {
      "epoch": 0.704,
      "grad_norm": 0.07983364164829254,
      "learning_rate": 0.00015464362850971922,
      "loss": 0.1484,
      "step": 220
    },
    {
      "epoch": 0.7072,
      "grad_norm": 0.0770895928144455,
      "learning_rate": 0.00015442764578833695,
      "loss": 0.1308,
      "step": 221
    },
    {
      "epoch": 0.7104,
      "grad_norm": 0.09385045617818832,
      "learning_rate": 0.00015421166306695466,
      "loss": 0.147,
      "step": 222
    },
    {
      "epoch": 0.7136,
      "grad_norm": 0.09331627190113068,
      "learning_rate": 0.00015399568034557237,
      "loss": 0.1234,
      "step": 223
    },
    {
      "epoch": 0.7168,
      "grad_norm": 0.11082423478364944,
      "learning_rate": 0.00015377969762419007,
      "loss": 0.1421,
      "step": 224
    },
    {
      "epoch": 0.72,
      "grad_norm": 0.10347947478294373,
      "learning_rate": 0.00015356371490280778,
      "loss": 0.1327,
      "step": 225
    },
    {
      "epoch": 0.7232,
      "grad_norm": 0.08602286875247955,
      "learning_rate": 0.00015334773218142548,
      "loss": 0.1158,
      "step": 226
    },
    {
      "epoch": 0.7264,
      "grad_norm": 0.0751379057765007,
      "learning_rate": 0.00015313174946004321,
      "loss": 0.1114,
      "step": 227
    },
    {
      "epoch": 0.7296,
      "grad_norm": 0.08718612790107727,
      "learning_rate": 0.0001529157667386609,
      "loss": 0.1259,
      "step": 228
    },
    {
      "epoch": 0.7328,
      "grad_norm": 0.09808294475078583,
      "learning_rate": 0.00015269978401727863,
      "loss": 0.1086,
      "step": 229
    },
    {
      "epoch": 0.736,
      "grad_norm": 0.08569410443305969,
      "learning_rate": 0.00015248380129589633,
      "loss": 0.1132,
      "step": 230
    },
    {
      "epoch": 0.7392,
      "grad_norm": 0.07641404867172241,
      "learning_rate": 0.00015226781857451404,
      "loss": 0.098,
      "step": 231
    },
    {
      "epoch": 0.7424,
      "grad_norm": 0.07934756577014923,
      "learning_rate": 0.00015205183585313177,
      "loss": 0.0977,
      "step": 232
    },
    {
      "epoch": 0.7456,
      "grad_norm": 0.0751657709479332,
      "learning_rate": 0.00015183585313174945,
      "loss": 0.1039,
      "step": 233
    },
    {
      "epoch": 0.7488,
      "grad_norm": 0.07843207567930222,
      "learning_rate": 0.00015161987041036718,
      "loss": 0.0844,
      "step": 234
    },
    {
      "epoch": 0.752,
      "grad_norm": 0.08769173175096512,
      "learning_rate": 0.0001514038876889849,
      "loss": 0.0866,
      "step": 235
    },
    {
      "epoch": 0.7552,
      "grad_norm": 0.0678262934088707,
      "learning_rate": 0.0001511879049676026,
      "loss": 0.0872,
      "step": 236
    },
    {
      "epoch": 0.7584,
      "grad_norm": 0.07057864964008331,
      "learning_rate": 0.0001509719222462203,
      "loss": 0.0758,
      "step": 237
    },
    {
      "epoch": 0.7616,
      "grad_norm": 0.08254123479127884,
      "learning_rate": 0.00015075593952483803,
      "loss": 0.0752,
      "step": 238
    },
    {
      "epoch": 0.7648,
      "grad_norm": 0.09618472307920456,
      "learning_rate": 0.00015053995680345574,
      "loss": 0.083,
      "step": 239
    },
    {
      "epoch": 0.768,
      "grad_norm": 0.07441812008619308,
      "learning_rate": 0.00015032397408207344,
      "loss": 0.0675,
      "step": 240
    },
    {
      "epoch": 0.7712,
      "grad_norm": 0.0733884796500206,
      "learning_rate": 0.00015010799136069115,
      "loss": 0.0701,
      "step": 241
    },
    {
      "epoch": 0.7744,
      "grad_norm": 0.0727105364203453,
      "learning_rate": 0.00014989200863930885,
      "loss": 0.0684,
      "step": 242
    },
    {
      "epoch": 0.7776,
      "grad_norm": 0.08703511208295822,
      "learning_rate": 0.00014967602591792659,
      "loss": 0.0711,
      "step": 243
    },
    {
      "epoch": 0.7808,
      "grad_norm": 0.0790431946516037,
      "learning_rate": 0.00014946004319654426,
      "loss": 0.0562,
      "step": 244
    },
    {
      "epoch": 0.784,
      "grad_norm": 0.07802541553974152,
      "learning_rate": 0.000149244060475162,
      "loss": 0.049,
      "step": 245
    },
    {
      "epoch": 0.7872,
      "grad_norm": 0.08394356817007065,
      "learning_rate": 0.0001490280777537797,
      "loss": 0.0476,
      "step": 246
    },
    {
      "epoch": 0.7904,
      "grad_norm": 0.08037304133176804,
      "learning_rate": 0.0001488120950323974,
      "loss": 0.0472,
      "step": 247
    },
    {
      "epoch": 0.7936,
      "grad_norm": 0.10269023478031158,
      "learning_rate": 0.00014859611231101514,
      "loss": 0.0422,
      "step": 248
    },
    {
      "epoch": 0.7968,
      "grad_norm": 0.07242658734321594,
      "learning_rate": 0.00014838012958963282,
      "loss": 0.0394,
      "step": 249
    },
    {
      "epoch": 0.8,
      "grad_norm": 0.07327985763549805,
      "learning_rate": 0.00014816414686825055,
      "loss": 0.0414,
      "step": 250
    },
    {
      "epoch": 0.8032,
      "grad_norm": 0.674603283405304,
      "learning_rate": 0.00014794816414686826,
      "loss": 0.4243,
      "step": 251
    },
    {
      "epoch": 0.8064,
      "grad_norm": 0.5341923236846924,
      "learning_rate": 0.00014773218142548596,
      "loss": 0.3671,
      "step": 252
    },
    {
      "epoch": 0.8096,
      "grad_norm": 0.39013245701789856,
      "learning_rate": 0.00014751619870410367,
      "loss": 0.3289,
      "step": 253
    },
    {
      "epoch": 0.8128,
      "grad_norm": 0.24735485017299652,
      "learning_rate": 0.0001473002159827214,
      "loss": 0.3106,
      "step": 254
    },
    {
      "epoch": 0.816,
      "grad_norm": 0.16006790101528168,
      "learning_rate": 0.0001470842332613391,
      "loss": 0.2906,
      "step": 255
    },
    {
      "epoch": 0.8192,
      "grad_norm": 0.23042429983615875,
      "learning_rate": 0.0001468682505399568,
      "loss": 0.2613,
      "step": 256
    },
    {
      "epoch": 0.8224,
      "grad_norm": 0.32497379183769226,
      "learning_rate": 0.00014665226781857452,
      "loss": 0.2567,
      "step": 257
    },
    {
      "epoch": 0.8256,
      "grad_norm": 0.30977165699005127,
      "learning_rate": 0.00014643628509719222,
      "loss": 0.2615,
      "step": 258
    },
    {
      "epoch": 0.8288,
      "grad_norm": 0.22327324748039246,
      "learning_rate": 0.00014622030237580996,
      "loss": 0.2311,
      "step": 259
    },
    {
      "epoch": 0.832,
      "grad_norm": 0.16415902972221375,
      "learning_rate": 0.00014600431965442764,
      "loss": 0.2293,
      "step": 260
    },
    {
      "epoch": 0.8352,
      "grad_norm": 0.1401928812265396,
      "learning_rate": 0.00014578833693304537,
      "loss": 0.2123,
      "step": 261
    },
    {
      "epoch": 0.8384,
      "grad_norm": 0.13532733917236328,
      "learning_rate": 0.00014557235421166307,
      "loss": 0.2154,
      "step": 262
    },
    {
      "epoch": 0.8416,
      "grad_norm": 0.11128929257392883,
      "learning_rate": 0.00014535637149028078,
      "loss": 0.2037,
      "step": 263
    },
    {
      "epoch": 0.8448,
      "grad_norm": 0.11990294605493546,
      "learning_rate": 0.0001451403887688985,
      "loss": 0.1933,
      "step": 264
    },
    {
      "epoch": 0.848,
      "grad_norm": 0.10572594404220581,
      "learning_rate": 0.0001449244060475162,
      "loss": 0.1852,
      "step": 265
    },
    {
      "epoch": 0.8512,
      "grad_norm": 0.09436079859733582,
      "learning_rate": 0.00014470842332613392,
      "loss": 0.1781,
      "step": 266
    },
    {
      "epoch": 0.8544,
      "grad_norm": 0.10206999629735947,
      "learning_rate": 0.00014449244060475163,
      "loss": 0.1661,
      "step": 267
    },
    {
      "epoch": 0.8576,
      "grad_norm": 0.09557899832725525,
      "learning_rate": 0.00014427645788336934,
      "loss": 0.1872,
      "step": 268
    },
    {
      "epoch": 0.8608,
      "grad_norm": 0.08524663001298904,
      "learning_rate": 0.00014406047516198704,
      "loss": 0.1535,
      "step": 269
    },
    {
      "epoch": 0.864,
      "grad_norm": 0.08678577840328217,
      "learning_rate": 0.00014384449244060477,
      "loss": 0.1437,
      "step": 270
    },
    {
      "epoch": 0.8672,
      "grad_norm": 0.09264212101697922,
      "learning_rate": 0.00014362850971922245,
      "loss": 0.1435,
      "step": 271
    },
    {
      "epoch": 0.8704,
      "grad_norm": 0.07844077050685883,
      "learning_rate": 0.00014341252699784018,
      "loss": 0.1366,
      "step": 272
    },
    {
      "epoch": 0.8736,
      "grad_norm": 0.08219824731349945,
      "learning_rate": 0.0001431965442764579,
      "loss": 0.1274,
      "step": 273
    },
    {
      "epoch": 0.8768,
      "grad_norm": 0.07055019587278366,
      "learning_rate": 0.0001429805615550756,
      "loss": 0.1333,
      "step": 274
    },
    {
      "epoch": 0.88,
      "grad_norm": 0.0853000283241272,
      "learning_rate": 0.00014276457883369333,
      "loss": 0.1297,
      "step": 275
    },
    {
      "epoch": 0.8832,
      "grad_norm": 0.0726621225476265,
      "learning_rate": 0.000142548596112311,
      "loss": 0.1198,
      "step": 276
    },
    {
      "epoch": 0.8864,
      "grad_norm": 0.06854601204395294,
      "learning_rate": 0.00014233261339092874,
      "loss": 0.1209,
      "step": 277
    },
    {
      "epoch": 0.8896,
      "grad_norm": 0.06103048473596573,
      "learning_rate": 0.00014211663066954645,
      "loss": 0.1115,
      "step": 278
    },
    {
      "epoch": 0.8928,
      "grad_norm": 0.07323264330625534,
      "learning_rate": 0.00014190064794816415,
      "loss": 0.1142,
      "step": 279
    },
    {
      "epoch": 0.896,
      "grad_norm": 0.07760905474424362,
      "learning_rate": 0.00014168466522678186,
      "loss": 0.1021,
      "step": 280
    },
    {
      "epoch": 0.8992,
      "grad_norm": 0.07650589197874069,
      "learning_rate": 0.00014146868250539956,
      "loss": 0.1161,
      "step": 281
    },
    {
      "epoch": 0.9024,
      "grad_norm": 0.07565004378557205,
      "learning_rate": 0.0001412526997840173,
      "loss": 0.1016,
      "step": 282
    },
    {
      "epoch": 0.9056,
      "grad_norm": 0.07086750864982605,
      "learning_rate": 0.000141036717062635,
      "loss": 0.0966,
      "step": 283
    },
    {
      "epoch": 0.9088,
      "grad_norm": 0.07491260766983032,
      "learning_rate": 0.0001408207343412527,
      "loss": 0.0973,
      "step": 284
    },
    {
      "epoch": 0.912,
      "grad_norm": 0.11182582378387451,
      "learning_rate": 0.0001406047516198704,
      "loss": 0.1014,
      "step": 285
    },
    {
      "epoch": 0.9152,
      "grad_norm": 0.06034325808286667,
      "learning_rate": 0.00014038876889848814,
      "loss": 0.0908,
      "step": 286
    },
    {
      "epoch": 0.9184,
      "grad_norm": 0.06185375899076462,
      "learning_rate": 0.00014017278617710582,
      "loss": 0.0921,
      "step": 287
    },
    {
      "epoch": 0.9216,
      "grad_norm": 0.10146602243185043,
      "learning_rate": 0.00013995680345572356,
      "loss": 0.0785,
      "step": 288
    },
    {
      "epoch": 0.9248,
      "grad_norm": 0.08139444142580032,
      "learning_rate": 0.00013974082073434126,
      "loss": 0.0734,
      "step": 289
    },
    {
      "epoch": 0.928,
      "grad_norm": 0.05981762707233429,
      "learning_rate": 0.00013952483801295897,
      "loss": 0.0745,
      "step": 290
    },
    {
      "epoch": 0.9312,
      "grad_norm": 0.06275299936532974,
      "learning_rate": 0.0001393088552915767,
      "loss": 0.0625,
      "step": 291
    },
    {
      "epoch": 0.9344,
      "grad_norm": 0.05989274010062218,
      "learning_rate": 0.00013909287257019438,
      "loss": 0.0654,
      "step": 292
    },
    {
      "epoch": 0.9376,
      "grad_norm": 0.06842871755361557,
      "learning_rate": 0.0001388768898488121,
      "loss": 0.0628,
      "step": 293
    },
    {
      "epoch": 0.9408,
      "grad_norm": 0.06952448189258575,
      "learning_rate": 0.00013866090712742982,
      "loss": 0.0563,
      "step": 294
    },
    {
      "epoch": 0.944,
      "grad_norm": 0.06065935641527176,
      "learning_rate": 0.00013844492440604752,
      "loss": 0.0528,
      "step": 295
    },
    {
      "epoch": 0.9472,
      "grad_norm": 0.061113715171813965,
      "learning_rate": 0.00013822894168466523,
      "loss": 0.0421,
      "step": 296
    },
    {
      "epoch": 0.9504,
      "grad_norm": 0.10677459836006165,
      "learning_rate": 0.00013801295896328293,
      "loss": 0.0394,
      "step": 297
    },
    {
      "epoch": 0.9536,
      "grad_norm": 0.08022179454565048,
      "learning_rate": 0.00013779697624190064,
      "loss": 0.041,
      "step": 298
    },
    {
      "epoch": 0.9568,
      "grad_norm": 0.06484784930944443,
      "learning_rate": 0.00013758099352051837,
      "loss": 0.0333,
      "step": 299
    },
    {
      "epoch": 0.96,
      "grad_norm": 0.06363236904144287,
      "learning_rate": 0.00013736501079913608,
      "loss": 0.0427,
      "step": 300
    },
    {
      "epoch": 0.9632,
      "grad_norm": 0.44433721899986267,
      "learning_rate": 0.00013714902807775378,
      "loss": 0.3584,
      "step": 301
    },
    {
      "epoch": 0.9664,
      "grad_norm": 0.24111196398735046,
      "learning_rate": 0.00013693304535637152,
      "loss": 0.2799,
      "step": 302
    },
    {
      "epoch": 0.9696,
      "grad_norm": 0.17339332401752472,
      "learning_rate": 0.0001367170626349892,
      "loss": 0.2343,
      "step": 303
    },
    {
      "epoch": 0.9728,
      "grad_norm": 0.1310667097568512,
      "learning_rate": 0.00013650107991360693,
      "loss": 0.1868,
      "step": 304
    },
    {
      "epoch": 0.976,
      "grad_norm": 0.10640361905097961,
      "learning_rate": 0.00013628509719222463,
      "loss": 0.1664,
      "step": 305
    },
    {
      "epoch": 0.9792,
      "grad_norm": 0.08672438561916351,
      "learning_rate": 0.00013606911447084234,
      "loss": 0.1479,
      "step": 306
    },
    {
      "epoch": 0.9824,
      "grad_norm": 0.08859781920909882,
      "learning_rate": 0.00013585313174946004,
      "loss": 0.1079,
      "step": 307
    },
    {
      "epoch": 0.9856,
      "grad_norm": 0.07510612905025482,
      "learning_rate": 0.00013563714902807775,
      "loss": 0.1101,
      "step": 308
    },
    {
      "epoch": 0.9888,
      "grad_norm": 0.09146241098642349,
      "learning_rate": 0.00013542116630669548,
      "loss": 0.0811,
      "step": 309
    },
    {
      "epoch": 0.992,
      "grad_norm": 0.09475333243608475,
      "learning_rate": 0.0001352051835853132,
      "loss": 0.0721,
      "step": 310
    },
    {
      "epoch": 0.9952,
      "grad_norm": 0.130778968334198,
      "learning_rate": 0.0001349892008639309,
      "loss": 0.0635,
      "step": 311
    },
    {
      "epoch": 0.9984,
      "grad_norm": 0.14115048944950104,
      "learning_rate": 0.0001347732181425486,
      "loss": 0.0497,
      "step": 312
    },
    {
      "epoch": 1.0016,
      "grad_norm": 0.11997000873088837,
      "learning_rate": 0.0001345572354211663,
      "loss": 0.2256,
      "step": 313
    },
    {
      "epoch": 1.0048,
      "grad_norm": 0.14707410335540771,
      "learning_rate": 0.000134341252699784,
      "loss": 0.3419,
      "step": 314
    },
    {
      "epoch": 1.008,
      "grad_norm": 0.17011718451976776,
      "learning_rate": 0.00013412526997840174,
      "loss": 0.2989,
      "step": 315
    },
    {
      "epoch": 1.0112,
      "grad_norm": 0.17345085740089417,
      "learning_rate": 0.00013390928725701945,
      "loss": 0.272,
      "step": 316
    },
    {
      "epoch": 1.0144,
      "grad_norm": 0.14271925389766693,
      "learning_rate": 0.00013369330453563715,
      "loss": 0.258,
      "step": 317
    },
    {
      "epoch": 1.0176,
      "grad_norm": 0.12236033380031586,
      "learning_rate": 0.0001334773218142549,
      "loss": 0.2353,
      "step": 318
    },
    {
      "epoch": 1.0208,
      "grad_norm": 0.11290924996137619,
      "learning_rate": 0.00013326133909287257,
      "loss": 0.2235,
      "step": 319
    },
    {
      "epoch": 1.024,
      "grad_norm": 0.11244039982557297,
      "learning_rate": 0.0001330453563714903,
      "loss": 0.2153,
      "step": 320
    },
    {
      "epoch": 1.0272,
      "grad_norm": 0.10569936782121658,
      "learning_rate": 0.000132829373650108,
      "loss": 0.2356,
      "step": 321
    },
    {
      "epoch": 1.0304,
      "grad_norm": 0.14320407807826996,
      "learning_rate": 0.0001326133909287257,
      "loss": 0.2225,
      "step": 322
    },
    {
      "epoch": 1.0336,
      "grad_norm": 0.11067403107881546,
      "learning_rate": 0.00013239740820734342,
      "loss": 0.2071,
      "step": 323
    },
    {
      "epoch": 1.0368,
      "grad_norm": 0.12213852256536484,
      "learning_rate": 0.00013218142548596112,
      "loss": 0.1933,
      "step": 324
    },
    {
      "epoch": 1.04,
      "grad_norm": 0.12561529874801636,
      "learning_rate": 0.00013196544276457885,
      "loss": 0.1827,
      "step": 325
    },
    {
      "epoch": 1.0432,
      "grad_norm": 0.14336295425891876,
      "learning_rate": 0.00013174946004319656,
      "loss": 0.186,
      "step": 326
    },
    {
      "epoch": 1.0464,
      "grad_norm": 0.12256599962711334,
      "learning_rate": 0.00013153347732181427,
      "loss": 0.1731,
      "step": 327
    },
    {
      "epoch": 1.0496,
      "grad_norm": 0.1231326013803482,
      "learning_rate": 0.00013131749460043197,
      "loss": 0.1803,
      "step": 328
    },
    {
      "epoch": 1.0528,
      "grad_norm": 0.12226280570030212,
      "learning_rate": 0.00013110151187904968,
      "loss": 0.1708,
      "step": 329
    },
    {
      "epoch": 1.056,
      "grad_norm": 0.10937453806400299,
      "learning_rate": 0.00013088552915766738,
      "loss": 0.1727,
      "step": 330
    },
    {
      "epoch": 1.0592,
      "grad_norm": 0.09929589182138443,
      "learning_rate": 0.00013066954643628511,
      "loss": 0.1452,
      "step": 331
    },
    {
      "epoch": 1.0624,
      "grad_norm": 0.1063147708773613,
      "learning_rate": 0.0001304535637149028,
      "loss": 0.1462,
      "step": 332
    },
    {
      "epoch": 1.0656,
      "grad_norm": 0.12070031464099884,
      "learning_rate": 0.00013023758099352053,
      "loss": 0.1282,
      "step": 333
    },
    {
      "epoch": 1.0688,
      "grad_norm": 0.10257162898778915,
      "learning_rate": 0.00013002159827213826,
      "loss": 0.1244,
      "step": 334
    },
    {
      "epoch": 1.072,
      "grad_norm": 0.08255746960639954,
      "learning_rate": 0.00012980561555075594,
      "loss": 0.1375,
      "step": 335
    },
    {
      "epoch": 1.0752,
      "grad_norm": 0.08366654813289642,
      "learning_rate": 0.00012958963282937367,
      "loss": 0.1286,
      "step": 336
    },
    {
      "epoch": 1.0784,
      "grad_norm": 0.09608781337738037,
      "learning_rate": 0.00012937365010799138,
      "loss": 0.1296,
      "step": 337
    },
    {
      "epoch": 1.0816,
      "grad_norm": 0.10134371370077133,
      "learning_rate": 0.00012915766738660908,
      "loss": 0.1292,
      "step": 338
    },
    {
      "epoch": 1.0848,
      "grad_norm": 0.07989580929279327,
      "learning_rate": 0.0001289416846652268,
      "loss": 0.1026,
      "step": 339
    },
    {
      "epoch": 1.088,
      "grad_norm": 0.0782737210392952,
      "learning_rate": 0.0001287257019438445,
      "loss": 0.1125,
      "step": 340
    },
    {
      "epoch": 1.0912,
      "grad_norm": 0.07798503339290619,
      "learning_rate": 0.0001285097192224622,
      "loss": 0.1091,
      "step": 341
    },
    {
      "epoch": 1.0944,
      "grad_norm": 0.08558062463998795,
      "learning_rate": 0.00012829373650107993,
      "loss": 0.1068,
      "step": 342
    },
    {
      "epoch": 1.0976,
      "grad_norm": 0.09030447155237198,
      "learning_rate": 0.00012807775377969764,
      "loss": 0.1019,
      "step": 343
    },
    {
      "epoch": 1.1008,
      "grad_norm": 0.07005513459444046,
      "learning_rate": 0.00012786177105831534,
      "loss": 0.0889,
      "step": 344
    },
    {
      "epoch": 1.104,
      "grad_norm": 0.08939553797245026,
      "learning_rate": 0.00012764578833693305,
      "loss": 0.1134,
      "step": 345
    },
    {
      "epoch": 1.1072,
      "grad_norm": 0.08030259609222412,
      "learning_rate": 0.00012742980561555075,
      "loss": 0.085,
      "step": 346
    },
    {
      "epoch": 1.1104,
      "grad_norm": 0.06897532194852829,
      "learning_rate": 0.00012721382289416849,
      "loss": 0.0772,
      "step": 347
    },
    {
      "epoch": 1.1136,
      "grad_norm": 0.08519865572452545,
      "learning_rate": 0.00012699784017278616,
      "loss": 0.0873,
      "step": 348
    },
    {
      "epoch": 1.1168,
      "grad_norm": 0.0698448196053505,
      "learning_rate": 0.0001267818574514039,
      "loss": 0.0809,
      "step": 349
    },
    {
      "epoch": 1.12,
      "grad_norm": 0.06411396712064743,
      "learning_rate": 0.0001265658747300216,
      "loss": 0.0748,
      "step": 350
    },
    {
      "epoch": 1.1232,
      "grad_norm": 0.0637090727686882,
      "learning_rate": 0.0001263498920086393,
      "loss": 0.0731,
      "step": 351
    },
    {
      "epoch": 1.1264,
      "grad_norm": 0.08060378581285477,
      "learning_rate": 0.00012613390928725704,
      "loss": 0.0784,
      "step": 352
    },
    {
      "epoch": 1.1296,
      "grad_norm": 0.06315179169178009,
      "learning_rate": 0.00012591792656587472,
      "loss": 0.0656,
      "step": 353
    },
    {
      "epoch": 1.1328,
      "grad_norm": 0.061039481312036514,
      "learning_rate": 0.00012570194384449245,
      "loss": 0.0639,
      "step": 354
    },
    {
      "epoch": 1.1360000000000001,
      "grad_norm": 0.06964940577745438,
      "learning_rate": 0.00012548596112311016,
      "loss": 0.0692,
      "step": 355
    },
    {
      "epoch": 1.1392,
      "grad_norm": 0.06637610495090485,
      "learning_rate": 0.00012526997840172786,
      "loss": 0.0498,
      "step": 356
    },
    {
      "epoch": 1.1424,
      "grad_norm": 0.06137174740433693,
      "learning_rate": 0.00012505399568034557,
      "loss": 0.0497,
      "step": 357
    },
    {
      "epoch": 1.1456,
      "grad_norm": 0.05875043570995331,
      "learning_rate": 0.0001248380129589633,
      "loss": 0.051,
      "step": 358
    },
    {
      "epoch": 1.1488,
      "grad_norm": 0.05981156975030899,
      "learning_rate": 0.00012462203023758098,
      "loss": 0.0481,
      "step": 359
    },
    {
      "epoch": 1.152,
      "grad_norm": 0.07167306542396545,
      "learning_rate": 0.0001244060475161987,
      "loss": 0.0365,
      "step": 360
    },
    {
      "epoch": 1.1552,
      "grad_norm": 0.07272736728191376,
      "learning_rate": 0.00012419006479481642,
      "loss": 0.0392,
      "step": 361
    },
    {
      "epoch": 1.1584,
      "grad_norm": 0.06320641934871674,
      "learning_rate": 0.00012397408207343412,
      "loss": 0.0378,
      "step": 362
    },
    {
      "epoch": 1.1616,
      "grad_norm": 0.19552962481975555,
      "learning_rate": 0.00012375809935205186,
      "loss": 0.2453,
      "step": 363
    },
    {
      "epoch": 1.1648,
      "grad_norm": 0.2649109363555908,
      "learning_rate": 0.00012354211663066954,
      "loss": 0.3521,
      "step": 364
    },
    {
      "epoch": 1.168,
      "grad_norm": 0.22942207753658295,
      "learning_rate": 0.00012332613390928727,
      "loss": 0.3139,
      "step": 365
    },
    {
      "epoch": 1.1712,
      "grad_norm": 0.20003998279571533,
      "learning_rate": 0.00012311015118790497,
      "loss": 0.2872,
      "step": 366
    },
    {
      "epoch": 1.1743999999999999,
      "grad_norm": 0.6745431423187256,
      "learning_rate": 0.00012289416846652268,
      "loss": 0.2839,
      "step": 367
    },
    {
      "epoch": 1.1776,
      "grad_norm": 0.17137108743190765,
      "learning_rate": 0.00012267818574514039,
      "loss": 0.2364,
      "step": 368
    },
    {
      "epoch": 1.1808,
      "grad_norm": 0.1637425720691681,
      "learning_rate": 0.0001224622030237581,
      "loss": 0.2488,
      "step": 369
    },
    {
      "epoch": 1.184,
      "grad_norm": 1.1395171880722046,
      "learning_rate": 0.00012224622030237582,
      "loss": 0.2203,
      "step": 370
    },
    {
      "epoch": 1.1872,
      "grad_norm": 0.12725816667079926,
      "learning_rate": 0.00012203023758099353,
      "loss": 0.2194,
      "step": 371
    },
    {
      "epoch": 1.1904,
      "grad_norm": 0.10857982933521271,
      "learning_rate": 0.00012181425485961125,
      "loss": 0.1918,
      "step": 372
    },
    {
      "epoch": 1.1936,
      "grad_norm": 1.2869576215744019,
      "learning_rate": 0.00012159827213822894,
      "loss": 0.2199,
      "step": 373
    },
    {
      "epoch": 1.1968,
      "grad_norm": 0.29680728912353516,
      "learning_rate": 0.00012138228941684666,
      "loss": 0.1891,
      "step": 374
    },
    {
      "epoch": 1.2,
      "grad_norm": 0.18445530533790588,
      "learning_rate": 0.00012116630669546437,
      "loss": 0.1877,
      "step": 375
    },
    {
      "epoch": 1.2032,
      "grad_norm": 0.13325351476669312,
      "learning_rate": 0.00012095032397408208,
      "loss": 0.1798,
      "step": 376
    },
    {
      "epoch": 1.2032,
      "eval_loss": 0.1591818630695343,
      "eval_runtime": 98.4691,
      "eval_samples_per_second": 11.262,
      "eval_steps_per_second": 0.711,
      "step": 376
    },
    {
      "epoch": 1.2064,
      "grad_norm": 0.6245388984680176,
      "learning_rate": 0.0001207343412526998,
      "loss": 0.1741,
      "step": 377
    },
    {
      "epoch": 1.2096,
      "grad_norm": 0.140795037150383,
      "learning_rate": 0.0001205183585313175,
      "loss": 0.1678,
      "step": 378
    },
    {
      "epoch": 1.2128,
      "grad_norm": 0.14888934791088104,
      "learning_rate": 0.00012030237580993522,
      "loss": 0.1667,
      "step": 379
    },
    {
      "epoch": 1.216,
      "grad_norm": 0.8047127723693848,
      "learning_rate": 0.00012008639308855292,
      "loss": 0.1602,
      "step": 380
    },
    {
      "epoch": 1.2192,
      "grad_norm": 0.11481031775474548,
      "learning_rate": 0.00011987041036717064,
      "loss": 0.1573,
      "step": 381
    },
    {
      "epoch": 1.2224,
      "grad_norm": 0.11270076036453247,
      "learning_rate": 0.00011965442764578833,
      "loss": 0.1526,
      "step": 382
    },
    {
      "epoch": 1.2256,
      "grad_norm": 0.09626296162605286,
      "learning_rate": 0.00011943844492440605,
      "loss": 0.139,
      "step": 383
    },
    {
      "epoch": 1.2288000000000001,
      "grad_norm": 0.10542811453342438,
      "learning_rate": 0.00011922246220302376,
      "loss": 0.1417,
      "step": 384
    },
    {
      "epoch": 1.232,
      "grad_norm": 0.8815963268280029,
      "learning_rate": 0.00011900647948164148,
      "loss": 0.129,
      "step": 385
    },
    {
      "epoch": 1.2352,
      "grad_norm": 0.09686513990163803,
      "learning_rate": 0.0001187904967602592,
      "loss": 0.1351,
      "step": 386
    },
    {
      "epoch": 1.2384,
      "grad_norm": 0.0849158987402916,
      "learning_rate": 0.00011857451403887689,
      "loss": 0.1189,
      "step": 387
    },
    {
      "epoch": 1.2416,
      "grad_norm": 0.09099497646093369,
      "learning_rate": 0.00011835853131749462,
      "loss": 0.1284,
      "step": 388
    },
    {
      "epoch": 1.2448,
      "grad_norm": 0.09169624000787735,
      "learning_rate": 0.00011814254859611231,
      "loss": 0.1294,
      "step": 389
    },
    {
      "epoch": 1.248,
      "grad_norm": 0.09578943997621536,
      "learning_rate": 0.00011792656587473003,
      "loss": 0.1111,
      "step": 390
    },
    {
      "epoch": 1.2511999999999999,
      "grad_norm": 0.10189872980117798,
      "learning_rate": 0.00011771058315334774,
      "loss": 0.1197,
      "step": 391
    },
    {
      "epoch": 1.2544,
      "grad_norm": 0.09034481644630432,
      "learning_rate": 0.00011749460043196546,
      "loss": 0.1052,
      "step": 392
    },
    {
      "epoch": 1.2576,
      "grad_norm": 0.08263595402240753,
      "learning_rate": 0.00011727861771058315,
      "loss": 0.1057,
      "step": 393
    },
    {
      "epoch": 1.2608,
      "grad_norm": 0.07547786086797714,
      "learning_rate": 0.00011706263498920087,
      "loss": 0.0993,
      "step": 394
    },
    {
      "epoch": 1.264,
      "grad_norm": 0.0667828842997551,
      "learning_rate": 0.00011684665226781859,
      "loss": 0.0927,
      "step": 395
    },
    {
      "epoch": 1.2671999999999999,
      "grad_norm": 0.06701038032770157,
      "learning_rate": 0.00011663066954643629,
      "loss": 0.0996,
      "step": 396
    },
    {
      "epoch": 1.2704,
      "grad_norm": 0.07439185678958893,
      "learning_rate": 0.00011641468682505401,
      "loss": 0.0956,
      "step": 397
    },
    {
      "epoch": 1.2736,
      "grad_norm": 0.06494053453207016,
      "learning_rate": 0.0001161987041036717,
      "loss": 0.0887,
      "step": 398
    },
    {
      "epoch": 1.2768,
      "grad_norm": 0.06355660408735275,
      "learning_rate": 0.00011598272138228942,
      "loss": 0.0867,
      "step": 399
    },
    {
      "epoch": 1.28,
      "grad_norm": 0.05834250897169113,
      "learning_rate": 0.00011576673866090713,
      "loss": 0.0772,
      "step": 400
    },
    {
      "epoch": 1.2832,
      "grad_norm": 0.06987501680850983,
      "learning_rate": 0.00011555075593952485,
      "loss": 0.077,
      "step": 401
    },
    {
      "epoch": 1.2864,
      "grad_norm": 0.05944366008043289,
      "learning_rate": 0.00011533477321814254,
      "loss": 0.0758,
      "step": 402
    },
    {
      "epoch": 1.2896,
      "grad_norm": 0.06509342044591904,
      "learning_rate": 0.00011511879049676026,
      "loss": 0.0589,
      "step": 403
    },
    {
      "epoch": 1.2928,
      "grad_norm": 0.059185851365327835,
      "learning_rate": 0.00011490280777537799,
      "loss": 0.0611,
      "step": 404
    },
    {
      "epoch": 1.296,
      "grad_norm": 0.06579429656267166,
      "learning_rate": 0.00011468682505399568,
      "loss": 0.061,
      "step": 405
    },
    {
      "epoch": 1.2992,
      "grad_norm": 0.053226109594106674,
      "learning_rate": 0.0001144708423326134,
      "loss": 0.0524,
      "step": 406
    },
    {
      "epoch": 1.3024,
      "grad_norm": 0.05422969162464142,
      "learning_rate": 0.00011425485961123111,
      "loss": 0.0463,
      "step": 407
    },
    {
      "epoch": 1.3056,
      "grad_norm": 0.060504183173179626,
      "learning_rate": 0.00011403887688984883,
      "loss": 0.049,
      "step": 408
    },
    {
      "epoch": 1.3088,
      "grad_norm": 0.05598820373415947,
      "learning_rate": 0.00011382289416846652,
      "loss": 0.0452,
      "step": 409
    },
    {
      "epoch": 1.312,
      "grad_norm": 0.08275706321001053,
      "learning_rate": 0.00011360691144708424,
      "loss": 0.0379,
      "step": 410
    },
    {
      "epoch": 1.3152,
      "grad_norm": 0.0849904716014862,
      "learning_rate": 0.00011339092872570194,
      "loss": 0.0382,
      "step": 411
    },
    {
      "epoch": 1.3184,
      "grad_norm": 0.0567975752055645,
      "learning_rate": 0.00011317494600431966,
      "loss": 0.0359,
      "step": 412
    },
    {
      "epoch": 1.3216,
      "grad_norm": 0.1660701036453247,
      "learning_rate": 0.00011295896328293738,
      "loss": 0.211,
      "step": 413
    },
    {
      "epoch": 1.3248,
      "grad_norm": 0.2668968737125397,
      "learning_rate": 0.00011274298056155507,
      "loss": 0.3317,
      "step": 414
    },
    {
      "epoch": 1.328,
      "grad_norm": 0.25007253885269165,
      "learning_rate": 0.0001125269978401728,
      "loss": 0.3164,
      "step": 415
    },
    {
      "epoch": 1.3312,
      "grad_norm": 0.22479476034641266,
      "learning_rate": 0.0001123110151187905,
      "loss": 0.3017,
      "step": 416
    },
    {
      "epoch": 1.3344,
      "grad_norm": 0.1871844083070755,
      "learning_rate": 0.00011209503239740822,
      "loss": 0.2792,
      "step": 417
    },
    {
      "epoch": 1.3376000000000001,
      "grad_norm": 0.1820467710494995,
      "learning_rate": 0.00011187904967602591,
      "loss": 0.2575,
      "step": 418
    },
    {
      "epoch": 1.3408,
      "grad_norm": 0.15836143493652344,
      "learning_rate": 0.00011166306695464363,
      "loss": 0.2606,
      "step": 419
    },
    {
      "epoch": 1.3439999999999999,
      "grad_norm": 0.14027643203735352,
      "learning_rate": 0.00011144708423326134,
      "loss": 0.2164,
      "step": 420
    },
    {
      "epoch": 1.3472,
      "grad_norm": 0.12318983674049377,
      "learning_rate": 0.00011123110151187905,
      "loss": 0.2269,
      "step": 421
    },
    {
      "epoch": 1.3504,
      "grad_norm": 0.1222982257604599,
      "learning_rate": 0.00011101511879049677,
      "loss": 0.1992,
      "step": 422
    },
    {
      "epoch": 1.3536000000000001,
      "grad_norm": 0.12370932847261429,
      "learning_rate": 0.00011079913606911448,
      "loss": 0.2037,
      "step": 423
    },
    {
      "epoch": 1.3568,
      "grad_norm": 0.11547400057315826,
      "learning_rate": 0.0001105831533477322,
      "loss": 0.1875,
      "step": 424
    },
    {
      "epoch": 1.3599999999999999,
      "grad_norm": 0.11140944808721542,
      "learning_rate": 0.00011036717062634989,
      "loss": 0.1833,
      "step": 425
    },
    {
      "epoch": 1.3632,
      "grad_norm": 0.11044490337371826,
      "learning_rate": 0.00011015118790496761,
      "loss": 0.1833,
      "step": 426
    },
    {
      "epoch": 1.3664,
      "grad_norm": 0.10642653703689575,
      "learning_rate": 0.00010993520518358532,
      "loss": 0.1732,
      "step": 427
    },
    {
      "epoch": 1.3696,
      "grad_norm": 0.10137318074703217,
      "learning_rate": 0.00010971922246220303,
      "loss": 0.1598,
      "step": 428
    },
    {
      "epoch": 1.3728,
      "grad_norm": 0.10386919975280762,
      "learning_rate": 0.00010950323974082073,
      "loss": 0.1527,
      "step": 429
    },
    {
      "epoch": 1.376,
      "grad_norm": 0.0981709286570549,
      "learning_rate": 0.00010928725701943845,
      "loss": 0.1408,
      "step": 430
    },
    {
      "epoch": 1.3792,
      "grad_norm": 0.1260475367307663,
      "learning_rate": 0.00010907127429805617,
      "loss": 0.1515,
      "step": 431
    },
    {
      "epoch": 1.3824,
      "grad_norm": 0.10942622274160385,
      "learning_rate": 0.00010885529157667387,
      "loss": 0.1428,
      "step": 432
    },
    {
      "epoch": 1.3856,
      "grad_norm": 0.10764030367136002,
      "learning_rate": 0.00010863930885529159,
      "loss": 0.1423,
      "step": 433
    },
    {
      "epoch": 1.3888,
      "grad_norm": 0.10711009800434113,
      "learning_rate": 0.00010842332613390928,
      "loss": 0.1313,
      "step": 434
    },
    {
      "epoch": 1.392,
      "grad_norm": 0.10206203907728195,
      "learning_rate": 0.000108207343412527,
      "loss": 0.1319,
      "step": 435
    },
    {
      "epoch": 1.3952,
      "grad_norm": 0.11249946057796478,
      "learning_rate": 0.00010799136069114471,
      "loss": 0.1306,
      "step": 436
    },
    {
      "epoch": 1.3984,
      "grad_norm": 0.09348681569099426,
      "learning_rate": 0.00010777537796976243,
      "loss": 0.1213,
      "step": 437
    },
    {
      "epoch": 1.4016,
      "grad_norm": 0.0990813672542572,
      "learning_rate": 0.00010755939524838012,
      "loss": 0.119,
      "step": 438
    },
    {
      "epoch": 1.4048,
      "grad_norm": 0.0866088792681694,
      "learning_rate": 0.00010734341252699785,
      "loss": 0.116,
      "step": 439
    },
    {
      "epoch": 1.408,
      "grad_norm": 0.094899021089077,
      "learning_rate": 0.00010712742980561557,
      "loss": 0.1166,
      "step": 440
    },
    {
      "epoch": 1.4112,
      "grad_norm": 0.08335098624229431,
      "learning_rate": 0.00010691144708423326,
      "loss": 0.1023,
      "step": 441
    },
    {
      "epoch": 1.4144,
      "grad_norm": 0.07794898003339767,
      "learning_rate": 0.00010669546436285098,
      "loss": 0.1031,
      "step": 442
    },
    {
      "epoch": 1.4176,
      "grad_norm": 0.08483266830444336,
      "learning_rate": 0.00010647948164146869,
      "loss": 0.1027,
      "step": 443
    },
    {
      "epoch": 1.4208,
      "grad_norm": 0.08544091880321503,
      "learning_rate": 0.0001062634989200864,
      "loss": 0.0945,
      "step": 444
    },
    {
      "epoch": 1.424,
      "grad_norm": 0.08047832548618317,
      "learning_rate": 0.0001060475161987041,
      "loss": 0.0979,
      "step": 445
    },
    {
      "epoch": 1.4272,
      "grad_norm": 0.07661319524049759,
      "learning_rate": 0.00010583153347732182,
      "loss": 0.0926,
      "step": 446
    },
    {
      "epoch": 1.4304000000000001,
      "grad_norm": 0.06809470057487488,
      "learning_rate": 0.00010561555075593954,
      "loss": 0.0869,
      "step": 447
    },
    {
      "epoch": 1.4336,
      "grad_norm": 0.07238508015871048,
      "learning_rate": 0.00010539956803455724,
      "loss": 0.0895,
      "step": 448
    },
    {
      "epoch": 1.4368,
      "grad_norm": 0.07382765412330627,
      "learning_rate": 0.00010518358531317496,
      "loss": 0.0842,
      "step": 449
    },
    {
      "epoch": 1.44,
      "grad_norm": 0.07234997302293777,
      "learning_rate": 0.00010496760259179265,
      "loss": 0.0845,
      "step": 450
    },
    {
      "epoch": 1.4432,
      "grad_norm": 0.0634097009897232,
      "learning_rate": 0.00010475161987041037,
      "loss": 0.0706,
      "step": 451
    },
    {
      "epoch": 1.4464000000000001,
      "grad_norm": 0.07205670326948166,
      "learning_rate": 0.00010453563714902808,
      "loss": 0.0689,
      "step": 452
    },
    {
      "epoch": 1.4496,
      "grad_norm": 0.07272996008396149,
      "learning_rate": 0.0001043196544276458,
      "loss": 0.0643,
      "step": 453
    },
    {
      "epoch": 1.4527999999999999,
      "grad_norm": 0.07861189544200897,
      "learning_rate": 0.00010410367170626349,
      "loss": 0.0714,
      "step": 454
    },
    {
      "epoch": 1.456,
      "grad_norm": 0.06273604184389114,
      "learning_rate": 0.00010388768898488121,
      "loss": 0.0603,
      "step": 455
    },
    {
      "epoch": 1.4592,
      "grad_norm": 0.07177518308162689,
      "learning_rate": 0.00010367170626349894,
      "loss": 0.0638,
      "step": 456
    },
    {
      "epoch": 1.4624,
      "grad_norm": 0.06667538732290268,
      "learning_rate": 0.00010345572354211663,
      "loss": 0.0502,
      "step": 457
    },
    {
      "epoch": 1.4656,
      "grad_norm": 0.07561179250478745,
      "learning_rate": 0.00010323974082073435,
      "loss": 0.0519,
      "step": 458
    },
    {
      "epoch": 1.4687999999999999,
      "grad_norm": 0.08505142480134964,
      "learning_rate": 0.00010302375809935206,
      "loss": 0.0539,
      "step": 459
    },
    {
      "epoch": 1.472,
      "grad_norm": 0.1028430238366127,
      "learning_rate": 0.00010280777537796978,
      "loss": 0.0394,
      "step": 460
    },
    {
      "epoch": 1.4752,
      "grad_norm": 0.07230327278375626,
      "learning_rate": 0.00010259179265658747,
      "loss": 0.0365,
      "step": 461
    },
    {
      "epoch": 1.4784,
      "grad_norm": 0.06497851014137268,
      "learning_rate": 0.00010237580993520519,
      "loss": 0.0332,
      "step": 462
    },
    {
      "epoch": 1.4816,
      "grad_norm": 0.24923889338970184,
      "learning_rate": 0.0001021598272138229,
      "loss": 0.2434,
      "step": 463
    },
    {
      "epoch": 1.4848,
      "grad_norm": 0.39703354239463806,
      "learning_rate": 0.00010194384449244061,
      "loss": 0.3476,
      "step": 464
    },
    {
      "epoch": 1.488,
      "grad_norm": 0.3577578663825989,
      "learning_rate": 0.00010172786177105833,
      "loss": 0.3116,
      "step": 465
    },
    {
      "epoch": 1.4912,
      "grad_norm": 0.27886849641799927,
      "learning_rate": 0.00010151187904967602,
      "loss": 0.2956,
      "step": 466
    },
    {
      "epoch": 1.4944,
      "grad_norm": 0.2198692262172699,
      "learning_rate": 0.00010129589632829374,
      "loss": 0.2692,
      "step": 467
    },
    {
      "epoch": 1.4976,
      "grad_norm": 0.1740591824054718,
      "learning_rate": 0.00010107991360691145,
      "loss": 0.2519,
      "step": 468
    },
    {
      "epoch": 1.5008,
      "grad_norm": 0.15478554368019104,
      "learning_rate": 0.00010086393088552917,
      "loss": 0.2413,
      "step": 469
    },
    {
      "epoch": 1.504,
      "grad_norm": 0.13286903500556946,
      "learning_rate": 0.00010064794816414686,
      "loss": 0.2235,
      "step": 470
    },
    {
      "epoch": 1.5072,
      "grad_norm": 0.1512717306613922,
      "learning_rate": 0.00010043196544276458,
      "loss": 0.2263,
      "step": 471
    },
    {
      "epoch": 1.5104,
      "grad_norm": 0.13422799110412598,
      "learning_rate": 0.00010021598272138229,
      "loss": 0.21,
      "step": 472
    },
    {
      "epoch": 1.5135999999999998,
      "grad_norm": 0.12860913574695587,
      "learning_rate": 0.0001,
      "loss": 0.207,
      "step": 473
    },
    {
      "epoch": 1.5168,
      "grad_norm": 0.1341039389371872,
      "learning_rate": 9.978401727861771e-05,
      "loss": 0.2048,
      "step": 474
    },
    {
      "epoch": 1.52,
      "grad_norm": 0.14098693430423737,
      "learning_rate": 9.956803455723543e-05,
      "loss": 0.1876,
      "step": 475
    },
    {
      "epoch": 1.5232,
      "grad_norm": 0.1215980276465416,
      "learning_rate": 9.935205183585314e-05,
      "loss": 0.1781,
      "step": 476
    },
    {
      "epoch": 1.5264,
      "grad_norm": 0.1203489676117897,
      "learning_rate": 9.913606911447084e-05,
      "loss": 0.1746,
      "step": 477
    },
    {
      "epoch": 1.5295999999999998,
      "grad_norm": 0.1114560142159462,
      "learning_rate": 9.892008639308856e-05,
      "loss": 0.1712,
      "step": 478
    },
    {
      "epoch": 1.5328,
      "grad_norm": 0.11476129293441772,
      "learning_rate": 9.870410367170627e-05,
      "loss": 0.1703,
      "step": 479
    },
    {
      "epoch": 1.536,
      "grad_norm": 0.11486128717660904,
      "learning_rate": 9.848812095032398e-05,
      "loss": 0.1586,
      "step": 480
    },
    {
      "epoch": 1.5392000000000001,
      "grad_norm": 0.1113382875919342,
      "learning_rate": 9.827213822894169e-05,
      "loss": 0.1634,
      "step": 481
    },
    {
      "epoch": 1.5424,
      "grad_norm": 0.11009319126605988,
      "learning_rate": 9.80561555075594e-05,
      "loss": 0.1523,
      "step": 482
    },
    {
      "epoch": 1.5455999999999999,
      "grad_norm": 0.11553367227315903,
      "learning_rate": 9.784017278617712e-05,
      "loss": 0.154,
      "step": 483
    },
    {
      "epoch": 1.5488,
      "grad_norm": 0.09562712907791138,
      "learning_rate": 9.762419006479482e-05,
      "loss": 0.1311,
      "step": 484
    },
    {
      "epoch": 1.552,
      "grad_norm": 0.10335250943899155,
      "learning_rate": 9.740820734341253e-05,
      "loss": 0.1437,
      "step": 485
    },
    {
      "epoch": 1.5552000000000001,
      "grad_norm": 0.10609379410743713,
      "learning_rate": 9.719222462203023e-05,
      "loss": 0.1347,
      "step": 486
    },
    {
      "epoch": 1.5584,
      "grad_norm": 0.09954191744327545,
      "learning_rate": 9.697624190064795e-05,
      "loss": 0.1284,
      "step": 487
    },
    {
      "epoch": 1.5615999999999999,
      "grad_norm": 0.10272558778524399,
      "learning_rate": 9.676025917926567e-05,
      "loss": 0.1263,
      "step": 488
    },
    {
      "epoch": 1.5648,
      "grad_norm": 0.10160176455974579,
      "learning_rate": 9.654427645788338e-05,
      "loss": 0.1191,
      "step": 489
    },
    {
      "epoch": 1.568,
      "grad_norm": 0.08763245493173599,
      "learning_rate": 9.632829373650108e-05,
      "loss": 0.1142,
      "step": 490
    },
    {
      "epoch": 1.5712000000000002,
      "grad_norm": 0.09454362094402313,
      "learning_rate": 9.61123110151188e-05,
      "loss": 0.1093,
      "step": 491
    },
    {
      "epoch": 1.5744,
      "grad_norm": 0.09002625197172165,
      "learning_rate": 9.58963282937365e-05,
      "loss": 0.1061,
      "step": 492
    },
    {
      "epoch": 1.5776,
      "grad_norm": 0.08534427732229233,
      "learning_rate": 9.568034557235421e-05,
      "loss": 0.103,
      "step": 493
    },
    {
      "epoch": 1.5808,
      "grad_norm": 0.08607471734285355,
      "learning_rate": 9.546436285097192e-05,
      "loss": 0.1115,
      "step": 494
    },
    {
      "epoch": 1.584,
      "grad_norm": 0.08401267975568771,
      "learning_rate": 9.524838012958964e-05,
      "loss": 0.0964,
      "step": 495
    },
    {
      "epoch": 1.5872000000000002,
      "grad_norm": 0.07432910799980164,
      "learning_rate": 9.503239740820736e-05,
      "loss": 0.09,
      "step": 496
    },
    {
      "epoch": 1.5904,
      "grad_norm": 0.08111179620027542,
      "learning_rate": 9.481641468682506e-05,
      "loss": 0.0856,
      "step": 497
    },
    {
      "epoch": 1.5936,
      "grad_norm": 0.08575079590082169,
      "learning_rate": 9.460043196544277e-05,
      "loss": 0.0897,
      "step": 498
    },
    {
      "epoch": 1.5968,
      "grad_norm": 0.07825019210577011,
      "learning_rate": 9.438444924406049e-05,
      "loss": 0.0808,
      "step": 499
    },
    {
      "epoch": 1.6,
      "grad_norm": 0.08180250227451324,
      "learning_rate": 9.416846652267819e-05,
      "loss": 0.0745,
      "step": 500
    },
    {
      "epoch": 1.6032,
      "grad_norm": 0.08143682777881622,
      "learning_rate": 9.39524838012959e-05,
      "loss": 0.0753,
      "step": 501
    },
    {
      "epoch": 1.6064,
      "grad_norm": 0.08158530294895172,
      "learning_rate": 9.37365010799136e-05,
      "loss": 0.0789,
      "step": 502
    },
    {
      "epoch": 1.6096,
      "grad_norm": 0.07187547534704208,
      "learning_rate": 9.352051835853132e-05,
      "loss": 0.0661,
      "step": 503
    },
    {
      "epoch": 1.6128,
      "grad_norm": 0.0706712156534195,
      "learning_rate": 9.330453563714904e-05,
      "loss": 0.0712,
      "step": 504
    },
    {
      "epoch": 1.616,
      "grad_norm": 0.07154963910579681,
      "learning_rate": 9.308855291576675e-05,
      "loss": 0.0664,
      "step": 505
    },
    {
      "epoch": 1.6192,
      "grad_norm": 0.10115194320678711,
      "learning_rate": 9.287257019438445e-05,
      "loss": 0.0563,
      "step": 506
    },
    {
      "epoch": 1.6223999999999998,
      "grad_norm": 0.08296545594930649,
      "learning_rate": 9.265658747300216e-05,
      "loss": 0.0515,
      "step": 507
    },
    {
      "epoch": 1.6256,
      "grad_norm": 0.07731545716524124,
      "learning_rate": 9.244060475161988e-05,
      "loss": 0.0538,
      "step": 508
    },
    {
      "epoch": 1.6288,
      "grad_norm": 0.09937188774347305,
      "learning_rate": 9.222462203023758e-05,
      "loss": 0.0472,
      "step": 509
    },
    {
      "epoch": 1.6320000000000001,
      "grad_norm": 0.11979135870933533,
      "learning_rate": 9.200863930885529e-05,
      "loss": 0.0435,
      "step": 510
    },
    {
      "epoch": 1.6352,
      "grad_norm": 0.08260580152273178,
      "learning_rate": 9.179265658747301e-05,
      "loss": 0.0406,
      "step": 511
    },
    {
      "epoch": 1.6383999999999999,
      "grad_norm": 0.09589828550815582,
      "learning_rate": 9.157667386609071e-05,
      "loss": 0.0387,
      "step": 512
    },
    {
      "epoch": 1.6416,
      "grad_norm": 0.35055404901504517,
      "learning_rate": 9.136069114470843e-05,
      "loss": 0.2302,
      "step": 513
    },
    {
      "epoch": 1.6448,
      "grad_norm": 0.5916114449501038,
      "learning_rate": 9.114470842332614e-05,
      "loss": 0.377,
      "step": 514
    },
    {
      "epoch": 1.6480000000000001,
      "grad_norm": 0.40840020775794983,
      "learning_rate": 9.092872570194384e-05,
      "loss": 0.3538,
      "step": 515
    },
    {
      "epoch": 1.6512,
      "grad_norm": 0.2491573691368103,
      "learning_rate": 9.071274298056156e-05,
      "loss": 0.2891,
      "step": 516
    },
    {
      "epoch": 1.6543999999999999,
      "grad_norm": 0.22952386736869812,
      "learning_rate": 9.049676025917927e-05,
      "loss": 0.2823,
      "step": 517
    },
    {
      "epoch": 1.6576,
      "grad_norm": 0.19585691392421722,
      "learning_rate": 9.028077753779697e-05,
      "loss": 0.2819,
      "step": 518
    },
    {
      "epoch": 1.6608,
      "grad_norm": 0.16297647356987,
      "learning_rate": 9.00647948164147e-05,
      "loss": 0.2593,
      "step": 519
    },
    {
      "epoch": 1.6640000000000001,
      "grad_norm": 0.15820850431919098,
      "learning_rate": 8.98488120950324e-05,
      "loss": 0.2388,
      "step": 520
    },
    {
      "epoch": 1.6672,
      "grad_norm": 0.13360115885734558,
      "learning_rate": 8.96328293736501e-05,
      "loss": 0.2273,
      "step": 521
    },
    {
      "epoch": 1.6703999999999999,
      "grad_norm": 0.141969233751297,
      "learning_rate": 8.941684665226782e-05,
      "loss": 0.2095,
      "step": 522
    },
    {
      "epoch": 1.6736,
      "grad_norm": 0.13650546967983246,
      "learning_rate": 8.920086393088553e-05,
      "loss": 0.2184,
      "step": 523
    },
    {
      "epoch": 1.6768,
      "grad_norm": 0.14284595847129822,
      "learning_rate": 8.898488120950325e-05,
      "loss": 0.1972,
      "step": 524
    },
    {
      "epoch": 1.6800000000000002,
      "grad_norm": 0.1204083114862442,
      "learning_rate": 8.876889848812095e-05,
      "loss": 0.1992,
      "step": 525
    },
    {
      "epoch": 1.6832,
      "grad_norm": 0.14227019250392914,
      "learning_rate": 8.855291576673866e-05,
      "loss": 0.1877,
      "step": 526
    },
    {
      "epoch": 1.6864,
      "grad_norm": 0.1178409531712532,
      "learning_rate": 8.833693304535638e-05,
      "loss": 0.1857,
      "step": 527
    },
    {
      "epoch": 1.6896,
      "grad_norm": 0.11625292897224426,
      "learning_rate": 8.812095032397409e-05,
      "loss": 0.1708,
      "step": 528
    },
    {
      "epoch": 1.6928,
      "grad_norm": 0.12011835724115372,
      "learning_rate": 8.790496760259179e-05,
      "loss": 0.1634,
      "step": 529
    },
    {
      "epoch": 1.696,
      "grad_norm": 0.1141217052936554,
      "learning_rate": 8.768898488120951e-05,
      "loss": 0.1685,
      "step": 530
    },
    {
      "epoch": 1.6992,
      "grad_norm": 0.12465504556894302,
      "learning_rate": 8.747300215982722e-05,
      "loss": 0.1594,
      "step": 531
    },
    {
      "epoch": 1.7024,
      "grad_norm": 0.11226052045822144,
      "learning_rate": 8.725701943844493e-05,
      "loss": 0.1494,
      "step": 532
    },
    {
      "epoch": 1.7056,
      "grad_norm": 0.11531741172075272,
      "learning_rate": 8.704103671706264e-05,
      "loss": 0.1388,
      "step": 533
    },
    {
      "epoch": 1.7088,
      "grad_norm": 0.11810331046581268,
      "learning_rate": 8.682505399568035e-05,
      "loss": 0.1397,
      "step": 534
    },
    {
      "epoch": 1.712,
      "grad_norm": 0.10694646835327148,
      "learning_rate": 8.660907127429807e-05,
      "loss": 0.1371,
      "step": 535
    },
    {
      "epoch": 1.7151999999999998,
      "grad_norm": 0.10647082328796387,
      "learning_rate": 8.639308855291577e-05,
      "loss": 0.1316,
      "step": 536
    },
    {
      "epoch": 1.7184,
      "grad_norm": 0.09423928707838058,
      "learning_rate": 8.617710583153348e-05,
      "loss": 0.1191,
      "step": 537
    },
    {
      "epoch": 1.7216,
      "grad_norm": 0.11012126505374908,
      "learning_rate": 8.596112311015118e-05,
      "loss": 0.128,
      "step": 538
    },
    {
      "epoch": 1.7248,
      "grad_norm": 0.09803846478462219,
      "learning_rate": 8.57451403887689e-05,
      "loss": 0.1247,
      "step": 539
    },
    {
      "epoch": 1.728,
      "grad_norm": 0.0852908119559288,
      "learning_rate": 8.552915766738662e-05,
      "loss": 0.0984,
      "step": 540
    },
    {
      "epoch": 1.7311999999999999,
      "grad_norm": 0.09111905097961426,
      "learning_rate": 8.531317494600433e-05,
      "loss": 0.1158,
      "step": 541
    },
    {
      "epoch": 1.7344,
      "grad_norm": 0.08378136903047562,
      "learning_rate": 8.509719222462203e-05,
      "loss": 0.1075,
      "step": 542
    },
    {
      "epoch": 1.7376,
      "grad_norm": 0.10072252154350281,
      "learning_rate": 8.488120950323975e-05,
      "loss": 0.1085,
      "step": 543
    },
    {
      "epoch": 1.7408000000000001,
      "grad_norm": 0.08883649110794067,
      "learning_rate": 8.466522678185746e-05,
      "loss": 0.1136,
      "step": 544
    },
    {
      "epoch": 1.744,
      "grad_norm": 0.09486796706914902,
      "learning_rate": 8.444924406047516e-05,
      "loss": 0.1052,
      "step": 545
    },
    {
      "epoch": 1.7471999999999999,
      "grad_norm": 0.09419810771942139,
      "learning_rate": 8.423326133909287e-05,
      "loss": 0.1009,
      "step": 546
    },
    {
      "epoch": 1.7504,
      "grad_norm": 0.07664548605680466,
      "learning_rate": 8.401727861771059e-05,
      "loss": 0.0878,
      "step": 547
    },
    {
      "epoch": 1.7536,
      "grad_norm": 0.07250615954399109,
      "learning_rate": 8.38012958963283e-05,
      "loss": 0.083,
      "step": 548
    },
    {
      "epoch": 1.7568000000000001,
      "grad_norm": 0.07041087746620178,
      "learning_rate": 8.358531317494601e-05,
      "loss": 0.0853,
      "step": 549
    },
    {
      "epoch": 1.76,
      "grad_norm": 0.07786690443754196,
      "learning_rate": 8.336933045356372e-05,
      "loss": 0.0834,
      "step": 550
    },
    {
      "epoch": 1.7631999999999999,
      "grad_norm": 0.0714854970574379,
      "learning_rate": 8.315334773218144e-05,
      "loss": 0.0788,
      "step": 551
    },
    {
      "epoch": 1.7664,
      "grad_norm": 0.07371851801872253,
      "learning_rate": 8.293736501079914e-05,
      "loss": 0.0713,
      "step": 552
    },
    {
      "epoch": 1.7696,
      "grad_norm": 0.08487867563962936,
      "learning_rate": 8.272138228941685e-05,
      "loss": 0.0741,
      "step": 553
    },
    {
      "epoch": 1.7728000000000002,
      "grad_norm": 0.08879470825195312,
      "learning_rate": 8.250539956803455e-05,
      "loss": 0.0715,
      "step": 554
    },
    {
      "epoch": 1.776,
      "grad_norm": 0.06918741762638092,
      "learning_rate": 8.228941684665227e-05,
      "loss": 0.0685,
      "step": 555
    },
    {
      "epoch": 1.7792,
      "grad_norm": 0.08087245374917984,
      "learning_rate": 8.207343412526999e-05,
      "loss": 0.066,
      "step": 556
    },
    {
      "epoch": 1.7824,
      "grad_norm": 0.07447051256895065,
      "learning_rate": 8.18574514038877e-05,
      "loss": 0.0586,
      "step": 557
    },
    {
      "epoch": 1.7856,
      "grad_norm": 0.07038711756467819,
      "learning_rate": 8.16414686825054e-05,
      "loss": 0.0476,
      "step": 558
    },
    {
      "epoch": 1.7888,
      "grad_norm": 0.05773020535707474,
      "learning_rate": 8.142548596112312e-05,
      "loss": 0.0501,
      "step": 559
    },
    {
      "epoch": 1.792,
      "grad_norm": 0.09203335642814636,
      "learning_rate": 8.120950323974083e-05,
      "loss": 0.037,
      "step": 560
    },
    {
      "epoch": 1.7952,
      "grad_norm": 0.0712483823299408,
      "learning_rate": 8.099352051835853e-05,
      "loss": 0.0382,
      "step": 561
    },
    {
      "epoch": 1.7984,
      "grad_norm": 0.0689968466758728,
      "learning_rate": 8.077753779697624e-05,
      "loss": 0.0366,
      "step": 562
    },
    {
      "epoch": 1.8016,
      "grad_norm": 0.24511437118053436,
      "learning_rate": 8.056155507559396e-05,
      "loss": 0.2278,
      "step": 563
    },
    {
      "epoch": 1.8048,
      "grad_norm": 0.38854292035102844,
      "learning_rate": 8.034557235421166e-05,
      "loss": 0.3432,
      "step": 564
    },
    {
      "epoch": 1.8048,
      "eval_loss": 0.15455137193202972,
      "eval_runtime": 98.6755,
      "eval_samples_per_second": 11.239,
      "eval_steps_per_second": 0.709,
      "step": 564
    },
    {
      "epoch": 1.808,
      "grad_norm": 0.3798759877681732,
      "learning_rate": 8.012958963282938e-05,
      "loss": 0.2961,
      "step": 565
    },
    {
      "epoch": 1.8112,
      "grad_norm": 0.23518724739551544,
      "learning_rate": 7.991360691144709e-05,
      "loss": 0.2946,
      "step": 566
    },
    {
      "epoch": 1.8144,
      "grad_norm": 0.22188539803028107,
      "learning_rate": 7.969762419006481e-05,
      "loss": 0.259,
      "step": 567
    },
    {
      "epoch": 1.8176,
      "grad_norm": 0.1729852557182312,
      "learning_rate": 7.948164146868251e-05,
      "loss": 0.2542,
      "step": 568
    },
    {
      "epoch": 1.8208,
      "grad_norm": 0.1621440201997757,
      "learning_rate": 7.926565874730022e-05,
      "loss": 0.2266,
      "step": 569
    },
    {
      "epoch": 1.8239999999999998,
      "grad_norm": 0.16381879150867462,
      "learning_rate": 7.904967602591792e-05,
      "loss": 0.2315,
      "step": 570
    },
    {
      "epoch": 1.8272,
      "grad_norm": 0.1519705057144165,
      "learning_rate": 7.883369330453564e-05,
      "loss": 0.2088,
      "step": 571
    },
    {
      "epoch": 1.8304,
      "grad_norm": 0.1401461362838745,
      "learning_rate": 7.861771058315335e-05,
      "loss": 0.1982,
      "step": 572
    },
    {
      "epoch": 1.8336000000000001,
      "grad_norm": 0.13829216361045837,
      "learning_rate": 7.840172786177106e-05,
      "loss": 0.1901,
      "step": 573
    },
    {
      "epoch": 1.8368,
      "grad_norm": 0.14486414194107056,
      "learning_rate": 7.818574514038877e-05,
      "loss": 0.1939,
      "step": 574
    },
    {
      "epoch": 1.8399999999999999,
      "grad_norm": 0.15187855064868927,
      "learning_rate": 7.796976241900648e-05,
      "loss": 0.197,
      "step": 575
    },
    {
      "epoch": 1.8432,
      "grad_norm": 0.13877016305923462,
      "learning_rate": 7.77537796976242e-05,
      "loss": 0.1785,
      "step": 576
    },
    {
      "epoch": 1.8464,
      "grad_norm": 0.12995542585849762,
      "learning_rate": 7.75377969762419e-05,
      "loss": 0.1606,
      "step": 577
    },
    {
      "epoch": 1.8496000000000001,
      "grad_norm": 0.1339365392923355,
      "learning_rate": 7.732181425485961e-05,
      "loss": 0.1674,
      "step": 578
    },
    {
      "epoch": 1.8528,
      "grad_norm": 0.12757405638694763,
      "learning_rate": 7.710583153347733e-05,
      "loss": 0.1631,
      "step": 579
    },
    {
      "epoch": 1.8559999999999999,
      "grad_norm": 0.13829480111598969,
      "learning_rate": 7.688984881209504e-05,
      "loss": 0.1586,
      "step": 580
    },
    {
      "epoch": 1.8592,
      "grad_norm": 0.15590350329875946,
      "learning_rate": 7.667386609071274e-05,
      "loss": 0.1547,
      "step": 581
    },
    {
      "epoch": 1.8624,
      "grad_norm": 0.11258657276630402,
      "learning_rate": 7.645788336933045e-05,
      "loss": 0.1396,
      "step": 582
    },
    {
      "epoch": 1.8656000000000001,
      "grad_norm": 0.11138877272605896,
      "learning_rate": 7.624190064794817e-05,
      "loss": 0.135,
      "step": 583
    },
    {
      "epoch": 1.8688,
      "grad_norm": 0.11214291304349899,
      "learning_rate": 7.602591792656588e-05,
      "loss": 0.1401,
      "step": 584
    },
    {
      "epoch": 1.8719999999999999,
      "grad_norm": 0.10863449424505234,
      "learning_rate": 7.580993520518359e-05,
      "loss": 0.1305,
      "step": 585
    },
    {
      "epoch": 1.8752,
      "grad_norm": 0.09817415475845337,
      "learning_rate": 7.55939524838013e-05,
      "loss": 0.1306,
      "step": 586
    },
    {
      "epoch": 1.8784,
      "grad_norm": 0.10106535255908966,
      "learning_rate": 7.537796976241902e-05,
      "loss": 0.1272,
      "step": 587
    },
    {
      "epoch": 1.8816000000000002,
      "grad_norm": 0.10363661497831345,
      "learning_rate": 7.516198704103672e-05,
      "loss": 0.1132,
      "step": 588
    },
    {
      "epoch": 1.8848,
      "grad_norm": 0.10587815940380096,
      "learning_rate": 7.494600431965443e-05,
      "loss": 0.1211,
      "step": 589
    },
    {
      "epoch": 1.888,
      "grad_norm": 0.08577200770378113,
      "learning_rate": 7.473002159827213e-05,
      "loss": 0.123,
      "step": 590
    },
    {
      "epoch": 1.8912,
      "grad_norm": 0.08998187631368637,
      "learning_rate": 7.451403887688985e-05,
      "loss": 0.1163,
      "step": 591
    },
    {
      "epoch": 1.8944,
      "grad_norm": 0.10452810674905777,
      "learning_rate": 7.429805615550757e-05,
      "loss": 0.1062,
      "step": 592
    },
    {
      "epoch": 1.8976,
      "grad_norm": 0.09956659376621246,
      "learning_rate": 7.408207343412528e-05,
      "loss": 0.1049,
      "step": 593
    },
    {
      "epoch": 1.9008,
      "grad_norm": 0.07682431489229202,
      "learning_rate": 7.386609071274298e-05,
      "loss": 0.0983,
      "step": 594
    },
    {
      "epoch": 1.904,
      "grad_norm": 0.09479042887687683,
      "learning_rate": 7.36501079913607e-05,
      "loss": 0.0995,
      "step": 595
    },
    {
      "epoch": 1.9072,
      "grad_norm": 0.07582845538854599,
      "learning_rate": 7.34341252699784e-05,
      "loss": 0.0959,
      "step": 596
    },
    {
      "epoch": 1.9104,
      "grad_norm": 0.08059787005186081,
      "learning_rate": 7.321814254859611e-05,
      "loss": 0.0932,
      "step": 597
    },
    {
      "epoch": 1.9136,
      "grad_norm": 0.07389502227306366,
      "learning_rate": 7.300215982721382e-05,
      "loss": 0.0859,
      "step": 598
    },
    {
      "epoch": 1.9167999999999998,
      "grad_norm": 0.07714010775089264,
      "learning_rate": 7.278617710583154e-05,
      "loss": 0.0828,
      "step": 599
    },
    {
      "epoch": 1.92,
      "grad_norm": 0.07672327011823654,
      "learning_rate": 7.257019438444926e-05,
      "loss": 0.0847,
      "step": 600
    },
    {
      "epoch": 1.9232,
      "grad_norm": 0.07006808370351791,
      "learning_rate": 7.235421166306696e-05,
      "loss": 0.0679,
      "step": 601
    },
    {
      "epoch": 1.9264000000000001,
      "grad_norm": 0.06612425297498703,
      "learning_rate": 7.213822894168467e-05,
      "loss": 0.0713,
      "step": 602
    },
    {
      "epoch": 1.9296,
      "grad_norm": 0.07288745045661926,
      "learning_rate": 7.192224622030239e-05,
      "loss": 0.0683,
      "step": 603
    },
    {
      "epoch": 1.9327999999999999,
      "grad_norm": 0.09013992547988892,
      "learning_rate": 7.170626349892009e-05,
      "loss": 0.0653,
      "step": 604
    },
    {
      "epoch": 1.936,
      "grad_norm": 0.06977345794439316,
      "learning_rate": 7.14902807775378e-05,
      "loss": 0.0678,
      "step": 605
    },
    {
      "epoch": 1.9392,
      "grad_norm": 0.07785487920045853,
      "learning_rate": 7.12742980561555e-05,
      "loss": 0.057,
      "step": 606
    },
    {
      "epoch": 1.9424000000000001,
      "grad_norm": 0.09363540261983871,
      "learning_rate": 7.105831533477322e-05,
      "loss": 0.0526,
      "step": 607
    },
    {
      "epoch": 1.9456,
      "grad_norm": 0.0838814377784729,
      "learning_rate": 7.084233261339093e-05,
      "loss": 0.0517,
      "step": 608
    },
    {
      "epoch": 1.9487999999999999,
      "grad_norm": 0.07685600221157074,
      "learning_rate": 7.062634989200865e-05,
      "loss": 0.0488,
      "step": 609
    },
    {
      "epoch": 1.952,
      "grad_norm": 0.08948629349470139,
      "learning_rate": 7.041036717062635e-05,
      "loss": 0.0369,
      "step": 610
    },
    {
      "epoch": 1.9552,
      "grad_norm": 0.08253494650125504,
      "learning_rate": 7.019438444924407e-05,
      "loss": 0.0399,
      "step": 611
    },
    {
      "epoch": 1.9584000000000001,
      "grad_norm": 0.07222139090299606,
      "learning_rate": 6.997840172786178e-05,
      "loss": 0.0407,
      "step": 612
    },
    {
      "epoch": 1.9616,
      "grad_norm": 0.13718093931674957,
      "learning_rate": 6.976241900647948e-05,
      "loss": 0.1914,
      "step": 613
    },
    {
      "epoch": 1.9647999999999999,
      "grad_norm": 0.23136843740940094,
      "learning_rate": 6.954643628509719e-05,
      "loss": 0.2594,
      "step": 614
    },
    {
      "epoch": 1.968,
      "grad_norm": 0.1781855970621109,
      "learning_rate": 6.933045356371491e-05,
      "loss": 0.219,
      "step": 615
    },
    {
      "epoch": 1.9712,
      "grad_norm": 0.1544959545135498,
      "learning_rate": 6.911447084233261e-05,
      "loss": 0.1962,
      "step": 616
    },
    {
      "epoch": 1.9744000000000002,
      "grad_norm": 0.13913296163082123,
      "learning_rate": 6.889848812095032e-05,
      "loss": 0.1676,
      "step": 617
    },
    {
      "epoch": 1.9776,
      "grad_norm": 0.12640081346035004,
      "learning_rate": 6.868250539956804e-05,
      "loss": 0.1365,
      "step": 618
    },
    {
      "epoch": 1.9808,
      "grad_norm": 0.10753028094768524,
      "learning_rate": 6.846652267818576e-05,
      "loss": 0.1188,
      "step": 619
    },
    {
      "epoch": 1.984,
      "grad_norm": 0.10328862816095352,
      "learning_rate": 6.825053995680346e-05,
      "loss": 0.1099,
      "step": 620
    },
    {
      "epoch": 1.9872,
      "grad_norm": 0.08222667127847672,
      "learning_rate": 6.803455723542117e-05,
      "loss": 0.0834,
      "step": 621
    },
    {
      "epoch": 1.9904,
      "grad_norm": 0.07155530154705048,
      "learning_rate": 6.781857451403887e-05,
      "loss": 0.0784,
      "step": 622
    },
    {
      "epoch": 1.9936,
      "grad_norm": 0.08450774103403091,
      "learning_rate": 6.76025917926566e-05,
      "loss": 0.067,
      "step": 623
    },
    {
      "epoch": 1.9968,
      "grad_norm": 0.06986336410045624,
      "learning_rate": 6.73866090712743e-05,
      "loss": 0.0483,
      "step": 624
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.12182948738336563,
      "learning_rate": 6.7170626349892e-05,
      "loss": 0.0528,
      "step": 625
    },
    {
      "epoch": 2.0032,
      "grad_norm": 0.21485474705696106,
      "learning_rate": 6.695464362850972e-05,
      "loss": 0.3176,
      "step": 626
    },
    {
      "epoch": 2.0064,
      "grad_norm": 0.21282453835010529,
      "learning_rate": 6.673866090712744e-05,
      "loss": 0.2924,
      "step": 627
    },
    {
      "epoch": 2.0096,
      "grad_norm": 0.19815070927143097,
      "learning_rate": 6.652267818574515e-05,
      "loss": 0.2624,
      "step": 628
    },
    {
      "epoch": 2.0128,
      "grad_norm": 0.1900617778301239,
      "learning_rate": 6.630669546436285e-05,
      "loss": 0.2326,
      "step": 629
    },
    {
      "epoch": 2.016,
      "grad_norm": 0.1547027975320816,
      "learning_rate": 6.609071274298056e-05,
      "loss": 0.2313,
      "step": 630
    },
    {
      "epoch": 2.0192,
      "grad_norm": 0.15883994102478027,
      "learning_rate": 6.587473002159828e-05,
      "loss": 0.2087,
      "step": 631
    },
    {
      "epoch": 2.0224,
      "grad_norm": 0.14328454434871674,
      "learning_rate": 6.565874730021599e-05,
      "loss": 0.2023,
      "step": 632
    },
    {
      "epoch": 2.0256,
      "grad_norm": 0.14063121378421783,
      "learning_rate": 6.544276457883369e-05,
      "loss": 0.1964,
      "step": 633
    },
    {
      "epoch": 2.0288,
      "grad_norm": 0.13863502442836761,
      "learning_rate": 6.52267818574514e-05,
      "loss": 0.1816,
      "step": 634
    },
    {
      "epoch": 2.032,
      "grad_norm": 0.1420655995607376,
      "learning_rate": 6.501079913606913e-05,
      "loss": 0.1657,
      "step": 635
    },
    {
      "epoch": 2.0352,
      "grad_norm": 0.15209457278251648,
      "learning_rate": 6.479481641468683e-05,
      "loss": 0.1767,
      "step": 636
    },
    {
      "epoch": 2.0384,
      "grad_norm": 0.14696547389030457,
      "learning_rate": 6.457883369330454e-05,
      "loss": 0.1592,
      "step": 637
    },
    {
      "epoch": 2.0416,
      "grad_norm": 0.15566587448120117,
      "learning_rate": 6.436285097192225e-05,
      "loss": 0.162,
      "step": 638
    },
    {
      "epoch": 2.0448,
      "grad_norm": 0.14631600677967072,
      "learning_rate": 6.414686825053997e-05,
      "loss": 0.1451,
      "step": 639
    },
    {
      "epoch": 2.048,
      "grad_norm": 0.15059977769851685,
      "learning_rate": 6.393088552915767e-05,
      "loss": 0.145,
      "step": 640
    },
    {
      "epoch": 2.0512,
      "grad_norm": 0.1729634553194046,
      "learning_rate": 6.371490280777538e-05,
      "loss": 0.1597,
      "step": 641
    },
    {
      "epoch": 2.0544,
      "grad_norm": 0.15901777148246765,
      "learning_rate": 6.349892008639308e-05,
      "loss": 0.1358,
      "step": 642
    },
    {
      "epoch": 2.0576,
      "grad_norm": 0.14063481986522675,
      "learning_rate": 6.32829373650108e-05,
      "loss": 0.1263,
      "step": 643
    },
    {
      "epoch": 2.0608,
      "grad_norm": 0.13899317383766174,
      "learning_rate": 6.306695464362852e-05,
      "loss": 0.1264,
      "step": 644
    },
    {
      "epoch": 2.064,
      "grad_norm": 0.14254115521907806,
      "learning_rate": 6.285097192224623e-05,
      "loss": 0.1278,
      "step": 645
    },
    {
      "epoch": 2.0672,
      "grad_norm": 0.14698676764965057,
      "learning_rate": 6.263498920086393e-05,
      "loss": 0.1205,
      "step": 646
    },
    {
      "epoch": 2.0704,
      "grad_norm": 0.13348756730556488,
      "learning_rate": 6.241900647948165e-05,
      "loss": 0.115,
      "step": 647
    },
    {
      "epoch": 2.0736,
      "grad_norm": 0.12698352336883545,
      "learning_rate": 6.220302375809936e-05,
      "loss": 0.1188,
      "step": 648
    },
    {
      "epoch": 2.0768,
      "grad_norm": 0.12209378182888031,
      "learning_rate": 6.198704103671706e-05,
      "loss": 0.1122,
      "step": 649
    },
    {
      "epoch": 2.08,
      "grad_norm": 0.11685489863157272,
      "learning_rate": 6.177105831533477e-05,
      "loss": 0.1085,
      "step": 650
    },
    {
      "epoch": 2.0832,
      "grad_norm": 0.11181914061307907,
      "learning_rate": 6.155507559395249e-05,
      "loss": 0.1035,
      "step": 651
    },
    {
      "epoch": 2.0864,
      "grad_norm": 0.1101808026432991,
      "learning_rate": 6.133909287257019e-05,
      "loss": 0.0966,
      "step": 652
    },
    {
      "epoch": 2.0896,
      "grad_norm": 0.10678301751613617,
      "learning_rate": 6.112311015118791e-05,
      "loss": 0.102,
      "step": 653
    },
    {
      "epoch": 2.0928,
      "grad_norm": 0.09967271983623505,
      "learning_rate": 6.0907127429805624e-05,
      "loss": 0.0949,
      "step": 654
    },
    {
      "epoch": 2.096,
      "grad_norm": 0.1096930131316185,
      "learning_rate": 6.069114470842333e-05,
      "loss": 0.0984,
      "step": 655
    },
    {
      "epoch": 2.0992,
      "grad_norm": 0.10135135054588318,
      "learning_rate": 6.047516198704104e-05,
      "loss": 0.0874,
      "step": 656
    },
    {
      "epoch": 2.1024,
      "grad_norm": 0.11995294690132141,
      "learning_rate": 6.025917926565875e-05,
      "loss": 0.0893,
      "step": 657
    },
    {
      "epoch": 2.1056,
      "grad_norm": 0.11153828352689743,
      "learning_rate": 6.004319654427646e-05,
      "loss": 0.0801,
      "step": 658
    },
    {
      "epoch": 2.1088,
      "grad_norm": 0.0917532816529274,
      "learning_rate": 5.9827213822894166e-05,
      "loss": 0.0869,
      "step": 659
    },
    {
      "epoch": 2.112,
      "grad_norm": 0.09878644347190857,
      "learning_rate": 5.961123110151188e-05,
      "loss": 0.0774,
      "step": 660
    },
    {
      "epoch": 2.1152,
      "grad_norm": 0.08821044117212296,
      "learning_rate": 5.93952483801296e-05,
      "loss": 0.0833,
      "step": 661
    },
    {
      "epoch": 2.1184,
      "grad_norm": 0.09172733873128891,
      "learning_rate": 5.917926565874731e-05,
      "loss": 0.0694,
      "step": 662
    },
    {
      "epoch": 2.1216,
      "grad_norm": 0.08634558320045471,
      "learning_rate": 5.8963282937365016e-05,
      "loss": 0.0671,
      "step": 663
    },
    {
      "epoch": 2.1248,
      "grad_norm": 0.09600189328193665,
      "learning_rate": 5.874730021598273e-05,
      "loss": 0.0737,
      "step": 664
    },
    {
      "epoch": 2.128,
      "grad_norm": 0.08632864058017731,
      "learning_rate": 5.8531317494600434e-05,
      "loss": 0.0664,
      "step": 665
    },
    {
      "epoch": 2.1312,
      "grad_norm": 0.08285550028085709,
      "learning_rate": 5.8315334773218146e-05,
      "loss": 0.0591,
      "step": 666
    },
    {
      "epoch": 2.1344,
      "grad_norm": 0.08246347308158875,
      "learning_rate": 5.809935205183585e-05,
      "loss": 0.068,
      "step": 667
    },
    {
      "epoch": 2.1376,
      "grad_norm": 0.07746779173612595,
      "learning_rate": 5.7883369330453564e-05,
      "loss": 0.0601,
      "step": 668
    },
    {
      "epoch": 2.1408,
      "grad_norm": 0.08443091809749603,
      "learning_rate": 5.766738660907127e-05,
      "loss": 0.0536,
      "step": 669
    },
    {
      "epoch": 2.144,
      "grad_norm": 0.07817403972148895,
      "learning_rate": 5.7451403887688996e-05,
      "loss": 0.0525,
      "step": 670
    },
    {
      "epoch": 2.1471999999999998,
      "grad_norm": 0.07569054514169693,
      "learning_rate": 5.72354211663067e-05,
      "loss": 0.0472,
      "step": 671
    },
    {
      "epoch": 2.1504,
      "grad_norm": 0.07550885528326035,
      "learning_rate": 5.7019438444924414e-05,
      "loss": 0.0415,
      "step": 672
    },
    {
      "epoch": 2.1536,
      "grad_norm": 0.08631633967161179,
      "learning_rate": 5.680345572354212e-05,
      "loss": 0.0369,
      "step": 673
    },
    {
      "epoch": 2.1568,
      "grad_norm": 0.08692370355129242,
      "learning_rate": 5.658747300215983e-05,
      "loss": 0.0389,
      "step": 674
    },
    {
      "epoch": 2.16,
      "grad_norm": 0.08637642860412598,
      "learning_rate": 5.637149028077754e-05,
      "loss": 0.0416,
      "step": 675
    },
    {
      "epoch": 2.1632,
      "grad_norm": 0.3150486946105957,
      "learning_rate": 5.615550755939525e-05,
      "loss": 0.3097,
      "step": 676
    },
    {
      "epoch": 2.1664,
      "grad_norm": 0.2768765389919281,
      "learning_rate": 5.5939524838012955e-05,
      "loss": 0.2902,
      "step": 677
    },
    {
      "epoch": 2.1696,
      "grad_norm": 0.24833029508590698,
      "learning_rate": 5.572354211663067e-05,
      "loss": 0.2622,
      "step": 678
    },
    {
      "epoch": 2.1728,
      "grad_norm": 0.22931402921676636,
      "learning_rate": 5.550755939524839e-05,
      "loss": 0.247,
      "step": 679
    },
    {
      "epoch": 2.176,
      "grad_norm": 0.2209731638431549,
      "learning_rate": 5.52915766738661e-05,
      "loss": 0.2239,
      "step": 680
    },
    {
      "epoch": 2.1792,
      "grad_norm": 0.21932345628738403,
      "learning_rate": 5.5075593952483805e-05,
      "loss": 0.2227,
      "step": 681
    },
    {
      "epoch": 2.1824,
      "grad_norm": 0.18809157609939575,
      "learning_rate": 5.485961123110152e-05,
      "loss": 0.2084,
      "step": 682
    },
    {
      "epoch": 2.1856,
      "grad_norm": 0.19216854870319366,
      "learning_rate": 5.464362850971922e-05,
      "loss": 0.2007,
      "step": 683
    },
    {
      "epoch": 2.1888,
      "grad_norm": 0.18065956234931946,
      "learning_rate": 5.4427645788336935e-05,
      "loss": 0.2018,
      "step": 684
    },
    {
      "epoch": 2.192,
      "grad_norm": 0.17621299624443054,
      "learning_rate": 5.421166306695464e-05,
      "loss": 0.1912,
      "step": 685
    },
    {
      "epoch": 2.1952,
      "grad_norm": 0.16206060349941254,
      "learning_rate": 5.3995680345572353e-05,
      "loss": 0.1807,
      "step": 686
    },
    {
      "epoch": 2.1984,
      "grad_norm": 0.1621609330177307,
      "learning_rate": 5.377969762419006e-05,
      "loss": 0.1757,
      "step": 687
    },
    {
      "epoch": 2.2016,
      "grad_norm": 0.17002031207084656,
      "learning_rate": 5.3563714902807785e-05,
      "loss": 0.1695,
      "step": 688
    },
    {
      "epoch": 2.2048,
      "grad_norm": 0.15702687203884125,
      "learning_rate": 5.334773218142549e-05,
      "loss": 0.1516,
      "step": 689
    },
    {
      "epoch": 2.208,
      "grad_norm": 0.15585163235664368,
      "learning_rate": 5.31317494600432e-05,
      "loss": 0.1587,
      "step": 690
    },
    {
      "epoch": 2.2112,
      "grad_norm": 0.14164604246616364,
      "learning_rate": 5.291576673866091e-05,
      "loss": 0.144,
      "step": 691
    },
    {
      "epoch": 2.2144,
      "grad_norm": 0.13723978400230408,
      "learning_rate": 5.269978401727862e-05,
      "loss": 0.1472,
      "step": 692
    },
    {
      "epoch": 2.2176,
      "grad_norm": 0.13875451683998108,
      "learning_rate": 5.248380129589633e-05,
      "loss": 0.1366,
      "step": 693
    },
    {
      "epoch": 2.2208,
      "grad_norm": 0.13335053622722626,
      "learning_rate": 5.226781857451404e-05,
      "loss": 0.1388,
      "step": 694
    },
    {
      "epoch": 2.224,
      "grad_norm": 0.1500721573829651,
      "learning_rate": 5.2051835853131745e-05,
      "loss": 0.1359,
      "step": 695
    },
    {
      "epoch": 2.2272,
      "grad_norm": 0.12894776463508606,
      "learning_rate": 5.183585313174947e-05,
      "loss": 0.1179,
      "step": 696
    },
    {
      "epoch": 2.2304,
      "grad_norm": 0.12912538647651672,
      "learning_rate": 5.1619870410367176e-05,
      "loss": 0.1234,
      "step": 697
    },
    {
      "epoch": 2.2336,
      "grad_norm": 0.14003194868564606,
      "learning_rate": 5.140388768898489e-05,
      "loss": 0.1124,
      "step": 698
    },
    {
      "epoch": 2.2368,
      "grad_norm": 0.14063040912151337,
      "learning_rate": 5.1187904967602594e-05,
      "loss": 0.1106,
      "step": 699
    },
    {
      "epoch": 2.24,
      "grad_norm": 0.13379035890102386,
      "learning_rate": 5.097192224622031e-05,
      "loss": 0.1145,
      "step": 700
    },
    {
      "epoch": 2.2432,
      "grad_norm": 0.13818082213401794,
      "learning_rate": 5.075593952483801e-05,
      "loss": 0.1095,
      "step": 701
    },
    {
      "epoch": 2.2464,
      "grad_norm": 0.12057525664567947,
      "learning_rate": 5.0539956803455725e-05,
      "loss": 0.1,
      "step": 702
    },
    {
      "epoch": 2.2496,
      "grad_norm": 0.13552822172641754,
      "learning_rate": 5.032397408207343e-05,
      "loss": 0.1052,
      "step": 703
    },
    {
      "epoch": 2.2528,
      "grad_norm": 0.11298465728759766,
      "learning_rate": 5.010799136069114e-05,
      "loss": 0.0907,
      "step": 704
    },
    {
      "epoch": 2.2560000000000002,
      "grad_norm": 0.12159287929534912,
      "learning_rate": 4.9892008639308855e-05,
      "loss": 0.0987,
      "step": 705
    },
    {
      "epoch": 2.2592,
      "grad_norm": 0.12513500452041626,
      "learning_rate": 4.967602591792657e-05,
      "loss": 0.0996,
      "step": 706
    },
    {
      "epoch": 2.2624,
      "grad_norm": 0.12159507721662521,
      "learning_rate": 4.946004319654428e-05,
      "loss": 0.0955,
      "step": 707
    },
    {
      "epoch": 2.2656,
      "grad_norm": 0.12080161273479462,
      "learning_rate": 4.924406047516199e-05,
      "loss": 0.0909,
      "step": 708
    },
    {
      "epoch": 2.2688,
      "grad_norm": 0.1053474023938179,
      "learning_rate": 4.90280777537797e-05,
      "loss": 0.0827,
      "step": 709
    },
    {
      "epoch": 2.2720000000000002,
      "grad_norm": 0.11075568944215775,
      "learning_rate": 4.881209503239741e-05,
      "loss": 0.0852,
      "step": 710
    },
    {
      "epoch": 2.2752,
      "grad_norm": 0.09885460883378983,
      "learning_rate": 4.8596112311015116e-05,
      "loss": 0.0793,
      "step": 711
    },
    {
      "epoch": 2.2784,
      "grad_norm": 0.11649411916732788,
      "learning_rate": 4.8380129589632835e-05,
      "loss": 0.0724,
      "step": 712
    },
    {
      "epoch": 2.2816,
      "grad_norm": 0.09865808486938477,
      "learning_rate": 4.816414686825054e-05,
      "loss": 0.0718,
      "step": 713
    },
    {
      "epoch": 2.2848,
      "grad_norm": 0.10835938900709152,
      "learning_rate": 4.794816414686825e-05,
      "loss": 0.0753,
      "step": 714
    },
    {
      "epoch": 2.288,
      "grad_norm": 0.0913906842470169,
      "learning_rate": 4.773218142548596e-05,
      "loss": 0.0648,
      "step": 715
    },
    {
      "epoch": 2.2912,
      "grad_norm": 0.09970086067914963,
      "learning_rate": 4.751619870410368e-05,
      "loss": 0.0615,
      "step": 716
    },
    {
      "epoch": 2.2944,
      "grad_norm": 0.12140193581581116,
      "learning_rate": 4.7300215982721384e-05,
      "loss": 0.0678,
      "step": 717
    },
    {
      "epoch": 2.2976,
      "grad_norm": 0.08068012446165085,
      "learning_rate": 4.7084233261339096e-05,
      "loss": 0.0563,
      "step": 718
    },
    {
      "epoch": 2.3008,
      "grad_norm": 0.09174259752035141,
      "learning_rate": 4.68682505399568e-05,
      "loss": 0.0505,
      "step": 719
    },
    {
      "epoch": 2.304,
      "grad_norm": 0.11462648212909698,
      "learning_rate": 4.665226781857452e-05,
      "loss": 0.0493,
      "step": 720
    },
    {
      "epoch": 2.3072,
      "grad_norm": 0.08834739029407501,
      "learning_rate": 4.6436285097192227e-05,
      "loss": 0.0468,
      "step": 721
    },
    {
      "epoch": 2.3104,
      "grad_norm": 0.1226147711277008,
      "learning_rate": 4.622030237580994e-05,
      "loss": 0.0414,
      "step": 722
    },
    {
      "epoch": 2.3136,
      "grad_norm": 0.0843595340847969,
      "learning_rate": 4.6004319654427645e-05,
      "loss": 0.0375,
      "step": 723
    },
    {
      "epoch": 2.3168,
      "grad_norm": 0.08732597529888153,
      "learning_rate": 4.578833693304536e-05,
      "loss": 0.0382,
      "step": 724
    },
    {
      "epoch": 2.32,
      "grad_norm": 0.08764883875846863,
      "learning_rate": 4.557235421166307e-05,
      "loss": 0.0448,
      "step": 725
    },
    {
      "epoch": 2.3232,
      "grad_norm": 0.39725857973098755,
      "learning_rate": 4.535637149028078e-05,
      "loss": 0.3338,
      "step": 726
    },
    {
      "epoch": 2.3264,
      "grad_norm": 0.3012124001979828,
      "learning_rate": 4.514038876889849e-05,
      "loss": 0.2671,
      "step": 727
    },
    {
      "epoch": 2.3296,
      "grad_norm": 0.2694571316242218,
      "learning_rate": 4.49244060475162e-05,
      "loss": 0.2519,
      "step": 728
    },
    {
      "epoch": 2.3327999999999998,
      "grad_norm": 0.2513049840927124,
      "learning_rate": 4.470842332613391e-05,
      "loss": 0.2425,
      "step": 729
    },
    {
      "epoch": 2.336,
      "grad_norm": 0.23179247975349426,
      "learning_rate": 4.4492440604751625e-05,
      "loss": 0.233,
      "step": 730
    },
    {
      "epoch": 2.3392,
      "grad_norm": 0.2324911504983902,
      "learning_rate": 4.427645788336933e-05,
      "loss": 0.2281,
      "step": 731
    },
    {
      "epoch": 2.3424,
      "grad_norm": 0.1942141354084015,
      "learning_rate": 4.406047516198704e-05,
      "loss": 0.2056,
      "step": 732
    },
    {
      "epoch": 2.3456,
      "grad_norm": 0.18909721076488495,
      "learning_rate": 4.3844492440604755e-05,
      "loss": 0.1915,
      "step": 733
    },
    {
      "epoch": 2.3487999999999998,
      "grad_norm": 0.18114371597766876,
      "learning_rate": 4.362850971922247e-05,
      "loss": 0.1884,
      "step": 734
    },
    {
      "epoch": 2.352,
      "grad_norm": 0.1852104514837265,
      "learning_rate": 4.341252699784017e-05,
      "loss": 0.1878,
      "step": 735
    },
    {
      "epoch": 2.3552,
      "grad_norm": 0.17727422714233398,
      "learning_rate": 4.3196544276457885e-05,
      "loss": 0.1806,
      "step": 736
    },
    {
      "epoch": 2.3584,
      "grad_norm": 0.16422376036643982,
      "learning_rate": 4.298056155507559e-05,
      "loss": 0.1778,
      "step": 737
    },
    {
      "epoch": 2.3616,
      "grad_norm": 0.17631520330905914,
      "learning_rate": 4.276457883369331e-05,
      "loss": 0.1713,
      "step": 738
    },
    {
      "epoch": 2.3648,
      "grad_norm": 0.14831188321113586,
      "learning_rate": 4.2548596112311016e-05,
      "loss": 0.1603,
      "step": 739
    },
    {
      "epoch": 2.368,
      "grad_norm": 0.15049102902412415,
      "learning_rate": 4.233261339092873e-05,
      "loss": 0.1537,
      "step": 740
    },
    {
      "epoch": 2.3712,
      "grad_norm": 0.15437766909599304,
      "learning_rate": 4.2116630669546434e-05,
      "loss": 0.1496,
      "step": 741
    },
    {
      "epoch": 2.3744,
      "grad_norm": 0.14086145162582397,
      "learning_rate": 4.190064794816415e-05,
      "loss": 0.146,
      "step": 742
    },
    {
      "epoch": 2.3776,
      "grad_norm": 0.1451355218887329,
      "learning_rate": 4.168466522678186e-05,
      "loss": 0.1469,
      "step": 743
    },
    {
      "epoch": 2.3808,
      "grad_norm": 0.1335173398256302,
      "learning_rate": 4.146868250539957e-05,
      "loss": 0.1311,
      "step": 744
    },
    {
      "epoch": 2.384,
      "grad_norm": 0.14164811372756958,
      "learning_rate": 4.125269978401728e-05,
      "loss": 0.1259,
      "step": 745
    },
    {
      "epoch": 2.3872,
      "grad_norm": 0.14108070731163025,
      "learning_rate": 4.1036717062634996e-05,
      "loss": 0.1267,
      "step": 746
    },
    {
      "epoch": 2.3904,
      "grad_norm": 0.1447277069091797,
      "learning_rate": 4.08207343412527e-05,
      "loss": 0.1241,
      "step": 747
    },
    {
      "epoch": 2.3936,
      "grad_norm": 0.12743854522705078,
      "learning_rate": 4.0604751619870414e-05,
      "loss": 0.12,
      "step": 748
    },
    {
      "epoch": 2.3968,
      "grad_norm": 0.13329988718032837,
      "learning_rate": 4.038876889848812e-05,
      "loss": 0.1203,
      "step": 749
    },
    {
      "epoch": 2.4,
      "grad_norm": 0.14377236366271973,
      "learning_rate": 4.017278617710583e-05,
      "loss": 0.1112,
      "step": 750
    },
    {
      "epoch": 2.4032,
      "grad_norm": 0.16236500442028046,
      "learning_rate": 3.9956803455723544e-05,
      "loss": 0.1235,
      "step": 751
    },
    {
      "epoch": 2.4064,
      "grad_norm": 0.130780428647995,
      "learning_rate": 3.974082073434126e-05,
      "loss": 0.1074,
      "step": 752
    },
    {
      "epoch": 2.4064,
      "eval_loss": 0.15772394835948944,
      "eval_runtime": 99.5424,
      "eval_samples_per_second": 11.141,
      "eval_steps_per_second": 0.703,
      "step": 752
    },
    {
      "epoch": 2.4096,
      "grad_norm": 0.1341335028409958,
      "learning_rate": 3.952483801295896e-05,
      "loss": 0.1124,
      "step": 753
    },
    {
      "epoch": 2.4128,
      "grad_norm": 0.12165698409080505,
      "learning_rate": 3.9308855291576675e-05,
      "loss": 0.0894,
      "step": 754
    },
    {
      "epoch": 2.416,
      "grad_norm": 0.14048254489898682,
      "learning_rate": 3.909287257019439e-05,
      "loss": 0.0934,
      "step": 755
    },
    {
      "epoch": 2.4192,
      "grad_norm": 0.1203276515007019,
      "learning_rate": 3.88768898488121e-05,
      "loss": 0.0897,
      "step": 756
    },
    {
      "epoch": 2.4224,
      "grad_norm": 0.14698557555675507,
      "learning_rate": 3.8660907127429805e-05,
      "loss": 0.1083,
      "step": 757
    },
    {
      "epoch": 2.4256,
      "grad_norm": 0.11951117217540741,
      "learning_rate": 3.844492440604752e-05,
      "loss": 0.0911,
      "step": 758
    },
    {
      "epoch": 2.4288,
      "grad_norm": 0.11983277648687363,
      "learning_rate": 3.822894168466522e-05,
      "loss": 0.0907,
      "step": 759
    },
    {
      "epoch": 2.432,
      "grad_norm": 0.1101461872458458,
      "learning_rate": 3.801295896328294e-05,
      "loss": 0.0855,
      "step": 760
    },
    {
      "epoch": 2.4352,
      "grad_norm": 0.11711747199296951,
      "learning_rate": 3.779697624190065e-05,
      "loss": 0.0827,
      "step": 761
    },
    {
      "epoch": 2.4384,
      "grad_norm": 0.11128281056880951,
      "learning_rate": 3.758099352051836e-05,
      "loss": 0.0811,
      "step": 762
    },
    {
      "epoch": 2.4416,
      "grad_norm": 0.12389536201953888,
      "learning_rate": 3.7365010799136066e-05,
      "loss": 0.0774,
      "step": 763
    },
    {
      "epoch": 2.4448,
      "grad_norm": 0.09233035892248154,
      "learning_rate": 3.7149028077753785e-05,
      "loss": 0.0673,
      "step": 764
    },
    {
      "epoch": 2.448,
      "grad_norm": 0.10512736439704895,
      "learning_rate": 3.693304535637149e-05,
      "loss": 0.0766,
      "step": 765
    },
    {
      "epoch": 2.4512,
      "grad_norm": 0.09805464744567871,
      "learning_rate": 3.67170626349892e-05,
      "loss": 0.0709,
      "step": 766
    },
    {
      "epoch": 2.4544,
      "grad_norm": 0.09595753252506256,
      "learning_rate": 3.650107991360691e-05,
      "loss": 0.0678,
      "step": 767
    },
    {
      "epoch": 2.4576000000000002,
      "grad_norm": 0.0970940887928009,
      "learning_rate": 3.628509719222463e-05,
      "loss": 0.0693,
      "step": 768
    },
    {
      "epoch": 2.4608,
      "grad_norm": 0.11944019049406052,
      "learning_rate": 3.6069114470842334e-05,
      "loss": 0.0639,
      "step": 769
    },
    {
      "epoch": 2.464,
      "grad_norm": 0.10206244140863419,
      "learning_rate": 3.5853131749460046e-05,
      "loss": 0.055,
      "step": 770
    },
    {
      "epoch": 2.4672,
      "grad_norm": 0.08787868171930313,
      "learning_rate": 3.563714902807775e-05,
      "loss": 0.0489,
      "step": 771
    },
    {
      "epoch": 2.4704,
      "grad_norm": 0.1139385923743248,
      "learning_rate": 3.5421166306695464e-05,
      "loss": 0.0471,
      "step": 772
    },
    {
      "epoch": 2.4736000000000002,
      "grad_norm": 0.1140502542257309,
      "learning_rate": 3.5205183585313177e-05,
      "loss": 0.0451,
      "step": 773
    },
    {
      "epoch": 2.4768,
      "grad_norm": 0.09999094903469086,
      "learning_rate": 3.498920086393089e-05,
      "loss": 0.0418,
      "step": 774
    },
    {
      "epoch": 2.48,
      "grad_norm": 0.09593232721090317,
      "learning_rate": 3.4773218142548595e-05,
      "loss": 0.0463,
      "step": 775
    },
    {
      "epoch": 2.4832,
      "grad_norm": 0.36582303047180176,
      "learning_rate": 3.455723542116631e-05,
      "loss": 0.3316,
      "step": 776
    },
    {
      "epoch": 2.4864,
      "grad_norm": 0.29561948776245117,
      "learning_rate": 3.434125269978402e-05,
      "loss": 0.2791,
      "step": 777
    },
    {
      "epoch": 2.4896,
      "grad_norm": 0.28371092677116394,
      "learning_rate": 3.412526997840173e-05,
      "loss": 0.2448,
      "step": 778
    },
    {
      "epoch": 2.4928,
      "grad_norm": 0.22806163132190704,
      "learning_rate": 3.390928725701944e-05,
      "loss": 0.2263,
      "step": 779
    },
    {
      "epoch": 2.496,
      "grad_norm": 0.23447364568710327,
      "learning_rate": 3.369330453563715e-05,
      "loss": 0.2419,
      "step": 780
    },
    {
      "epoch": 2.4992,
      "grad_norm": 0.2209642231464386,
      "learning_rate": 3.347732181425486e-05,
      "loss": 0.2184,
      "step": 781
    },
    {
      "epoch": 2.5023999999999997,
      "grad_norm": 0.21145261824131012,
      "learning_rate": 3.3261339092872575e-05,
      "loss": 0.1971,
      "step": 782
    },
    {
      "epoch": 2.5056000000000003,
      "grad_norm": 0.21573157608509064,
      "learning_rate": 3.304535637149028e-05,
      "loss": 0.2033,
      "step": 783
    },
    {
      "epoch": 2.5088,
      "grad_norm": 0.20693305134773254,
      "learning_rate": 3.282937365010799e-05,
      "loss": 0.1903,
      "step": 784
    },
    {
      "epoch": 2.512,
      "grad_norm": 0.19380688667297363,
      "learning_rate": 3.26133909287257e-05,
      "loss": 0.178,
      "step": 785
    },
    {
      "epoch": 2.5152,
      "grad_norm": 0.18519894778728485,
      "learning_rate": 3.239740820734342e-05,
      "loss": 0.1866,
      "step": 786
    },
    {
      "epoch": 2.5183999999999997,
      "grad_norm": 0.17977797985076904,
      "learning_rate": 3.218142548596112e-05,
      "loss": 0.1756,
      "step": 787
    },
    {
      "epoch": 2.5216,
      "grad_norm": 0.17349271476268768,
      "learning_rate": 3.1965442764578836e-05,
      "loss": 0.1686,
      "step": 788
    },
    {
      "epoch": 2.5248,
      "grad_norm": 0.17866250872612,
      "learning_rate": 3.174946004319654e-05,
      "loss": 0.1797,
      "step": 789
    },
    {
      "epoch": 2.528,
      "grad_norm": 0.1654186099767685,
      "learning_rate": 3.153347732181426e-05,
      "loss": 0.1586,
      "step": 790
    },
    {
      "epoch": 2.5312,
      "grad_norm": 0.15822580456733704,
      "learning_rate": 3.1317494600431966e-05,
      "loss": 0.1398,
      "step": 791
    },
    {
      "epoch": 2.5343999999999998,
      "grad_norm": 0.15062500536441803,
      "learning_rate": 3.110151187904968e-05,
      "loss": 0.1528,
      "step": 792
    },
    {
      "epoch": 2.5376,
      "grad_norm": 0.14847388863563538,
      "learning_rate": 3.0885529157667384e-05,
      "loss": 0.142,
      "step": 793
    },
    {
      "epoch": 2.5408,
      "grad_norm": 0.15090975165367126,
      "learning_rate": 3.0669546436285096e-05,
      "loss": 0.1319,
      "step": 794
    },
    {
      "epoch": 2.544,
      "grad_norm": 0.1493859738111496,
      "learning_rate": 3.0453563714902812e-05,
      "loss": 0.1255,
      "step": 795
    },
    {
      "epoch": 2.5472,
      "grad_norm": 0.16549192368984222,
      "learning_rate": 3.023758099352052e-05,
      "loss": 0.1397,
      "step": 796
    },
    {
      "epoch": 2.5504,
      "grad_norm": 0.13174007833003998,
      "learning_rate": 3.002159827213823e-05,
      "loss": 0.125,
      "step": 797
    },
    {
      "epoch": 2.5536,
      "grad_norm": 0.13549922406673431,
      "learning_rate": 2.980561555075594e-05,
      "loss": 0.1177,
      "step": 798
    },
    {
      "epoch": 2.5568,
      "grad_norm": 0.12907634675502777,
      "learning_rate": 2.9589632829373655e-05,
      "loss": 0.1174,
      "step": 799
    },
    {
      "epoch": 2.56,
      "grad_norm": 0.120298832654953,
      "learning_rate": 2.9373650107991364e-05,
      "loss": 0.1079,
      "step": 800
    },
    {
      "epoch": 2.5632,
      "grad_norm": 0.12258123606443405,
      "learning_rate": 2.9157667386609073e-05,
      "loss": 0.1109,
      "step": 801
    },
    {
      "epoch": 2.5664,
      "grad_norm": 0.14330869913101196,
      "learning_rate": 2.8941684665226782e-05,
      "loss": 0.1122,
      "step": 802
    },
    {
      "epoch": 2.5696,
      "grad_norm": 0.12605610489845276,
      "learning_rate": 2.8725701943844498e-05,
      "loss": 0.1019,
      "step": 803
    },
    {
      "epoch": 2.5728,
      "grad_norm": 0.13955774903297424,
      "learning_rate": 2.8509719222462207e-05,
      "loss": 0.1002,
      "step": 804
    },
    {
      "epoch": 2.576,
      "grad_norm": 0.1233225092291832,
      "learning_rate": 2.8293736501079916e-05,
      "loss": 0.1012,
      "step": 805
    },
    {
      "epoch": 2.5792,
      "grad_norm": 0.1294841319322586,
      "learning_rate": 2.8077753779697625e-05,
      "loss": 0.1023,
      "step": 806
    },
    {
      "epoch": 2.5824,
      "grad_norm": 0.11727478355169296,
      "learning_rate": 2.7861771058315334e-05,
      "loss": 0.0918,
      "step": 807
    },
    {
      "epoch": 2.5856,
      "grad_norm": 0.13125987350940704,
      "learning_rate": 2.764578833693305e-05,
      "loss": 0.0979,
      "step": 808
    },
    {
      "epoch": 2.5888,
      "grad_norm": 0.1083776205778122,
      "learning_rate": 2.742980561555076e-05,
      "loss": 0.084,
      "step": 809
    },
    {
      "epoch": 2.592,
      "grad_norm": 0.13057638704776764,
      "learning_rate": 2.7213822894168468e-05,
      "loss": 0.0976,
      "step": 810
    },
    {
      "epoch": 2.5952,
      "grad_norm": 0.11634261161088943,
      "learning_rate": 2.6997840172786177e-05,
      "loss": 0.0807,
      "step": 811
    },
    {
      "epoch": 2.5984,
      "grad_norm": 0.11692791432142258,
      "learning_rate": 2.6781857451403893e-05,
      "loss": 0.083,
      "step": 812
    },
    {
      "epoch": 2.6016,
      "grad_norm": 0.1028442233800888,
      "learning_rate": 2.65658747300216e-05,
      "loss": 0.0775,
      "step": 813
    },
    {
      "epoch": 2.6048,
      "grad_norm": 0.10524721443653107,
      "learning_rate": 2.634989200863931e-05,
      "loss": 0.0774,
      "step": 814
    },
    {
      "epoch": 2.608,
      "grad_norm": 0.09703822433948517,
      "learning_rate": 2.613390928725702e-05,
      "loss": 0.0658,
      "step": 815
    },
    {
      "epoch": 2.6112,
      "grad_norm": 0.09542513638734818,
      "learning_rate": 2.5917926565874735e-05,
      "loss": 0.0698,
      "step": 816
    },
    {
      "epoch": 2.6144,
      "grad_norm": 0.10897335410118103,
      "learning_rate": 2.5701943844492444e-05,
      "loss": 0.0686,
      "step": 817
    },
    {
      "epoch": 2.6176,
      "grad_norm": 0.12239304929971695,
      "learning_rate": 2.5485961123110153e-05,
      "loss": 0.0692,
      "step": 818
    },
    {
      "epoch": 2.6208,
      "grad_norm": 0.11846344918012619,
      "learning_rate": 2.5269978401727862e-05,
      "loss": 0.0584,
      "step": 819
    },
    {
      "epoch": 2.624,
      "grad_norm": 0.11537090688943863,
      "learning_rate": 2.505399568034557e-05,
      "loss": 0.059,
      "step": 820
    },
    {
      "epoch": 2.6272,
      "grad_norm": 0.11187752336263657,
      "learning_rate": 2.4838012958963284e-05,
      "loss": 0.0518,
      "step": 821
    },
    {
      "epoch": 2.6304,
      "grad_norm": 0.11371037364006042,
      "learning_rate": 2.4622030237580996e-05,
      "loss": 0.0487,
      "step": 822
    },
    {
      "epoch": 2.6336,
      "grad_norm": 0.13884443044662476,
      "learning_rate": 2.4406047516198705e-05,
      "loss": 0.042,
      "step": 823
    },
    {
      "epoch": 2.6368,
      "grad_norm": 0.13732759654521942,
      "learning_rate": 2.4190064794816418e-05,
      "loss": 0.0412,
      "step": 824
    },
    {
      "epoch": 2.64,
      "grad_norm": 0.12649796903133392,
      "learning_rate": 2.3974082073434127e-05,
      "loss": 0.0498,
      "step": 825
    },
    {
      "epoch": 2.6432,
      "grad_norm": 0.2969752550125122,
      "learning_rate": 2.375809935205184e-05,
      "loss": 0.3201,
      "step": 826
    },
    {
      "epoch": 2.6464,
      "grad_norm": 0.2613941729068756,
      "learning_rate": 2.3542116630669548e-05,
      "loss": 0.2687,
      "step": 827
    },
    {
      "epoch": 2.6496,
      "grad_norm": 0.26244738698005676,
      "learning_rate": 2.332613390928726e-05,
      "loss": 0.2481,
      "step": 828
    },
    {
      "epoch": 2.6528,
      "grad_norm": 0.24566911160945892,
      "learning_rate": 2.311015118790497e-05,
      "loss": 0.2521,
      "step": 829
    },
    {
      "epoch": 2.656,
      "grad_norm": 0.24355582892894745,
      "learning_rate": 2.289416846652268e-05,
      "loss": 0.2366,
      "step": 830
    },
    {
      "epoch": 2.6592000000000002,
      "grad_norm": 0.2317725121974945,
      "learning_rate": 2.267818574514039e-05,
      "loss": 0.2142,
      "step": 831
    },
    {
      "epoch": 2.6624,
      "grad_norm": 0.21619531512260437,
      "learning_rate": 2.24622030237581e-05,
      "loss": 0.2107,
      "step": 832
    },
    {
      "epoch": 2.6656,
      "grad_norm": 0.19514796137809753,
      "learning_rate": 2.2246220302375812e-05,
      "loss": 0.1972,
      "step": 833
    },
    {
      "epoch": 2.6688,
      "grad_norm": 0.19775131344795227,
      "learning_rate": 2.203023758099352e-05,
      "loss": 0.1821,
      "step": 834
    },
    {
      "epoch": 2.672,
      "grad_norm": 0.18267297744750977,
      "learning_rate": 2.1814254859611234e-05,
      "loss": 0.186,
      "step": 835
    },
    {
      "epoch": 2.6752000000000002,
      "grad_norm": 0.18218350410461426,
      "learning_rate": 2.1598272138228943e-05,
      "loss": 0.1698,
      "step": 836
    },
    {
      "epoch": 2.6784,
      "grad_norm": 0.19310437142848969,
      "learning_rate": 2.1382289416846655e-05,
      "loss": 0.1669,
      "step": 837
    },
    {
      "epoch": 2.6816,
      "grad_norm": 0.17387984693050385,
      "learning_rate": 2.1166306695464364e-05,
      "loss": 0.156,
      "step": 838
    },
    {
      "epoch": 2.6848,
      "grad_norm": 0.16975240409374237,
      "learning_rate": 2.0950323974082077e-05,
      "loss": 0.1572,
      "step": 839
    },
    {
      "epoch": 2.6879999999999997,
      "grad_norm": 0.15988050401210785,
      "learning_rate": 2.0734341252699786e-05,
      "loss": 0.144,
      "step": 840
    },
    {
      "epoch": 2.6912000000000003,
      "grad_norm": 0.16885139048099518,
      "learning_rate": 2.0518358531317498e-05,
      "loss": 0.1469,
      "step": 841
    },
    {
      "epoch": 2.6944,
      "grad_norm": 0.1501711905002594,
      "learning_rate": 2.0302375809935207e-05,
      "loss": 0.1409,
      "step": 842
    },
    {
      "epoch": 2.6976,
      "grad_norm": 0.16758105158805847,
      "learning_rate": 2.0086393088552916e-05,
      "loss": 0.1426,
      "step": 843
    },
    {
      "epoch": 2.7008,
      "grad_norm": 0.14013351500034332,
      "learning_rate": 1.987041036717063e-05,
      "loss": 0.1217,
      "step": 844
    },
    {
      "epoch": 2.7039999999999997,
      "grad_norm": 0.14713069796562195,
      "learning_rate": 1.9654427645788337e-05,
      "loss": 0.1294,
      "step": 845
    },
    {
      "epoch": 2.7072000000000003,
      "grad_norm": 0.13226692378520966,
      "learning_rate": 1.943844492440605e-05,
      "loss": 0.1132,
      "step": 846
    },
    {
      "epoch": 2.7104,
      "grad_norm": 0.14332397282123566,
      "learning_rate": 1.922246220302376e-05,
      "loss": 0.1276,
      "step": 847
    },
    {
      "epoch": 2.7136,
      "grad_norm": 0.14073650538921356,
      "learning_rate": 1.900647948164147e-05,
      "loss": 0.1185,
      "step": 848
    },
    {
      "epoch": 2.7168,
      "grad_norm": 0.13172310590744019,
      "learning_rate": 1.879049676025918e-05,
      "loss": 0.1165,
      "step": 849
    },
    {
      "epoch": 2.7199999999999998,
      "grad_norm": 0.13586093485355377,
      "learning_rate": 1.8574514038876893e-05,
      "loss": 0.1126,
      "step": 850
    },
    {
      "epoch": 2.7232,
      "grad_norm": 0.14281995594501495,
      "learning_rate": 1.83585313174946e-05,
      "loss": 0.1124,
      "step": 851
    },
    {
      "epoch": 2.7264,
      "grad_norm": 0.13835787773132324,
      "learning_rate": 1.8142548596112314e-05,
      "loss": 0.1083,
      "step": 852
    },
    {
      "epoch": 2.7296,
      "grad_norm": 0.11988100409507751,
      "learning_rate": 1.7926565874730023e-05,
      "loss": 0.1034,
      "step": 853
    },
    {
      "epoch": 2.7328,
      "grad_norm": 0.11488315463066101,
      "learning_rate": 1.7710583153347732e-05,
      "loss": 0.1018,
      "step": 854
    },
    {
      "epoch": 2.7359999999999998,
      "grad_norm": 0.12677572667598724,
      "learning_rate": 1.7494600431965444e-05,
      "loss": 0.0993,
      "step": 855
    },
    {
      "epoch": 2.7392,
      "grad_norm": 0.11538853496313095,
      "learning_rate": 1.7278617710583154e-05,
      "loss": 0.0924,
      "step": 856
    },
    {
      "epoch": 2.7424,
      "grad_norm": 0.13631924986839294,
      "learning_rate": 1.7062634989200866e-05,
      "loss": 0.0911,
      "step": 857
    },
    {
      "epoch": 2.7456,
      "grad_norm": 0.11918346583843231,
      "learning_rate": 1.6846652267818575e-05,
      "loss": 0.0968,
      "step": 858
    },
    {
      "epoch": 2.7488,
      "grad_norm": 0.12230443954467773,
      "learning_rate": 1.6630669546436287e-05,
      "loss": 0.0856,
      "step": 859
    },
    {
      "epoch": 2.752,
      "grad_norm": 0.11245699226856232,
      "learning_rate": 1.6414686825053996e-05,
      "loss": 0.0768,
      "step": 860
    },
    {
      "epoch": 2.7552,
      "grad_norm": 0.11839310079813004,
      "learning_rate": 1.619870410367171e-05,
      "loss": 0.088,
      "step": 861
    },
    {
      "epoch": 2.7584,
      "grad_norm": 0.11135007441043854,
      "learning_rate": 1.5982721382289418e-05,
      "loss": 0.0746,
      "step": 862
    },
    {
      "epoch": 2.7616,
      "grad_norm": 0.10180454701185226,
      "learning_rate": 1.576673866090713e-05,
      "loss": 0.072,
      "step": 863
    },
    {
      "epoch": 2.7648,
      "grad_norm": 0.10360454022884369,
      "learning_rate": 1.555075593952484e-05,
      "loss": 0.0718,
      "step": 864
    },
    {
      "epoch": 2.768,
      "grad_norm": 0.10681334882974625,
      "learning_rate": 1.5334773218142548e-05,
      "loss": 0.0664,
      "step": 865
    },
    {
      "epoch": 2.7712,
      "grad_norm": 0.10491461306810379,
      "learning_rate": 1.511879049676026e-05,
      "loss": 0.0658,
      "step": 866
    },
    {
      "epoch": 2.7744,
      "grad_norm": 0.10549873113632202,
      "learning_rate": 1.490280777537797e-05,
      "loss": 0.0637,
      "step": 867
    },
    {
      "epoch": 2.7776,
      "grad_norm": 0.10947345942258835,
      "learning_rate": 1.4686825053995682e-05,
      "loss": 0.0581,
      "step": 868
    },
    {
      "epoch": 2.7808,
      "grad_norm": 0.09260623157024384,
      "learning_rate": 1.4470842332613391e-05,
      "loss": 0.0502,
      "step": 869
    },
    {
      "epoch": 2.784,
      "grad_norm": 0.11560041457414627,
      "learning_rate": 1.4254859611231103e-05,
      "loss": 0.0558,
      "step": 870
    },
    {
      "epoch": 2.7872,
      "grad_norm": 0.10883894562721252,
      "learning_rate": 1.4038876889848812e-05,
      "loss": 0.0532,
      "step": 871
    },
    {
      "epoch": 2.7904,
      "grad_norm": 0.12838850915431976,
      "learning_rate": 1.3822894168466525e-05,
      "loss": 0.0492,
      "step": 872
    },
    {
      "epoch": 2.7936,
      "grad_norm": 0.13005909323692322,
      "learning_rate": 1.3606911447084234e-05,
      "loss": 0.0468,
      "step": 873
    },
    {
      "epoch": 2.7968,
      "grad_norm": 0.11864806711673737,
      "learning_rate": 1.3390928725701946e-05,
      "loss": 0.0443,
      "step": 874
    },
    {
      "epoch": 2.8,
      "grad_norm": 0.11461834609508514,
      "learning_rate": 1.3174946004319655e-05,
      "loss": 0.049,
      "step": 875
    },
    {
      "epoch": 2.8032,
      "grad_norm": 0.31931498646736145,
      "learning_rate": 1.2958963282937368e-05,
      "loss": 0.3128,
      "step": 876
    },
    {
      "epoch": 2.8064,
      "grad_norm": 0.28787630796432495,
      "learning_rate": 1.2742980561555077e-05,
      "loss": 0.2788,
      "step": 877
    },
    {
      "epoch": 2.8096,
      "grad_norm": 0.25727397203445435,
      "learning_rate": 1.2526997840172786e-05,
      "loss": 0.253,
      "step": 878
    },
    {
      "epoch": 2.8128,
      "grad_norm": 0.24664194881916046,
      "learning_rate": 1.2311015118790498e-05,
      "loss": 0.2299,
      "step": 879
    },
    {
      "epoch": 2.816,
      "grad_norm": 0.22439797222614288,
      "learning_rate": 1.2095032397408209e-05,
      "loss": 0.2226,
      "step": 880
    },
    {
      "epoch": 2.8192,
      "grad_norm": 0.23044945299625397,
      "learning_rate": 1.187904967602592e-05,
      "loss": 0.2247,
      "step": 881
    },
    {
      "epoch": 2.8224,
      "grad_norm": 0.21268147230148315,
      "learning_rate": 1.166306695464363e-05,
      "loss": 0.2077,
      "step": 882
    },
    {
      "epoch": 2.8256,
      "grad_norm": 0.21470804512500763,
      "learning_rate": 1.144708423326134e-05,
      "loss": 0.2074,
      "step": 883
    },
    {
      "epoch": 2.8288,
      "grad_norm": 0.21320751309394836,
      "learning_rate": 1.123110151187905e-05,
      "loss": 0.2058,
      "step": 884
    },
    {
      "epoch": 2.832,
      "grad_norm": 0.20308518409729004,
      "learning_rate": 1.101511879049676e-05,
      "loss": 0.1807,
      "step": 885
    },
    {
      "epoch": 2.8352,
      "grad_norm": 0.19879984855651855,
      "learning_rate": 1.0799136069114471e-05,
      "loss": 0.1727,
      "step": 886
    },
    {
      "epoch": 2.8384,
      "grad_norm": 0.1805485039949417,
      "learning_rate": 1.0583153347732182e-05,
      "loss": 0.1818,
      "step": 887
    },
    {
      "epoch": 2.8416,
      "grad_norm": 0.18083924055099487,
      "learning_rate": 1.0367170626349893e-05,
      "loss": 0.165,
      "step": 888
    },
    {
      "epoch": 2.8448,
      "grad_norm": 0.19337712228298187,
      "learning_rate": 1.0151187904967603e-05,
      "loss": 0.1619,
      "step": 889
    },
    {
      "epoch": 2.848,
      "grad_norm": 0.18335779011249542,
      "learning_rate": 9.935205183585314e-06,
      "loss": 0.1535,
      "step": 890
    },
    {
      "epoch": 2.8512,
      "grad_norm": 0.1572374850511551,
      "learning_rate": 9.719222462203025e-06,
      "loss": 0.1524,
      "step": 891
    },
    {
      "epoch": 2.8544,
      "grad_norm": 0.18347567319869995,
      "learning_rate": 9.503239740820736e-06,
      "loss": 0.1603,
      "step": 892
    },
    {
      "epoch": 2.8576,
      "grad_norm": 0.15932327508926392,
      "learning_rate": 9.287257019438446e-06,
      "loss": 0.1427,
      "step": 893
    },
    {
      "epoch": 2.8608000000000002,
      "grad_norm": 0.15586335957050323,
      "learning_rate": 9.071274298056157e-06,
      "loss": 0.1357,
      "step": 894
    },
    {
      "epoch": 2.864,
      "grad_norm": 0.15122151374816895,
      "learning_rate": 8.855291576673866e-06,
      "loss": 0.1331,
      "step": 895
    },
    {
      "epoch": 2.8672,
      "grad_norm": 0.15237925946712494,
      "learning_rate": 8.639308855291577e-06,
      "loss": 0.1273,
      "step": 896
    },
    {
      "epoch": 2.8704,
      "grad_norm": 0.1483955681324005,
      "learning_rate": 8.423326133909287e-06,
      "loss": 0.1402,
      "step": 897
    },
    {
      "epoch": 2.8736,
      "grad_norm": 0.1408195197582245,
      "learning_rate": 8.207343412526998e-06,
      "loss": 0.1208,
      "step": 898
    },
    {
      "epoch": 2.8768000000000002,
      "grad_norm": 0.14622369408607483,
      "learning_rate": 7.991360691144709e-06,
      "loss": 0.1268,
      "step": 899
    },
    {
      "epoch": 2.88,
      "grad_norm": 0.13877056539058685,
      "learning_rate": 7.77537796976242e-06,
      "loss": 0.1254,
      "step": 900
    },
    {
      "epoch": 2.8832,
      "grad_norm": 0.12816564738750458,
      "learning_rate": 7.55939524838013e-06,
      "loss": 0.1161,
      "step": 901
    },
    {
      "epoch": 2.8864,
      "grad_norm": 0.15358401834964752,
      "learning_rate": 7.343412526997841e-06,
      "loss": 0.1058,
      "step": 902
    },
    {
      "epoch": 2.8895999999999997,
      "grad_norm": 0.1353873759508133,
      "learning_rate": 7.127429805615552e-06,
      "loss": 0.1088,
      "step": 903
    },
    {
      "epoch": 2.8928000000000003,
      "grad_norm": 0.14457595348358154,
      "learning_rate": 6.911447084233262e-06,
      "loss": 0.1038,
      "step": 904
    },
    {
      "epoch": 2.896,
      "grad_norm": 0.13466805219650269,
      "learning_rate": 6.695464362850973e-06,
      "loss": 0.1031,
      "step": 905
    },
    {
      "epoch": 2.8992,
      "grad_norm": 0.12174401432275772,
      "learning_rate": 6.479481641468684e-06,
      "loss": 0.103,
      "step": 906
    },
    {
      "epoch": 2.9024,
      "grad_norm": 0.1094335988163948,
      "learning_rate": 6.263498920086393e-06,
      "loss": 0.0984,
      "step": 907
    },
    {
      "epoch": 2.9055999999999997,
      "grad_norm": 0.13142022490501404,
      "learning_rate": 6.047516198704104e-06,
      "loss": 0.0997,
      "step": 908
    },
    {
      "epoch": 2.9088000000000003,
      "grad_norm": 0.12021956592798233,
      "learning_rate": 5.831533477321815e-06,
      "loss": 0.0927,
      "step": 909
    },
    {
      "epoch": 2.912,
      "grad_norm": 0.10934996604919434,
      "learning_rate": 5.615550755939525e-06,
      "loss": 0.0832,
      "step": 910
    },
    {
      "epoch": 2.9152,
      "grad_norm": 0.09859300404787064,
      "learning_rate": 5.399568034557236e-06,
      "loss": 0.0803,
      "step": 911
    },
    {
      "epoch": 2.9184,
      "grad_norm": 0.11087987571954727,
      "learning_rate": 5.183585313174946e-06,
      "loss": 0.0861,
      "step": 912
    },
    {
      "epoch": 2.9215999999999998,
      "grad_norm": 0.1129394918680191,
      "learning_rate": 4.967602591792657e-06,
      "loss": 0.0768,
      "step": 913
    },
    {
      "epoch": 2.9248,
      "grad_norm": 0.11549843847751617,
      "learning_rate": 4.751619870410368e-06,
      "loss": 0.0737,
      "step": 914
    },
    {
      "epoch": 2.928,
      "grad_norm": 0.1095946803689003,
      "learning_rate": 4.5356371490280785e-06,
      "loss": 0.0748,
      "step": 915
    },
    {
      "epoch": 2.9312,
      "grad_norm": 0.10904207080602646,
      "learning_rate": 4.319654427645788e-06,
      "loss": 0.0751,
      "step": 916
    },
    {
      "epoch": 2.9344,
      "grad_norm": 0.11399673670530319,
      "learning_rate": 4.103671706263499e-06,
      "loss": 0.0681,
      "step": 917
    },
    {
      "epoch": 2.9375999999999998,
      "grad_norm": 0.11016026884317398,
      "learning_rate": 3.88768898488121e-06,
      "loss": 0.0672,
      "step": 918
    },
    {
      "epoch": 2.9408,
      "grad_norm": 0.11834641546010971,
      "learning_rate": 3.6717062634989205e-06,
      "loss": 0.0589,
      "step": 919
    },
    {
      "epoch": 2.944,
      "grad_norm": 0.12334676831960678,
      "learning_rate": 3.455723542116631e-06,
      "loss": 0.0531,
      "step": 920
    },
    {
      "epoch": 2.9472,
      "grad_norm": 0.12524719536304474,
      "learning_rate": 3.239740820734342e-06,
      "loss": 0.0551,
      "step": 921
    },
    {
      "epoch": 2.9504,
      "grad_norm": 0.13059194386005402,
      "learning_rate": 3.023758099352052e-06,
      "loss": 0.0542,
      "step": 922
    },
    {
      "epoch": 2.9536,
      "grad_norm": 0.1309802532196045,
      "learning_rate": 2.8077753779697625e-06,
      "loss": 0.0451,
      "step": 923
    },
    {
      "epoch": 2.9568,
      "grad_norm": 0.16120564937591553,
      "learning_rate": 2.591792656587473e-06,
      "loss": 0.0423,
      "step": 924
    },
    {
      "epoch": 2.96,
      "grad_norm": 0.13344590365886688,
      "learning_rate": 2.375809935205184e-06,
      "loss": 0.0532,
      "step": 925
    },
    {
      "epoch": 2.9632,
      "grad_norm": 0.2511063814163208,
      "learning_rate": 2.159827213822894e-06,
      "loss": 0.2743,
      "step": 926
    },
    {
      "epoch": 2.9664,
      "grad_norm": 0.20441991090774536,
      "learning_rate": 1.943844492440605e-06,
      "loss": 0.2058,
      "step": 927
    },
    {
      "epoch": 2.9696,
      "grad_norm": 0.21656681597232819,
      "learning_rate": 1.7278617710583156e-06,
      "loss": 0.1942,
      "step": 928
    },
    {
      "epoch": 2.9728,
      "grad_norm": 0.17374096810817719,
      "learning_rate": 1.511879049676026e-06,
      "loss": 0.1636,
      "step": 929
    },
    {
      "epoch": 2.976,
      "grad_norm": 0.18308919668197632,
      "learning_rate": 1.2958963282937366e-06,
      "loss": 0.1419,
      "step": 930
    },
    {
      "epoch": 2.9792,
      "grad_norm": 0.1591521054506302,
      "learning_rate": 1.079913606911447e-06,
      "loss": 0.1265,
      "step": 931
    },
    {
      "epoch": 2.9824,
      "grad_norm": 0.1440308541059494,
      "learning_rate": 8.639308855291578e-07,
      "loss": 0.1114,
      "step": 932
    },
    {
      "epoch": 2.9856,
      "grad_norm": 0.12756496667861938,
      "learning_rate": 6.479481641468683e-07,
      "loss": 0.1101,
      "step": 933
    },
    {
      "epoch": 2.9888,
      "grad_norm": 0.13424651324748993,
      "learning_rate": 4.319654427645789e-07,
      "loss": 0.0934,
      "step": 934
    },
    {
      "epoch": 2.992,
      "grad_norm": 0.11600705236196518,
      "learning_rate": 2.1598272138228945e-07,
      "loss": 0.0804,
      "step": 935
    },
    {
      "epoch": 2.9952,
      "grad_norm": 0.10207013785839081,
      "learning_rate": 0.0,
      "loss": 0.0614,
      "step": 936
    }
  ],
  "logging_steps": 1,
  "max_steps": 936,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3.603475061641052e+17,
  "train_batch_size": 16,
  "trial_name": null,
  "trial_params": null
}
