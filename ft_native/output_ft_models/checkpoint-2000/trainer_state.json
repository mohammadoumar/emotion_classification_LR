{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 2.8388928317956,
  "eval_steps": 423,
  "global_step": 2000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0014194464158978,
      "grad_norm": 3.8753199577331543,
      "learning_rate": 2e-05,
      "loss": 2.6848,
      "step": 1
    },
    {
      "epoch": 0.0028388928317956,
      "grad_norm": 4.06846809387207,
      "learning_rate": 4e-05,
      "loss": 2.6312,
      "step": 2
    },
    {
      "epoch": 0.0042583392476933995,
      "grad_norm": 3.860471248626709,
      "learning_rate": 6e-05,
      "loss": 2.6378,
      "step": 3
    },
    {
      "epoch": 0.0056777856635912,
      "grad_norm": 2.591989517211914,
      "learning_rate": 8e-05,
      "loss": 2.4154,
      "step": 4
    },
    {
      "epoch": 0.007097232079488999,
      "grad_norm": 2.2637760639190674,
      "learning_rate": 0.0001,
      "loss": 2.2543,
      "step": 5
    },
    {
      "epoch": 0.008516678495386799,
      "grad_norm": 1.7446980476379395,
      "learning_rate": 0.00012,
      "loss": 2.0235,
      "step": 6
    },
    {
      "epoch": 0.0099361249112846,
      "grad_norm": 1.61598801612854,
      "learning_rate": 0.00014,
      "loss": 1.8347,
      "step": 7
    },
    {
      "epoch": 0.0113555713271824,
      "grad_norm": 1.692824363708496,
      "learning_rate": 0.00016,
      "loss": 1.5201,
      "step": 8
    },
    {
      "epoch": 0.0127750177430802,
      "grad_norm": 1.7318449020385742,
      "learning_rate": 0.00018,
      "loss": 1.1865,
      "step": 9
    },
    {
      "epoch": 0.014194464158977998,
      "grad_norm": 2.3941969871520996,
      "learning_rate": 0.0002,
      "loss": 0.8478,
      "step": 10
    },
    {
      "epoch": 0.015613910574875798,
      "grad_norm": 2.488072395324707,
      "learning_rate": 0.0001999048525214082,
      "loss": 0.5818,
      "step": 11
    },
    {
      "epoch": 0.017033356990773598,
      "grad_norm": 5.200738906860352,
      "learning_rate": 0.00019980970504281638,
      "loss": 0.4402,
      "step": 12
    },
    {
      "epoch": 0.018452803406671398,
      "grad_norm": 1.2015495300292969,
      "learning_rate": 0.00019971455756422456,
      "loss": 0.363,
      "step": 13
    },
    {
      "epoch": 0.0198722498225692,
      "grad_norm": 0.5281029939651489,
      "learning_rate": 0.00019961941008563274,
      "loss": 0.329,
      "step": 14
    },
    {
      "epoch": 0.021291696238467,
      "grad_norm": 0.5371572375297546,
      "learning_rate": 0.0001995242626070409,
      "loss": 0.2941,
      "step": 15
    },
    {
      "epoch": 0.0227111426543648,
      "grad_norm": 0.6878210306167603,
      "learning_rate": 0.0001994291151284491,
      "loss": 0.3151,
      "step": 16
    },
    {
      "epoch": 0.0241305890702626,
      "grad_norm": 0.7888450026512146,
      "learning_rate": 0.00019933396764985727,
      "loss": 0.2811,
      "step": 17
    },
    {
      "epoch": 0.0255500354861604,
      "grad_norm": 0.9834377765655518,
      "learning_rate": 0.00019923882017126548,
      "loss": 0.2909,
      "step": 18
    },
    {
      "epoch": 0.0269694819020582,
      "grad_norm": 1.111398696899414,
      "learning_rate": 0.00019914367269267363,
      "loss": 0.2877,
      "step": 19
    },
    {
      "epoch": 0.028388928317955996,
      "grad_norm": 1.3554736375808716,
      "learning_rate": 0.00019904852521408184,
      "loss": 0.2168,
      "step": 20
    },
    {
      "epoch": 0.029808374733853796,
      "grad_norm": 2.863246202468872,
      "learning_rate": 0.00019895337773549,
      "loss": 0.2247,
      "step": 21
    },
    {
      "epoch": 0.031227821149751596,
      "grad_norm": 0.4769139587879181,
      "learning_rate": 0.0001988582302568982,
      "loss": 0.2451,
      "step": 22
    },
    {
      "epoch": 0.032647267565649396,
      "grad_norm": 0.3291550576686859,
      "learning_rate": 0.00019876308277830637,
      "loss": 0.1798,
      "step": 23
    },
    {
      "epoch": 0.034066713981547196,
      "grad_norm": 0.3495163321495056,
      "learning_rate": 0.00019866793529971458,
      "loss": 0.1882,
      "step": 24
    },
    {
      "epoch": 0.035486160397444996,
      "grad_norm": 0.4018864035606384,
      "learning_rate": 0.00019857278782112273,
      "loss": 0.1911,
      "step": 25
    },
    {
      "epoch": 0.036905606813342796,
      "grad_norm": 0.5376371145248413,
      "learning_rate": 0.00019847764034253094,
      "loss": 0.156,
      "step": 26
    },
    {
      "epoch": 0.0383250532292406,
      "grad_norm": 0.8540717959403992,
      "learning_rate": 0.0001983824928639391,
      "loss": 0.1568,
      "step": 27
    },
    {
      "epoch": 0.0397444996451384,
      "grad_norm": 0.9595521092414856,
      "learning_rate": 0.0001982873453853473,
      "loss": 0.1599,
      "step": 28
    },
    {
      "epoch": 0.0411639460610362,
      "grad_norm": 0.7133545875549316,
      "learning_rate": 0.00019819219790675547,
      "loss": 0.1678,
      "step": 29
    },
    {
      "epoch": 0.042583392476934,
      "grad_norm": 0.43954452872276306,
      "learning_rate": 0.00019809705042816368,
      "loss": 0.1581,
      "step": 30
    },
    {
      "epoch": 0.0440028388928318,
      "grad_norm": 0.51581871509552,
      "learning_rate": 0.00019800190294957183,
      "loss": 0.1458,
      "step": 31
    },
    {
      "epoch": 0.0454222853087296,
      "grad_norm": 0.5623205900192261,
      "learning_rate": 0.00019790675547098004,
      "loss": 0.1473,
      "step": 32
    },
    {
      "epoch": 0.0468417317246274,
      "grad_norm": 0.6040207147598267,
      "learning_rate": 0.0001978116079923882,
      "loss": 0.1368,
      "step": 33
    },
    {
      "epoch": 0.0482611781405252,
      "grad_norm": 0.556030809879303,
      "learning_rate": 0.0001977164605137964,
      "loss": 0.1352,
      "step": 34
    },
    {
      "epoch": 0.049680624556423,
      "grad_norm": 0.49861204624176025,
      "learning_rate": 0.00019762131303520457,
      "loss": 0.1348,
      "step": 35
    },
    {
      "epoch": 0.0511000709723208,
      "grad_norm": 0.27800312638282776,
      "learning_rate": 0.00019752616555661275,
      "loss": 0.109,
      "step": 36
    },
    {
      "epoch": 0.0525195173882186,
      "grad_norm": 0.2570524215698242,
      "learning_rate": 0.00019743101807802093,
      "loss": 0.1141,
      "step": 37
    },
    {
      "epoch": 0.0539389638041164,
      "grad_norm": 0.3502003848552704,
      "learning_rate": 0.00019733587059942912,
      "loss": 0.1413,
      "step": 38
    },
    {
      "epoch": 0.05535841022001419,
      "grad_norm": 0.3913830518722534,
      "learning_rate": 0.0001972407231208373,
      "loss": 0.0961,
      "step": 39
    },
    {
      "epoch": 0.05677785663591199,
      "grad_norm": 0.4328113794326782,
      "learning_rate": 0.00019714557564224548,
      "loss": 0.097,
      "step": 40
    },
    {
      "epoch": 0.05819730305180979,
      "grad_norm": 0.38531455397605896,
      "learning_rate": 0.00019705042816365367,
      "loss": 0.1106,
      "step": 41
    },
    {
      "epoch": 0.05961674946770759,
      "grad_norm": 0.35614219307899475,
      "learning_rate": 0.00019695528068506185,
      "loss": 0.0956,
      "step": 42
    },
    {
      "epoch": 0.06103619588360539,
      "grad_norm": 0.26061660051345825,
      "learning_rate": 0.00019686013320647003,
      "loss": 0.0981,
      "step": 43
    },
    {
      "epoch": 0.06245564229950319,
      "grad_norm": 0.16854628920555115,
      "learning_rate": 0.00019676498572787822,
      "loss": 0.0877,
      "step": 44
    },
    {
      "epoch": 0.063875088715401,
      "grad_norm": 0.359539657831192,
      "learning_rate": 0.0001966698382492864,
      "loss": 0.0816,
      "step": 45
    },
    {
      "epoch": 0.06529453513129879,
      "grad_norm": 0.2330690622329712,
      "learning_rate": 0.00019657469077069458,
      "loss": 0.101,
      "step": 46
    },
    {
      "epoch": 0.0667139815471966,
      "grad_norm": 0.25437015295028687,
      "learning_rate": 0.00019647954329210277,
      "loss": 0.1031,
      "step": 47
    },
    {
      "epoch": 0.06813342796309439,
      "grad_norm": 0.2970932722091675,
      "learning_rate": 0.00019638439581351095,
      "loss": 0.0731,
      "step": 48
    },
    {
      "epoch": 0.0695528743789922,
      "grad_norm": 0.2961924374103546,
      "learning_rate": 0.00019628924833491913,
      "loss": 0.0669,
      "step": 49
    },
    {
      "epoch": 0.07097232079488999,
      "grad_norm": 0.288361519575119,
      "learning_rate": 0.00019619410085632732,
      "loss": 0.0664,
      "step": 50
    },
    {
      "epoch": 0.0723917672107878,
      "grad_norm": 0.8922231197357178,
      "learning_rate": 0.0001960989533777355,
      "loss": 0.5873,
      "step": 51
    },
    {
      "epoch": 0.07381121362668559,
      "grad_norm": 0.5401262044906616,
      "learning_rate": 0.00019600380589914368,
      "loss": 0.447,
      "step": 52
    },
    {
      "epoch": 0.07523066004258339,
      "grad_norm": 1.9218655824661255,
      "learning_rate": 0.00019590865842055187,
      "loss": 0.3904,
      "step": 53
    },
    {
      "epoch": 0.0766501064584812,
      "grad_norm": 0.5285252928733826,
      "learning_rate": 0.00019581351094196005,
      "loss": 0.3745,
      "step": 54
    },
    {
      "epoch": 0.07806955287437899,
      "grad_norm": 0.31439849734306335,
      "learning_rate": 0.00019571836346336823,
      "loss": 0.3307,
      "step": 55
    },
    {
      "epoch": 0.0794889992902768,
      "grad_norm": 0.4290097951889038,
      "learning_rate": 0.00019562321598477642,
      "loss": 0.3252,
      "step": 56
    },
    {
      "epoch": 0.08090844570617459,
      "grad_norm": 0.3310398459434509,
      "learning_rate": 0.0001955280685061846,
      "loss": 0.3135,
      "step": 57
    },
    {
      "epoch": 0.0823278921220724,
      "grad_norm": 0.32844847440719604,
      "learning_rate": 0.00019543292102759278,
      "loss": 0.2843,
      "step": 58
    },
    {
      "epoch": 0.08374733853797019,
      "grad_norm": 0.3329128324985504,
      "learning_rate": 0.00019533777354900097,
      "loss": 0.271,
      "step": 59
    },
    {
      "epoch": 0.085166784953868,
      "grad_norm": 0.2432798445224762,
      "learning_rate": 0.00019524262607040915,
      "loss": 0.2519,
      "step": 60
    },
    {
      "epoch": 0.08658623136976579,
      "grad_norm": 0.2554329037666321,
      "learning_rate": 0.00019514747859181733,
      "loss": 0.2459,
      "step": 61
    },
    {
      "epoch": 0.0880056777856636,
      "grad_norm": 0.2721186876296997,
      "learning_rate": 0.00019505233111322552,
      "loss": 0.2464,
      "step": 62
    },
    {
      "epoch": 0.08942512420156139,
      "grad_norm": 0.22954344749450684,
      "learning_rate": 0.0001949571836346337,
      "loss": 0.2196,
      "step": 63
    },
    {
      "epoch": 0.0908445706174592,
      "grad_norm": 0.26764383912086487,
      "learning_rate": 0.00019486203615604188,
      "loss": 0.228,
      "step": 64
    },
    {
      "epoch": 0.09226401703335699,
      "grad_norm": 0.22909002006053925,
      "learning_rate": 0.00019476688867745007,
      "loss": 0.2039,
      "step": 65
    },
    {
      "epoch": 0.0936834634492548,
      "grad_norm": 0.2602919340133667,
      "learning_rate": 0.00019467174119885825,
      "loss": 0.2007,
      "step": 66
    },
    {
      "epoch": 0.09510290986515259,
      "grad_norm": 0.22124329209327698,
      "learning_rate": 0.0001945765937202664,
      "loss": 0.1886,
      "step": 67
    },
    {
      "epoch": 0.0965223562810504,
      "grad_norm": 0.21159298717975616,
      "learning_rate": 0.00019448144624167462,
      "loss": 0.2003,
      "step": 68
    },
    {
      "epoch": 0.09794180269694819,
      "grad_norm": 0.2761073112487793,
      "learning_rate": 0.00019438629876308277,
      "loss": 0.2093,
      "step": 69
    },
    {
      "epoch": 0.099361249112846,
      "grad_norm": 0.20292288064956665,
      "learning_rate": 0.00019429115128449098,
      "loss": 0.1712,
      "step": 70
    },
    {
      "epoch": 0.10078069552874379,
      "grad_norm": 0.19448138773441315,
      "learning_rate": 0.00019419600380589914,
      "loss": 0.1434,
      "step": 71
    },
    {
      "epoch": 0.1022001419446416,
      "grad_norm": 0.20379109680652618,
      "learning_rate": 0.00019410085632730735,
      "loss": 0.1644,
      "step": 72
    },
    {
      "epoch": 0.10361958836053939,
      "grad_norm": 0.223442941904068,
      "learning_rate": 0.0001940057088487155,
      "loss": 0.1879,
      "step": 73
    },
    {
      "epoch": 0.1050390347764372,
      "grad_norm": 0.1932620406150818,
      "learning_rate": 0.00019391056137012372,
      "loss": 0.1413,
      "step": 74
    },
    {
      "epoch": 0.10645848119233499,
      "grad_norm": 0.2164372056722641,
      "learning_rate": 0.00019381541389153187,
      "loss": 0.1703,
      "step": 75
    },
    {
      "epoch": 0.1078779276082328,
      "grad_norm": 0.1846214234828949,
      "learning_rate": 0.00019372026641294008,
      "loss": 0.1742,
      "step": 76
    },
    {
      "epoch": 0.10929737402413059,
      "grad_norm": 0.16839642822742462,
      "learning_rate": 0.00019362511893434824,
      "loss": 0.1471,
      "step": 77
    },
    {
      "epoch": 0.11071682044002838,
      "grad_norm": 0.2075975090265274,
      "learning_rate": 0.00019352997145575645,
      "loss": 0.1939,
      "step": 78
    },
    {
      "epoch": 0.11213626685592619,
      "grad_norm": 0.21337558329105377,
      "learning_rate": 0.0001934348239771646,
      "loss": 0.1677,
      "step": 79
    },
    {
      "epoch": 0.11355571327182398,
      "grad_norm": 0.1997154951095581,
      "learning_rate": 0.00019333967649857282,
      "loss": 0.1525,
      "step": 80
    },
    {
      "epoch": 0.11497515968772179,
      "grad_norm": 0.20924781262874603,
      "learning_rate": 0.00019324452901998097,
      "loss": 0.1234,
      "step": 81
    },
    {
      "epoch": 0.11639460610361958,
      "grad_norm": 0.20264357328414917,
      "learning_rate": 0.00019314938154138916,
      "loss": 0.1459,
      "step": 82
    },
    {
      "epoch": 0.11781405251951739,
      "grad_norm": 0.15568575263023376,
      "learning_rate": 0.00019305423406279734,
      "loss": 0.1262,
      "step": 83
    },
    {
      "epoch": 0.11923349893541518,
      "grad_norm": 0.18003098666667938,
      "learning_rate": 0.00019295908658420552,
      "loss": 0.1073,
      "step": 84
    },
    {
      "epoch": 0.12065294535131299,
      "grad_norm": 0.18673446774482727,
      "learning_rate": 0.0001928639391056137,
      "loss": 0.1248,
      "step": 85
    },
    {
      "epoch": 0.12207239176721078,
      "grad_norm": 0.16824555397033691,
      "learning_rate": 0.0001927687916270219,
      "loss": 0.1181,
      "step": 86
    },
    {
      "epoch": 0.12349183818310859,
      "grad_norm": 0.15063299238681793,
      "learning_rate": 0.00019267364414843007,
      "loss": 0.1117,
      "step": 87
    },
    {
      "epoch": 0.12491128459900638,
      "grad_norm": 0.16303810477256775,
      "learning_rate": 0.00019257849666983826,
      "loss": 0.105,
      "step": 88
    },
    {
      "epoch": 0.1263307310149042,
      "grad_norm": 0.16362158954143524,
      "learning_rate": 0.00019248334919124644,
      "loss": 0.0931,
      "step": 89
    },
    {
      "epoch": 0.127750177430802,
      "grad_norm": 0.15042024850845337,
      "learning_rate": 0.00019238820171265462,
      "loss": 0.0981,
      "step": 90
    },
    {
      "epoch": 0.12916962384669978,
      "grad_norm": 0.14741793274879456,
      "learning_rate": 0.0001922930542340628,
      "loss": 0.0986,
      "step": 91
    },
    {
      "epoch": 0.13058907026259758,
      "grad_norm": 0.158548966050148,
      "learning_rate": 0.000192197906755471,
      "loss": 0.0884,
      "step": 92
    },
    {
      "epoch": 0.1320085166784954,
      "grad_norm": 0.14750920236110687,
      "learning_rate": 0.00019210275927687917,
      "loss": 0.0886,
      "step": 93
    },
    {
      "epoch": 0.1334279630943932,
      "grad_norm": 0.12116905301809311,
      "learning_rate": 0.00019200761179828736,
      "loss": 0.0841,
      "step": 94
    },
    {
      "epoch": 0.13484740951029098,
      "grad_norm": 0.12684527039527893,
      "learning_rate": 0.00019191246431969554,
      "loss": 0.0836,
      "step": 95
    },
    {
      "epoch": 0.13626685592618878,
      "grad_norm": 0.13355328142642975,
      "learning_rate": 0.00019181731684110372,
      "loss": 0.0852,
      "step": 96
    },
    {
      "epoch": 0.1376863023420866,
      "grad_norm": 0.11786916851997375,
      "learning_rate": 0.0001917221693625119,
      "loss": 0.066,
      "step": 97
    },
    {
      "epoch": 0.1391057487579844,
      "grad_norm": 0.1535901427268982,
      "learning_rate": 0.00019162702188392006,
      "loss": 0.0697,
      "step": 98
    },
    {
      "epoch": 0.14052519517388218,
      "grad_norm": 0.10121750086545944,
      "learning_rate": 0.00019153187440532827,
      "loss": 0.0604,
      "step": 99
    },
    {
      "epoch": 0.14194464158977999,
      "grad_norm": 0.09791870415210724,
      "learning_rate": 0.00019143672692673643,
      "loss": 0.0422,
      "step": 100
    },
    {
      "epoch": 0.1433640880056778,
      "grad_norm": 0.8991482853889465,
      "learning_rate": 0.00019134157944814464,
      "loss": 0.6245,
      "step": 101
    },
    {
      "epoch": 0.1447835344215756,
      "grad_norm": 0.536517858505249,
      "learning_rate": 0.0001912464319695528,
      "loss": 0.4928,
      "step": 102
    },
    {
      "epoch": 0.14620298083747338,
      "grad_norm": 0.3712337911128998,
      "learning_rate": 0.000191151284490961,
      "loss": 0.4136,
      "step": 103
    },
    {
      "epoch": 0.14762242725337119,
      "grad_norm": 0.29865872859954834,
      "learning_rate": 0.00019105613701236916,
      "loss": 0.3265,
      "step": 104
    },
    {
      "epoch": 0.149041873669269,
      "grad_norm": 0.3236014246940613,
      "learning_rate": 0.00019096098953377737,
      "loss": 0.3386,
      "step": 105
    },
    {
      "epoch": 0.15046132008516677,
      "grad_norm": 0.31175604462623596,
      "learning_rate": 0.00019086584205518553,
      "loss": 0.3567,
      "step": 106
    },
    {
      "epoch": 0.15188076650106458,
      "grad_norm": 0.3100660741329193,
      "learning_rate": 0.00019077069457659374,
      "loss": 0.3257,
      "step": 107
    },
    {
      "epoch": 0.1533002129169624,
      "grad_norm": 0.30780166387557983,
      "learning_rate": 0.0001906755470980019,
      "loss": 0.3294,
      "step": 108
    },
    {
      "epoch": 0.1547196593328602,
      "grad_norm": 0.2866743505001068,
      "learning_rate": 0.0001905803996194101,
      "loss": 0.3086,
      "step": 109
    },
    {
      "epoch": 0.15613910574875797,
      "grad_norm": 0.25725001096725464,
      "learning_rate": 0.00019048525214081826,
      "loss": 0.2569,
      "step": 110
    },
    {
      "epoch": 0.15755855216465578,
      "grad_norm": 0.2841548025608063,
      "learning_rate": 0.00019039010466222647,
      "loss": 0.2833,
      "step": 111
    },
    {
      "epoch": 0.1589779985805536,
      "grad_norm": 0.21969172358512878,
      "learning_rate": 0.00019029495718363463,
      "loss": 0.213,
      "step": 112
    },
    {
      "epoch": 0.1603974449964514,
      "grad_norm": 0.2883078455924988,
      "learning_rate": 0.00019019980970504284,
      "loss": 0.2762,
      "step": 113
    },
    {
      "epoch": 0.16181689141234917,
      "grad_norm": 0.21333856880664825,
      "learning_rate": 0.000190104662226451,
      "loss": 0.2362,
      "step": 114
    },
    {
      "epoch": 0.16323633782824698,
      "grad_norm": 0.2552758753299713,
      "learning_rate": 0.0001900095147478592,
      "loss": 0.2223,
      "step": 115
    },
    {
      "epoch": 0.1646557842441448,
      "grad_norm": 0.23104964196681976,
      "learning_rate": 0.00018991436726926736,
      "loss": 0.1887,
      "step": 116
    },
    {
      "epoch": 0.1660752306600426,
      "grad_norm": 0.27122604846954346,
      "learning_rate": 0.00018981921979067558,
      "loss": 0.2177,
      "step": 117
    },
    {
      "epoch": 0.16749467707594037,
      "grad_norm": 0.2582598626613617,
      "learning_rate": 0.00018972407231208373,
      "loss": 0.2375,
      "step": 118
    },
    {
      "epoch": 0.16891412349183818,
      "grad_norm": 0.2079077512025833,
      "learning_rate": 0.00018962892483349191,
      "loss": 0.1653,
      "step": 119
    },
    {
      "epoch": 0.170333569907736,
      "grad_norm": 0.2133801132440567,
      "learning_rate": 0.0001895337773549001,
      "loss": 0.1939,
      "step": 120
    },
    {
      "epoch": 0.1717530163236338,
      "grad_norm": 0.25759196281433105,
      "learning_rate": 0.00018943862987630828,
      "loss": 0.2134,
      "step": 121
    },
    {
      "epoch": 0.17317246273953157,
      "grad_norm": 0.1881779581308365,
      "learning_rate": 0.00018934348239771646,
      "loss": 0.1756,
      "step": 122
    },
    {
      "epoch": 0.17459190915542938,
      "grad_norm": 0.227374866604805,
      "learning_rate": 0.00018924833491912465,
      "loss": 0.1788,
      "step": 123
    },
    {
      "epoch": 0.1760113555713272,
      "grad_norm": 0.21692273020744324,
      "learning_rate": 0.00018915318744053283,
      "loss": 0.1817,
      "step": 124
    },
    {
      "epoch": 0.177430801987225,
      "grad_norm": 0.22101150453090668,
      "learning_rate": 0.00018905803996194101,
      "loss": 0.176,
      "step": 125
    },
    {
      "epoch": 0.17885024840312277,
      "grad_norm": 0.1902722418308258,
      "learning_rate": 0.0001889628924833492,
      "loss": 0.1228,
      "step": 126
    },
    {
      "epoch": 0.18026969481902058,
      "grad_norm": 0.20391739904880524,
      "learning_rate": 0.00018886774500475738,
      "loss": 0.1454,
      "step": 127
    },
    {
      "epoch": 0.1816891412349184,
      "grad_norm": 0.16759422421455383,
      "learning_rate": 0.00018877259752616556,
      "loss": 0.115,
      "step": 128
    },
    {
      "epoch": 0.18310858765081617,
      "grad_norm": 0.17888155579566956,
      "learning_rate": 0.00018867745004757375,
      "loss": 0.1211,
      "step": 129
    },
    {
      "epoch": 0.18452803406671398,
      "grad_norm": 0.1745491474866867,
      "learning_rate": 0.00018858230256898193,
      "loss": 0.133,
      "step": 130
    },
    {
      "epoch": 0.18594748048261178,
      "grad_norm": 0.168021097779274,
      "learning_rate": 0.00018848715509039012,
      "loss": 0.137,
      "step": 131
    },
    {
      "epoch": 0.1873669268985096,
      "grad_norm": 0.2074587345123291,
      "learning_rate": 0.0001883920076117983,
      "loss": 0.1185,
      "step": 132
    },
    {
      "epoch": 0.18878637331440737,
      "grad_norm": 0.18290935456752777,
      "learning_rate": 0.00018829686013320648,
      "loss": 0.1491,
      "step": 133
    },
    {
      "epoch": 0.19020581973030518,
      "grad_norm": 0.20074568688869476,
      "learning_rate": 0.00018820171265461467,
      "loss": 0.1425,
      "step": 134
    },
    {
      "epoch": 0.19162526614620298,
      "grad_norm": 0.16316284239292145,
      "learning_rate": 0.00018810656517602285,
      "loss": 0.1285,
      "step": 135
    },
    {
      "epoch": 0.1930447125621008,
      "grad_norm": 0.13204751908779144,
      "learning_rate": 0.00018801141769743103,
      "loss": 0.1088,
      "step": 136
    },
    {
      "epoch": 0.19446415897799857,
      "grad_norm": 0.17812924087047577,
      "learning_rate": 0.00018791627021883922,
      "loss": 0.1283,
      "step": 137
    },
    {
      "epoch": 0.19588360539389638,
      "grad_norm": 0.17065832018852234,
      "learning_rate": 0.0001878211227402474,
      "loss": 0.1153,
      "step": 138
    },
    {
      "epoch": 0.19730305180979418,
      "grad_norm": 0.14613445103168488,
      "learning_rate": 0.00018772597526165558,
      "loss": 0.1048,
      "step": 139
    },
    {
      "epoch": 0.198722498225692,
      "grad_norm": 0.1457911729812622,
      "learning_rate": 0.00018763082778306377,
      "loss": 0.0918,
      "step": 140
    },
    {
      "epoch": 0.20014194464158977,
      "grad_norm": 0.13844063878059387,
      "learning_rate": 0.00018753568030447195,
      "loss": 0.0909,
      "step": 141
    },
    {
      "epoch": 0.20156139105748758,
      "grad_norm": 0.14067940413951874,
      "learning_rate": 0.00018744053282588013,
      "loss": 0.1034,
      "step": 142
    },
    {
      "epoch": 0.20298083747338538,
      "grad_norm": 0.12681680917739868,
      "learning_rate": 0.00018734538534728832,
      "loss": 0.0947,
      "step": 143
    },
    {
      "epoch": 0.2044002838892832,
      "grad_norm": 0.14871253073215485,
      "learning_rate": 0.0001872502378686965,
      "loss": 0.108,
      "step": 144
    },
    {
      "epoch": 0.20581973030518097,
      "grad_norm": 0.20518921315670013,
      "learning_rate": 0.00018715509039010468,
      "loss": 0.1038,
      "step": 145
    },
    {
      "epoch": 0.20723917672107878,
      "grad_norm": 0.17616136372089386,
      "learning_rate": 0.00018705994291151287,
      "loss": 0.0715,
      "step": 146
    },
    {
      "epoch": 0.20865862313697658,
      "grad_norm": 0.11235202103853226,
      "learning_rate": 0.00018696479543292105,
      "loss": 0.0786,
      "step": 147
    },
    {
      "epoch": 0.2100780695528744,
      "grad_norm": 0.12890516221523285,
      "learning_rate": 0.00018686964795432923,
      "loss": 0.0701,
      "step": 148
    },
    {
      "epoch": 0.21149751596877217,
      "grad_norm": 0.09201624244451523,
      "learning_rate": 0.00018677450047573742,
      "loss": 0.0552,
      "step": 149
    },
    {
      "epoch": 0.21291696238466998,
      "grad_norm": 0.1286080777645111,
      "learning_rate": 0.00018667935299714557,
      "loss": 0.0494,
      "step": 150
    },
    {
      "epoch": 0.21433640880056778,
      "grad_norm": 0.6585749387741089,
      "learning_rate": 0.00018658420551855376,
      "loss": 0.7252,
      "step": 151
    },
    {
      "epoch": 0.2157558552164656,
      "grad_norm": 0.5025274157524109,
      "learning_rate": 0.00018648905803996194,
      "loss": 0.5642,
      "step": 152
    },
    {
      "epoch": 0.21717530163236337,
      "grad_norm": 0.3372633755207062,
      "learning_rate": 0.00018639391056137012,
      "loss": 0.4086,
      "step": 153
    },
    {
      "epoch": 0.21859474804826118,
      "grad_norm": 0.298308789730072,
      "learning_rate": 0.0001862987630827783,
      "loss": 0.3602,
      "step": 154
    },
    {
      "epoch": 0.22001419446415899,
      "grad_norm": 0.254146933555603,
      "learning_rate": 0.0001862036156041865,
      "loss": 0.3183,
      "step": 155
    },
    {
      "epoch": 0.22143364088005676,
      "grad_norm": 0.2620442509651184,
      "learning_rate": 0.00018610846812559467,
      "loss": 0.3252,
      "step": 156
    },
    {
      "epoch": 0.22285308729595457,
      "grad_norm": 0.2677079737186432,
      "learning_rate": 0.00018601332064700286,
      "loss": 0.3069,
      "step": 157
    },
    {
      "epoch": 0.22427253371185238,
      "grad_norm": 0.26175227761268616,
      "learning_rate": 0.00018591817316841104,
      "loss": 0.3178,
      "step": 158
    },
    {
      "epoch": 0.22569198012775019,
      "grad_norm": 0.2651664614677429,
      "learning_rate": 0.00018582302568981922,
      "loss": 0.3102,
      "step": 159
    },
    {
      "epoch": 0.22711142654364797,
      "grad_norm": 0.24599607288837433,
      "learning_rate": 0.0001857278782112274,
      "loss": 0.2539,
      "step": 160
    },
    {
      "epoch": 0.22853087295954577,
      "grad_norm": 0.28982409834861755,
      "learning_rate": 0.0001856327307326356,
      "loss": 0.236,
      "step": 161
    },
    {
      "epoch": 0.22995031937544358,
      "grad_norm": 0.24877092242240906,
      "learning_rate": 0.00018553758325404377,
      "loss": 0.2827,
      "step": 162
    },
    {
      "epoch": 0.2313697657913414,
      "grad_norm": 0.31241002678871155,
      "learning_rate": 0.00018544243577545196,
      "loss": 0.2639,
      "step": 163
    },
    {
      "epoch": 0.23278921220723917,
      "grad_norm": 0.23131708800792694,
      "learning_rate": 0.00018534728829686014,
      "loss": 0.2099,
      "step": 164
    },
    {
      "epoch": 0.23420865862313697,
      "grad_norm": 0.24522876739501953,
      "learning_rate": 0.00018525214081826832,
      "loss": 0.2242,
      "step": 165
    },
    {
      "epoch": 0.23562810503903478,
      "grad_norm": 0.23746757209300995,
      "learning_rate": 0.0001851569933396765,
      "loss": 0.2173,
      "step": 166
    },
    {
      "epoch": 0.2370475514549326,
      "grad_norm": 0.19376133382320404,
      "learning_rate": 0.0001850618458610847,
      "loss": 0.1677,
      "step": 167
    },
    {
      "epoch": 0.23846699787083037,
      "grad_norm": 0.24670350551605225,
      "learning_rate": 0.00018496669838249287,
      "loss": 0.1954,
      "step": 168
    },
    {
      "epoch": 0.23988644428672817,
      "grad_norm": 0.2263229489326477,
      "learning_rate": 0.00018487155090390106,
      "loss": 0.1983,
      "step": 169
    },
    {
      "epoch": 0.24130589070262598,
      "grad_norm": 0.2091400921344757,
      "learning_rate": 0.00018477640342530924,
      "loss": 0.186,
      "step": 170
    },
    {
      "epoch": 0.2427253371185238,
      "grad_norm": 0.20422160625457764,
      "learning_rate": 0.00018468125594671742,
      "loss": 0.1713,
      "step": 171
    },
    {
      "epoch": 0.24414478353442157,
      "grad_norm": 0.21125274896621704,
      "learning_rate": 0.0001845861084681256,
      "loss": 0.1888,
      "step": 172
    },
    {
      "epoch": 0.24556422995031937,
      "grad_norm": 0.2694367468357086,
      "learning_rate": 0.0001844909609895338,
      "loss": 0.2226,
      "step": 173
    },
    {
      "epoch": 0.24698367636621718,
      "grad_norm": 0.18443642556667328,
      "learning_rate": 0.00018439581351094197,
      "loss": 0.1524,
      "step": 174
    },
    {
      "epoch": 0.248403122782115,
      "grad_norm": 0.20904751121997833,
      "learning_rate": 0.00018430066603235016,
      "loss": 0.1829,
      "step": 175
    },
    {
      "epoch": 0.24982256919801277,
      "grad_norm": 0.19625532627105713,
      "learning_rate": 0.00018420551855375834,
      "loss": 0.2097,
      "step": 176
    },
    {
      "epoch": 0.2512420156139106,
      "grad_norm": 0.19554728269577026,
      "learning_rate": 0.00018411037107516652,
      "loss": 0.1498,
      "step": 177
    },
    {
      "epoch": 0.2526614620298084,
      "grad_norm": 0.19032320380210876,
      "learning_rate": 0.0001840152235965747,
      "loss": 0.1684,
      "step": 178
    },
    {
      "epoch": 0.2540809084457062,
      "grad_norm": 0.1574752926826477,
      "learning_rate": 0.00018392007611798286,
      "loss": 0.1511,
      "step": 179
    },
    {
      "epoch": 0.255500354861604,
      "grad_norm": 0.15841525793075562,
      "learning_rate": 0.00018382492863939107,
      "loss": 0.1521,
      "step": 180
    },
    {
      "epoch": 0.25691980127750175,
      "grad_norm": 0.22403736412525177,
      "learning_rate": 0.00018372978116079923,
      "loss": 0.1683,
      "step": 181
    },
    {
      "epoch": 0.25833924769339955,
      "grad_norm": 0.1463969349861145,
      "learning_rate": 0.00018363463368220744,
      "loss": 0.1201,
      "step": 182
    },
    {
      "epoch": 0.25975869410929736,
      "grad_norm": 0.182897686958313,
      "learning_rate": 0.0001835394862036156,
      "loss": 0.1254,
      "step": 183
    },
    {
      "epoch": 0.26117814052519517,
      "grad_norm": 0.16555581986904144,
      "learning_rate": 0.0001834443387250238,
      "loss": 0.1435,
      "step": 184
    },
    {
      "epoch": 0.262597586941093,
      "grad_norm": 0.16409210860729218,
      "learning_rate": 0.00018334919124643196,
      "loss": 0.1144,
      "step": 185
    },
    {
      "epoch": 0.2640170333569908,
      "grad_norm": 0.15717332065105438,
      "learning_rate": 0.00018325404376784017,
      "loss": 0.1227,
      "step": 186
    },
    {
      "epoch": 0.2654364797728886,
      "grad_norm": 0.14730651676654816,
      "learning_rate": 0.00018315889628924833,
      "loss": 0.1139,
      "step": 187
    },
    {
      "epoch": 0.2668559261887864,
      "grad_norm": 0.15004900097846985,
      "learning_rate": 0.00018306374881065654,
      "loss": 0.1141,
      "step": 188
    },
    {
      "epoch": 0.26827537260468415,
      "grad_norm": 0.14527259767055511,
      "learning_rate": 0.0001829686013320647,
      "loss": 0.1227,
      "step": 189
    },
    {
      "epoch": 0.26969481902058196,
      "grad_norm": 0.12058791518211365,
      "learning_rate": 0.0001828734538534729,
      "loss": 0.0967,
      "step": 190
    },
    {
      "epoch": 0.27111426543647976,
      "grad_norm": 0.15104159712791443,
      "learning_rate": 0.00018277830637488106,
      "loss": 0.1133,
      "step": 191
    },
    {
      "epoch": 0.27253371185237757,
      "grad_norm": 0.1208389550447464,
      "learning_rate": 0.00018268315889628927,
      "loss": 0.0909,
      "step": 192
    },
    {
      "epoch": 0.2739531582682754,
      "grad_norm": 0.13636882603168488,
      "learning_rate": 0.00018258801141769743,
      "loss": 0.1006,
      "step": 193
    },
    {
      "epoch": 0.2753726046841732,
      "grad_norm": 0.14653749763965607,
      "learning_rate": 0.00018249286393910564,
      "loss": 0.1028,
      "step": 194
    },
    {
      "epoch": 0.276792051100071,
      "grad_norm": 0.13390621542930603,
      "learning_rate": 0.0001823977164605138,
      "loss": 0.1013,
      "step": 195
    },
    {
      "epoch": 0.2782114975159688,
      "grad_norm": 0.17225858569145203,
      "learning_rate": 0.000182302568981922,
      "loss": 0.0982,
      "step": 196
    },
    {
      "epoch": 0.27963094393186655,
      "grad_norm": 0.1274978071451187,
      "learning_rate": 0.00018220742150333016,
      "loss": 0.0923,
      "step": 197
    },
    {
      "epoch": 0.28105039034776436,
      "grad_norm": 0.1318695843219757,
      "learning_rate": 0.00018211227402473837,
      "loss": 0.0639,
      "step": 198
    },
    {
      "epoch": 0.28246983676366216,
      "grad_norm": 0.13581371307373047,
      "learning_rate": 0.00018201712654614653,
      "loss": 0.0774,
      "step": 199
    },
    {
      "epoch": 0.28388928317955997,
      "grad_norm": 0.10221122205257416,
      "learning_rate": 0.00018192197906755474,
      "loss": 0.0662,
      "step": 200
    },
    {
      "epoch": 0.2853087295954578,
      "grad_norm": 0.5923755764961243,
      "learning_rate": 0.0001818268315889629,
      "loss": 0.5019,
      "step": 201
    },
    {
      "epoch": 0.2867281760113556,
      "grad_norm": 0.47801506519317627,
      "learning_rate": 0.00018173168411037108,
      "loss": 0.4882,
      "step": 202
    },
    {
      "epoch": 0.2881476224272534,
      "grad_norm": 0.416513055562973,
      "learning_rate": 0.00018163653663177926,
      "loss": 0.4185,
      "step": 203
    },
    {
      "epoch": 0.2895670688431512,
      "grad_norm": 0.30151981115341187,
      "learning_rate": 0.00018154138915318745,
      "loss": 0.3634,
      "step": 204
    },
    {
      "epoch": 0.29098651525904895,
      "grad_norm": 0.3366330564022064,
      "learning_rate": 0.00018144624167459563,
      "loss": 0.3766,
      "step": 205
    },
    {
      "epoch": 0.29240596167494676,
      "grad_norm": 0.3388964831829071,
      "learning_rate": 0.0001813510941960038,
      "loss": 0.3425,
      "step": 206
    },
    {
      "epoch": 0.29382540809084456,
      "grad_norm": 0.28328460454940796,
      "learning_rate": 0.000181255946717412,
      "loss": 0.3147,
      "step": 207
    },
    {
      "epoch": 0.29524485450674237,
      "grad_norm": 0.27872422337532043,
      "learning_rate": 0.00018116079923882018,
      "loss": 0.3018,
      "step": 208
    },
    {
      "epoch": 0.2966643009226402,
      "grad_norm": 0.26885318756103516,
      "learning_rate": 0.00018106565176022836,
      "loss": 0.3222,
      "step": 209
    },
    {
      "epoch": 0.298083747338538,
      "grad_norm": 0.26485034823417664,
      "learning_rate": 0.00018097050428163655,
      "loss": 0.3397,
      "step": 210
    },
    {
      "epoch": 0.2995031937544358,
      "grad_norm": 0.2604244351387024,
      "learning_rate": 0.00018087535680304473,
      "loss": 0.2642,
      "step": 211
    },
    {
      "epoch": 0.30092264017033354,
      "grad_norm": 0.22035960853099823,
      "learning_rate": 0.0001807802093244529,
      "loss": 0.2307,
      "step": 212
    },
    {
      "epoch": 0.30234208658623135,
      "grad_norm": 0.21684938669204712,
      "learning_rate": 0.0001806850618458611,
      "loss": 0.2531,
      "step": 213
    },
    {
      "epoch": 0.30376153300212916,
      "grad_norm": 0.24349994957447052,
      "learning_rate": 0.00018058991436726928,
      "loss": 0.2496,
      "step": 214
    },
    {
      "epoch": 0.30518097941802697,
      "grad_norm": 0.23300093412399292,
      "learning_rate": 0.00018049476688867746,
      "loss": 0.2167,
      "step": 215
    },
    {
      "epoch": 0.3066004258339248,
      "grad_norm": 0.20153824985027313,
      "learning_rate": 0.00018039961941008565,
      "loss": 0.1941,
      "step": 216
    },
    {
      "epoch": 0.3080198722498226,
      "grad_norm": 0.22291982173919678,
      "learning_rate": 0.00018030447193149383,
      "loss": 0.2212,
      "step": 217
    },
    {
      "epoch": 0.3094393186657204,
      "grad_norm": 0.2710703909397125,
      "learning_rate": 0.000180209324452902,
      "loss": 0.2324,
      "step": 218
    },
    {
      "epoch": 0.3108587650816182,
      "grad_norm": 0.22228169441223145,
      "learning_rate": 0.0001801141769743102,
      "loss": 0.2219,
      "step": 219
    },
    {
      "epoch": 0.31227821149751595,
      "grad_norm": 0.19645242393016815,
      "learning_rate": 0.00018001902949571838,
      "loss": 0.1829,
      "step": 220
    },
    {
      "epoch": 0.31369765791341375,
      "grad_norm": 0.2148899883031845,
      "learning_rate": 0.00017992388201712656,
      "loss": 0.1846,
      "step": 221
    },
    {
      "epoch": 0.31511710432931156,
      "grad_norm": 0.21384695172309875,
      "learning_rate": 0.00017982873453853472,
      "loss": 0.1629,
      "step": 222
    },
    {
      "epoch": 0.31653655074520937,
      "grad_norm": 0.20600435137748718,
      "learning_rate": 0.00017973358705994293,
      "loss": 0.1472,
      "step": 223
    },
    {
      "epoch": 0.3179559971611072,
      "grad_norm": 0.15907453000545502,
      "learning_rate": 0.00017963843958135109,
      "loss": 0.1376,
      "step": 224
    },
    {
      "epoch": 0.319375443577005,
      "grad_norm": 0.2737395763397217,
      "learning_rate": 0.0001795432921027593,
      "loss": 0.1338,
      "step": 225
    },
    {
      "epoch": 0.3207948899929028,
      "grad_norm": 0.18210864067077637,
      "learning_rate": 0.00017944814462416745,
      "loss": 0.1591,
      "step": 226
    },
    {
      "epoch": 0.3222143364088006,
      "grad_norm": 0.18131813406944275,
      "learning_rate": 0.00017935299714557566,
      "loss": 0.1454,
      "step": 227
    },
    {
      "epoch": 0.32363378282469835,
      "grad_norm": 0.1633884608745575,
      "learning_rate": 0.00017925784966698382,
      "loss": 0.1188,
      "step": 228
    },
    {
      "epoch": 0.32505322924059615,
      "grad_norm": 0.1833418607711792,
      "learning_rate": 0.00017916270218839203,
      "loss": 0.1387,
      "step": 229
    },
    {
      "epoch": 0.32647267565649396,
      "grad_norm": 0.20457161962985992,
      "learning_rate": 0.0001790675547098002,
      "loss": 0.1397,
      "step": 230
    },
    {
      "epoch": 0.32789212207239177,
      "grad_norm": 0.16873478889465332,
      "learning_rate": 0.00017897240723120837,
      "loss": 0.1374,
      "step": 231
    },
    {
      "epoch": 0.3293115684882896,
      "grad_norm": 0.16101014614105225,
      "learning_rate": 0.00017887725975261655,
      "loss": 0.1483,
      "step": 232
    },
    {
      "epoch": 0.3307310149041874,
      "grad_norm": 0.18449218571186066,
      "learning_rate": 0.00017878211227402474,
      "loss": 0.1686,
      "step": 233
    },
    {
      "epoch": 0.3321504613200852,
      "grad_norm": 0.154401496052742,
      "learning_rate": 0.00017868696479543292,
      "loss": 0.1242,
      "step": 234
    },
    {
      "epoch": 0.33356990773598294,
      "grad_norm": 0.1769985854625702,
      "learning_rate": 0.0001785918173168411,
      "loss": 0.1254,
      "step": 235
    },
    {
      "epoch": 0.33498935415188075,
      "grad_norm": 0.1648237556219101,
      "learning_rate": 0.0001784966698382493,
      "loss": 0.1232,
      "step": 236
    },
    {
      "epoch": 0.33640880056777855,
      "grad_norm": 0.1461450308561325,
      "learning_rate": 0.00017840152235965747,
      "loss": 0.1055,
      "step": 237
    },
    {
      "epoch": 0.33782824698367636,
      "grad_norm": 0.15033933520317078,
      "learning_rate": 0.00017830637488106565,
      "loss": 0.1054,
      "step": 238
    },
    {
      "epoch": 0.33924769339957417,
      "grad_norm": 0.11923829466104507,
      "learning_rate": 0.00017821122740247384,
      "loss": 0.1106,
      "step": 239
    },
    {
      "epoch": 0.340667139815472,
      "grad_norm": 0.13053034245967865,
      "learning_rate": 0.00017811607992388202,
      "loss": 0.1153,
      "step": 240
    },
    {
      "epoch": 0.3420865862313698,
      "grad_norm": 0.12194915115833282,
      "learning_rate": 0.0001780209324452902,
      "loss": 0.0878,
      "step": 241
    },
    {
      "epoch": 0.3435060326472676,
      "grad_norm": 0.12302149087190628,
      "learning_rate": 0.0001779257849666984,
      "loss": 0.1003,
      "step": 242
    },
    {
      "epoch": 0.34492547906316534,
      "grad_norm": 0.10759289562702179,
      "learning_rate": 0.00017783063748810657,
      "loss": 0.0948,
      "step": 243
    },
    {
      "epoch": 0.34634492547906315,
      "grad_norm": 0.11200319230556488,
      "learning_rate": 0.00017773549000951475,
      "loss": 0.0835,
      "step": 244
    },
    {
      "epoch": 0.34776437189496096,
      "grad_norm": 0.12429293245077133,
      "learning_rate": 0.00017764034253092294,
      "loss": 0.0768,
      "step": 245
    },
    {
      "epoch": 0.34918381831085876,
      "grad_norm": 0.08911320567131042,
      "learning_rate": 0.00017754519505233112,
      "loss": 0.0817,
      "step": 246
    },
    {
      "epoch": 0.35060326472675657,
      "grad_norm": 0.12302954494953156,
      "learning_rate": 0.0001774500475737393,
      "loss": 0.0935,
      "step": 247
    },
    {
      "epoch": 0.3520227111426544,
      "grad_norm": 0.09873057901859283,
      "learning_rate": 0.0001773549000951475,
      "loss": 0.0701,
      "step": 248
    },
    {
      "epoch": 0.3534421575585522,
      "grad_norm": 0.0786275565624237,
      "learning_rate": 0.00017725975261655567,
      "loss": 0.0564,
      "step": 249
    },
    {
      "epoch": 0.35486160397445,
      "grad_norm": 0.09582041203975677,
      "learning_rate": 0.00017716460513796385,
      "loss": 0.0691,
      "step": 250
    },
    {
      "epoch": 0.35628105039034774,
      "grad_norm": 0.5196399688720703,
      "learning_rate": 0.00017706945765937204,
      "loss": 0.5333,
      "step": 251
    },
    {
      "epoch": 0.35770049680624555,
      "grad_norm": 0.444915771484375,
      "learning_rate": 0.00017697431018078022,
      "loss": 0.4228,
      "step": 252
    },
    {
      "epoch": 0.35911994322214336,
      "grad_norm": 0.3279583752155304,
      "learning_rate": 0.0001768791627021884,
      "loss": 0.3702,
      "step": 253
    },
    {
      "epoch": 0.36053938963804116,
      "grad_norm": 0.295723021030426,
      "learning_rate": 0.0001767840152235966,
      "loss": 0.3464,
      "step": 254
    },
    {
      "epoch": 0.36195883605393897,
      "grad_norm": 0.32640308141708374,
      "learning_rate": 0.00017668886774500477,
      "loss": 0.3714,
      "step": 255
    },
    {
      "epoch": 0.3633782824698368,
      "grad_norm": 0.23948761820793152,
      "learning_rate": 0.00017659372026641295,
      "loss": 0.3377,
      "step": 256
    },
    {
      "epoch": 0.3647977288857346,
      "grad_norm": 0.2686794102191925,
      "learning_rate": 0.00017649857278782114,
      "loss": 0.3323,
      "step": 257
    },
    {
      "epoch": 0.36621717530163234,
      "grad_norm": 0.2624031603336334,
      "learning_rate": 0.00017640342530922932,
      "loss": 0.2537,
      "step": 258
    },
    {
      "epoch": 0.36763662171753014,
      "grad_norm": 0.2933129668235779,
      "learning_rate": 0.0001763082778306375,
      "loss": 0.3,
      "step": 259
    },
    {
      "epoch": 0.36905606813342795,
      "grad_norm": 0.23483724892139435,
      "learning_rate": 0.0001762131303520457,
      "loss": 0.2649,
      "step": 260
    },
    {
      "epoch": 0.37047551454932576,
      "grad_norm": 0.23871733248233795,
      "learning_rate": 0.00017611798287345387,
      "loss": 0.2487,
      "step": 261
    },
    {
      "epoch": 0.37189496096522356,
      "grad_norm": 0.2093488723039627,
      "learning_rate": 0.00017602283539486203,
      "loss": 0.2416,
      "step": 262
    },
    {
      "epoch": 0.37331440738112137,
      "grad_norm": 0.33524712920188904,
      "learning_rate": 0.00017592768791627024,
      "loss": 0.2572,
      "step": 263
    },
    {
      "epoch": 0.3747338537970192,
      "grad_norm": 0.2671527564525604,
      "learning_rate": 0.0001758325404376784,
      "loss": 0.2473,
      "step": 264
    },
    {
      "epoch": 0.376153300212917,
      "grad_norm": 0.21593506634235382,
      "learning_rate": 0.0001757373929590866,
      "loss": 0.2438,
      "step": 265
    },
    {
      "epoch": 0.37757274662881474,
      "grad_norm": 0.245122030377388,
      "learning_rate": 0.00017564224548049476,
      "loss": 0.2661,
      "step": 266
    },
    {
      "epoch": 0.37899219304471254,
      "grad_norm": 0.2078303098678589,
      "learning_rate": 0.00017554709800190297,
      "loss": 0.2199,
      "step": 267
    },
    {
      "epoch": 0.38041163946061035,
      "grad_norm": 0.22816424071788788,
      "learning_rate": 0.00017545195052331113,
      "loss": 0.2178,
      "step": 268
    },
    {
      "epoch": 0.38183108587650816,
      "grad_norm": 0.19368530809879303,
      "learning_rate": 0.00017535680304471934,
      "loss": 0.2057,
      "step": 269
    },
    {
      "epoch": 0.38325053229240597,
      "grad_norm": 0.25312456488609314,
      "learning_rate": 0.0001752616555661275,
      "loss": 0.1906,
      "step": 270
    },
    {
      "epoch": 0.3846699787083038,
      "grad_norm": 0.20021282136440277,
      "learning_rate": 0.0001751665080875357,
      "loss": 0.1888,
      "step": 271
    },
    {
      "epoch": 0.3860894251242016,
      "grad_norm": 0.19169561564922333,
      "learning_rate": 0.00017507136060894386,
      "loss": 0.2014,
      "step": 272
    },
    {
      "epoch": 0.3875088715400994,
      "grad_norm": 0.23282867670059204,
      "learning_rate": 0.00017497621313035207,
      "loss": 0.2081,
      "step": 273
    },
    {
      "epoch": 0.38892831795599714,
      "grad_norm": 0.15645594894886017,
      "learning_rate": 0.00017488106565176023,
      "loss": 0.14,
      "step": 274
    },
    {
      "epoch": 0.39034776437189495,
      "grad_norm": 0.20146551728248596,
      "learning_rate": 0.00017478591817316844,
      "loss": 0.1751,
      "step": 275
    },
    {
      "epoch": 0.39176721078779275,
      "grad_norm": 0.15790389478206635,
      "learning_rate": 0.0001746907706945766,
      "loss": 0.1428,
      "step": 276
    },
    {
      "epoch": 0.39318665720369056,
      "grad_norm": 0.13402463495731354,
      "learning_rate": 0.0001745956232159848,
      "loss": 0.1508,
      "step": 277
    },
    {
      "epoch": 0.39460610361958837,
      "grad_norm": 0.19556523859500885,
      "learning_rate": 0.00017450047573739296,
      "loss": 0.1774,
      "step": 278
    },
    {
      "epoch": 0.3960255500354862,
      "grad_norm": 0.26072943210601807,
      "learning_rate": 0.00017440532825880117,
      "loss": 0.1569,
      "step": 279
    },
    {
      "epoch": 0.397444996451384,
      "grad_norm": 0.14136475324630737,
      "learning_rate": 0.00017431018078020933,
      "loss": 0.1252,
      "step": 280
    },
    {
      "epoch": 0.3988644428672818,
      "grad_norm": 0.17014621198177338,
      "learning_rate": 0.00017421503330161754,
      "loss": 0.1561,
      "step": 281
    },
    {
      "epoch": 0.40028388928317954,
      "grad_norm": 0.16529017686843872,
      "learning_rate": 0.0001741198858230257,
      "loss": 0.1521,
      "step": 282
    },
    {
      "epoch": 0.40170333569907735,
      "grad_norm": 0.17142519354820251,
      "learning_rate": 0.00017402473834443388,
      "loss": 0.1061,
      "step": 283
    },
    {
      "epoch": 0.40312278211497515,
      "grad_norm": 0.1425708681344986,
      "learning_rate": 0.00017392959086584206,
      "loss": 0.1285,
      "step": 284
    },
    {
      "epoch": 0.40454222853087296,
      "grad_norm": 0.18765318393707275,
      "learning_rate": 0.00017383444338725024,
      "loss": 0.1233,
      "step": 285
    },
    {
      "epoch": 0.40596167494677077,
      "grad_norm": 0.14780974388122559,
      "learning_rate": 0.00017373929590865843,
      "loss": 0.1043,
      "step": 286
    },
    {
      "epoch": 0.4073811213626686,
      "grad_norm": 0.14468996226787567,
      "learning_rate": 0.0001736441484300666,
      "loss": 0.112,
      "step": 287
    },
    {
      "epoch": 0.4088005677785664,
      "grad_norm": 0.12419953942298889,
      "learning_rate": 0.0001735490009514748,
      "loss": 0.0896,
      "step": 288
    },
    {
      "epoch": 0.41022001419446413,
      "grad_norm": 0.1314602941274643,
      "learning_rate": 0.00017345385347288298,
      "loss": 0.1008,
      "step": 289
    },
    {
      "epoch": 0.41163946061036194,
      "grad_norm": 0.15486060082912445,
      "learning_rate": 0.00017335870599429116,
      "loss": 0.0982,
      "step": 290
    },
    {
      "epoch": 0.41305890702625975,
      "grad_norm": 0.12233155220746994,
      "learning_rate": 0.00017326355851569934,
      "loss": 0.0964,
      "step": 291
    },
    {
      "epoch": 0.41447835344215755,
      "grad_norm": 0.1104344055056572,
      "learning_rate": 0.00017316841103710753,
      "loss": 0.0955,
      "step": 292
    },
    {
      "epoch": 0.41589779985805536,
      "grad_norm": 0.0978473499417305,
      "learning_rate": 0.00017307326355851568,
      "loss": 0.0746,
      "step": 293
    },
    {
      "epoch": 0.41731724627395317,
      "grad_norm": 0.10647564381361008,
      "learning_rate": 0.0001729781160799239,
      "loss": 0.0688,
      "step": 294
    },
    {
      "epoch": 0.418736692689851,
      "grad_norm": 0.1035827174782753,
      "learning_rate": 0.00017288296860133205,
      "loss": 0.0785,
      "step": 295
    },
    {
      "epoch": 0.4201561391057488,
      "grad_norm": 0.12469320744276047,
      "learning_rate": 0.00017278782112274026,
      "loss": 0.0889,
      "step": 296
    },
    {
      "epoch": 0.42157558552164653,
      "grad_norm": 0.11159732937812805,
      "learning_rate": 0.00017269267364414842,
      "loss": 0.0786,
      "step": 297
    },
    {
      "epoch": 0.42299503193754434,
      "grad_norm": 0.09567723423242569,
      "learning_rate": 0.00017259752616555663,
      "loss": 0.0618,
      "step": 298
    },
    {
      "epoch": 0.42441447835344215,
      "grad_norm": 0.11323487013578415,
      "learning_rate": 0.00017250237868696478,
      "loss": 0.0735,
      "step": 299
    },
    {
      "epoch": 0.42583392476933996,
      "grad_norm": 0.36350539326667786,
      "learning_rate": 0.000172407231208373,
      "loss": 0.0587,
      "step": 300
    },
    {
      "epoch": 0.42725337118523776,
      "grad_norm": 0.4785170257091522,
      "learning_rate": 0.00017231208372978115,
      "loss": 0.518,
      "step": 301
    },
    {
      "epoch": 0.42867281760113557,
      "grad_norm": 0.4595721960067749,
      "learning_rate": 0.00017221693625118936,
      "loss": 0.4631,
      "step": 302
    },
    {
      "epoch": 0.4300922640170334,
      "grad_norm": 0.3478117287158966,
      "learning_rate": 0.00017212178877259752,
      "loss": 0.4587,
      "step": 303
    },
    {
      "epoch": 0.4315117104329312,
      "grad_norm": 0.33338478207588196,
      "learning_rate": 0.00017202664129400573,
      "loss": 0.3858,
      "step": 304
    },
    {
      "epoch": 0.43293115684882894,
      "grad_norm": 0.27139970660209656,
      "learning_rate": 0.00017193149381541388,
      "loss": 0.3406,
      "step": 305
    },
    {
      "epoch": 0.43435060326472674,
      "grad_norm": 0.24543721973896027,
      "learning_rate": 0.0001718363463368221,
      "loss": 0.3331,
      "step": 306
    },
    {
      "epoch": 0.43577004968062455,
      "grad_norm": 0.2112877368927002,
      "learning_rate": 0.00017174119885823025,
      "loss": 0.2871,
      "step": 307
    },
    {
      "epoch": 0.43718949609652236,
      "grad_norm": 0.304798424243927,
      "learning_rate": 0.00017164605137963846,
      "loss": 0.3329,
      "step": 308
    },
    {
      "epoch": 0.43860894251242016,
      "grad_norm": 0.21619813144207,
      "learning_rate": 0.00017155090390104662,
      "loss": 0.2867,
      "step": 309
    },
    {
      "epoch": 0.44002838892831797,
      "grad_norm": 0.24975015223026276,
      "learning_rate": 0.00017145575642245483,
      "loss": 0.3055,
      "step": 310
    },
    {
      "epoch": 0.4414478353442158,
      "grad_norm": 0.21540270745754242,
      "learning_rate": 0.00017136060894386298,
      "loss": 0.2333,
      "step": 311
    },
    {
      "epoch": 0.44286728176011353,
      "grad_norm": 0.18148262798786163,
      "learning_rate": 0.0001712654614652712,
      "loss": 0.2232,
      "step": 312
    },
    {
      "epoch": 0.44428672817601134,
      "grad_norm": 0.20767313241958618,
      "learning_rate": 0.00017117031398667935,
      "loss": 0.204,
      "step": 313
    },
    {
      "epoch": 0.44570617459190914,
      "grad_norm": 0.22825096547603607,
      "learning_rate": 0.00017107516650808753,
      "loss": 0.2421,
      "step": 314
    },
    {
      "epoch": 0.44712562100780695,
      "grad_norm": 0.23413901031017303,
      "learning_rate": 0.00017098001902949572,
      "loss": 0.2224,
      "step": 315
    },
    {
      "epoch": 0.44854506742370476,
      "grad_norm": 0.20314759016036987,
      "learning_rate": 0.0001708848715509039,
      "loss": 0.1989,
      "step": 316
    },
    {
      "epoch": 0.44996451383960256,
      "grad_norm": 0.19911476969718933,
      "learning_rate": 0.00017078972407231208,
      "loss": 0.2239,
      "step": 317
    },
    {
      "epoch": 0.45138396025550037,
      "grad_norm": 0.20485487580299377,
      "learning_rate": 0.00017069457659372027,
      "loss": 0.21,
      "step": 318
    },
    {
      "epoch": 0.4528034066713982,
      "grad_norm": 0.16163139045238495,
      "learning_rate": 0.00017059942911512845,
      "loss": 0.154,
      "step": 319
    },
    {
      "epoch": 0.45422285308729593,
      "grad_norm": 0.17442378401756287,
      "learning_rate": 0.00017050428163653663,
      "loss": 0.1623,
      "step": 320
    },
    {
      "epoch": 0.45564229950319374,
      "grad_norm": 0.1901973932981491,
      "learning_rate": 0.00017040913415794482,
      "loss": 0.1975,
      "step": 321
    },
    {
      "epoch": 0.45706174591909154,
      "grad_norm": 0.17201147973537445,
      "learning_rate": 0.000170313986679353,
      "loss": 0.1577,
      "step": 322
    },
    {
      "epoch": 0.45848119233498935,
      "grad_norm": 0.24799390137195587,
      "learning_rate": 0.00017021883920076119,
      "loss": 0.1743,
      "step": 323
    },
    {
      "epoch": 0.45990063875088716,
      "grad_norm": 0.1515909880399704,
      "learning_rate": 0.00017012369172216937,
      "loss": 0.1395,
      "step": 324
    },
    {
      "epoch": 0.46132008516678497,
      "grad_norm": 0.3801301419734955,
      "learning_rate": 0.00017002854424357755,
      "loss": 0.2097,
      "step": 325
    },
    {
      "epoch": 0.4627395315826828,
      "grad_norm": 0.2507898807525635,
      "learning_rate": 0.00016993339676498574,
      "loss": 0.1767,
      "step": 326
    },
    {
      "epoch": 0.4641589779985806,
      "grad_norm": 0.1987512707710266,
      "learning_rate": 0.00016983824928639392,
      "loss": 0.1428,
      "step": 327
    },
    {
      "epoch": 0.46557842441447833,
      "grad_norm": 0.1773303896188736,
      "learning_rate": 0.0001697431018078021,
      "loss": 0.167,
      "step": 328
    },
    {
      "epoch": 0.46699787083037614,
      "grad_norm": 0.15946342051029205,
      "learning_rate": 0.00016964795432921029,
      "loss": 0.1494,
      "step": 329
    },
    {
      "epoch": 0.46841731724627395,
      "grad_norm": 0.1415938287973404,
      "learning_rate": 0.00016955280685061847,
      "loss": 0.1197,
      "step": 330
    },
    {
      "epoch": 0.46983676366217175,
      "grad_norm": 0.1696440577507019,
      "learning_rate": 0.00016945765937202665,
      "loss": 0.1348,
      "step": 331
    },
    {
      "epoch": 0.47125621007806956,
      "grad_norm": 0.17328503727912903,
      "learning_rate": 0.00016936251189343484,
      "loss": 0.1301,
      "step": 332
    },
    {
      "epoch": 0.47267565649396737,
      "grad_norm": 0.14782625436782837,
      "learning_rate": 0.00016926736441484302,
      "loss": 0.1398,
      "step": 333
    },
    {
      "epoch": 0.4740951029098652,
      "grad_norm": 0.1644575446844101,
      "learning_rate": 0.0001691722169362512,
      "loss": 0.1488,
      "step": 334
    },
    {
      "epoch": 0.4755145493257629,
      "grad_norm": 0.13526898622512817,
      "learning_rate": 0.00016907706945765939,
      "loss": 0.1213,
      "step": 335
    },
    {
      "epoch": 0.47693399574166073,
      "grad_norm": 0.14072123169898987,
      "learning_rate": 0.00016898192197906757,
      "loss": 0.1249,
      "step": 336
    },
    {
      "epoch": 0.47835344215755854,
      "grad_norm": 0.12543220818042755,
      "learning_rate": 0.00016888677450047575,
      "loss": 0.1191,
      "step": 337
    },
    {
      "epoch": 0.47977288857345635,
      "grad_norm": 0.13075068593025208,
      "learning_rate": 0.00016879162702188394,
      "loss": 0.113,
      "step": 338
    },
    {
      "epoch": 0.48119233498935415,
      "grad_norm": 0.1628408282995224,
      "learning_rate": 0.00016869647954329212,
      "loss": 0.1083,
      "step": 339
    },
    {
      "epoch": 0.48261178140525196,
      "grad_norm": 0.12735773622989655,
      "learning_rate": 0.0001686013320647003,
      "loss": 0.0759,
      "step": 340
    },
    {
      "epoch": 0.48403122782114977,
      "grad_norm": 0.13596266508102417,
      "learning_rate": 0.00016850618458610849,
      "loss": 0.1073,
      "step": 341
    },
    {
      "epoch": 0.4854506742370476,
      "grad_norm": 0.10148149728775024,
      "learning_rate": 0.00016841103710751667,
      "loss": 0.0714,
      "step": 342
    },
    {
      "epoch": 0.4868701206529453,
      "grad_norm": 0.10629739612340927,
      "learning_rate": 0.00016831588962892485,
      "loss": 0.083,
      "step": 343
    },
    {
      "epoch": 0.48828956706884313,
      "grad_norm": 0.16768154501914978,
      "learning_rate": 0.00016822074215033304,
      "loss": 0.1239,
      "step": 344
    },
    {
      "epoch": 0.48970901348474094,
      "grad_norm": 0.15854743123054504,
      "learning_rate": 0.0001681255946717412,
      "loss": 0.0868,
      "step": 345
    },
    {
      "epoch": 0.49112845990063875,
      "grad_norm": 0.126465305685997,
      "learning_rate": 0.0001680304471931494,
      "loss": 0.0918,
      "step": 346
    },
    {
      "epoch": 0.49254790631653655,
      "grad_norm": 0.14185777306556702,
      "learning_rate": 0.00016793529971455756,
      "loss": 0.0713,
      "step": 347
    },
    {
      "epoch": 0.49396735273243436,
      "grad_norm": 0.10624835640192032,
      "learning_rate": 0.00016784015223596577,
      "loss": 0.0697,
      "step": 348
    },
    {
      "epoch": 0.49538679914833217,
      "grad_norm": 0.11770059168338776,
      "learning_rate": 0.00016774500475737393,
      "loss": 0.0699,
      "step": 349
    },
    {
      "epoch": 0.49680624556423,
      "grad_norm": 0.0840187519788742,
      "learning_rate": 0.00016764985727878214,
      "loss": 0.0478,
      "step": 350
    },
    {
      "epoch": 0.4982256919801277,
      "grad_norm": 0.6331724524497986,
      "learning_rate": 0.0001675547098001903,
      "loss": 0.5741,
      "step": 351
    },
    {
      "epoch": 0.49964513839602553,
      "grad_norm": 0.488374799489975,
      "learning_rate": 0.0001674595623215985,
      "loss": 0.446,
      "step": 352
    },
    {
      "epoch": 0.5010645848119234,
      "grad_norm": 0.38088110089302063,
      "learning_rate": 0.00016736441484300666,
      "loss": 0.4208,
      "step": 353
    },
    {
      "epoch": 0.5024840312278211,
      "grad_norm": 0.3756221830844879,
      "learning_rate": 0.00016726926736441487,
      "loss": 0.3902,
      "step": 354
    },
    {
      "epoch": 0.5039034776437189,
      "grad_norm": 0.2471628040075302,
      "learning_rate": 0.00016717411988582303,
      "loss": 0.3258,
      "step": 355
    },
    {
      "epoch": 0.5053229240596168,
      "grad_norm": 0.23875264823436737,
      "learning_rate": 0.00016707897240723124,
      "loss": 0.32,
      "step": 356
    },
    {
      "epoch": 0.5067423704755145,
      "grad_norm": 0.2846216559410095,
      "learning_rate": 0.0001669838249286394,
      "loss": 0.3069,
      "step": 357
    },
    {
      "epoch": 0.5081618168914124,
      "grad_norm": 0.2903178036212921,
      "learning_rate": 0.0001668886774500476,
      "loss": 0.2584,
      "step": 358
    },
    {
      "epoch": 0.5095812633073101,
      "grad_norm": 0.21420243382453918,
      "learning_rate": 0.00016679352997145576,
      "loss": 0.2632,
      "step": 359
    },
    {
      "epoch": 0.511000709723208,
      "grad_norm": 0.2651553452014923,
      "learning_rate": 0.00016669838249286397,
      "loss": 0.3097,
      "step": 360
    },
    {
      "epoch": 0.5124201561391057,
      "grad_norm": 0.22691239416599274,
      "learning_rate": 0.00016660323501427213,
      "loss": 0.258,
      "step": 361
    },
    {
      "epoch": 0.5138396025550035,
      "grad_norm": 0.2461789846420288,
      "learning_rate": 0.0001665080875356803,
      "loss": 0.2406,
      "step": 362
    },
    {
      "epoch": 0.5152590489709014,
      "grad_norm": 0.21322080492973328,
      "learning_rate": 0.0001664129400570885,
      "loss": 0.2157,
      "step": 363
    },
    {
      "epoch": 0.5166784953867991,
      "grad_norm": 0.22189216315746307,
      "learning_rate": 0.00016631779257849668,
      "loss": 0.245,
      "step": 364
    },
    {
      "epoch": 0.518097941802697,
      "grad_norm": 0.21649399399757385,
      "learning_rate": 0.00016622264509990486,
      "loss": 0.2343,
      "step": 365
    },
    {
      "epoch": 0.5195173882185947,
      "grad_norm": 0.2525191307067871,
      "learning_rate": 0.00016612749762131304,
      "loss": 0.212,
      "step": 366
    },
    {
      "epoch": 0.5209368346344926,
      "grad_norm": 0.2230282723903656,
      "learning_rate": 0.00016603235014272123,
      "loss": 0.2404,
      "step": 367
    },
    {
      "epoch": 0.5223562810503903,
      "grad_norm": 0.19831006228923798,
      "learning_rate": 0.0001659372026641294,
      "loss": 0.183,
      "step": 368
    },
    {
      "epoch": 0.5237757274662882,
      "grad_norm": 0.2026294767856598,
      "learning_rate": 0.0001658420551855376,
      "loss": 0.1909,
      "step": 369
    },
    {
      "epoch": 0.525195173882186,
      "grad_norm": 0.18979088962078094,
      "learning_rate": 0.00016574690770694578,
      "loss": 0.1757,
      "step": 370
    },
    {
      "epoch": 0.5266146202980837,
      "grad_norm": 0.21828339993953705,
      "learning_rate": 0.00016565176022835396,
      "loss": 0.1836,
      "step": 371
    },
    {
      "epoch": 0.5280340667139816,
      "grad_norm": 0.17623811960220337,
      "learning_rate": 0.00016555661274976214,
      "loss": 0.1601,
      "step": 372
    },
    {
      "epoch": 0.5294535131298793,
      "grad_norm": 0.14808376133441925,
      "learning_rate": 0.00016546146527117033,
      "loss": 0.1293,
      "step": 373
    },
    {
      "epoch": 0.5308729595457772,
      "grad_norm": 0.17691995203495026,
      "learning_rate": 0.00016536631779257848,
      "loss": 0.1526,
      "step": 374
    },
    {
      "epoch": 0.5322924059616749,
      "grad_norm": 0.17905530333518982,
      "learning_rate": 0.0001652711703139867,
      "loss": 0.1266,
      "step": 375
    },
    {
      "epoch": 0.5337118523775728,
      "grad_norm": 0.16115914285182953,
      "learning_rate": 0.00016517602283539485,
      "loss": 0.1249,
      "step": 376
    },
    {
      "epoch": 0.5351312987934705,
      "grad_norm": 0.1475640833377838,
      "learning_rate": 0.00016508087535680306,
      "loss": 0.1325,
      "step": 377
    },
    {
      "epoch": 0.5365507452093683,
      "grad_norm": 0.1882176697254181,
      "learning_rate": 0.00016498572787821122,
      "loss": 0.158,
      "step": 378
    },
    {
      "epoch": 0.5379701916252662,
      "grad_norm": 0.16310134530067444,
      "learning_rate": 0.00016489058039961943,
      "loss": 0.1441,
      "step": 379
    },
    {
      "epoch": 0.5393896380411639,
      "grad_norm": 0.1749742180109024,
      "learning_rate": 0.00016479543292102758,
      "loss": 0.1537,
      "step": 380
    },
    {
      "epoch": 0.5408090844570618,
      "grad_norm": 0.18974344432353973,
      "learning_rate": 0.0001647002854424358,
      "loss": 0.1483,
      "step": 381
    },
    {
      "epoch": 0.5422285308729595,
      "grad_norm": 0.15348966419696808,
      "learning_rate": 0.00016460513796384395,
      "loss": 0.1261,
      "step": 382
    },
    {
      "epoch": 0.5436479772888574,
      "grad_norm": 0.13731154799461365,
      "learning_rate": 0.00016450999048525216,
      "loss": 0.0929,
      "step": 383
    },
    {
      "epoch": 0.5450674237047551,
      "grad_norm": 0.1556616723537445,
      "learning_rate": 0.00016441484300666032,
      "loss": 0.1203,
      "step": 384
    },
    {
      "epoch": 0.5464868701206529,
      "grad_norm": 0.13559553027153015,
      "learning_rate": 0.00016431969552806853,
      "loss": 0.1004,
      "step": 385
    },
    {
      "epoch": 0.5479063165365508,
      "grad_norm": 0.13521873950958252,
      "learning_rate": 0.00016422454804947668,
      "loss": 0.122,
      "step": 386
    },
    {
      "epoch": 0.5493257629524485,
      "grad_norm": 0.13034307956695557,
      "learning_rate": 0.0001641294005708849,
      "loss": 0.1071,
      "step": 387
    },
    {
      "epoch": 0.5507452093683464,
      "grad_norm": 0.1721305400133133,
      "learning_rate": 0.00016403425309229305,
      "loss": 0.1119,
      "step": 388
    },
    {
      "epoch": 0.5521646557842441,
      "grad_norm": 0.15545590221881866,
      "learning_rate": 0.00016393910561370126,
      "loss": 0.0976,
      "step": 389
    },
    {
      "epoch": 0.553584102200142,
      "grad_norm": 0.1454247236251831,
      "learning_rate": 0.00016384395813510942,
      "loss": 0.0913,
      "step": 390
    },
    {
      "epoch": 0.5550035486160397,
      "grad_norm": 0.13429966568946838,
      "learning_rate": 0.00016374881065651763,
      "loss": 0.0924,
      "step": 391
    },
    {
      "epoch": 0.5564229950319376,
      "grad_norm": 0.1830882728099823,
      "learning_rate": 0.00016365366317792578,
      "loss": 0.0892,
      "step": 392
    },
    {
      "epoch": 0.5578424414478353,
      "grad_norm": 0.12371549755334854,
      "learning_rate": 0.000163558515699334,
      "loss": 0.0632,
      "step": 393
    },
    {
      "epoch": 0.5592618878637331,
      "grad_norm": 0.11389799416065216,
      "learning_rate": 0.00016346336822074215,
      "loss": 0.0821,
      "step": 394
    },
    {
      "epoch": 0.560681334279631,
      "grad_norm": 0.1552814394235611,
      "learning_rate": 0.00016336822074215033,
      "loss": 0.0961,
      "step": 395
    },
    {
      "epoch": 0.5621007806955287,
      "grad_norm": 0.1489168405532837,
      "learning_rate": 0.00016327307326355852,
      "loss": 0.0927,
      "step": 396
    },
    {
      "epoch": 0.5635202271114266,
      "grad_norm": 0.147613987326622,
      "learning_rate": 0.0001631779257849667,
      "loss": 0.091,
      "step": 397
    },
    {
      "epoch": 0.5649396735273243,
      "grad_norm": 0.10550914704799652,
      "learning_rate": 0.00016308277830637488,
      "loss": 0.0645,
      "step": 398
    },
    {
      "epoch": 0.5663591199432222,
      "grad_norm": 0.11544385552406311,
      "learning_rate": 0.00016298763082778307,
      "loss": 0.0705,
      "step": 399
    },
    {
      "epoch": 0.5677785663591199,
      "grad_norm": 0.09835031628608704,
      "learning_rate": 0.00016289248334919125,
      "loss": 0.0551,
      "step": 400
    },
    {
      "epoch": 0.5691980127750177,
      "grad_norm": 0.5734143257141113,
      "learning_rate": 0.00016279733587059943,
      "loss": 0.5311,
      "step": 401
    },
    {
      "epoch": 0.5706174591909156,
      "grad_norm": 0.3967488706111908,
      "learning_rate": 0.00016270218839200762,
      "loss": 0.42,
      "step": 402
    },
    {
      "epoch": 0.5720369056068133,
      "grad_norm": 0.3979634940624237,
      "learning_rate": 0.0001626070409134158,
      "loss": 0.4227,
      "step": 403
    },
    {
      "epoch": 0.5734563520227112,
      "grad_norm": 0.3215809464454651,
      "learning_rate": 0.00016251189343482398,
      "loss": 0.3438,
      "step": 404
    },
    {
      "epoch": 0.5748757984386089,
      "grad_norm": 0.28150179982185364,
      "learning_rate": 0.00016241674595623217,
      "loss": 0.3409,
      "step": 405
    },
    {
      "epoch": 0.5762952448545068,
      "grad_norm": 0.23088285326957703,
      "learning_rate": 0.00016232159847764035,
      "loss": 0.2856,
      "step": 406
    },
    {
      "epoch": 0.5777146912704045,
      "grad_norm": 0.24941927194595337,
      "learning_rate": 0.00016222645099904853,
      "loss": 0.3215,
      "step": 407
    },
    {
      "epoch": 0.5791341376863024,
      "grad_norm": 0.2608039975166321,
      "learning_rate": 0.00016213130352045672,
      "loss": 0.2984,
      "step": 408
    },
    {
      "epoch": 0.5805535841022001,
      "grad_norm": 0.2782004177570343,
      "learning_rate": 0.0001620361560418649,
      "loss": 0.3123,
      "step": 409
    },
    {
      "epoch": 0.5819730305180979,
      "grad_norm": 0.29299309849739075,
      "learning_rate": 0.00016194100856327308,
      "loss": 0.3224,
      "step": 410
    },
    {
      "epoch": 0.5833924769339958,
      "grad_norm": 0.2324187159538269,
      "learning_rate": 0.00016184586108468127,
      "loss": 0.2626,
      "step": 411
    },
    {
      "epoch": 0.5848119233498935,
      "grad_norm": 0.31871795654296875,
      "learning_rate": 0.00016175071360608945,
      "loss": 0.3211,
      "step": 412
    },
    {
      "epoch": 0.5862313697657914,
      "grad_norm": 0.2733614146709442,
      "learning_rate": 0.00016165556612749763,
      "loss": 0.288,
      "step": 413
    },
    {
      "epoch": 0.5876508161816891,
      "grad_norm": 0.2656559348106384,
      "learning_rate": 0.00016156041864890582,
      "loss": 0.2593,
      "step": 414
    },
    {
      "epoch": 0.589070262597587,
      "grad_norm": 0.2410850077867508,
      "learning_rate": 0.000161465271170314,
      "loss": 0.2606,
      "step": 415
    },
    {
      "epoch": 0.5904897090134847,
      "grad_norm": 0.2182936817407608,
      "learning_rate": 0.00016137012369172218,
      "loss": 0.2177,
      "step": 416
    },
    {
      "epoch": 0.5919091554293825,
      "grad_norm": 0.2415715456008911,
      "learning_rate": 0.00016127497621313037,
      "loss": 0.2419,
      "step": 417
    },
    {
      "epoch": 0.5933286018452804,
      "grad_norm": 0.18864968419075012,
      "learning_rate": 0.00016117982873453855,
      "loss": 0.1927,
      "step": 418
    },
    {
      "epoch": 0.5947480482611781,
      "grad_norm": 0.19137367606163025,
      "learning_rate": 0.00016108468125594673,
      "loss": 0.1817,
      "step": 419
    },
    {
      "epoch": 0.596167494677076,
      "grad_norm": 0.222140371799469,
      "learning_rate": 0.00016098953377735492,
      "loss": 0.2109,
      "step": 420
    },
    {
      "epoch": 0.5975869410929737,
      "grad_norm": 0.23214884102344513,
      "learning_rate": 0.0001608943862987631,
      "loss": 0.2105,
      "step": 421
    },
    {
      "epoch": 0.5990063875088716,
      "grad_norm": 0.19018523395061493,
      "learning_rate": 0.00016079923882017128,
      "loss": 0.1821,
      "step": 422
    },
    {
      "epoch": 0.6004258339247693,
      "grad_norm": 0.19725508987903595,
      "learning_rate": 0.00016070409134157947,
      "loss": 0.1668,
      "step": 423
    },
    {
      "epoch": 0.6004258339247693,
      "eval_loss": 0.18132758140563965,
      "eval_runtime": 350.2051,
      "eval_samples_per_second": 3.018,
      "eval_steps_per_second": 1.008,
      "step": 423
    },
    {
      "epoch": 0.6018452803406671,
      "grad_norm": 0.20987409353256226,
      "learning_rate": 0.00016060894386298765,
      "loss": 0.1645,
      "step": 424
    },
    {
      "epoch": 0.603264726756565,
      "grad_norm": 0.19481807947158813,
      "learning_rate": 0.00016051379638439583,
      "loss": 0.1913,
      "step": 425
    },
    {
      "epoch": 0.6046841731724627,
      "grad_norm": 0.2456100434064865,
      "learning_rate": 0.000160418648905804,
      "loss": 0.2019,
      "step": 426
    },
    {
      "epoch": 0.6061036195883606,
      "grad_norm": 0.21857067942619324,
      "learning_rate": 0.0001603235014272122,
      "loss": 0.198,
      "step": 427
    },
    {
      "epoch": 0.6075230660042583,
      "grad_norm": 0.1517402082681656,
      "learning_rate": 0.00016022835394862036,
      "loss": 0.1194,
      "step": 428
    },
    {
      "epoch": 0.6089425124201562,
      "grad_norm": 0.1431589126586914,
      "learning_rate": 0.00016013320647002857,
      "loss": 0.1407,
      "step": 429
    },
    {
      "epoch": 0.6103619588360539,
      "grad_norm": 0.1423029750585556,
      "learning_rate": 0.00016003805899143672,
      "loss": 0.1311,
      "step": 430
    },
    {
      "epoch": 0.6117814052519518,
      "grad_norm": 0.1388556808233261,
      "learning_rate": 0.0001599429115128449,
      "loss": 0.127,
      "step": 431
    },
    {
      "epoch": 0.6132008516678495,
      "grad_norm": 0.15656495094299316,
      "learning_rate": 0.0001598477640342531,
      "loss": 0.1219,
      "step": 432
    },
    {
      "epoch": 0.6146202980837473,
      "grad_norm": 0.1460929661989212,
      "learning_rate": 0.00015975261655566127,
      "loss": 0.1175,
      "step": 433
    },
    {
      "epoch": 0.6160397444996452,
      "grad_norm": 0.14177191257476807,
      "learning_rate": 0.00015965746907706946,
      "loss": 0.122,
      "step": 434
    },
    {
      "epoch": 0.6174591909155429,
      "grad_norm": 0.163153275847435,
      "learning_rate": 0.00015956232159847764,
      "loss": 0.1422,
      "step": 435
    },
    {
      "epoch": 0.6188786373314408,
      "grad_norm": 0.14655143022537231,
      "learning_rate": 0.00015946717411988582,
      "loss": 0.1119,
      "step": 436
    },
    {
      "epoch": 0.6202980837473385,
      "grad_norm": 0.1259157657623291,
      "learning_rate": 0.000159372026641294,
      "loss": 0.0865,
      "step": 437
    },
    {
      "epoch": 0.6217175301632364,
      "grad_norm": 0.14728648960590363,
      "learning_rate": 0.0001592768791627022,
      "loss": 0.1048,
      "step": 438
    },
    {
      "epoch": 0.6231369765791341,
      "grad_norm": 0.1266816407442093,
      "learning_rate": 0.00015918173168411037,
      "loss": 0.1025,
      "step": 439
    },
    {
      "epoch": 0.6245564229950319,
      "grad_norm": 0.12114408612251282,
      "learning_rate": 0.00015908658420551856,
      "loss": 0.1071,
      "step": 440
    },
    {
      "epoch": 0.6259758694109298,
      "grad_norm": 0.14592917263507843,
      "learning_rate": 0.00015899143672692674,
      "loss": 0.1268,
      "step": 441
    },
    {
      "epoch": 0.6273953158268275,
      "grad_norm": 0.12582072615623474,
      "learning_rate": 0.00015889628924833492,
      "loss": 0.0941,
      "step": 442
    },
    {
      "epoch": 0.6288147622427254,
      "grad_norm": 0.12897424399852753,
      "learning_rate": 0.0001588011417697431,
      "loss": 0.086,
      "step": 443
    },
    {
      "epoch": 0.6302342086586231,
      "grad_norm": 0.11238119751214981,
      "learning_rate": 0.0001587059942911513,
      "loss": 0.0952,
      "step": 444
    },
    {
      "epoch": 0.631653655074521,
      "grad_norm": 0.1291157603263855,
      "learning_rate": 0.00015861084681255947,
      "loss": 0.1033,
      "step": 445
    },
    {
      "epoch": 0.6330731014904187,
      "grad_norm": 0.0873965173959732,
      "learning_rate": 0.00015851569933396766,
      "loss": 0.0675,
      "step": 446
    },
    {
      "epoch": 0.6344925479063165,
      "grad_norm": 0.13512372970581055,
      "learning_rate": 0.00015842055185537584,
      "loss": 0.0967,
      "step": 447
    },
    {
      "epoch": 0.6359119943222143,
      "grad_norm": 0.14364920556545258,
      "learning_rate": 0.00015832540437678402,
      "loss": 0.0781,
      "step": 448
    },
    {
      "epoch": 0.6373314407381121,
      "grad_norm": 0.0919160544872284,
      "learning_rate": 0.0001582302568981922,
      "loss": 0.0646,
      "step": 449
    },
    {
      "epoch": 0.63875088715401,
      "grad_norm": 0.09288521856069565,
      "learning_rate": 0.0001581351094196004,
      "loss": 0.0578,
      "step": 450
    },
    {
      "epoch": 0.6401703335699077,
      "grad_norm": 0.5773417949676514,
      "learning_rate": 0.00015803996194100857,
      "loss": 0.6209,
      "step": 451
    },
    {
      "epoch": 0.6415897799858056,
      "grad_norm": 0.39017078280448914,
      "learning_rate": 0.00015794481446241676,
      "loss": 0.393,
      "step": 452
    },
    {
      "epoch": 0.6430092264017033,
      "grad_norm": 0.29224568605422974,
      "learning_rate": 0.00015784966698382494,
      "loss": 0.3267,
      "step": 453
    },
    {
      "epoch": 0.6444286728176012,
      "grad_norm": 0.3001405596733093,
      "learning_rate": 0.00015775451950523312,
      "loss": 0.3313,
      "step": 454
    },
    {
      "epoch": 0.6458481192334989,
      "grad_norm": 0.2834407389163971,
      "learning_rate": 0.0001576593720266413,
      "loss": 0.342,
      "step": 455
    },
    {
      "epoch": 0.6472675656493967,
      "grad_norm": 0.23928727209568024,
      "learning_rate": 0.0001575642245480495,
      "loss": 0.2847,
      "step": 456
    },
    {
      "epoch": 0.6486870120652946,
      "grad_norm": 0.2410030961036682,
      "learning_rate": 0.00015746907706945765,
      "loss": 0.3095,
      "step": 457
    },
    {
      "epoch": 0.6501064584811923,
      "grad_norm": 0.22346051037311554,
      "learning_rate": 0.00015737392959086586,
      "loss": 0.288,
      "step": 458
    },
    {
      "epoch": 0.6515259048970902,
      "grad_norm": 0.23195426166057587,
      "learning_rate": 0.00015727878211227401,
      "loss": 0.2825,
      "step": 459
    },
    {
      "epoch": 0.6529453513129879,
      "grad_norm": 0.20546920597553253,
      "learning_rate": 0.00015718363463368222,
      "loss": 0.251,
      "step": 460
    },
    {
      "epoch": 0.6543647977288858,
      "grad_norm": 0.24309487640857697,
      "learning_rate": 0.00015708848715509038,
      "loss": 0.2642,
      "step": 461
    },
    {
      "epoch": 0.6557842441447835,
      "grad_norm": 0.2406681925058365,
      "learning_rate": 0.0001569933396764986,
      "loss": 0.2356,
      "step": 462
    },
    {
      "epoch": 0.6572036905606813,
      "grad_norm": 0.21421076357364655,
      "learning_rate": 0.00015689819219790675,
      "loss": 0.2481,
      "step": 463
    },
    {
      "epoch": 0.6586231369765791,
      "grad_norm": 0.24608848989009857,
      "learning_rate": 0.00015680304471931496,
      "loss": 0.2426,
      "step": 464
    },
    {
      "epoch": 0.6600425833924769,
      "grad_norm": 0.22647297382354736,
      "learning_rate": 0.00015670789724072311,
      "loss": 0.2414,
      "step": 465
    },
    {
      "epoch": 0.6614620298083748,
      "grad_norm": 0.20979394018650055,
      "learning_rate": 0.00015661274976213132,
      "loss": 0.2067,
      "step": 466
    },
    {
      "epoch": 0.6628814762242725,
      "grad_norm": 0.1916702538728714,
      "learning_rate": 0.00015651760228353948,
      "loss": 0.1637,
      "step": 467
    },
    {
      "epoch": 0.6643009226401704,
      "grad_norm": 0.21306970715522766,
      "learning_rate": 0.0001564224548049477,
      "loss": 0.2083,
      "step": 468
    },
    {
      "epoch": 0.6657203690560681,
      "grad_norm": 0.18934592604637146,
      "learning_rate": 0.00015632730732635585,
      "loss": 0.1861,
      "step": 469
    },
    {
      "epoch": 0.6671398154719659,
      "grad_norm": 0.16308657824993134,
      "learning_rate": 0.00015623215984776406,
      "loss": 0.146,
      "step": 470
    },
    {
      "epoch": 0.6685592618878637,
      "grad_norm": 0.18044470250606537,
      "learning_rate": 0.00015613701236917221,
      "loss": 0.176,
      "step": 471
    },
    {
      "epoch": 0.6699787083037615,
      "grad_norm": 0.1645916849374771,
      "learning_rate": 0.00015604186489058042,
      "loss": 0.1327,
      "step": 472
    },
    {
      "epoch": 0.6713981547196594,
      "grad_norm": 0.15306499600410461,
      "learning_rate": 0.00015594671741198858,
      "loss": 0.1419,
      "step": 473
    },
    {
      "epoch": 0.6728176011355571,
      "grad_norm": 0.17582587897777557,
      "learning_rate": 0.0001558515699333968,
      "loss": 0.1426,
      "step": 474
    },
    {
      "epoch": 0.674237047551455,
      "grad_norm": 0.16724251210689545,
      "learning_rate": 0.00015575642245480495,
      "loss": 0.1479,
      "step": 475
    },
    {
      "epoch": 0.6756564939673527,
      "grad_norm": 0.14700044691562653,
      "learning_rate": 0.00015566127497621316,
      "loss": 0.136,
      "step": 476
    },
    {
      "epoch": 0.6770759403832506,
      "grad_norm": 0.1487903892993927,
      "learning_rate": 0.00015556612749762131,
      "loss": 0.1351,
      "step": 477
    },
    {
      "epoch": 0.6784953867991483,
      "grad_norm": 0.20699326694011688,
      "learning_rate": 0.0001554709800190295,
      "loss": 0.1403,
      "step": 478
    },
    {
      "epoch": 0.6799148332150461,
      "grad_norm": 0.15087048709392548,
      "learning_rate": 0.00015537583254043768,
      "loss": 0.1224,
      "step": 479
    },
    {
      "epoch": 0.681334279630944,
      "grad_norm": 0.1315482258796692,
      "learning_rate": 0.00015528068506184586,
      "loss": 0.1058,
      "step": 480
    },
    {
      "epoch": 0.6827537260468417,
      "grad_norm": 0.18645484745502472,
      "learning_rate": 0.00015518553758325405,
      "loss": 0.1342,
      "step": 481
    },
    {
      "epoch": 0.6841731724627396,
      "grad_norm": 0.19662021100521088,
      "learning_rate": 0.00015509039010466223,
      "loss": 0.1463,
      "step": 482
    },
    {
      "epoch": 0.6855926188786373,
      "grad_norm": 0.12735840678215027,
      "learning_rate": 0.00015499524262607041,
      "loss": 0.0963,
      "step": 483
    },
    {
      "epoch": 0.6870120652945352,
      "grad_norm": 0.1387183964252472,
      "learning_rate": 0.0001549000951474786,
      "loss": 0.0957,
      "step": 484
    },
    {
      "epoch": 0.6884315117104329,
      "grad_norm": 0.13882651925086975,
      "learning_rate": 0.00015480494766888678,
      "loss": 0.1095,
      "step": 485
    },
    {
      "epoch": 0.6898509581263307,
      "grad_norm": 0.17697463929653168,
      "learning_rate": 0.00015470980019029496,
      "loss": 0.0947,
      "step": 486
    },
    {
      "epoch": 0.6912704045422285,
      "grad_norm": 0.12020111083984375,
      "learning_rate": 0.00015461465271170315,
      "loss": 0.0928,
      "step": 487
    },
    {
      "epoch": 0.6926898509581263,
      "grad_norm": 0.11944457143545151,
      "learning_rate": 0.00015451950523311133,
      "loss": 0.0909,
      "step": 488
    },
    {
      "epoch": 0.6941092973740242,
      "grad_norm": 0.12041766196489334,
      "learning_rate": 0.00015442435775451951,
      "loss": 0.0907,
      "step": 489
    },
    {
      "epoch": 0.6955287437899219,
      "grad_norm": 0.11162368953227997,
      "learning_rate": 0.0001543292102759277,
      "loss": 0.084,
      "step": 490
    },
    {
      "epoch": 0.6969481902058198,
      "grad_norm": 0.11690417677164078,
      "learning_rate": 0.00015423406279733588,
      "loss": 0.0778,
      "step": 491
    },
    {
      "epoch": 0.6983676366217175,
      "grad_norm": 0.13275593519210815,
      "learning_rate": 0.00015413891531874406,
      "loss": 0.1012,
      "step": 492
    },
    {
      "epoch": 0.6997870830376153,
      "grad_norm": 0.12870202958583832,
      "learning_rate": 0.00015404376784015225,
      "loss": 0.0807,
      "step": 493
    },
    {
      "epoch": 0.7012065294535131,
      "grad_norm": 0.13579553365707397,
      "learning_rate": 0.00015394862036156043,
      "loss": 0.0824,
      "step": 494
    },
    {
      "epoch": 0.7026259758694109,
      "grad_norm": 0.1251567006111145,
      "learning_rate": 0.00015385347288296861,
      "loss": 0.0808,
      "step": 495
    },
    {
      "epoch": 0.7040454222853088,
      "grad_norm": 0.10862187296152115,
      "learning_rate": 0.0001537583254043768,
      "loss": 0.0766,
      "step": 496
    },
    {
      "epoch": 0.7054648687012065,
      "grad_norm": 0.10223617404699326,
      "learning_rate": 0.00015366317792578498,
      "loss": 0.0595,
      "step": 497
    },
    {
      "epoch": 0.7068843151171044,
      "grad_norm": 0.125504732131958,
      "learning_rate": 0.00015356803044719316,
      "loss": 0.0711,
      "step": 498
    },
    {
      "epoch": 0.7083037615330021,
      "grad_norm": 0.09378299862146378,
      "learning_rate": 0.00015347288296860135,
      "loss": 0.0569,
      "step": 499
    },
    {
      "epoch": 0.7097232079489,
      "grad_norm": 0.09513435512781143,
      "learning_rate": 0.00015337773549000953,
      "loss": 0.0519,
      "step": 500
    },
    {
      "epoch": 0.7111426543647977,
      "grad_norm": 0.5318410396575928,
      "learning_rate": 0.00015328258801141772,
      "loss": 0.5911,
      "step": 501
    },
    {
      "epoch": 0.7125621007806955,
      "grad_norm": 0.4041525423526764,
      "learning_rate": 0.00015318744053282587,
      "loss": 0.4444,
      "step": 502
    },
    {
      "epoch": 0.7139815471965933,
      "grad_norm": 0.36350980401039124,
      "learning_rate": 0.00015309229305423408,
      "loss": 0.4043,
      "step": 503
    },
    {
      "epoch": 0.7154009936124911,
      "grad_norm": 0.3149220049381256,
      "learning_rate": 0.00015299714557564224,
      "loss": 0.388,
      "step": 504
    },
    {
      "epoch": 0.716820440028389,
      "grad_norm": 0.3172186315059662,
      "learning_rate": 0.00015290199809705045,
      "loss": 0.3572,
      "step": 505
    },
    {
      "epoch": 0.7182398864442867,
      "grad_norm": 0.21996641159057617,
      "learning_rate": 0.0001528068506184586,
      "loss": 0.3169,
      "step": 506
    },
    {
      "epoch": 0.7196593328601846,
      "grad_norm": 0.2433776557445526,
      "learning_rate": 0.00015271170313986682,
      "loss": 0.2814,
      "step": 507
    },
    {
      "epoch": 0.7210787792760823,
      "grad_norm": 0.22343401610851288,
      "learning_rate": 0.00015261655566127497,
      "loss": 0.2988,
      "step": 508
    },
    {
      "epoch": 0.7224982256919801,
      "grad_norm": 0.3074162006378174,
      "learning_rate": 0.00015252140818268315,
      "loss": 0.2837,
      "step": 509
    },
    {
      "epoch": 0.7239176721078779,
      "grad_norm": 0.3100600242614746,
      "learning_rate": 0.00015242626070409134,
      "loss": 0.3222,
      "step": 510
    },
    {
      "epoch": 0.7253371185237757,
      "grad_norm": 0.2587689757347107,
      "learning_rate": 0.00015233111322549952,
      "loss": 0.3424,
      "step": 511
    },
    {
      "epoch": 0.7267565649396736,
      "grad_norm": 0.32157301902770996,
      "learning_rate": 0.0001522359657469077,
      "loss": 0.2827,
      "step": 512
    },
    {
      "epoch": 0.7281760113555713,
      "grad_norm": 0.1988394856452942,
      "learning_rate": 0.0001521408182683159,
      "loss": 0.2079,
      "step": 513
    },
    {
      "epoch": 0.7295954577714692,
      "grad_norm": 0.19347591698169708,
      "learning_rate": 0.00015204567078972407,
      "loss": 0.2008,
      "step": 514
    },
    {
      "epoch": 0.7310149041873669,
      "grad_norm": 0.26390835642814636,
      "learning_rate": 0.00015195052331113226,
      "loss": 0.2376,
      "step": 515
    },
    {
      "epoch": 0.7324343506032647,
      "grad_norm": 0.2353726327419281,
      "learning_rate": 0.00015185537583254044,
      "loss": 0.2092,
      "step": 516
    },
    {
      "epoch": 0.7338537970191625,
      "grad_norm": 0.19849388301372528,
      "learning_rate": 0.00015176022835394862,
      "loss": 0.2198,
      "step": 517
    },
    {
      "epoch": 0.7352732434350603,
      "grad_norm": 0.18831156194210052,
      "learning_rate": 0.0001516650808753568,
      "loss": 0.1801,
      "step": 518
    },
    {
      "epoch": 0.7366926898509581,
      "grad_norm": 0.20285390317440033,
      "learning_rate": 0.000151569933396765,
      "loss": 0.2012,
      "step": 519
    },
    {
      "epoch": 0.7381121362668559,
      "grad_norm": 0.19947929680347443,
      "learning_rate": 0.00015147478591817317,
      "loss": 0.168,
      "step": 520
    },
    {
      "epoch": 0.7395315826827538,
      "grad_norm": 0.18636251986026764,
      "learning_rate": 0.00015137963843958136,
      "loss": 0.1753,
      "step": 521
    },
    {
      "epoch": 0.7409510290986515,
      "grad_norm": 0.19068703055381775,
      "learning_rate": 0.00015128449096098954,
      "loss": 0.175,
      "step": 522
    },
    {
      "epoch": 0.7423704755145494,
      "grad_norm": 0.14717668294906616,
      "learning_rate": 0.00015118934348239772,
      "loss": 0.1273,
      "step": 523
    },
    {
      "epoch": 0.7437899219304471,
      "grad_norm": 0.2050025761127472,
      "learning_rate": 0.0001510941960038059,
      "loss": 0.1684,
      "step": 524
    },
    {
      "epoch": 0.7452093683463449,
      "grad_norm": 0.1962311714887619,
      "learning_rate": 0.0001509990485252141,
      "loss": 0.1756,
      "step": 525
    },
    {
      "epoch": 0.7466288147622427,
      "grad_norm": 0.19705097377300262,
      "learning_rate": 0.00015090390104662227,
      "loss": 0.1744,
      "step": 526
    },
    {
      "epoch": 0.7480482611781405,
      "grad_norm": 0.16563889384269714,
      "learning_rate": 0.00015080875356803046,
      "loss": 0.1331,
      "step": 527
    },
    {
      "epoch": 0.7494677075940384,
      "grad_norm": 0.15262381732463837,
      "learning_rate": 0.00015071360608943864,
      "loss": 0.1118,
      "step": 528
    },
    {
      "epoch": 0.7508871540099361,
      "grad_norm": 0.16130661964416504,
      "learning_rate": 0.00015061845861084682,
      "loss": 0.1514,
      "step": 529
    },
    {
      "epoch": 0.752306600425834,
      "grad_norm": 0.1357499212026596,
      "learning_rate": 0.000150523311132255,
      "loss": 0.1327,
      "step": 530
    },
    {
      "epoch": 0.7537260468417317,
      "grad_norm": 0.171394482254982,
      "learning_rate": 0.0001504281636536632,
      "loss": 0.139,
      "step": 531
    },
    {
      "epoch": 0.7551454932576295,
      "grad_norm": 0.1561088263988495,
      "learning_rate": 0.00015033301617507137,
      "loss": 0.1381,
      "step": 532
    },
    {
      "epoch": 0.7565649396735273,
      "grad_norm": 0.14966213703155518,
      "learning_rate": 0.00015023786869647956,
      "loss": 0.143,
      "step": 533
    },
    {
      "epoch": 0.7579843860894251,
      "grad_norm": 0.14861424267292023,
      "learning_rate": 0.00015014272121788774,
      "loss": 0.1309,
      "step": 534
    },
    {
      "epoch": 0.759403832505323,
      "grad_norm": 0.12363504618406296,
      "learning_rate": 0.00015004757373929592,
      "loss": 0.1117,
      "step": 535
    },
    {
      "epoch": 0.7608232789212207,
      "grad_norm": 0.1706092208623886,
      "learning_rate": 0.0001499524262607041,
      "loss": 0.1281,
      "step": 536
    },
    {
      "epoch": 0.7622427253371186,
      "grad_norm": 0.27571582794189453,
      "learning_rate": 0.0001498572787821123,
      "loss": 0.1283,
      "step": 537
    },
    {
      "epoch": 0.7636621717530163,
      "grad_norm": 0.12227420508861542,
      "learning_rate": 0.00014976213130352045,
      "loss": 0.1099,
      "step": 538
    },
    {
      "epoch": 0.7650816181689141,
      "grad_norm": 0.1284242570400238,
      "learning_rate": 0.00014966698382492866,
      "loss": 0.0966,
      "step": 539
    },
    {
      "epoch": 0.7665010645848119,
      "grad_norm": 0.1298067420721054,
      "learning_rate": 0.0001495718363463368,
      "loss": 0.1141,
      "step": 540
    },
    {
      "epoch": 0.7679205110007097,
      "grad_norm": 0.10451561957597733,
      "learning_rate": 0.00014947668886774502,
      "loss": 0.0854,
      "step": 541
    },
    {
      "epoch": 0.7693399574166075,
      "grad_norm": 0.10955027490854263,
      "learning_rate": 0.00014938154138915318,
      "loss": 0.0872,
      "step": 542
    },
    {
      "epoch": 0.7707594038325053,
      "grad_norm": 0.11931219696998596,
      "learning_rate": 0.0001492863939105614,
      "loss": 0.0724,
      "step": 543
    },
    {
      "epoch": 0.7721788502484032,
      "grad_norm": 0.10852313041687012,
      "learning_rate": 0.00014919124643196955,
      "loss": 0.0737,
      "step": 544
    },
    {
      "epoch": 0.7735982966643009,
      "grad_norm": 0.1705986112356186,
      "learning_rate": 0.00014909609895337776,
      "loss": 0.1093,
      "step": 545
    },
    {
      "epoch": 0.7750177430801988,
      "grad_norm": 0.12562288343906403,
      "learning_rate": 0.0001490009514747859,
      "loss": 0.0799,
      "step": 546
    },
    {
      "epoch": 0.7764371894960965,
      "grad_norm": 0.15277740359306335,
      "learning_rate": 0.00014890580399619412,
      "loss": 0.0744,
      "step": 547
    },
    {
      "epoch": 0.7778566359119943,
      "grad_norm": 0.15108178555965424,
      "learning_rate": 0.00014881065651760228,
      "loss": 0.0825,
      "step": 548
    },
    {
      "epoch": 0.7792760823278921,
      "grad_norm": 0.08579418808221817,
      "learning_rate": 0.0001487155090390105,
      "loss": 0.0609,
      "step": 549
    },
    {
      "epoch": 0.7806955287437899,
      "grad_norm": 0.07670292258262634,
      "learning_rate": 0.00014862036156041865,
      "loss": 0.0521,
      "step": 550
    },
    {
      "epoch": 0.7821149751596878,
      "grad_norm": 0.5727811455726624,
      "learning_rate": 0.00014852521408182686,
      "loss": 0.6004,
      "step": 551
    },
    {
      "epoch": 0.7835344215755855,
      "grad_norm": 0.4237355887889862,
      "learning_rate": 0.000148430066603235,
      "loss": 0.4187,
      "step": 552
    },
    {
      "epoch": 0.7849538679914834,
      "grad_norm": 0.38377857208251953,
      "learning_rate": 0.00014833491912464322,
      "loss": 0.4589,
      "step": 553
    },
    {
      "epoch": 0.7863733144073811,
      "grad_norm": 0.30041319131851196,
      "learning_rate": 0.00014823977164605138,
      "loss": 0.3742,
      "step": 554
    },
    {
      "epoch": 0.7877927608232789,
      "grad_norm": 0.2506842017173767,
      "learning_rate": 0.0001481446241674596,
      "loss": 0.3193,
      "step": 555
    },
    {
      "epoch": 0.7892122072391767,
      "grad_norm": 0.28410252928733826,
      "learning_rate": 0.00014804947668886775,
      "loss": 0.3387,
      "step": 556
    },
    {
      "epoch": 0.7906316536550745,
      "grad_norm": 0.2975349724292755,
      "learning_rate": 0.00014795432921027596,
      "loss": 0.3203,
      "step": 557
    },
    {
      "epoch": 0.7920511000709723,
      "grad_norm": 0.2852706015110016,
      "learning_rate": 0.0001478591817316841,
      "loss": 0.3333,
      "step": 558
    },
    {
      "epoch": 0.7934705464868701,
      "grad_norm": 0.24174071848392487,
      "learning_rate": 0.0001477640342530923,
      "loss": 0.2559,
      "step": 559
    },
    {
      "epoch": 0.794889992902768,
      "grad_norm": 0.24527372419834137,
      "learning_rate": 0.00014766888677450048,
      "loss": 0.2817,
      "step": 560
    },
    {
      "epoch": 0.7963094393186657,
      "grad_norm": 0.24680295586585999,
      "learning_rate": 0.00014757373929590866,
      "loss": 0.259,
      "step": 561
    },
    {
      "epoch": 0.7977288857345636,
      "grad_norm": 0.26462438702583313,
      "learning_rate": 0.00014747859181731685,
      "loss": 0.238,
      "step": 562
    },
    {
      "epoch": 0.7991483321504613,
      "grad_norm": 0.22806233167648315,
      "learning_rate": 0.00014738344433872503,
      "loss": 0.2437,
      "step": 563
    },
    {
      "epoch": 0.8005677785663591,
      "grad_norm": 0.2250288724899292,
      "learning_rate": 0.0001472882968601332,
      "loss": 0.2275,
      "step": 564
    },
    {
      "epoch": 0.8019872249822569,
      "grad_norm": 0.2455858588218689,
      "learning_rate": 0.0001471931493815414,
      "loss": 0.2286,
      "step": 565
    },
    {
      "epoch": 0.8034066713981547,
      "grad_norm": 0.21104341745376587,
      "learning_rate": 0.00014709800190294958,
      "loss": 0.1818,
      "step": 566
    },
    {
      "epoch": 0.8048261178140526,
      "grad_norm": 0.26976826786994934,
      "learning_rate": 0.00014700285442435776,
      "loss": 0.2443,
      "step": 567
    },
    {
      "epoch": 0.8062455642299503,
      "grad_norm": 0.29840242862701416,
      "learning_rate": 0.00014690770694576595,
      "loss": 0.2517,
      "step": 568
    },
    {
      "epoch": 0.8076650106458482,
      "grad_norm": 0.22859202325344086,
      "learning_rate": 0.00014681255946717413,
      "loss": 0.2014,
      "step": 569
    },
    {
      "epoch": 0.8090844570617459,
      "grad_norm": 0.1852816492319107,
      "learning_rate": 0.0001467174119885823,
      "loss": 0.1858,
      "step": 570
    },
    {
      "epoch": 0.8105039034776437,
      "grad_norm": 0.18705476820468903,
      "learning_rate": 0.00014662226450999047,
      "loss": 0.1809,
      "step": 571
    },
    {
      "epoch": 0.8119233498935415,
      "grad_norm": 0.17783915996551514,
      "learning_rate": 0.00014652711703139868,
      "loss": 0.1493,
      "step": 572
    },
    {
      "epoch": 0.8133427963094393,
      "grad_norm": 0.2490440160036087,
      "learning_rate": 0.00014643196955280684,
      "loss": 0.2033,
      "step": 573
    },
    {
      "epoch": 0.8147622427253371,
      "grad_norm": 0.19925494492053986,
      "learning_rate": 0.00014633682207421505,
      "loss": 0.1792,
      "step": 574
    },
    {
      "epoch": 0.8161816891412349,
      "grad_norm": 0.15136615931987762,
      "learning_rate": 0.0001462416745956232,
      "loss": 0.146,
      "step": 575
    },
    {
      "epoch": 0.8176011355571328,
      "grad_norm": 0.16686761379241943,
      "learning_rate": 0.0001461465271170314,
      "loss": 0.1605,
      "step": 576
    },
    {
      "epoch": 0.8190205819730305,
      "grad_norm": 0.17302149534225464,
      "learning_rate": 0.00014605137963843957,
      "loss": 0.1574,
      "step": 577
    },
    {
      "epoch": 0.8204400283889283,
      "grad_norm": 0.1727643460035324,
      "learning_rate": 0.00014595623215984778,
      "loss": 0.1601,
      "step": 578
    },
    {
      "epoch": 0.8218594748048261,
      "grad_norm": 0.19917789101600647,
      "learning_rate": 0.00014586108468125594,
      "loss": 0.1617,
      "step": 579
    },
    {
      "epoch": 0.8232789212207239,
      "grad_norm": 0.15862904489040375,
      "learning_rate": 0.00014576593720266415,
      "loss": 0.127,
      "step": 580
    },
    {
      "epoch": 0.8246983676366217,
      "grad_norm": 0.19296683371067047,
      "learning_rate": 0.0001456707897240723,
      "loss": 0.169,
      "step": 581
    },
    {
      "epoch": 0.8261178140525195,
      "grad_norm": 0.1642456352710724,
      "learning_rate": 0.0001455756422454805,
      "loss": 0.1349,
      "step": 582
    },
    {
      "epoch": 0.8275372604684174,
      "grad_norm": 0.15335328876972198,
      "learning_rate": 0.00014548049476688867,
      "loss": 0.1295,
      "step": 583
    },
    {
      "epoch": 0.8289567068843151,
      "grad_norm": 0.18904566764831543,
      "learning_rate": 0.00014538534728829688,
      "loss": 0.1352,
      "step": 584
    },
    {
      "epoch": 0.830376153300213,
      "grad_norm": 0.17180368304252625,
      "learning_rate": 0.00014529019980970504,
      "loss": 0.1422,
      "step": 585
    },
    {
      "epoch": 0.8317955997161107,
      "grad_norm": 0.17974838614463806,
      "learning_rate": 0.00014519505233111325,
      "loss": 0.1232,
      "step": 586
    },
    {
      "epoch": 0.8332150461320085,
      "grad_norm": 0.16072316467761993,
      "learning_rate": 0.0001450999048525214,
      "loss": 0.1128,
      "step": 587
    },
    {
      "epoch": 0.8346344925479063,
      "grad_norm": 0.14107412099838257,
      "learning_rate": 0.0001450047573739296,
      "loss": 0.096,
      "step": 588
    },
    {
      "epoch": 0.8360539389638041,
      "grad_norm": 0.13276265561580658,
      "learning_rate": 0.00014490960989533777,
      "loss": 0.1049,
      "step": 589
    },
    {
      "epoch": 0.837473385379702,
      "grad_norm": 0.15781976282596588,
      "learning_rate": 0.00014481446241674595,
      "loss": 0.1152,
      "step": 590
    },
    {
      "epoch": 0.8388928317955997,
      "grad_norm": 0.1590309888124466,
      "learning_rate": 0.00014471931493815414,
      "loss": 0.1101,
      "step": 591
    },
    {
      "epoch": 0.8403122782114976,
      "grad_norm": 0.1210738867521286,
      "learning_rate": 0.00014462416745956232,
      "loss": 0.0638,
      "step": 592
    },
    {
      "epoch": 0.8417317246273953,
      "grad_norm": 0.14234381914138794,
      "learning_rate": 0.0001445290199809705,
      "loss": 0.1131,
      "step": 593
    },
    {
      "epoch": 0.8431511710432931,
      "grad_norm": 0.11053589731454849,
      "learning_rate": 0.00014443387250237869,
      "loss": 0.0849,
      "step": 594
    },
    {
      "epoch": 0.8445706174591909,
      "grad_norm": 0.11865406483411789,
      "learning_rate": 0.00014433872502378687,
      "loss": 0.0836,
      "step": 595
    },
    {
      "epoch": 0.8459900638750887,
      "grad_norm": 0.11474697291851044,
      "learning_rate": 0.00014424357754519505,
      "loss": 0.0784,
      "step": 596
    },
    {
      "epoch": 0.8474095102909865,
      "grad_norm": 0.11239312589168549,
      "learning_rate": 0.00014414843006660324,
      "loss": 0.0788,
      "step": 597
    },
    {
      "epoch": 0.8488289567068843,
      "grad_norm": 0.093875452876091,
      "learning_rate": 0.00014405328258801142,
      "loss": 0.0711,
      "step": 598
    },
    {
      "epoch": 0.8502484031227822,
      "grad_norm": 0.09124719351530075,
      "learning_rate": 0.0001439581351094196,
      "loss": 0.0503,
      "step": 599
    },
    {
      "epoch": 0.8516678495386799,
      "grad_norm": 0.09440264850854874,
      "learning_rate": 0.0001438629876308278,
      "loss": 0.0669,
      "step": 600
    },
    {
      "epoch": 0.8530872959545777,
      "grad_norm": 0.6293944120407104,
      "learning_rate": 0.00014376784015223597,
      "loss": 0.5435,
      "step": 601
    },
    {
      "epoch": 0.8545067423704755,
      "grad_norm": 0.4291839897632599,
      "learning_rate": 0.00014367269267364415,
      "loss": 0.5023,
      "step": 602
    },
    {
      "epoch": 0.8559261887863733,
      "grad_norm": 0.38119930028915405,
      "learning_rate": 0.00014357754519505234,
      "loss": 0.4098,
      "step": 603
    },
    {
      "epoch": 0.8573456352022711,
      "grad_norm": 0.3160819411277771,
      "learning_rate": 0.00014348239771646052,
      "loss": 0.3627,
      "step": 604
    },
    {
      "epoch": 0.8587650816181689,
      "grad_norm": 0.3257262706756592,
      "learning_rate": 0.0001433872502378687,
      "loss": 0.3849,
      "step": 605
    },
    {
      "epoch": 0.8601845280340668,
      "grad_norm": 0.313341349363327,
      "learning_rate": 0.0001432921027592769,
      "loss": 0.3333,
      "step": 606
    },
    {
      "epoch": 0.8616039744499645,
      "grad_norm": 0.24657322466373444,
      "learning_rate": 0.00014319695528068507,
      "loss": 0.3,
      "step": 607
    },
    {
      "epoch": 0.8630234208658624,
      "grad_norm": 0.23522303998470306,
      "learning_rate": 0.00014310180780209325,
      "loss": 0.2958,
      "step": 608
    },
    {
      "epoch": 0.8644428672817601,
      "grad_norm": 0.24196945130825043,
      "learning_rate": 0.00014300666032350144,
      "loss": 0.2626,
      "step": 609
    },
    {
      "epoch": 0.8658623136976579,
      "grad_norm": 0.25085851550102234,
      "learning_rate": 0.00014291151284490962,
      "loss": 0.2921,
      "step": 610
    },
    {
      "epoch": 0.8672817601135557,
      "grad_norm": 0.27037549018859863,
      "learning_rate": 0.0001428163653663178,
      "loss": 0.2503,
      "step": 611
    },
    {
      "epoch": 0.8687012065294535,
      "grad_norm": 0.22163833677768707,
      "learning_rate": 0.000142721217887726,
      "loss": 0.2544,
      "step": 612
    },
    {
      "epoch": 0.8701206529453513,
      "grad_norm": 0.22180965542793274,
      "learning_rate": 0.00014262607040913417,
      "loss": 0.2629,
      "step": 613
    },
    {
      "epoch": 0.8715400993612491,
      "grad_norm": 0.21035288274288177,
      "learning_rate": 0.00014253092293054235,
      "loss": 0.2249,
      "step": 614
    },
    {
      "epoch": 0.872959545777147,
      "grad_norm": 0.23706668615341187,
      "learning_rate": 0.00014243577545195054,
      "loss": 0.237,
      "step": 615
    },
    {
      "epoch": 0.8743789921930447,
      "grad_norm": 0.21941930055618286,
      "learning_rate": 0.00014234062797335872,
      "loss": 0.2174,
      "step": 616
    },
    {
      "epoch": 0.8757984386089425,
      "grad_norm": 0.19976352155208588,
      "learning_rate": 0.0001422454804947669,
      "loss": 0.2056,
      "step": 617
    },
    {
      "epoch": 0.8772178850248403,
      "grad_norm": 0.21921448409557343,
      "learning_rate": 0.0001421503330161751,
      "loss": 0.2323,
      "step": 618
    },
    {
      "epoch": 0.8786373314407381,
      "grad_norm": 0.20175445079803467,
      "learning_rate": 0.00014205518553758327,
      "loss": 0.19,
      "step": 619
    },
    {
      "epoch": 0.8800567778566359,
      "grad_norm": 0.16209647059440613,
      "learning_rate": 0.00014196003805899145,
      "loss": 0.1979,
      "step": 620
    },
    {
      "epoch": 0.8814762242725337,
      "grad_norm": 0.17713458836078644,
      "learning_rate": 0.0001418648905803996,
      "loss": 0.1737,
      "step": 621
    },
    {
      "epoch": 0.8828956706884316,
      "grad_norm": 0.18487538397312164,
      "learning_rate": 0.00014176974310180782,
      "loss": 0.1931,
      "step": 622
    },
    {
      "epoch": 0.8843151171043293,
      "grad_norm": 0.15035980939865112,
      "learning_rate": 0.00014167459562321598,
      "loss": 0.1376,
      "step": 623
    },
    {
      "epoch": 0.8857345635202271,
      "grad_norm": 0.158620685338974,
      "learning_rate": 0.0001415794481446242,
      "loss": 0.1449,
      "step": 624
    },
    {
      "epoch": 0.8871540099361249,
      "grad_norm": 0.18606463074684143,
      "learning_rate": 0.00014148430066603234,
      "loss": 0.1711,
      "step": 625
    },
    {
      "epoch": 0.8885734563520227,
      "grad_norm": 0.17129795253276825,
      "learning_rate": 0.00014138915318744055,
      "loss": 0.1586,
      "step": 626
    },
    {
      "epoch": 0.8899929027679205,
      "grad_norm": 0.15806519985198975,
      "learning_rate": 0.0001412940057088487,
      "loss": 0.1509,
      "step": 627
    },
    {
      "epoch": 0.8914123491838183,
      "grad_norm": 0.18260568380355835,
      "learning_rate": 0.00014119885823025692,
      "loss": 0.178,
      "step": 628
    },
    {
      "epoch": 0.8928317955997161,
      "grad_norm": 0.1893806755542755,
      "learning_rate": 0.00014110371075166508,
      "loss": 0.1496,
      "step": 629
    },
    {
      "epoch": 0.8942512420156139,
      "grad_norm": 0.16832983493804932,
      "learning_rate": 0.0001410085632730733,
      "loss": 0.1352,
      "step": 630
    },
    {
      "epoch": 0.8956706884315118,
      "grad_norm": 0.1850036233663559,
      "learning_rate": 0.00014091341579448144,
      "loss": 0.1513,
      "step": 631
    },
    {
      "epoch": 0.8970901348474095,
      "grad_norm": 0.14322754740715027,
      "learning_rate": 0.00014081826831588965,
      "loss": 0.1216,
      "step": 632
    },
    {
      "epoch": 0.8985095812633073,
      "grad_norm": 0.1574215441942215,
      "learning_rate": 0.0001407231208372978,
      "loss": 0.1369,
      "step": 633
    },
    {
      "epoch": 0.8999290276792051,
      "grad_norm": 0.1363058090209961,
      "learning_rate": 0.00014062797335870602,
      "loss": 0.1099,
      "step": 634
    },
    {
      "epoch": 0.9013484740951029,
      "grad_norm": 0.14761459827423096,
      "learning_rate": 0.00014053282588011418,
      "loss": 0.1092,
      "step": 635
    },
    {
      "epoch": 0.9027679205110007,
      "grad_norm": 0.1275966316461563,
      "learning_rate": 0.0001404376784015224,
      "loss": 0.0984,
      "step": 636
    },
    {
      "epoch": 0.9041873669268985,
      "grad_norm": 0.1448143869638443,
      "learning_rate": 0.00014034253092293054,
      "loss": 0.1154,
      "step": 637
    },
    {
      "epoch": 0.9056068133427964,
      "grad_norm": 0.15729297697544098,
      "learning_rate": 0.00014024738344433875,
      "loss": 0.1186,
      "step": 638
    },
    {
      "epoch": 0.9070262597586941,
      "grad_norm": 0.12504489719867706,
      "learning_rate": 0.0001401522359657469,
      "loss": 0.0992,
      "step": 639
    },
    {
      "epoch": 0.9084457061745919,
      "grad_norm": 0.12685270607471466,
      "learning_rate": 0.00014005708848715512,
      "loss": 0.0977,
      "step": 640
    },
    {
      "epoch": 0.9098651525904897,
      "grad_norm": 0.1121777817606926,
      "learning_rate": 0.00013996194100856328,
      "loss": 0.0849,
      "step": 641
    },
    {
      "epoch": 0.9112845990063875,
      "grad_norm": 0.11588460206985474,
      "learning_rate": 0.00013986679352997146,
      "loss": 0.0805,
      "step": 642
    },
    {
      "epoch": 0.9127040454222853,
      "grad_norm": 0.12288933247327805,
      "learning_rate": 0.00013977164605137964,
      "loss": 0.0942,
      "step": 643
    },
    {
      "epoch": 0.9141234918381831,
      "grad_norm": 0.11066070199012756,
      "learning_rate": 0.00013967649857278783,
      "loss": 0.0844,
      "step": 644
    },
    {
      "epoch": 0.915542938254081,
      "grad_norm": 0.11849730461835861,
      "learning_rate": 0.000139581351094196,
      "loss": 0.0903,
      "step": 645
    },
    {
      "epoch": 0.9169623846699787,
      "grad_norm": 0.14358580112457275,
      "learning_rate": 0.0001394862036156042,
      "loss": 0.0851,
      "step": 646
    },
    {
      "epoch": 0.9183818310858765,
      "grad_norm": 0.11204153299331665,
      "learning_rate": 0.00013939105613701238,
      "loss": 0.0682,
      "step": 647
    },
    {
      "epoch": 0.9198012775017743,
      "grad_norm": 0.10966405272483826,
      "learning_rate": 0.00013929590865842056,
      "loss": 0.0794,
      "step": 648
    },
    {
      "epoch": 0.9212207239176721,
      "grad_norm": 0.09689820557832718,
      "learning_rate": 0.00013920076117982874,
      "loss": 0.0618,
      "step": 649
    },
    {
      "epoch": 0.9226401703335699,
      "grad_norm": 0.1057770848274231,
      "learning_rate": 0.00013910561370123693,
      "loss": 0.0544,
      "step": 650
    },
    {
      "epoch": 0.9240596167494677,
      "grad_norm": 0.43264076113700867,
      "learning_rate": 0.0001390104662226451,
      "loss": 0.5093,
      "step": 651
    },
    {
      "epoch": 0.9254790631653655,
      "grad_norm": 0.37290966510772705,
      "learning_rate": 0.00013891531874405327,
      "loss": 0.3851,
      "step": 652
    },
    {
      "epoch": 0.9268985095812633,
      "grad_norm": 0.35502201318740845,
      "learning_rate": 0.00013882017126546148,
      "loss": 0.3743,
      "step": 653
    },
    {
      "epoch": 0.9283179559971612,
      "grad_norm": 0.36546796560287476,
      "learning_rate": 0.00013872502378686963,
      "loss": 0.4008,
      "step": 654
    },
    {
      "epoch": 0.9297374024130589,
      "grad_norm": 0.3241652846336365,
      "learning_rate": 0.00013862987630827784,
      "loss": 0.3426,
      "step": 655
    },
    {
      "epoch": 0.9311568488289567,
      "grad_norm": 0.27071768045425415,
      "learning_rate": 0.000138534728829686,
      "loss": 0.3561,
      "step": 656
    },
    {
      "epoch": 0.9325762952448545,
      "grad_norm": 0.24140791594982147,
      "learning_rate": 0.0001384395813510942,
      "loss": 0.3154,
      "step": 657
    },
    {
      "epoch": 0.9339957416607523,
      "grad_norm": 0.2794343829154968,
      "learning_rate": 0.00013834443387250237,
      "loss": 0.3443,
      "step": 658
    },
    {
      "epoch": 0.9354151880766501,
      "grad_norm": 0.2668094336986542,
      "learning_rate": 0.00013824928639391058,
      "loss": 0.2793,
      "step": 659
    },
    {
      "epoch": 0.9368346344925479,
      "grad_norm": 0.23154842853546143,
      "learning_rate": 0.00013815413891531873,
      "loss": 0.2511,
      "step": 660
    },
    {
      "epoch": 0.9382540809084458,
      "grad_norm": 0.2238822877407074,
      "learning_rate": 0.00013805899143672694,
      "loss": 0.2304,
      "step": 661
    },
    {
      "epoch": 0.9396735273243435,
      "grad_norm": 0.23827619850635529,
      "learning_rate": 0.0001379638439581351,
      "loss": 0.232,
      "step": 662
    },
    {
      "epoch": 0.9410929737402413,
      "grad_norm": 0.22854574024677277,
      "learning_rate": 0.0001378686964795433,
      "loss": 0.2643,
      "step": 663
    },
    {
      "epoch": 0.9425124201561391,
      "grad_norm": 0.26202166080474854,
      "learning_rate": 0.00013777354900095147,
      "loss": 0.2451,
      "step": 664
    },
    {
      "epoch": 0.9439318665720369,
      "grad_norm": 0.2784346044063568,
      "learning_rate": 0.00013767840152235968,
      "loss": 0.2481,
      "step": 665
    },
    {
      "epoch": 0.9453513129879347,
      "grad_norm": 0.24547907710075378,
      "learning_rate": 0.00013758325404376783,
      "loss": 0.2185,
      "step": 666
    },
    {
      "epoch": 0.9467707594038325,
      "grad_norm": 0.17956671118736267,
      "learning_rate": 0.00013748810656517604,
      "loss": 0.1563,
      "step": 667
    },
    {
      "epoch": 0.9481902058197303,
      "grad_norm": 0.19529953598976135,
      "learning_rate": 0.0001373929590865842,
      "loss": 0.2116,
      "step": 668
    },
    {
      "epoch": 0.9496096522356281,
      "grad_norm": 0.18870826065540314,
      "learning_rate": 0.0001372978116079924,
      "loss": 0.1932,
      "step": 669
    },
    {
      "epoch": 0.9510290986515259,
      "grad_norm": 0.1614811271429062,
      "learning_rate": 0.00013720266412940057,
      "loss": 0.1405,
      "step": 670
    },
    {
      "epoch": 0.9524485450674237,
      "grad_norm": 0.17701715230941772,
      "learning_rate": 0.00013710751665080878,
      "loss": 0.1635,
      "step": 671
    },
    {
      "epoch": 0.9538679914833215,
      "grad_norm": 0.25134775042533875,
      "learning_rate": 0.00013701236917221693,
      "loss": 0.2015,
      "step": 672
    },
    {
      "epoch": 0.9552874378992193,
      "grad_norm": 0.16321654617786407,
      "learning_rate": 0.00013691722169362512,
      "loss": 0.1383,
      "step": 673
    },
    {
      "epoch": 0.9567068843151171,
      "grad_norm": 0.26167479157447815,
      "learning_rate": 0.0001368220742150333,
      "loss": 0.1783,
      "step": 674
    },
    {
      "epoch": 0.9581263307310149,
      "grad_norm": 0.18638291954994202,
      "learning_rate": 0.00013672692673644148,
      "loss": 0.161,
      "step": 675
    },
    {
      "epoch": 0.9595457771469127,
      "grad_norm": 0.1568879634141922,
      "learning_rate": 0.00013663177925784967,
      "loss": 0.1449,
      "step": 676
    },
    {
      "epoch": 0.9609652235628106,
      "grad_norm": 0.21420665085315704,
      "learning_rate": 0.00013653663177925785,
      "loss": 0.1969,
      "step": 677
    },
    {
      "epoch": 0.9623846699787083,
      "grad_norm": 0.14903709292411804,
      "learning_rate": 0.00013644148430066603,
      "loss": 0.1236,
      "step": 678
    },
    {
      "epoch": 0.9638041163946061,
      "grad_norm": 0.1530379056930542,
      "learning_rate": 0.00013634633682207422,
      "loss": 0.1267,
      "step": 679
    },
    {
      "epoch": 0.9652235628105039,
      "grad_norm": 0.15803420543670654,
      "learning_rate": 0.0001362511893434824,
      "loss": 0.133,
      "step": 680
    },
    {
      "epoch": 0.9666430092264017,
      "grad_norm": 0.14294297993183136,
      "learning_rate": 0.00013615604186489058,
      "loss": 0.1071,
      "step": 681
    },
    {
      "epoch": 0.9680624556422995,
      "grad_norm": 0.2188594788312912,
      "learning_rate": 0.00013606089438629877,
      "loss": 0.1488,
      "step": 682
    },
    {
      "epoch": 0.9694819020581973,
      "grad_norm": 0.2593432068824768,
      "learning_rate": 0.00013596574690770695,
      "loss": 0.1371,
      "step": 683
    },
    {
      "epoch": 0.9709013484740951,
      "grad_norm": 0.1615634709596634,
      "learning_rate": 0.00013587059942911513,
      "loss": 0.1343,
      "step": 684
    },
    {
      "epoch": 0.9723207948899929,
      "grad_norm": 0.14472191035747528,
      "learning_rate": 0.00013577545195052332,
      "loss": 0.104,
      "step": 685
    },
    {
      "epoch": 0.9737402413058907,
      "grad_norm": 0.11434387415647507,
      "learning_rate": 0.0001356803044719315,
      "loss": 0.0982,
      "step": 686
    },
    {
      "epoch": 0.9751596877217885,
      "grad_norm": 0.14229045808315277,
      "learning_rate": 0.00013558515699333968,
      "loss": 0.0915,
      "step": 687
    },
    {
      "epoch": 0.9765791341376863,
      "grad_norm": 0.14187514781951904,
      "learning_rate": 0.00013549000951474787,
      "loss": 0.1274,
      "step": 688
    },
    {
      "epoch": 0.9779985805535841,
      "grad_norm": 0.1579059362411499,
      "learning_rate": 0.00013539486203615605,
      "loss": 0.1116,
      "step": 689
    },
    {
      "epoch": 0.9794180269694819,
      "grad_norm": 0.10884373635053635,
      "learning_rate": 0.00013529971455756423,
      "loss": 0.0702,
      "step": 690
    },
    {
      "epoch": 0.9808374733853797,
      "grad_norm": 0.11126106232404709,
      "learning_rate": 0.00013520456707897242,
      "loss": 0.0892,
      "step": 691
    },
    {
      "epoch": 0.9822569198012775,
      "grad_norm": 0.10765853524208069,
      "learning_rate": 0.0001351094196003806,
      "loss": 0.0815,
      "step": 692
    },
    {
      "epoch": 0.9836763662171752,
      "grad_norm": 0.09569776058197021,
      "learning_rate": 0.00013501427212178879,
      "loss": 0.0667,
      "step": 693
    },
    {
      "epoch": 0.9850958126330731,
      "grad_norm": 0.12836676836013794,
      "learning_rate": 0.00013491912464319697,
      "loss": 0.0831,
      "step": 694
    },
    {
      "epoch": 0.9865152590489709,
      "grad_norm": 0.1558128148317337,
      "learning_rate": 0.00013482397716460515,
      "loss": 0.0659,
      "step": 695
    },
    {
      "epoch": 0.9879347054648687,
      "grad_norm": 0.10963049530982971,
      "learning_rate": 0.00013472882968601334,
      "loss": 0.0729,
      "step": 696
    },
    {
      "epoch": 0.9893541518807665,
      "grad_norm": 0.10026916861534119,
      "learning_rate": 0.00013463368220742152,
      "loss": 0.0732,
      "step": 697
    },
    {
      "epoch": 0.9907735982966643,
      "grad_norm": 0.09855934232473373,
      "learning_rate": 0.0001345385347288297,
      "loss": 0.0822,
      "step": 698
    },
    {
      "epoch": 0.9921930447125621,
      "grad_norm": 0.11414436995983124,
      "learning_rate": 0.00013444338725023789,
      "loss": 0.0719,
      "step": 699
    },
    {
      "epoch": 0.99361249112846,
      "grad_norm": 0.10073580592870712,
      "learning_rate": 0.00013434823977164607,
      "loss": 0.0562,
      "step": 700
    },
    {
      "epoch": 0.9950319375443577,
      "grad_norm": 0.38939520716667175,
      "learning_rate": 0.00013425309229305425,
      "loss": 0.396,
      "step": 701
    },
    {
      "epoch": 0.9964513839602555,
      "grad_norm": 0.24623413383960724,
      "learning_rate": 0.00013415794481446244,
      "loss": 0.2127,
      "step": 702
    },
    {
      "epoch": 0.9978708303761533,
      "grad_norm": 0.16796725988388062,
      "learning_rate": 0.00013406279733587062,
      "loss": 0.1338,
      "step": 703
    },
    {
      "epoch": 0.9992902767920511,
      "grad_norm": 0.11252884566783905,
      "learning_rate": 0.00013396764985727877,
      "loss": 0.0768,
      "step": 704
    },
    {
      "epoch": 1.000709723207949,
      "grad_norm": 0.34894123673439026,
      "learning_rate": 0.00013387250237868699,
      "loss": 0.3404,
      "step": 705
    },
    {
      "epoch": 1.0021291696238468,
      "grad_norm": 0.3218008577823639,
      "learning_rate": 0.00013377735490009514,
      "loss": 0.409,
      "step": 706
    },
    {
      "epoch": 1.0035486160397444,
      "grad_norm": 0.32862189412117004,
      "learning_rate": 0.00013368220742150335,
      "loss": 0.3043,
      "step": 707
    },
    {
      "epoch": 1.0049680624556423,
      "grad_norm": 0.24026300013065338,
      "learning_rate": 0.0001335870599429115,
      "loss": 0.2644,
      "step": 708
    },
    {
      "epoch": 1.0063875088715402,
      "grad_norm": 0.5634042024612427,
      "learning_rate": 0.00013349191246431972,
      "loss": 0.279,
      "step": 709
    },
    {
      "epoch": 1.0078069552874378,
      "grad_norm": 0.20807743072509766,
      "learning_rate": 0.00013339676498572788,
      "loss": 0.233,
      "step": 710
    },
    {
      "epoch": 1.0092264017033357,
      "grad_norm": 0.20175105333328247,
      "learning_rate": 0.00013330161750713606,
      "loss": 0.2394,
      "step": 711
    },
    {
      "epoch": 1.0106458481192335,
      "grad_norm": 0.25610262155532837,
      "learning_rate": 0.00013320647002854424,
      "loss": 0.2639,
      "step": 712
    },
    {
      "epoch": 1.0120652945351314,
      "grad_norm": 0.28106600046157837,
      "learning_rate": 0.00013311132254995243,
      "loss": 0.3007,
      "step": 713
    },
    {
      "epoch": 1.013484740951029,
      "grad_norm": 0.2338162213563919,
      "learning_rate": 0.0001330161750713606,
      "loss": 0.225,
      "step": 714
    },
    {
      "epoch": 1.014904187366927,
      "grad_norm": 0.24868424236774445,
      "learning_rate": 0.0001329210275927688,
      "loss": 0.2076,
      "step": 715
    },
    {
      "epoch": 1.0163236337828248,
      "grad_norm": 0.2331075370311737,
      "learning_rate": 0.00013282588011417698,
      "loss": 0.1801,
      "step": 716
    },
    {
      "epoch": 1.0177430801987224,
      "grad_norm": 0.2903284430503845,
      "learning_rate": 0.00013273073263558516,
      "loss": 0.2158,
      "step": 717
    },
    {
      "epoch": 1.0191625266146203,
      "grad_norm": 0.2172362208366394,
      "learning_rate": 0.00013263558515699334,
      "loss": 0.1947,
      "step": 718
    },
    {
      "epoch": 1.0205819730305181,
      "grad_norm": 0.20478737354278564,
      "learning_rate": 0.00013254043767840153,
      "loss": 0.1783,
      "step": 719
    },
    {
      "epoch": 1.022001419446416,
      "grad_norm": 0.24831555783748627,
      "learning_rate": 0.0001324452901998097,
      "loss": 0.1681,
      "step": 720
    },
    {
      "epoch": 1.0234208658623136,
      "grad_norm": 0.1960899829864502,
      "learning_rate": 0.0001323501427212179,
      "loss": 0.1545,
      "step": 721
    },
    {
      "epoch": 1.0248403122782115,
      "grad_norm": 0.20280173420906067,
      "learning_rate": 0.00013225499524262608,
      "loss": 0.1574,
      "step": 722
    },
    {
      "epoch": 1.0262597586941093,
      "grad_norm": 0.19267137348651886,
      "learning_rate": 0.00013215984776403426,
      "loss": 0.159,
      "step": 723
    },
    {
      "epoch": 1.027679205110007,
      "grad_norm": 0.26803693175315857,
      "learning_rate": 0.00013206470028544244,
      "loss": 0.1873,
      "step": 724
    },
    {
      "epoch": 1.0290986515259049,
      "grad_norm": 0.18435527384281158,
      "learning_rate": 0.00013196955280685063,
      "loss": 0.1352,
      "step": 725
    },
    {
      "epoch": 1.0305180979418027,
      "grad_norm": 0.25576648116111755,
      "learning_rate": 0.0001318744053282588,
      "loss": 0.1614,
      "step": 726
    },
    {
      "epoch": 1.0319375443577006,
      "grad_norm": 0.17966328561306,
      "learning_rate": 0.000131779257849667,
      "loss": 0.1374,
      "step": 727
    },
    {
      "epoch": 1.0333569907735982,
      "grad_norm": 0.20831584930419922,
      "learning_rate": 0.00013168411037107518,
      "loss": 0.1387,
      "step": 728
    },
    {
      "epoch": 1.034776437189496,
      "grad_norm": 0.2099025398492813,
      "learning_rate": 0.00013158896289248336,
      "loss": 0.1407,
      "step": 729
    },
    {
      "epoch": 1.036195883605394,
      "grad_norm": 0.18164142966270447,
      "learning_rate": 0.00013149381541389154,
      "loss": 0.1244,
      "step": 730
    },
    {
      "epoch": 1.0376153300212918,
      "grad_norm": 0.19698351621627808,
      "learning_rate": 0.00013139866793529973,
      "loss": 0.1299,
      "step": 731
    },
    {
      "epoch": 1.0390347764371894,
      "grad_norm": 0.1799570471048355,
      "learning_rate": 0.0001313035204567079,
      "loss": 0.144,
      "step": 732
    },
    {
      "epoch": 1.0404542228530873,
      "grad_norm": 0.19182667136192322,
      "learning_rate": 0.00013120837297811607,
      "loss": 0.1299,
      "step": 733
    },
    {
      "epoch": 1.0418736692689852,
      "grad_norm": 0.16167569160461426,
      "learning_rate": 0.00013111322549952428,
      "loss": 0.1237,
      "step": 734
    },
    {
      "epoch": 1.0432931156848828,
      "grad_norm": 0.15219692885875702,
      "learning_rate": 0.00013101807802093243,
      "loss": 0.1093,
      "step": 735
    },
    {
      "epoch": 1.0447125621007807,
      "grad_norm": 0.21530640125274658,
      "learning_rate": 0.00013092293054234064,
      "loss": 0.1192,
      "step": 736
    },
    {
      "epoch": 1.0461320085166785,
      "grad_norm": 0.1552867889404297,
      "learning_rate": 0.0001308277830637488,
      "loss": 0.0846,
      "step": 737
    },
    {
      "epoch": 1.0475514549325764,
      "grad_norm": 0.2005973905324936,
      "learning_rate": 0.000130732635585157,
      "loss": 0.1068,
      "step": 738
    },
    {
      "epoch": 1.048970901348474,
      "grad_norm": 0.18921338021755219,
      "learning_rate": 0.00013063748810656517,
      "loss": 0.1038,
      "step": 739
    },
    {
      "epoch": 1.050390347764372,
      "grad_norm": 0.11821583658456802,
      "learning_rate": 0.00013054234062797338,
      "loss": 0.0803,
      "step": 740
    },
    {
      "epoch": 1.0518097941802698,
      "grad_norm": 0.15257775783538818,
      "learning_rate": 0.00013044719314938153,
      "loss": 0.0961,
      "step": 741
    },
    {
      "epoch": 1.0532292405961674,
      "grad_norm": 0.17699576914310455,
      "learning_rate": 0.00013035204567078974,
      "loss": 0.1068,
      "step": 742
    },
    {
      "epoch": 1.0546486870120653,
      "grad_norm": 0.14737801253795624,
      "learning_rate": 0.0001302568981921979,
      "loss": 0.0924,
      "step": 743
    },
    {
      "epoch": 1.0560681334279631,
      "grad_norm": 0.12181035429239273,
      "learning_rate": 0.0001301617507136061,
      "loss": 0.0738,
      "step": 744
    },
    {
      "epoch": 1.057487579843861,
      "grad_norm": 0.13820628821849823,
      "learning_rate": 0.00013006660323501427,
      "loss": 0.0939,
      "step": 745
    },
    {
      "epoch": 1.0589070262597586,
      "grad_norm": 0.15334512293338776,
      "learning_rate": 0.00012997145575642248,
      "loss": 0.0737,
      "step": 746
    },
    {
      "epoch": 1.0603264726756565,
      "grad_norm": 0.11891650408506393,
      "learning_rate": 0.00012987630827783063,
      "loss": 0.0696,
      "step": 747
    },
    {
      "epoch": 1.0617459190915544,
      "grad_norm": 0.12140782922506332,
      "learning_rate": 0.00012978116079923884,
      "loss": 0.0674,
      "step": 748
    },
    {
      "epoch": 1.063165365507452,
      "grad_norm": 0.1472795605659485,
      "learning_rate": 0.000129686013320647,
      "loss": 0.0733,
      "step": 749
    },
    {
      "epoch": 1.0645848119233499,
      "grad_norm": 0.13995087146759033,
      "learning_rate": 0.0001295908658420552,
      "loss": 0.065,
      "step": 750
    },
    {
      "epoch": 1.0660042583392477,
      "grad_norm": 0.11231423914432526,
      "learning_rate": 0.00012949571836346337,
      "loss": 0.0616,
      "step": 751
    },
    {
      "epoch": 1.0674237047551456,
      "grad_norm": 0.15325985848903656,
      "learning_rate": 0.00012940057088487158,
      "loss": 0.0594,
      "step": 752
    },
    {
      "epoch": 1.0688431511710432,
      "grad_norm": 0.13390852510929108,
      "learning_rate": 0.00012930542340627973,
      "loss": 0.058,
      "step": 753
    },
    {
      "epoch": 1.070262597586941,
      "grad_norm": 0.10027240961790085,
      "learning_rate": 0.00012921027592768792,
      "loss": 0.0473,
      "step": 754
    },
    {
      "epoch": 1.071682044002839,
      "grad_norm": 0.4430636465549469,
      "learning_rate": 0.0001291151284490961,
      "loss": 0.2616,
      "step": 755
    },
    {
      "epoch": 1.0731014904187366,
      "grad_norm": 0.5103023052215576,
      "learning_rate": 0.00012901998097050428,
      "loss": 0.3646,
      "step": 756
    },
    {
      "epoch": 1.0745209368346345,
      "grad_norm": 0.5317808389663696,
      "learning_rate": 0.00012892483349191247,
      "loss": 0.2895,
      "step": 757
    },
    {
      "epoch": 1.0759403832505323,
      "grad_norm": 0.3523619472980499,
      "learning_rate": 0.00012882968601332065,
      "loss": 0.2638,
      "step": 758
    },
    {
      "epoch": 1.0773598296664302,
      "grad_norm": 0.32921886444091797,
      "learning_rate": 0.00012873453853472883,
      "loss": 0.2775,
      "step": 759
    },
    {
      "epoch": 1.0787792760823278,
      "grad_norm": 0.3308810591697693,
      "learning_rate": 0.00012863939105613702,
      "loss": 0.2494,
      "step": 760
    },
    {
      "epoch": 1.0801987224982257,
      "grad_norm": 0.26525869965553284,
      "learning_rate": 0.0001285442435775452,
      "loss": 0.225,
      "step": 761
    },
    {
      "epoch": 1.0816181689141235,
      "grad_norm": 0.22003328800201416,
      "learning_rate": 0.00012844909609895338,
      "loss": 0.2023,
      "step": 762
    },
    {
      "epoch": 1.0830376153300212,
      "grad_norm": 0.27551794052124023,
      "learning_rate": 0.00012835394862036157,
      "loss": 0.2594,
      "step": 763
    },
    {
      "epoch": 1.084457061745919,
      "grad_norm": 0.23424625396728516,
      "learning_rate": 0.00012825880114176975,
      "loss": 0.194,
      "step": 764
    },
    {
      "epoch": 1.085876508161817,
      "grad_norm": 0.19375629723072052,
      "learning_rate": 0.00012816365366317793,
      "loss": 0.1736,
      "step": 765
    },
    {
      "epoch": 1.0872959545777148,
      "grad_norm": 0.21587787568569183,
      "learning_rate": 0.00012806850618458612,
      "loss": 0.2051,
      "step": 766
    },
    {
      "epoch": 1.0887154009936124,
      "grad_norm": 0.17300263047218323,
      "learning_rate": 0.0001279733587059943,
      "loss": 0.1539,
      "step": 767
    },
    {
      "epoch": 1.0901348474095103,
      "grad_norm": 0.21971891820430756,
      "learning_rate": 0.00012787821122740248,
      "loss": 0.1885,
      "step": 768
    },
    {
      "epoch": 1.0915542938254081,
      "grad_norm": 0.2105615884065628,
      "learning_rate": 0.00012778306374881067,
      "loss": 0.1787,
      "step": 769
    },
    {
      "epoch": 1.0929737402413058,
      "grad_norm": 0.1858362853527069,
      "learning_rate": 0.00012768791627021885,
      "loss": 0.1558,
      "step": 770
    },
    {
      "epoch": 1.0943931866572036,
      "grad_norm": 0.21708060801029205,
      "learning_rate": 0.00012759276879162703,
      "loss": 0.1839,
      "step": 771
    },
    {
      "epoch": 1.0958126330731015,
      "grad_norm": 0.24080990254878998,
      "learning_rate": 0.00012749762131303522,
      "loss": 0.1655,
      "step": 772
    },
    {
      "epoch": 1.0972320794889994,
      "grad_norm": 0.20318418741226196,
      "learning_rate": 0.0001274024738344434,
      "loss": 0.1605,
      "step": 773
    },
    {
      "epoch": 1.098651525904897,
      "grad_norm": 0.16543614864349365,
      "learning_rate": 0.00012730732635585158,
      "loss": 0.119,
      "step": 774
    },
    {
      "epoch": 1.1000709723207949,
      "grad_norm": 0.1733444780111313,
      "learning_rate": 0.00012721217887725977,
      "loss": 0.1315,
      "step": 775
    },
    {
      "epoch": 1.1014904187366927,
      "grad_norm": 0.19277739524841309,
      "learning_rate": 0.00012711703139866795,
      "loss": 0.1363,
      "step": 776
    },
    {
      "epoch": 1.1029098651525904,
      "grad_norm": 0.15131960809230804,
      "learning_rate": 0.00012702188392007613,
      "loss": 0.101,
      "step": 777
    },
    {
      "epoch": 1.1043293115684882,
      "grad_norm": 0.1636597216129303,
      "learning_rate": 0.00012692673644148432,
      "loss": 0.1102,
      "step": 778
    },
    {
      "epoch": 1.105748757984386,
      "grad_norm": 0.15759044885635376,
      "learning_rate": 0.0001268315889628925,
      "loss": 0.1049,
      "step": 779
    },
    {
      "epoch": 1.107168204400284,
      "grad_norm": 0.18001241981983185,
      "learning_rate": 0.00012673644148430068,
      "loss": 0.1056,
      "step": 780
    },
    {
      "epoch": 1.1085876508161816,
      "grad_norm": 0.20508702099323273,
      "learning_rate": 0.00012664129400570887,
      "loss": 0.1495,
      "step": 781
    },
    {
      "epoch": 1.1100070972320795,
      "grad_norm": 0.1843666434288025,
      "learning_rate": 0.00012654614652711702,
      "loss": 0.1228,
      "step": 782
    },
    {
      "epoch": 1.1114265436479773,
      "grad_norm": 0.21471849083900452,
      "learning_rate": 0.00012645099904852523,
      "loss": 0.1481,
      "step": 783
    },
    {
      "epoch": 1.1128459900638752,
      "grad_norm": 0.20250990986824036,
      "learning_rate": 0.0001263558515699334,
      "loss": 0.1289,
      "step": 784
    },
    {
      "epoch": 1.1142654364797728,
      "grad_norm": 0.15460456907749176,
      "learning_rate": 0.00012626070409134157,
      "loss": 0.0891,
      "step": 785
    },
    {
      "epoch": 1.1156848828956707,
      "grad_norm": 0.16691207885742188,
      "learning_rate": 0.00012616555661274976,
      "loss": 0.1069,
      "step": 786
    },
    {
      "epoch": 1.1171043293115686,
      "grad_norm": 0.1643488109111786,
      "learning_rate": 0.00012607040913415794,
      "loss": 0.1221,
      "step": 787
    },
    {
      "epoch": 1.1185237757274662,
      "grad_norm": 0.17483361065387726,
      "learning_rate": 0.00012597526165556612,
      "loss": 0.1084,
      "step": 788
    },
    {
      "epoch": 1.119943222143364,
      "grad_norm": 0.1809265911579132,
      "learning_rate": 0.0001258801141769743,
      "loss": 0.1008,
      "step": 789
    },
    {
      "epoch": 1.121362668559262,
      "grad_norm": 0.14745040237903595,
      "learning_rate": 0.0001257849666983825,
      "loss": 0.0913,
      "step": 790
    },
    {
      "epoch": 1.1227821149751598,
      "grad_norm": 0.17484422028064728,
      "learning_rate": 0.00012568981921979067,
      "loss": 0.1124,
      "step": 791
    },
    {
      "epoch": 1.1242015613910574,
      "grad_norm": 0.1568085104227066,
      "learning_rate": 0.00012559467174119886,
      "loss": 0.1049,
      "step": 792
    },
    {
      "epoch": 1.1256210078069553,
      "grad_norm": 0.1401190161705017,
      "learning_rate": 0.00012549952426260704,
      "loss": 0.0804,
      "step": 793
    },
    {
      "epoch": 1.1270404542228531,
      "grad_norm": 0.15491287410259247,
      "learning_rate": 0.00012540437678401522,
      "loss": 0.09,
      "step": 794
    },
    {
      "epoch": 1.1284599006387508,
      "grad_norm": 0.12146248668432236,
      "learning_rate": 0.0001253092293054234,
      "loss": 0.0736,
      "step": 795
    },
    {
      "epoch": 1.1298793470546487,
      "grad_norm": 0.13991238176822662,
      "learning_rate": 0.0001252140818268316,
      "loss": 0.092,
      "step": 796
    },
    {
      "epoch": 1.1312987934705465,
      "grad_norm": 0.13818562030792236,
      "learning_rate": 0.00012511893434823977,
      "loss": 0.0625,
      "step": 797
    },
    {
      "epoch": 1.1327182398864444,
      "grad_norm": 0.16054265201091766,
      "learning_rate": 0.00012502378686964796,
      "loss": 0.0788,
      "step": 798
    },
    {
      "epoch": 1.134137686302342,
      "grad_norm": 0.16226358711719513,
      "learning_rate": 0.00012492863939105614,
      "loss": 0.0799,
      "step": 799
    },
    {
      "epoch": 1.1355571327182399,
      "grad_norm": 0.14217671751976013,
      "learning_rate": 0.00012483349191246432,
      "loss": 0.0872,
      "step": 800
    },
    {
      "epoch": 1.1369765791341377,
      "grad_norm": 0.11626420170068741,
      "learning_rate": 0.0001247383444338725,
      "loss": 0.0631,
      "step": 801
    },
    {
      "epoch": 1.1383960255500356,
      "grad_norm": 0.13634441792964935,
      "learning_rate": 0.0001246431969552807,
      "loss": 0.0714,
      "step": 802
    },
    {
      "epoch": 1.1398154719659332,
      "grad_norm": 0.1397586613893509,
      "learning_rate": 0.00012454804947668887,
      "loss": 0.0749,
      "step": 803
    },
    {
      "epoch": 1.141234918381831,
      "grad_norm": 0.1097993329167366,
      "learning_rate": 0.00012445290199809706,
      "loss": 0.0519,
      "step": 804
    },
    {
      "epoch": 1.142654364797729,
      "grad_norm": 0.5238766074180603,
      "learning_rate": 0.00012435775451950524,
      "loss": 0.2286,
      "step": 805
    },
    {
      "epoch": 1.1440738112136266,
      "grad_norm": 4.3247270584106445,
      "learning_rate": 0.00012426260704091342,
      "loss": 0.3534,
      "step": 806
    },
    {
      "epoch": 1.1454932576295245,
      "grad_norm": 0.48436227440834045,
      "learning_rate": 0.0001241674595623216,
      "loss": 0.3187,
      "step": 807
    },
    {
      "epoch": 1.1469127040454223,
      "grad_norm": 1.8418009281158447,
      "learning_rate": 0.0001240723120837298,
      "loss": 0.3589,
      "step": 808
    },
    {
      "epoch": 1.1483321504613202,
      "grad_norm": 1.3584396839141846,
      "learning_rate": 0.00012397716460513797,
      "loss": 0.3722,
      "step": 809
    },
    {
      "epoch": 1.1497515968772178,
      "grad_norm": 3.1571452617645264,
      "learning_rate": 0.00012388201712654616,
      "loss": 0.3251,
      "step": 810
    },
    {
      "epoch": 1.1511710432931157,
      "grad_norm": 2.3034403324127197,
      "learning_rate": 0.00012378686964795434,
      "loss": 0.2542,
      "step": 811
    },
    {
      "epoch": 1.1525904897090136,
      "grad_norm": 2.939605236053467,
      "learning_rate": 0.00012369172216936252,
      "loss": 0.3349,
      "step": 812
    },
    {
      "epoch": 1.1540099361249112,
      "grad_norm": 1.1878662109375,
      "learning_rate": 0.0001235965746907707,
      "loss": 0.3345,
      "step": 813
    },
    {
      "epoch": 1.155429382540809,
      "grad_norm": 2.1705081462860107,
      "learning_rate": 0.0001235014272121789,
      "loss": 0.3751,
      "step": 814
    },
    {
      "epoch": 1.156848828956707,
      "grad_norm": 1.0016138553619385,
      "learning_rate": 0.00012340627973358707,
      "loss": 0.2843,
      "step": 815
    },
    {
      "epoch": 1.1582682753726048,
      "grad_norm": 0.37471213936805725,
      "learning_rate": 0.00012331113225499523,
      "loss": 0.2343,
      "step": 816
    },
    {
      "epoch": 1.1596877217885024,
      "grad_norm": 0.29775741696357727,
      "learning_rate": 0.00012321598477640344,
      "loss": 0.2155,
      "step": 817
    },
    {
      "epoch": 1.1611071682044003,
      "grad_norm": 0.27718648314476013,
      "learning_rate": 0.0001231208372978116,
      "loss": 0.2223,
      "step": 818
    },
    {
      "epoch": 1.1625266146202982,
      "grad_norm": 0.32277005910873413,
      "learning_rate": 0.0001230256898192198,
      "loss": 0.179,
      "step": 819
    },
    {
      "epoch": 1.1639460610361958,
      "grad_norm": 0.28431737422943115,
      "learning_rate": 0.00012293054234062796,
      "loss": 0.2003,
      "step": 820
    },
    {
      "epoch": 1.1653655074520937,
      "grad_norm": 0.302427738904953,
      "learning_rate": 0.00012283539486203617,
      "loss": 0.1774,
      "step": 821
    },
    {
      "epoch": 1.1667849538679915,
      "grad_norm": 0.2704111933708191,
      "learning_rate": 0.00012274024738344433,
      "loss": 0.1794,
      "step": 822
    },
    {
      "epoch": 1.1682044002838894,
      "grad_norm": 0.2798808217048645,
      "learning_rate": 0.00012264509990485254,
      "loss": 0.1905,
      "step": 823
    },
    {
      "epoch": 1.169623846699787,
      "grad_norm": 0.22384904325008392,
      "learning_rate": 0.0001225499524262607,
      "loss": 0.1456,
      "step": 824
    },
    {
      "epoch": 1.171043293115685,
      "grad_norm": 0.2550671398639679,
      "learning_rate": 0.0001224548049476689,
      "loss": 0.157,
      "step": 825
    },
    {
      "epoch": 1.1724627395315828,
      "grad_norm": 0.25126901268959045,
      "learning_rate": 0.00012235965746907706,
      "loss": 0.1754,
      "step": 826
    },
    {
      "epoch": 1.1738821859474804,
      "grad_norm": 0.21364066004753113,
      "learning_rate": 0.00012226450999048527,
      "loss": 0.1414,
      "step": 827
    },
    {
      "epoch": 1.1753016323633783,
      "grad_norm": 0.2681367099285126,
      "learning_rate": 0.00012216936251189343,
      "loss": 0.1462,
      "step": 828
    },
    {
      "epoch": 1.1767210787792761,
      "grad_norm": 0.19708532094955444,
      "learning_rate": 0.00012207421503330164,
      "loss": 0.1355,
      "step": 829
    },
    {
      "epoch": 1.178140525195174,
      "grad_norm": 0.19216667115688324,
      "learning_rate": 0.0001219790675547098,
      "loss": 0.132,
      "step": 830
    },
    {
      "epoch": 1.1795599716110716,
      "grad_norm": 0.19203446805477142,
      "learning_rate": 0.000121883920076118,
      "loss": 0.1141,
      "step": 831
    },
    {
      "epoch": 1.1809794180269695,
      "grad_norm": 0.21067063510417938,
      "learning_rate": 0.00012178877259752616,
      "loss": 0.1384,
      "step": 832
    },
    {
      "epoch": 1.1823988644428673,
      "grad_norm": 0.18489737808704376,
      "learning_rate": 0.00012169362511893436,
      "loss": 0.1209,
      "step": 833
    },
    {
      "epoch": 1.183818310858765,
      "grad_norm": 0.1805708408355713,
      "learning_rate": 0.00012159847764034253,
      "loss": 0.1103,
      "step": 834
    },
    {
      "epoch": 1.1852377572746629,
      "grad_norm": 0.1872464120388031,
      "learning_rate": 0.00012150333016175073,
      "loss": 0.1254,
      "step": 835
    },
    {
      "epoch": 1.1866572036905607,
      "grad_norm": 0.19910162687301636,
      "learning_rate": 0.0001214081826831589,
      "loss": 0.1246,
      "step": 836
    },
    {
      "epoch": 1.1880766501064586,
      "grad_norm": 0.1490432769060135,
      "learning_rate": 0.0001213130352045671,
      "loss": 0.1023,
      "step": 837
    },
    {
      "epoch": 1.1894960965223562,
      "grad_norm": 0.1573842316865921,
      "learning_rate": 0.00012121788772597526,
      "loss": 0.1006,
      "step": 838
    },
    {
      "epoch": 1.190915542938254,
      "grad_norm": 0.17362456023693085,
      "learning_rate": 0.00012112274024738346,
      "loss": 0.112,
      "step": 839
    },
    {
      "epoch": 1.192334989354152,
      "grad_norm": 0.15489695966243744,
      "learning_rate": 0.00012102759276879163,
      "loss": 0.101,
      "step": 840
    },
    {
      "epoch": 1.1937544357700496,
      "grad_norm": 0.198067769408226,
      "learning_rate": 0.00012093244529019983,
      "loss": 0.0953,
      "step": 841
    },
    {
      "epoch": 1.1951738821859474,
      "grad_norm": 0.18136829137802124,
      "learning_rate": 0.000120837297811608,
      "loss": 0.1047,
      "step": 842
    },
    {
      "epoch": 1.1965933286018453,
      "grad_norm": 0.14393557608127594,
      "learning_rate": 0.0001207421503330162,
      "loss": 0.0921,
      "step": 843
    },
    {
      "epoch": 1.1980127750177432,
      "grad_norm": 0.14543643593788147,
      "learning_rate": 0.00012064700285442436,
      "loss": 0.0875,
      "step": 844
    },
    {
      "epoch": 1.1994322214336408,
      "grad_norm": 0.15259906649589539,
      "learning_rate": 0.00012055185537583256,
      "loss": 0.0879,
      "step": 845
    },
    {
      "epoch": 1.2008516678495387,
      "grad_norm": 0.1388278603553772,
      "learning_rate": 0.00012045670789724073,
      "loss": 0.0761,
      "step": 846
    },
    {
      "epoch": 1.2008516678495387,
      "eval_loss": 0.18455936014652252,
      "eval_runtime": 350.5514,
      "eval_samples_per_second": 3.015,
      "eval_steps_per_second": 1.007,
      "step": 846
    },
    {
      "epoch": 1.2022711142654365,
      "grad_norm": 0.1468459814786911,
      "learning_rate": 0.00012036156041864893,
      "loss": 0.0842,
      "step": 847
    },
    {
      "epoch": 1.2036905606813342,
      "grad_norm": 0.14862684905529022,
      "learning_rate": 0.0001202664129400571,
      "loss": 0.0872,
      "step": 848
    },
    {
      "epoch": 1.205110007097232,
      "grad_norm": 0.13126423954963684,
      "learning_rate": 0.00012017126546146528,
      "loss": 0.0787,
      "step": 849
    },
    {
      "epoch": 1.20652945351313,
      "grad_norm": 0.12527677416801453,
      "learning_rate": 0.00012007611798287346,
      "loss": 0.0689,
      "step": 850
    },
    {
      "epoch": 1.2079488999290278,
      "grad_norm": 0.13559743762016296,
      "learning_rate": 0.00011998097050428163,
      "loss": 0.0718,
      "step": 851
    },
    {
      "epoch": 1.2093683463449254,
      "grad_norm": 0.14549559354782104,
      "learning_rate": 0.00011988582302568983,
      "loss": 0.0758,
      "step": 852
    },
    {
      "epoch": 1.2107877927608233,
      "grad_norm": 0.17298828065395355,
      "learning_rate": 0.000119790675547098,
      "loss": 0.0708,
      "step": 853
    },
    {
      "epoch": 1.2122072391767211,
      "grad_norm": 0.09239975363016129,
      "learning_rate": 0.00011969552806850618,
      "loss": 0.0474,
      "step": 854
    },
    {
      "epoch": 1.2136266855926188,
      "grad_norm": 0.3613687753677368,
      "learning_rate": 0.00011960038058991437,
      "loss": 0.2991,
      "step": 855
    },
    {
      "epoch": 1.2150461320085166,
      "grad_norm": 0.41135674715042114,
      "learning_rate": 0.00011950523311132255,
      "loss": 0.3662,
      "step": 856
    },
    {
      "epoch": 1.2164655784244145,
      "grad_norm": 0.414628803730011,
      "learning_rate": 0.00011941008563273073,
      "loss": 0.3604,
      "step": 857
    },
    {
      "epoch": 1.2178850248403124,
      "grad_norm": 0.7705197930335999,
      "learning_rate": 0.00011931493815413892,
      "loss": 0.315,
      "step": 858
    },
    {
      "epoch": 1.21930447125621,
      "grad_norm": 0.32966041564941406,
      "learning_rate": 0.00011921979067554709,
      "loss": 0.285,
      "step": 859
    },
    {
      "epoch": 1.2207239176721079,
      "grad_norm": 0.31775933504104614,
      "learning_rate": 0.00011912464319695528,
      "loss": 0.2339,
      "step": 860
    },
    {
      "epoch": 1.2221433640880057,
      "grad_norm": 0.3154318928718567,
      "learning_rate": 0.00011902949571836345,
      "loss": 0.2703,
      "step": 861
    },
    {
      "epoch": 1.2235628105039034,
      "grad_norm": 0.25369754433631897,
      "learning_rate": 0.00011893434823977165,
      "loss": 0.2151,
      "step": 862
    },
    {
      "epoch": 1.2249822569198012,
      "grad_norm": 0.3355623781681061,
      "learning_rate": 0.00011883920076117982,
      "loss": 0.2431,
      "step": 863
    },
    {
      "epoch": 1.226401703335699,
      "grad_norm": 0.29226014018058777,
      "learning_rate": 0.00011874405328258802,
      "loss": 0.223,
      "step": 864
    },
    {
      "epoch": 1.227821149751597,
      "grad_norm": 0.2907524108886719,
      "learning_rate": 0.00011864890580399619,
      "loss": 0.199,
      "step": 865
    },
    {
      "epoch": 1.2292405961674946,
      "grad_norm": 0.24319379031658173,
      "learning_rate": 0.00011855375832540438,
      "loss": 0.1955,
      "step": 866
    },
    {
      "epoch": 1.2306600425833925,
      "grad_norm": 0.26656320691108704,
      "learning_rate": 0.00011845861084681255,
      "loss": 0.1926,
      "step": 867
    },
    {
      "epoch": 1.2320794889992903,
      "grad_norm": 0.33448919653892517,
      "learning_rate": 0.00011836346336822075,
      "loss": 0.2131,
      "step": 868
    },
    {
      "epoch": 1.233498935415188,
      "grad_norm": 0.24090465903282166,
      "learning_rate": 0.00011826831588962892,
      "loss": 0.1623,
      "step": 869
    },
    {
      "epoch": 1.2349183818310858,
      "grad_norm": 0.21560679376125336,
      "learning_rate": 0.00011817316841103712,
      "loss": 0.1456,
      "step": 870
    },
    {
      "epoch": 1.2363378282469837,
      "grad_norm": 0.21122831106185913,
      "learning_rate": 0.00011807802093244529,
      "loss": 0.1626,
      "step": 871
    },
    {
      "epoch": 1.2377572746628815,
      "grad_norm": 0.20528544485569,
      "learning_rate": 0.00011798287345385348,
      "loss": 0.144,
      "step": 872
    },
    {
      "epoch": 1.2391767210787792,
      "grad_norm": 0.26725128293037415,
      "learning_rate": 0.00011788772597526165,
      "loss": 0.1551,
      "step": 873
    },
    {
      "epoch": 1.240596167494677,
      "grad_norm": 0.23850692808628082,
      "learning_rate": 0.00011779257849666985,
      "loss": 0.1374,
      "step": 874
    },
    {
      "epoch": 1.242015613910575,
      "grad_norm": 0.20298372209072113,
      "learning_rate": 0.00011769743101807802,
      "loss": 0.1562,
      "step": 875
    },
    {
      "epoch": 1.2434350603264726,
      "grad_norm": 0.19317595660686493,
      "learning_rate": 0.00011760228353948622,
      "loss": 0.1305,
      "step": 876
    },
    {
      "epoch": 1.2448545067423704,
      "grad_norm": 0.2484245002269745,
      "learning_rate": 0.00011750713606089439,
      "loss": 0.1654,
      "step": 877
    },
    {
      "epoch": 1.2462739531582683,
      "grad_norm": 0.20022067427635193,
      "learning_rate": 0.00011741198858230259,
      "loss": 0.1336,
      "step": 878
    },
    {
      "epoch": 1.2476933995741661,
      "grad_norm": 0.22455428540706635,
      "learning_rate": 0.00011731684110371075,
      "loss": 0.1362,
      "step": 879
    },
    {
      "epoch": 1.2491128459900638,
      "grad_norm": 0.22041738033294678,
      "learning_rate": 0.00011722169362511894,
      "loss": 0.1561,
      "step": 880
    },
    {
      "epoch": 1.2505322924059616,
      "grad_norm": 0.19736546277999878,
      "learning_rate": 0.00011712654614652712,
      "loss": 0.1307,
      "step": 881
    },
    {
      "epoch": 1.2519517388218595,
      "grad_norm": 0.30246126651763916,
      "learning_rate": 0.0001170313986679353,
      "loss": 0.1394,
      "step": 882
    },
    {
      "epoch": 1.2533711852377571,
      "grad_norm": 0.19718517363071442,
      "learning_rate": 0.00011693625118934349,
      "loss": 0.117,
      "step": 883
    },
    {
      "epoch": 1.254790631653655,
      "grad_norm": 0.15760773420333862,
      "learning_rate": 0.00011684110371075167,
      "loss": 0.1133,
      "step": 884
    },
    {
      "epoch": 1.2562100780695529,
      "grad_norm": 0.16693347692489624,
      "learning_rate": 0.00011674595623215984,
      "loss": 0.1052,
      "step": 885
    },
    {
      "epoch": 1.2576295244854507,
      "grad_norm": 0.1633010059595108,
      "learning_rate": 0.00011665080875356804,
      "loss": 0.0975,
      "step": 886
    },
    {
      "epoch": 1.2590489709013486,
      "grad_norm": 0.19909369945526123,
      "learning_rate": 0.00011655566127497621,
      "loss": 0.0996,
      "step": 887
    },
    {
      "epoch": 1.2604684173172462,
      "grad_norm": 0.15285266935825348,
      "learning_rate": 0.0001164605137963844,
      "loss": 0.1074,
      "step": 888
    },
    {
      "epoch": 1.261887863733144,
      "grad_norm": 0.1329864263534546,
      "learning_rate": 0.00011636536631779257,
      "loss": 0.0821,
      "step": 889
    },
    {
      "epoch": 1.2633073101490417,
      "grad_norm": 0.16709905862808228,
      "learning_rate": 0.00011627021883920077,
      "loss": 0.0971,
      "step": 890
    },
    {
      "epoch": 1.2647267565649396,
      "grad_norm": 0.14971426129341125,
      "learning_rate": 0.00011617507136060894,
      "loss": 0.0882,
      "step": 891
    },
    {
      "epoch": 1.2661462029808375,
      "grad_norm": 0.14370709657669067,
      "learning_rate": 0.00011607992388201714,
      "loss": 0.0927,
      "step": 892
    },
    {
      "epoch": 1.2675656493967353,
      "grad_norm": 0.15775443613529205,
      "learning_rate": 0.00011598477640342531,
      "loss": 0.0914,
      "step": 893
    },
    {
      "epoch": 1.2689850958126332,
      "grad_norm": 0.16949594020843506,
      "learning_rate": 0.0001158896289248335,
      "loss": 0.0863,
      "step": 894
    },
    {
      "epoch": 1.2704045422285308,
      "grad_norm": 0.13893845677375793,
      "learning_rate": 0.00011579448144624168,
      "loss": 0.0671,
      "step": 895
    },
    {
      "epoch": 1.2718239886444287,
      "grad_norm": 0.12404394149780273,
      "learning_rate": 0.00011569933396764987,
      "loss": 0.0703,
      "step": 896
    },
    {
      "epoch": 1.2732434350603263,
      "grad_norm": 0.14517563581466675,
      "learning_rate": 0.00011560418648905804,
      "loss": 0.0767,
      "step": 897
    },
    {
      "epoch": 1.2746628814762242,
      "grad_norm": 0.12821710109710693,
      "learning_rate": 0.00011550903901046624,
      "loss": 0.08,
      "step": 898
    },
    {
      "epoch": 1.276082327892122,
      "grad_norm": 0.11503323912620544,
      "learning_rate": 0.00011541389153187441,
      "loss": 0.0609,
      "step": 899
    },
    {
      "epoch": 1.27750177430802,
      "grad_norm": 0.20002481341362,
      "learning_rate": 0.0001153187440532826,
      "loss": 0.0826,
      "step": 900
    },
    {
      "epoch": 1.2789212207239178,
      "grad_norm": 0.13138751685619354,
      "learning_rate": 0.00011522359657469078,
      "loss": 0.0779,
      "step": 901
    },
    {
      "epoch": 1.2803406671398154,
      "grad_norm": 0.15923921763896942,
      "learning_rate": 0.00011512844909609897,
      "loss": 0.0719,
      "step": 902
    },
    {
      "epoch": 1.2817601135557133,
      "grad_norm": 0.12470070272684097,
      "learning_rate": 0.00011503330161750714,
      "loss": 0.0613,
      "step": 903
    },
    {
      "epoch": 1.2831795599716112,
      "grad_norm": 0.09330444782972336,
      "learning_rate": 0.00011493815413891534,
      "loss": 0.047,
      "step": 904
    },
    {
      "epoch": 1.2845990063875088,
      "grad_norm": 0.41469815373420715,
      "learning_rate": 0.00011484300666032351,
      "loss": 0.3099,
      "step": 905
    },
    {
      "epoch": 1.2860184528034067,
      "grad_norm": 0.5250797867774963,
      "learning_rate": 0.00011474785918173169,
      "loss": 0.4028,
      "step": 906
    },
    {
      "epoch": 1.2874378992193045,
      "grad_norm": 0.43176230788230896,
      "learning_rate": 0.00011465271170313988,
      "loss": 0.3241,
      "step": 907
    },
    {
      "epoch": 1.2888573456352024,
      "grad_norm": 0.36824601888656616,
      "learning_rate": 0.00011455756422454806,
      "loss": 0.2948,
      "step": 908
    },
    {
      "epoch": 1.2902767920511,
      "grad_norm": 0.3636336028575897,
      "learning_rate": 0.00011446241674595624,
      "loss": 0.2767,
      "step": 909
    },
    {
      "epoch": 1.2916962384669979,
      "grad_norm": 0.3332219123840332,
      "learning_rate": 0.00011436726926736443,
      "loss": 0.2558,
      "step": 910
    },
    {
      "epoch": 1.2931156848828957,
      "grad_norm": 0.37777385115623474,
      "learning_rate": 0.0001142721217887726,
      "loss": 0.2921,
      "step": 911
    },
    {
      "epoch": 1.2945351312987934,
      "grad_norm": 0.2618837058544159,
      "learning_rate": 0.00011417697431018079,
      "loss": 0.2338,
      "step": 912
    },
    {
      "epoch": 1.2959545777146912,
      "grad_norm": 0.31428951025009155,
      "learning_rate": 0.00011408182683158896,
      "loss": 0.276,
      "step": 913
    },
    {
      "epoch": 1.297374024130589,
      "grad_norm": 0.3028257191181183,
      "learning_rate": 0.00011398667935299716,
      "loss": 0.221,
      "step": 914
    },
    {
      "epoch": 1.298793470546487,
      "grad_norm": 0.2830982208251953,
      "learning_rate": 0.00011389153187440533,
      "loss": 0.2374,
      "step": 915
    },
    {
      "epoch": 1.3002129169623846,
      "grad_norm": 0.2544098198413849,
      "learning_rate": 0.00011379638439581353,
      "loss": 0.2136,
      "step": 916
    },
    {
      "epoch": 1.3016323633782825,
      "grad_norm": 0.2352762669324875,
      "learning_rate": 0.0001137012369172217,
      "loss": 0.199,
      "step": 917
    },
    {
      "epoch": 1.3030518097941803,
      "grad_norm": 0.27317607402801514,
      "learning_rate": 0.00011360608943862989,
      "loss": 0.225,
      "step": 918
    },
    {
      "epoch": 1.304471256210078,
      "grad_norm": 0.22307567298412323,
      "learning_rate": 0.00011351094196003806,
      "loss": 0.2079,
      "step": 919
    },
    {
      "epoch": 1.3058907026259758,
      "grad_norm": 0.20242930948734283,
      "learning_rate": 0.00011341579448144626,
      "loss": 0.1508,
      "step": 920
    },
    {
      "epoch": 1.3073101490418737,
      "grad_norm": 0.24381127953529358,
      "learning_rate": 0.00011332064700285443,
      "loss": 0.1564,
      "step": 921
    },
    {
      "epoch": 1.3087295954577716,
      "grad_norm": 0.2172638177871704,
      "learning_rate": 0.0001132254995242626,
      "loss": 0.1559,
      "step": 922
    },
    {
      "epoch": 1.3101490418736692,
      "grad_norm": 0.24027672410011292,
      "learning_rate": 0.0001131303520456708,
      "loss": 0.1673,
      "step": 923
    },
    {
      "epoch": 1.311568488289567,
      "grad_norm": 0.22242408990859985,
      "learning_rate": 0.00011303520456707897,
      "loss": 0.1452,
      "step": 924
    },
    {
      "epoch": 1.312987934705465,
      "grad_norm": 0.18609297275543213,
      "learning_rate": 0.00011294005708848716,
      "loss": 0.1344,
      "step": 925
    },
    {
      "epoch": 1.3144073811213626,
      "grad_norm": 0.20028699934482574,
      "learning_rate": 0.00011284490960989533,
      "loss": 0.1419,
      "step": 926
    },
    {
      "epoch": 1.3158268275372604,
      "grad_norm": 0.2508096992969513,
      "learning_rate": 0.00011274976213130353,
      "loss": 0.1573,
      "step": 927
    },
    {
      "epoch": 1.3172462739531583,
      "grad_norm": 0.20852036774158478,
      "learning_rate": 0.0001126546146527117,
      "loss": 0.1254,
      "step": 928
    },
    {
      "epoch": 1.3186657203690562,
      "grad_norm": 0.18415208160877228,
      "learning_rate": 0.0001125594671741199,
      "loss": 0.1218,
      "step": 929
    },
    {
      "epoch": 1.3200851667849538,
      "grad_norm": 0.2075454443693161,
      "learning_rate": 0.00011246431969552807,
      "loss": 0.1186,
      "step": 930
    },
    {
      "epoch": 1.3215046132008517,
      "grad_norm": 0.1952759474515915,
      "learning_rate": 0.00011236917221693626,
      "loss": 0.1249,
      "step": 931
    },
    {
      "epoch": 1.3229240596167495,
      "grad_norm": 0.18214453756809235,
      "learning_rate": 0.00011227402473834443,
      "loss": 0.1086,
      "step": 932
    },
    {
      "epoch": 1.3243435060326472,
      "grad_norm": 0.3730047643184662,
      "learning_rate": 0.00011217887725975263,
      "loss": 0.161,
      "step": 933
    },
    {
      "epoch": 1.325762952448545,
      "grad_norm": 0.18995772302150726,
      "learning_rate": 0.0001120837297811608,
      "loss": 0.1267,
      "step": 934
    },
    {
      "epoch": 1.327182398864443,
      "grad_norm": 0.16369737684726715,
      "learning_rate": 0.000111988582302569,
      "loss": 0.103,
      "step": 935
    },
    {
      "epoch": 1.3286018452803408,
      "grad_norm": 0.1697535663843155,
      "learning_rate": 0.00011189343482397717,
      "loss": 0.1065,
      "step": 936
    },
    {
      "epoch": 1.3300212916962384,
      "grad_norm": 0.17384809255599976,
      "learning_rate": 0.00011179828734538535,
      "loss": 0.0987,
      "step": 937
    },
    {
      "epoch": 1.3314407381121363,
      "grad_norm": 0.17038893699645996,
      "learning_rate": 0.00011170313986679353,
      "loss": 0.1065,
      "step": 938
    },
    {
      "epoch": 1.3328601845280341,
      "grad_norm": 0.16880187392234802,
      "learning_rate": 0.00011160799238820172,
      "loss": 0.0989,
      "step": 939
    },
    {
      "epoch": 1.3342796309439318,
      "grad_norm": 0.1523292064666748,
      "learning_rate": 0.0001115128449096099,
      "loss": 0.093,
      "step": 940
    },
    {
      "epoch": 1.3356990773598296,
      "grad_norm": 0.16526508331298828,
      "learning_rate": 0.00011141769743101808,
      "loss": 0.0995,
      "step": 941
    },
    {
      "epoch": 1.3371185237757275,
      "grad_norm": 0.14615917205810547,
      "learning_rate": 0.00011132254995242625,
      "loss": 0.084,
      "step": 942
    },
    {
      "epoch": 1.3385379701916253,
      "grad_norm": 0.16288377344608307,
      "learning_rate": 0.00011122740247383445,
      "loss": 0.1018,
      "step": 943
    },
    {
      "epoch": 1.339957416607523,
      "grad_norm": 0.11590070277452469,
      "learning_rate": 0.00011113225499524262,
      "loss": 0.0675,
      "step": 944
    },
    {
      "epoch": 1.3413768630234209,
      "grad_norm": 0.14819401502609253,
      "learning_rate": 0.00011103710751665082,
      "loss": 0.0888,
      "step": 945
    },
    {
      "epoch": 1.3427963094393187,
      "grad_norm": 0.15838517248630524,
      "learning_rate": 0.00011094196003805899,
      "loss": 0.0911,
      "step": 946
    },
    {
      "epoch": 1.3442157558552164,
      "grad_norm": 0.12764696776866913,
      "learning_rate": 0.00011084681255946718,
      "loss": 0.0687,
      "step": 947
    },
    {
      "epoch": 1.3456352022711142,
      "grad_norm": 0.14489686489105225,
      "learning_rate": 0.00011075166508087535,
      "loss": 0.0789,
      "step": 948
    },
    {
      "epoch": 1.347054648687012,
      "grad_norm": 0.13333258032798767,
      "learning_rate": 0.00011065651760228355,
      "loss": 0.0777,
      "step": 949
    },
    {
      "epoch": 1.34847409510291,
      "grad_norm": 0.11465080082416534,
      "learning_rate": 0.00011056137012369172,
      "loss": 0.0659,
      "step": 950
    },
    {
      "epoch": 1.3498935415188076,
      "grad_norm": 0.16852301359176636,
      "learning_rate": 0.00011046622264509992,
      "loss": 0.073,
      "step": 951
    },
    {
      "epoch": 1.3513129879347054,
      "grad_norm": 0.1426130086183548,
      "learning_rate": 0.00011037107516650809,
      "loss": 0.0715,
      "step": 952
    },
    {
      "epoch": 1.3527324343506033,
      "grad_norm": 0.1167796328663826,
      "learning_rate": 0.00011027592768791628,
      "loss": 0.0661,
      "step": 953
    },
    {
      "epoch": 1.354151880766501,
      "grad_norm": 0.09697416424751282,
      "learning_rate": 0.00011018078020932445,
      "loss": 0.0565,
      "step": 954
    },
    {
      "epoch": 1.3555713271823988,
      "grad_norm": 0.3888132870197296,
      "learning_rate": 0.00011008563273073265,
      "loss": 0.2377,
      "step": 955
    },
    {
      "epoch": 1.3569907735982967,
      "grad_norm": 0.5223394632339478,
      "learning_rate": 0.00010999048525214082,
      "loss": 0.3662,
      "step": 956
    },
    {
      "epoch": 1.3584102200141945,
      "grad_norm": 0.4344301223754883,
      "learning_rate": 0.00010989533777354902,
      "loss": 0.3116,
      "step": 957
    },
    {
      "epoch": 1.3598296664300924,
      "grad_norm": 0.4458158612251282,
      "learning_rate": 0.00010980019029495719,
      "loss": 0.2812,
      "step": 958
    },
    {
      "epoch": 1.36124911284599,
      "grad_norm": 0.43694454431533813,
      "learning_rate": 0.00010970504281636538,
      "loss": 0.2976,
      "step": 959
    },
    {
      "epoch": 1.362668559261888,
      "grad_norm": 0.36991703510284424,
      "learning_rate": 0.00010960989533777355,
      "loss": 0.3088,
      "step": 960
    },
    {
      "epoch": 1.3640880056777855,
      "grad_norm": 0.28801313042640686,
      "learning_rate": 0.00010951474785918175,
      "loss": 0.2514,
      "step": 961
    },
    {
      "epoch": 1.3655074520936834,
      "grad_norm": 0.3378835916519165,
      "learning_rate": 0.00010941960038058992,
      "loss": 0.277,
      "step": 962
    },
    {
      "epoch": 1.3669268985095813,
      "grad_norm": 0.3637514114379883,
      "learning_rate": 0.0001093244529019981,
      "loss": 0.2407,
      "step": 963
    },
    {
      "epoch": 1.3683463449254791,
      "grad_norm": 0.29766184091567993,
      "learning_rate": 0.00010922930542340629,
      "loss": 0.2391,
      "step": 964
    },
    {
      "epoch": 1.369765791341377,
      "grad_norm": 0.2889477610588074,
      "learning_rate": 0.00010913415794481447,
      "loss": 0.2171,
      "step": 965
    },
    {
      "epoch": 1.3711852377572746,
      "grad_norm": 0.3491814136505127,
      "learning_rate": 0.00010903901046622264,
      "loss": 0.2042,
      "step": 966
    },
    {
      "epoch": 1.3726046841731725,
      "grad_norm": 0.26398664712905884,
      "learning_rate": 0.00010894386298763084,
      "loss": 0.1969,
      "step": 967
    },
    {
      "epoch": 1.3740241305890701,
      "grad_norm": 0.2262667566537857,
      "learning_rate": 0.000108848715509039,
      "loss": 0.1871,
      "step": 968
    },
    {
      "epoch": 1.375443577004968,
      "grad_norm": 0.26554760336875916,
      "learning_rate": 0.0001087535680304472,
      "loss": 0.199,
      "step": 969
    },
    {
      "epoch": 1.3768630234208659,
      "grad_norm": 0.28888365626335144,
      "learning_rate": 0.00010865842055185537,
      "loss": 0.1879,
      "step": 970
    },
    {
      "epoch": 1.3782824698367637,
      "grad_norm": 0.25305262207984924,
      "learning_rate": 0.00010856327307326357,
      "loss": 0.1964,
      "step": 971
    },
    {
      "epoch": 1.3797019162526616,
      "grad_norm": 0.21910499036312103,
      "learning_rate": 0.00010846812559467174,
      "loss": 0.1632,
      "step": 972
    },
    {
      "epoch": 1.3811213626685592,
      "grad_norm": 0.27938950061798096,
      "learning_rate": 0.00010837297811607994,
      "loss": 0.1908,
      "step": 973
    },
    {
      "epoch": 1.382540809084457,
      "grad_norm": 0.2013658583164215,
      "learning_rate": 0.0001082778306374881,
      "loss": 0.1546,
      "step": 974
    },
    {
      "epoch": 1.3839602555003547,
      "grad_norm": 0.2206653654575348,
      "learning_rate": 0.0001081826831588963,
      "loss": 0.1439,
      "step": 975
    },
    {
      "epoch": 1.3853797019162526,
      "grad_norm": 0.20507606863975525,
      "learning_rate": 0.00010808753568030447,
      "loss": 0.1412,
      "step": 976
    },
    {
      "epoch": 1.3867991483321505,
      "grad_norm": 0.18784716725349426,
      "learning_rate": 0.00010799238820171267,
      "loss": 0.1515,
      "step": 977
    },
    {
      "epoch": 1.3882185947480483,
      "grad_norm": 0.22592055797576904,
      "learning_rate": 0.00010789724072312084,
      "loss": 0.1531,
      "step": 978
    },
    {
      "epoch": 1.3896380411639462,
      "grad_norm": 0.2258600890636444,
      "learning_rate": 0.00010780209324452904,
      "loss": 0.1415,
      "step": 979
    },
    {
      "epoch": 1.3910574875798438,
      "grad_norm": 0.17608104646205902,
      "learning_rate": 0.0001077069457659372,
      "loss": 0.1362,
      "step": 980
    },
    {
      "epoch": 1.3924769339957417,
      "grad_norm": 0.19677606225013733,
      "learning_rate": 0.0001076117982873454,
      "loss": 0.1097,
      "step": 981
    },
    {
      "epoch": 1.3938963804116393,
      "grad_norm": 0.19371064007282257,
      "learning_rate": 0.00010751665080875357,
      "loss": 0.1152,
      "step": 982
    },
    {
      "epoch": 1.3953158268275372,
      "grad_norm": 0.2072717845439911,
      "learning_rate": 0.00010742150333016177,
      "loss": 0.151,
      "step": 983
    },
    {
      "epoch": 1.396735273243435,
      "grad_norm": 0.16814540326595306,
      "learning_rate": 0.00010732635585156994,
      "loss": 0.1152,
      "step": 984
    },
    {
      "epoch": 1.398154719659333,
      "grad_norm": 0.1665930449962616,
      "learning_rate": 0.00010723120837297814,
      "loss": 0.102,
      "step": 985
    },
    {
      "epoch": 1.3995741660752308,
      "grad_norm": 0.18988312780857086,
      "learning_rate": 0.00010713606089438631,
      "loss": 0.1158,
      "step": 986
    },
    {
      "epoch": 1.4009936124911284,
      "grad_norm": 0.17646914720535278,
      "learning_rate": 0.0001070409134157945,
      "loss": 0.1093,
      "step": 987
    },
    {
      "epoch": 1.4024130589070263,
      "grad_norm": 0.19804489612579346,
      "learning_rate": 0.00010694576593720267,
      "loss": 0.1181,
      "step": 988
    },
    {
      "epoch": 1.4038325053229241,
      "grad_norm": 0.20103871822357178,
      "learning_rate": 0.00010685061845861086,
      "loss": 0.1283,
      "step": 989
    },
    {
      "epoch": 1.4052519517388218,
      "grad_norm": 0.15522681176662445,
      "learning_rate": 0.00010675547098001904,
      "loss": 0.0956,
      "step": 990
    },
    {
      "epoch": 1.4066713981547196,
      "grad_norm": 0.17475688457489014,
      "learning_rate": 0.00010666032350142721,
      "loss": 0.0893,
      "step": 991
    },
    {
      "epoch": 1.4080908445706175,
      "grad_norm": 0.17609241604804993,
      "learning_rate": 0.0001065651760228354,
      "loss": 0.1003,
      "step": 992
    },
    {
      "epoch": 1.4095102909865154,
      "grad_norm": 0.14097237586975098,
      "learning_rate": 0.00010647002854424358,
      "loss": 0.0727,
      "step": 993
    },
    {
      "epoch": 1.410929737402413,
      "grad_norm": 0.16441726684570312,
      "learning_rate": 0.00010637488106565176,
      "loss": 0.0683,
      "step": 994
    },
    {
      "epoch": 1.4123491838183109,
      "grad_norm": 0.1344594657421112,
      "learning_rate": 0.00010627973358705994,
      "loss": 0.077,
      "step": 995
    },
    {
      "epoch": 1.4137686302342087,
      "grad_norm": 0.12010911107063293,
      "learning_rate": 0.00010618458610846813,
      "loss": 0.0771,
      "step": 996
    },
    {
      "epoch": 1.4151880766501064,
      "grad_norm": 0.12441962212324142,
      "learning_rate": 0.0001060894386298763,
      "loss": 0.0749,
      "step": 997
    },
    {
      "epoch": 1.4166075230660042,
      "grad_norm": 0.13136529922485352,
      "learning_rate": 0.0001059942911512845,
      "loss": 0.0762,
      "step": 998
    },
    {
      "epoch": 1.418026969481902,
      "grad_norm": 0.1276669204235077,
      "learning_rate": 0.00010589914367269266,
      "loss": 0.0713,
      "step": 999
    },
    {
      "epoch": 1.4194464158978,
      "grad_norm": 0.1353890597820282,
      "learning_rate": 0.00010580399619410086,
      "loss": 0.0798,
      "step": 1000
    },
    {
      "epoch": 1.4208658623136976,
      "grad_norm": 0.13912644982337952,
      "learning_rate": 0.00010570884871550903,
      "loss": 0.0829,
      "step": 1001
    },
    {
      "epoch": 1.4222853087295955,
      "grad_norm": 0.13375432789325714,
      "learning_rate": 0.00010561370123691723,
      "loss": 0.0604,
      "step": 1002
    },
    {
      "epoch": 1.4237047551454933,
      "grad_norm": 0.09155597537755966,
      "learning_rate": 0.0001055185537583254,
      "loss": 0.0581,
      "step": 1003
    },
    {
      "epoch": 1.425124201561391,
      "grad_norm": 0.10369319468736649,
      "learning_rate": 0.0001054234062797336,
      "loss": 0.0658,
      "step": 1004
    },
    {
      "epoch": 1.4265436479772888,
      "grad_norm": 0.3376222550868988,
      "learning_rate": 0.00010532825880114176,
      "loss": 0.2506,
      "step": 1005
    },
    {
      "epoch": 1.4279630943931867,
      "grad_norm": 0.4509850740432739,
      "learning_rate": 0.00010523311132254996,
      "loss": 0.3579,
      "step": 1006
    },
    {
      "epoch": 1.4293825408090846,
      "grad_norm": 0.40659192204475403,
      "learning_rate": 0.00010513796384395813,
      "loss": 0.31,
      "step": 1007
    },
    {
      "epoch": 1.4308019872249822,
      "grad_norm": 0.3683106303215027,
      "learning_rate": 0.00010504281636536633,
      "loss": 0.319,
      "step": 1008
    },
    {
      "epoch": 1.43222143364088,
      "grad_norm": 0.36644962430000305,
      "learning_rate": 0.0001049476688867745,
      "loss": 0.2997,
      "step": 1009
    },
    {
      "epoch": 1.433640880056778,
      "grad_norm": 0.3919818103313446,
      "learning_rate": 0.0001048525214081827,
      "loss": 0.2534,
      "step": 1010
    },
    {
      "epoch": 1.4350603264726756,
      "grad_norm": 0.3645288050174713,
      "learning_rate": 0.00010475737392959086,
      "loss": 0.2621,
      "step": 1011
    },
    {
      "epoch": 1.4364797728885734,
      "grad_norm": 0.3770142197608948,
      "learning_rate": 0.00010466222645099906,
      "loss": 0.285,
      "step": 1012
    },
    {
      "epoch": 1.4378992193044713,
      "grad_norm": 0.3619225025177002,
      "learning_rate": 0.00010456707897240723,
      "loss": 0.2305,
      "step": 1013
    },
    {
      "epoch": 1.4393186657203692,
      "grad_norm": 0.34071770310401917,
      "learning_rate": 0.00010447193149381543,
      "loss": 0.2578,
      "step": 1014
    },
    {
      "epoch": 1.4407381121362668,
      "grad_norm": 0.2711794078350067,
      "learning_rate": 0.0001043767840152236,
      "loss": 0.2085,
      "step": 1015
    },
    {
      "epoch": 1.4421575585521647,
      "grad_norm": 0.2684991955757141,
      "learning_rate": 0.0001042816365366318,
      "loss": 0.2103,
      "step": 1016
    },
    {
      "epoch": 1.4435770049680625,
      "grad_norm": 0.29930415749549866,
      "learning_rate": 0.00010418648905803996,
      "loss": 0.2201,
      "step": 1017
    },
    {
      "epoch": 1.4449964513839602,
      "grad_norm": 0.2571880519390106,
      "learning_rate": 0.00010409134157944815,
      "loss": 0.1968,
      "step": 1018
    },
    {
      "epoch": 1.446415897799858,
      "grad_norm": 0.25899988412857056,
      "learning_rate": 0.00010399619410085633,
      "loss": 0.1941,
      "step": 1019
    },
    {
      "epoch": 1.4478353442157559,
      "grad_norm": 0.25768035650253296,
      "learning_rate": 0.00010390104662226451,
      "loss": 0.1974,
      "step": 1020
    },
    {
      "epoch": 1.4492547906316537,
      "grad_norm": 0.22968943417072296,
      "learning_rate": 0.0001038058991436727,
      "loss": 0.1749,
      "step": 1021
    },
    {
      "epoch": 1.4506742370475514,
      "grad_norm": 0.2698192000389099,
      "learning_rate": 0.00010371075166508088,
      "loss": 0.1845,
      "step": 1022
    },
    {
      "epoch": 1.4520936834634492,
      "grad_norm": 0.23746562004089355,
      "learning_rate": 0.00010361560418648905,
      "loss": 0.1689,
      "step": 1023
    },
    {
      "epoch": 1.453513129879347,
      "grad_norm": 0.21293583512306213,
      "learning_rate": 0.00010352045670789725,
      "loss": 0.1559,
      "step": 1024
    },
    {
      "epoch": 1.4549325762952448,
      "grad_norm": 0.23344440758228302,
      "learning_rate": 0.00010342530922930542,
      "loss": 0.1725,
      "step": 1025
    },
    {
      "epoch": 1.4563520227111426,
      "grad_norm": 0.24162785708904266,
      "learning_rate": 0.00010333016175071361,
      "loss": 0.1508,
      "step": 1026
    },
    {
      "epoch": 1.4577714691270405,
      "grad_norm": 0.22754700481891632,
      "learning_rate": 0.00010323501427212178,
      "loss": 0.1437,
      "step": 1027
    },
    {
      "epoch": 1.4591909155429383,
      "grad_norm": 0.1913580745458603,
      "learning_rate": 0.00010313986679352998,
      "loss": 0.139,
      "step": 1028
    },
    {
      "epoch": 1.460610361958836,
      "grad_norm": 0.23378515243530273,
      "learning_rate": 0.00010304471931493815,
      "loss": 0.1506,
      "step": 1029
    },
    {
      "epoch": 1.4620298083747338,
      "grad_norm": 0.22524982690811157,
      "learning_rate": 0.00010294957183634635,
      "loss": 0.1583,
      "step": 1030
    },
    {
      "epoch": 1.4634492547906317,
      "grad_norm": 0.2045893371105194,
      "learning_rate": 0.00010285442435775452,
      "loss": 0.1202,
      "step": 1031
    },
    {
      "epoch": 1.4648687012065293,
      "grad_norm": 0.22323796153068542,
      "learning_rate": 0.00010275927687916271,
      "loss": 0.1408,
      "step": 1032
    },
    {
      "epoch": 1.4662881476224272,
      "grad_norm": 0.1934976726770401,
      "learning_rate": 0.00010266412940057088,
      "loss": 0.1299,
      "step": 1033
    },
    {
      "epoch": 1.467707594038325,
      "grad_norm": 0.19308976829051971,
      "learning_rate": 0.00010256898192197908,
      "loss": 0.1217,
      "step": 1034
    },
    {
      "epoch": 1.469127040454223,
      "grad_norm": 0.25783419609069824,
      "learning_rate": 0.00010247383444338725,
      "loss": 0.1286,
      "step": 1035
    },
    {
      "epoch": 1.4705464868701206,
      "grad_norm": 0.17706920206546783,
      "learning_rate": 0.00010237868696479545,
      "loss": 0.1067,
      "step": 1036
    },
    {
      "epoch": 1.4719659332860184,
      "grad_norm": 0.1756499707698822,
      "learning_rate": 0.00010228353948620362,
      "loss": 0.1211,
      "step": 1037
    },
    {
      "epoch": 1.4733853797019163,
      "grad_norm": 0.18017417192459106,
      "learning_rate": 0.00010218839200761181,
      "loss": 0.1259,
      "step": 1038
    },
    {
      "epoch": 1.474804826117814,
      "grad_norm": 0.15759436786174774,
      "learning_rate": 0.00010209324452901998,
      "loss": 0.0964,
      "step": 1039
    },
    {
      "epoch": 1.4762242725337118,
      "grad_norm": 0.16425994038581848,
      "learning_rate": 0.00010199809705042818,
      "loss": 0.1019,
      "step": 1040
    },
    {
      "epoch": 1.4776437189496097,
      "grad_norm": 0.19559040665626526,
      "learning_rate": 0.00010190294957183635,
      "loss": 0.1081,
      "step": 1041
    },
    {
      "epoch": 1.4790631653655075,
      "grad_norm": 0.15828199684619904,
      "learning_rate": 0.00010180780209324455,
      "loss": 0.0905,
      "step": 1042
    },
    {
      "epoch": 1.4804826117814054,
      "grad_norm": 0.12704646587371826,
      "learning_rate": 0.00010171265461465272,
      "loss": 0.0718,
      "step": 1043
    },
    {
      "epoch": 1.481902058197303,
      "grad_norm": 0.18000349402427673,
      "learning_rate": 0.0001016175071360609,
      "loss": 0.0908,
      "step": 1044
    },
    {
      "epoch": 1.483321504613201,
      "grad_norm": 0.16595911979675293,
      "learning_rate": 0.00010152235965746908,
      "loss": 0.0771,
      "step": 1045
    },
    {
      "epoch": 1.4847409510290985,
      "grad_norm": 0.11309801787137985,
      "learning_rate": 0.00010142721217887727,
      "loss": 0.0667,
      "step": 1046
    },
    {
      "epoch": 1.4861603974449964,
      "grad_norm": 0.1495676040649414,
      "learning_rate": 0.00010133206470028545,
      "loss": 0.0778,
      "step": 1047
    },
    {
      "epoch": 1.4875798438608943,
      "grad_norm": 0.1155170425772667,
      "learning_rate": 0.00010123691722169363,
      "loss": 0.0674,
      "step": 1048
    },
    {
      "epoch": 1.4889992902767921,
      "grad_norm": 0.12232993543148041,
      "learning_rate": 0.0001011417697431018,
      "loss": 0.0638,
      "step": 1049
    },
    {
      "epoch": 1.49041873669269,
      "grad_norm": 0.13771972060203552,
      "learning_rate": 0.00010104662226451,
      "loss": 0.0699,
      "step": 1050
    },
    {
      "epoch": 1.4918381831085876,
      "grad_norm": 0.1295737624168396,
      "learning_rate": 0.00010095147478591817,
      "loss": 0.0815,
      "step": 1051
    },
    {
      "epoch": 1.4932576295244855,
      "grad_norm": 0.1449989527463913,
      "learning_rate": 0.00010085632730732637,
      "loss": 0.0664,
      "step": 1052
    },
    {
      "epoch": 1.4946770759403831,
      "grad_norm": 0.11564434319734573,
      "learning_rate": 0.00010076117982873454,
      "loss": 0.0628,
      "step": 1053
    },
    {
      "epoch": 1.496096522356281,
      "grad_norm": 0.12688419222831726,
      "learning_rate": 0.00010066603235014273,
      "loss": 0.0649,
      "step": 1054
    },
    {
      "epoch": 1.4975159687721789,
      "grad_norm": 0.35974642634391785,
      "learning_rate": 0.0001005708848715509,
      "loss": 0.2482,
      "step": 1055
    },
    {
      "epoch": 1.4989354151880767,
      "grad_norm": 0.5284035205841064,
      "learning_rate": 0.0001004757373929591,
      "loss": 0.4025,
      "step": 1056
    },
    {
      "epoch": 1.5003548616039746,
      "grad_norm": 0.47878873348236084,
      "learning_rate": 0.00010038058991436727,
      "loss": 0.3925,
      "step": 1057
    },
    {
      "epoch": 1.5017743080198722,
      "grad_norm": 0.42729562520980835,
      "learning_rate": 0.00010028544243577547,
      "loss": 0.3297,
      "step": 1058
    },
    {
      "epoch": 1.50319375443577,
      "grad_norm": 0.37489771842956543,
      "learning_rate": 0.00010019029495718364,
      "loss": 0.3079,
      "step": 1059
    },
    {
      "epoch": 1.5046132008516677,
      "grad_norm": 0.393646776676178,
      "learning_rate": 0.00010009514747859183,
      "loss": 0.2765,
      "step": 1060
    },
    {
      "epoch": 1.5060326472675656,
      "grad_norm": 0.3352445363998413,
      "learning_rate": 0.0001,
      "loss": 0.2416,
      "step": 1061
    },
    {
      "epoch": 1.5074520936834634,
      "grad_norm": 0.29460179805755615,
      "learning_rate": 9.990485252140819e-05,
      "loss": 0.2044,
      "step": 1062
    },
    {
      "epoch": 1.5088715400993613,
      "grad_norm": 0.26033997535705566,
      "learning_rate": 9.980970504281637e-05,
      "loss": 0.2455,
      "step": 1063
    },
    {
      "epoch": 1.5102909865152592,
      "grad_norm": 0.27495765686035156,
      "learning_rate": 9.971455756422455e-05,
      "loss": 0.2085,
      "step": 1064
    },
    {
      "epoch": 1.5117104329311568,
      "grad_norm": 0.2626189589500427,
      "learning_rate": 9.961941008563274e-05,
      "loss": 0.1922,
      "step": 1065
    },
    {
      "epoch": 1.5131298793470547,
      "grad_norm": 0.2729925811290741,
      "learning_rate": 9.952426260704092e-05,
      "loss": 0.1938,
      "step": 1066
    },
    {
      "epoch": 1.5145493257629523,
      "grad_norm": 0.2547912299633026,
      "learning_rate": 9.94291151284491e-05,
      "loss": 0.1955,
      "step": 1067
    },
    {
      "epoch": 1.5159687721788502,
      "grad_norm": 0.24400480091571808,
      "learning_rate": 9.933396764985729e-05,
      "loss": 0.1699,
      "step": 1068
    },
    {
      "epoch": 1.517388218594748,
      "grad_norm": 0.2243364304304123,
      "learning_rate": 9.923882017126547e-05,
      "loss": 0.1407,
      "step": 1069
    },
    {
      "epoch": 1.518807665010646,
      "grad_norm": 0.24018153548240662,
      "learning_rate": 9.914367269267366e-05,
      "loss": 0.1781,
      "step": 1070
    },
    {
      "epoch": 1.5202271114265438,
      "grad_norm": 0.24494323134422302,
      "learning_rate": 9.904852521408184e-05,
      "loss": 0.165,
      "step": 1071
    },
    {
      "epoch": 1.5216465578424414,
      "grad_norm": 0.22337570786476135,
      "learning_rate": 9.895337773549002e-05,
      "loss": 0.1531,
      "step": 1072
    },
    {
      "epoch": 1.5230660042583393,
      "grad_norm": 0.28711211681365967,
      "learning_rate": 9.88582302568982e-05,
      "loss": 0.1314,
      "step": 1073
    },
    {
      "epoch": 1.524485450674237,
      "grad_norm": 0.1958283931016922,
      "learning_rate": 9.876308277830637e-05,
      "loss": 0.1528,
      "step": 1074
    },
    {
      "epoch": 1.5259048970901348,
      "grad_norm": 0.21554893255233765,
      "learning_rate": 9.866793529971456e-05,
      "loss": 0.146,
      "step": 1075
    },
    {
      "epoch": 1.5273243435060326,
      "grad_norm": 0.1957639753818512,
      "learning_rate": 9.857278782112274e-05,
      "loss": 0.1265,
      "step": 1076
    },
    {
      "epoch": 1.5287437899219305,
      "grad_norm": 0.18297803401947021,
      "learning_rate": 9.847764034253093e-05,
      "loss": 0.1125,
      "step": 1077
    },
    {
      "epoch": 1.5301632363378284,
      "grad_norm": 0.21720577776432037,
      "learning_rate": 9.838249286393911e-05,
      "loss": 0.1159,
      "step": 1078
    },
    {
      "epoch": 1.531582682753726,
      "grad_norm": 0.1975031942129135,
      "learning_rate": 9.828734538534729e-05,
      "loss": 0.1214,
      "step": 1079
    },
    {
      "epoch": 1.5330021291696239,
      "grad_norm": 0.17435821890830994,
      "learning_rate": 9.819219790675548e-05,
      "loss": 0.1081,
      "step": 1080
    },
    {
      "epoch": 1.5344215755855215,
      "grad_norm": 0.17254918813705444,
      "learning_rate": 9.809705042816366e-05,
      "loss": 0.1243,
      "step": 1081
    },
    {
      "epoch": 1.5358410220014194,
      "grad_norm": 0.1871403455734253,
      "learning_rate": 9.800190294957184e-05,
      "loss": 0.1001,
      "step": 1082
    },
    {
      "epoch": 1.5372604684173172,
      "grad_norm": 0.18306584656238556,
      "learning_rate": 9.790675547098003e-05,
      "loss": 0.1255,
      "step": 1083
    },
    {
      "epoch": 1.538679914833215,
      "grad_norm": 0.1560664176940918,
      "learning_rate": 9.781160799238821e-05,
      "loss": 0.0946,
      "step": 1084
    },
    {
      "epoch": 1.540099361249113,
      "grad_norm": 0.19846776127815247,
      "learning_rate": 9.771646051379639e-05,
      "loss": 0.1042,
      "step": 1085
    },
    {
      "epoch": 1.5415188076650106,
      "grad_norm": 0.18451741337776184,
      "learning_rate": 9.762131303520458e-05,
      "loss": 0.1097,
      "step": 1086
    },
    {
      "epoch": 1.5429382540809085,
      "grad_norm": 0.1477707177400589,
      "learning_rate": 9.752616555661276e-05,
      "loss": 0.0905,
      "step": 1087
    },
    {
      "epoch": 1.544357700496806,
      "grad_norm": 0.1747155487537384,
      "learning_rate": 9.743101807802094e-05,
      "loss": 0.1156,
      "step": 1088
    },
    {
      "epoch": 1.545777146912704,
      "grad_norm": 0.18101200461387634,
      "learning_rate": 9.733587059942913e-05,
      "loss": 0.1041,
      "step": 1089
    },
    {
      "epoch": 1.5471965933286018,
      "grad_norm": 0.1895354688167572,
      "learning_rate": 9.724072312083731e-05,
      "loss": 0.1207,
      "step": 1090
    },
    {
      "epoch": 1.5486160397444997,
      "grad_norm": 0.14425978064537048,
      "learning_rate": 9.714557564224549e-05,
      "loss": 0.0906,
      "step": 1091
    },
    {
      "epoch": 1.5500354861603975,
      "grad_norm": 0.1297040432691574,
      "learning_rate": 9.705042816365368e-05,
      "loss": 0.0748,
      "step": 1092
    },
    {
      "epoch": 1.5514549325762954,
      "grad_norm": 0.18801988661289215,
      "learning_rate": 9.695528068506186e-05,
      "loss": 0.0885,
      "step": 1093
    },
    {
      "epoch": 1.552874378992193,
      "grad_norm": 0.12559044361114502,
      "learning_rate": 9.686013320647004e-05,
      "loss": 0.077,
      "step": 1094
    },
    {
      "epoch": 1.5542938254080907,
      "grad_norm": 0.13570553064346313,
      "learning_rate": 9.676498572787823e-05,
      "loss": 0.0763,
      "step": 1095
    },
    {
      "epoch": 1.5557132718239886,
      "grad_norm": 0.1427968144416809,
      "learning_rate": 9.666983824928641e-05,
      "loss": 0.0971,
      "step": 1096
    },
    {
      "epoch": 1.5571327182398864,
      "grad_norm": 0.13289742171764374,
      "learning_rate": 9.657469077069458e-05,
      "loss": 0.0907,
      "step": 1097
    },
    {
      "epoch": 1.5585521646557843,
      "grad_norm": 0.14509588479995728,
      "learning_rate": 9.647954329210276e-05,
      "loss": 0.0858,
      "step": 1098
    },
    {
      "epoch": 1.5599716110716821,
      "grad_norm": 0.15524929761886597,
      "learning_rate": 9.638439581351095e-05,
      "loss": 0.0871,
      "step": 1099
    },
    {
      "epoch": 1.56139105748758,
      "grad_norm": 0.11721380054950714,
      "learning_rate": 9.628924833491913e-05,
      "loss": 0.077,
      "step": 1100
    },
    {
      "epoch": 1.5628105039034776,
      "grad_norm": 0.11220785975456238,
      "learning_rate": 9.619410085632731e-05,
      "loss": 0.0648,
      "step": 1101
    },
    {
      "epoch": 1.5642299503193753,
      "grad_norm": 0.10674542933702469,
      "learning_rate": 9.60989533777355e-05,
      "loss": 0.0692,
      "step": 1102
    },
    {
      "epoch": 1.5656493967352731,
      "grad_norm": 0.10572240501642227,
      "learning_rate": 9.600380589914368e-05,
      "loss": 0.0549,
      "step": 1103
    },
    {
      "epoch": 1.567068843151171,
      "grad_norm": 0.0960398018360138,
      "learning_rate": 9.590865842055186e-05,
      "loss": 0.0504,
      "step": 1104
    },
    {
      "epoch": 1.5684882895670689,
      "grad_norm": 0.39577287435531616,
      "learning_rate": 9.581351094196003e-05,
      "loss": 0.2883,
      "step": 1105
    },
    {
      "epoch": 1.5699077359829667,
      "grad_norm": 0.4439944624900818,
      "learning_rate": 9.571836346336822e-05,
      "loss": 0.3205,
      "step": 1106
    },
    {
      "epoch": 1.5713271823988646,
      "grad_norm": 0.38351961970329285,
      "learning_rate": 9.56232159847764e-05,
      "loss": 0.3093,
      "step": 1107
    },
    {
      "epoch": 1.5727466288147622,
      "grad_norm": 0.4045942425727844,
      "learning_rate": 9.552806850618458e-05,
      "loss": 0.2846,
      "step": 1108
    },
    {
      "epoch": 1.5741660752306599,
      "grad_norm": 0.3734223544597626,
      "learning_rate": 9.543292102759277e-05,
      "loss": 0.2873,
      "step": 1109
    },
    {
      "epoch": 1.5755855216465577,
      "grad_norm": 0.3392872214317322,
      "learning_rate": 9.533777354900095e-05,
      "loss": 0.2807,
      "step": 1110
    },
    {
      "epoch": 1.5770049680624556,
      "grad_norm": 0.31974202394485474,
      "learning_rate": 9.524262607040913e-05,
      "loss": 0.2684,
      "step": 1111
    },
    {
      "epoch": 1.5784244144783535,
      "grad_norm": 0.36096569895744324,
      "learning_rate": 9.514747859181732e-05,
      "loss": 0.2552,
      "step": 1112
    },
    {
      "epoch": 1.5798438608942513,
      "grad_norm": 0.3601045608520508,
      "learning_rate": 9.50523311132255e-05,
      "loss": 0.2453,
      "step": 1113
    },
    {
      "epoch": 1.5812633073101492,
      "grad_norm": 0.271644651889801,
      "learning_rate": 9.495718363463368e-05,
      "loss": 0.2239,
      "step": 1114
    },
    {
      "epoch": 1.5826827537260468,
      "grad_norm": 0.3106287121772766,
      "learning_rate": 9.486203615604187e-05,
      "loss": 0.2637,
      "step": 1115
    },
    {
      "epoch": 1.5841022001419447,
      "grad_norm": 0.27016833424568176,
      "learning_rate": 9.476688867745005e-05,
      "loss": 0.1836,
      "step": 1116
    },
    {
      "epoch": 1.5855216465578423,
      "grad_norm": 0.26268041133880615,
      "learning_rate": 9.467174119885823e-05,
      "loss": 0.201,
      "step": 1117
    },
    {
      "epoch": 1.5869410929737402,
      "grad_norm": 0.24914787709712982,
      "learning_rate": 9.457659372026642e-05,
      "loss": 0.1886,
      "step": 1118
    },
    {
      "epoch": 1.588360539389638,
      "grad_norm": 0.23750141263008118,
      "learning_rate": 9.44814462416746e-05,
      "loss": 0.1888,
      "step": 1119
    },
    {
      "epoch": 1.589779985805536,
      "grad_norm": 0.2056935727596283,
      "learning_rate": 9.438629876308278e-05,
      "loss": 0.1568,
      "step": 1120
    },
    {
      "epoch": 1.5911994322214338,
      "grad_norm": 0.25478532910346985,
      "learning_rate": 9.429115128449097e-05,
      "loss": 0.1956,
      "step": 1121
    },
    {
      "epoch": 1.5926188786373314,
      "grad_norm": 0.2456052452325821,
      "learning_rate": 9.419600380589915e-05,
      "loss": 0.1934,
      "step": 1122
    },
    {
      "epoch": 1.5940383250532293,
      "grad_norm": 0.28367647528648376,
      "learning_rate": 9.410085632730733e-05,
      "loss": 0.1722,
      "step": 1123
    },
    {
      "epoch": 1.595457771469127,
      "grad_norm": 0.2271275669336319,
      "learning_rate": 9.400570884871552e-05,
      "loss": 0.1617,
      "step": 1124
    },
    {
      "epoch": 1.5968772178850248,
      "grad_norm": 0.26219385862350464,
      "learning_rate": 9.39105613701237e-05,
      "loss": 0.1676,
      "step": 1125
    },
    {
      "epoch": 1.5982966643009227,
      "grad_norm": 0.2154684215784073,
      "learning_rate": 9.381541389153188e-05,
      "loss": 0.1286,
      "step": 1126
    },
    {
      "epoch": 1.5997161107168205,
      "grad_norm": 0.22306138277053833,
      "learning_rate": 9.372026641294007e-05,
      "loss": 0.1555,
      "step": 1127
    },
    {
      "epoch": 1.6011355571327184,
      "grad_norm": 0.20479682087898254,
      "learning_rate": 9.362511893434825e-05,
      "loss": 0.1301,
      "step": 1128
    },
    {
      "epoch": 1.602555003548616,
      "grad_norm": 0.21258917450904846,
      "learning_rate": 9.352997145575643e-05,
      "loss": 0.1388,
      "step": 1129
    },
    {
      "epoch": 1.6039744499645139,
      "grad_norm": 0.18370363116264343,
      "learning_rate": 9.343482397716462e-05,
      "loss": 0.1336,
      "step": 1130
    },
    {
      "epoch": 1.6053938963804115,
      "grad_norm": 0.17901065945625305,
      "learning_rate": 9.333967649857279e-05,
      "loss": 0.1226,
      "step": 1131
    },
    {
      "epoch": 1.6068133427963094,
      "grad_norm": 0.19235804677009583,
      "learning_rate": 9.324452901998097e-05,
      "loss": 0.149,
      "step": 1132
    },
    {
      "epoch": 1.6082327892122072,
      "grad_norm": 0.21273356676101685,
      "learning_rate": 9.314938154138915e-05,
      "loss": 0.1274,
      "step": 1133
    },
    {
      "epoch": 1.609652235628105,
      "grad_norm": 0.18899813294410706,
      "learning_rate": 9.305423406279734e-05,
      "loss": 0.115,
      "step": 1134
    },
    {
      "epoch": 1.611071682044003,
      "grad_norm": 0.1690155267715454,
      "learning_rate": 9.295908658420552e-05,
      "loss": 0.1126,
      "step": 1135
    },
    {
      "epoch": 1.6124911284599006,
      "grad_norm": 0.16782383620738983,
      "learning_rate": 9.28639391056137e-05,
      "loss": 0.1104,
      "step": 1136
    },
    {
      "epoch": 1.6139105748757985,
      "grad_norm": 0.17566779255867004,
      "learning_rate": 9.276879162702189e-05,
      "loss": 0.0998,
      "step": 1137
    },
    {
      "epoch": 1.6153300212916961,
      "grad_norm": 0.17106449604034424,
      "learning_rate": 9.267364414843007e-05,
      "loss": 0.1126,
      "step": 1138
    },
    {
      "epoch": 1.616749467707594,
      "grad_norm": 0.18861639499664307,
      "learning_rate": 9.257849666983825e-05,
      "loss": 0.1203,
      "step": 1139
    },
    {
      "epoch": 1.6181689141234918,
      "grad_norm": 0.1776580810546875,
      "learning_rate": 9.248334919124644e-05,
      "loss": 0.0833,
      "step": 1140
    },
    {
      "epoch": 1.6195883605393897,
      "grad_norm": 0.13469579815864563,
      "learning_rate": 9.238820171265462e-05,
      "loss": 0.0979,
      "step": 1141
    },
    {
      "epoch": 1.6210078069552876,
      "grad_norm": 0.186883807182312,
      "learning_rate": 9.22930542340628e-05,
      "loss": 0.0985,
      "step": 1142
    },
    {
      "epoch": 1.6224272533711852,
      "grad_norm": 0.2528599500656128,
      "learning_rate": 9.219790675547099e-05,
      "loss": 0.1045,
      "step": 1143
    },
    {
      "epoch": 1.623846699787083,
      "grad_norm": 0.16331806778907776,
      "learning_rate": 9.210275927687917e-05,
      "loss": 0.096,
      "step": 1144
    },
    {
      "epoch": 1.6252661462029807,
      "grad_norm": 0.15830108523368835,
      "learning_rate": 9.200761179828735e-05,
      "loss": 0.0932,
      "step": 1145
    },
    {
      "epoch": 1.6266855926188786,
      "grad_norm": 0.16993558406829834,
      "learning_rate": 9.191246431969554e-05,
      "loss": 0.0888,
      "step": 1146
    },
    {
      "epoch": 1.6281050390347764,
      "grad_norm": 0.1768576055765152,
      "learning_rate": 9.181731684110372e-05,
      "loss": 0.0961,
      "step": 1147
    },
    {
      "epoch": 1.6295244854506743,
      "grad_norm": 0.12979647517204285,
      "learning_rate": 9.17221693625119e-05,
      "loss": 0.0804,
      "step": 1148
    },
    {
      "epoch": 1.6309439318665722,
      "grad_norm": 0.16028013825416565,
      "learning_rate": 9.162702188392009e-05,
      "loss": 0.084,
      "step": 1149
    },
    {
      "epoch": 1.6323633782824698,
      "grad_norm": 0.16149309277534485,
      "learning_rate": 9.153187440532827e-05,
      "loss": 0.0956,
      "step": 1150
    },
    {
      "epoch": 1.6337828246983677,
      "grad_norm": 0.13212089240550995,
      "learning_rate": 9.143672692673645e-05,
      "loss": 0.0775,
      "step": 1151
    },
    {
      "epoch": 1.6352022711142653,
      "grad_norm": 0.1310039907693863,
      "learning_rate": 9.134157944814464e-05,
      "loss": 0.0716,
      "step": 1152
    },
    {
      "epoch": 1.6366217175301632,
      "grad_norm": 0.13007889688014984,
      "learning_rate": 9.124643196955282e-05,
      "loss": 0.0604,
      "step": 1153
    },
    {
      "epoch": 1.638041163946061,
      "grad_norm": 0.12361201643943787,
      "learning_rate": 9.1151284490961e-05,
      "loss": 0.0679,
      "step": 1154
    },
    {
      "epoch": 1.639460610361959,
      "grad_norm": 0.40215152502059937,
      "learning_rate": 9.105613701236919e-05,
      "loss": 0.2445,
      "step": 1155
    },
    {
      "epoch": 1.6408800567778568,
      "grad_norm": 0.47390490770339966,
      "learning_rate": 9.096098953377737e-05,
      "loss": 0.323,
      "step": 1156
    },
    {
      "epoch": 1.6422995031937544,
      "grad_norm": 0.4263153374195099,
      "learning_rate": 9.086584205518554e-05,
      "loss": 0.3116,
      "step": 1157
    },
    {
      "epoch": 1.6437189496096523,
      "grad_norm": 0.42216771841049194,
      "learning_rate": 9.077069457659372e-05,
      "loss": 0.296,
      "step": 1158
    },
    {
      "epoch": 1.64513839602555,
      "grad_norm": 0.41625288128852844,
      "learning_rate": 9.06755470980019e-05,
      "loss": 0.2533,
      "step": 1159
    },
    {
      "epoch": 1.6465578424414478,
      "grad_norm": 0.3583560585975647,
      "learning_rate": 9.058039961941009e-05,
      "loss": 0.2797,
      "step": 1160
    },
    {
      "epoch": 1.6479772888573456,
      "grad_norm": 0.33342236280441284,
      "learning_rate": 9.048525214081827e-05,
      "loss": 0.2264,
      "step": 1161
    },
    {
      "epoch": 1.6493967352732435,
      "grad_norm": 0.34095385670661926,
      "learning_rate": 9.039010466222646e-05,
      "loss": 0.2124,
      "step": 1162
    },
    {
      "epoch": 1.6508161816891413,
      "grad_norm": 0.30777162313461304,
      "learning_rate": 9.029495718363464e-05,
      "loss": 0.1948,
      "step": 1163
    },
    {
      "epoch": 1.652235628105039,
      "grad_norm": 0.33276137709617615,
      "learning_rate": 9.019980970504282e-05,
      "loss": 0.2266,
      "step": 1164
    },
    {
      "epoch": 1.6536550745209369,
      "grad_norm": 0.3561549484729767,
      "learning_rate": 9.0104662226451e-05,
      "loss": 0.1992,
      "step": 1165
    },
    {
      "epoch": 1.6550745209368345,
      "grad_norm": 0.32363876700401306,
      "learning_rate": 9.000951474785919e-05,
      "loss": 0.1941,
      "step": 1166
    },
    {
      "epoch": 1.6564939673527324,
      "grad_norm": 0.2753477990627289,
      "learning_rate": 8.991436726926736e-05,
      "loss": 0.1701,
      "step": 1167
    },
    {
      "epoch": 1.6579134137686302,
      "grad_norm": 0.2797068953514099,
      "learning_rate": 8.981921979067554e-05,
      "loss": 0.1681,
      "step": 1168
    },
    {
      "epoch": 1.659332860184528,
      "grad_norm": 0.22651182115077972,
      "learning_rate": 8.972407231208373e-05,
      "loss": 0.1616,
      "step": 1169
    },
    {
      "epoch": 1.660752306600426,
      "grad_norm": 0.2519342303276062,
      "learning_rate": 8.962892483349191e-05,
      "loss": 0.1598,
      "step": 1170
    },
    {
      "epoch": 1.6621717530163236,
      "grad_norm": 0.24193108081817627,
      "learning_rate": 8.95337773549001e-05,
      "loss": 0.1747,
      "step": 1171
    },
    {
      "epoch": 1.6635911994322214,
      "grad_norm": 0.229801207780838,
      "learning_rate": 8.943862987630828e-05,
      "loss": 0.1756,
      "step": 1172
    },
    {
      "epoch": 1.665010645848119,
      "grad_norm": 0.25531861186027527,
      "learning_rate": 8.934348239771646e-05,
      "loss": 0.1632,
      "step": 1173
    },
    {
      "epoch": 1.666430092264017,
      "grad_norm": 0.20510223507881165,
      "learning_rate": 8.924833491912464e-05,
      "loss": 0.1462,
      "step": 1174
    },
    {
      "epoch": 1.6678495386799148,
      "grad_norm": 0.21905170381069183,
      "learning_rate": 8.915318744053283e-05,
      "loss": 0.1348,
      "step": 1175
    },
    {
      "epoch": 1.6692689850958127,
      "grad_norm": 0.2136707901954651,
      "learning_rate": 8.905803996194101e-05,
      "loss": 0.1687,
      "step": 1176
    },
    {
      "epoch": 1.6706884315117105,
      "grad_norm": 0.19990460574626923,
      "learning_rate": 8.89628924833492e-05,
      "loss": 0.1266,
      "step": 1177
    },
    {
      "epoch": 1.6721078779276084,
      "grad_norm": 0.22707079350948334,
      "learning_rate": 8.886774500475738e-05,
      "loss": 0.1225,
      "step": 1178
    },
    {
      "epoch": 1.673527324343506,
      "grad_norm": 0.19313934445381165,
      "learning_rate": 8.877259752616556e-05,
      "loss": 0.1561,
      "step": 1179
    },
    {
      "epoch": 1.6749467707594037,
      "grad_norm": 0.20268777012825012,
      "learning_rate": 8.867745004757374e-05,
      "loss": 0.1184,
      "step": 1180
    },
    {
      "epoch": 1.6763662171753015,
      "grad_norm": 0.2176579087972641,
      "learning_rate": 8.858230256898193e-05,
      "loss": 0.1603,
      "step": 1181
    },
    {
      "epoch": 1.6777856635911994,
      "grad_norm": 0.1778584122657776,
      "learning_rate": 8.848715509039011e-05,
      "loss": 0.1147,
      "step": 1182
    },
    {
      "epoch": 1.6792051100070973,
      "grad_norm": 0.1905093491077423,
      "learning_rate": 8.83920076117983e-05,
      "loss": 0.1196,
      "step": 1183
    },
    {
      "epoch": 1.6806245564229951,
      "grad_norm": 0.21993334591388702,
      "learning_rate": 8.829686013320648e-05,
      "loss": 0.1228,
      "step": 1184
    },
    {
      "epoch": 1.682044002838893,
      "grad_norm": 0.20666565001010895,
      "learning_rate": 8.820171265461466e-05,
      "loss": 0.1167,
      "step": 1185
    },
    {
      "epoch": 1.6834634492547906,
      "grad_norm": 0.1573011875152588,
      "learning_rate": 8.810656517602284e-05,
      "loss": 0.1041,
      "step": 1186
    },
    {
      "epoch": 1.6848828956706883,
      "grad_norm": 0.16789160668849945,
      "learning_rate": 8.801141769743101e-05,
      "loss": 0.1195,
      "step": 1187
    },
    {
      "epoch": 1.6863023420865861,
      "grad_norm": 0.16880875825881958,
      "learning_rate": 8.79162702188392e-05,
      "loss": 0.1162,
      "step": 1188
    },
    {
      "epoch": 1.687721788502484,
      "grad_norm": 0.1994628757238388,
      "learning_rate": 8.782112274024738e-05,
      "loss": 0.0968,
      "step": 1189
    },
    {
      "epoch": 1.6891412349183819,
      "grad_norm": 0.1818031221628189,
      "learning_rate": 8.772597526165556e-05,
      "loss": 0.0912,
      "step": 1190
    },
    {
      "epoch": 1.6905606813342797,
      "grad_norm": 0.1715688556432724,
      "learning_rate": 8.763082778306375e-05,
      "loss": 0.1044,
      "step": 1191
    },
    {
      "epoch": 1.6919801277501776,
      "grad_norm": 0.1778375655412674,
      "learning_rate": 8.753568030447193e-05,
      "loss": 0.1127,
      "step": 1192
    },
    {
      "epoch": 1.6933995741660752,
      "grad_norm": 0.14485497772693634,
      "learning_rate": 8.744053282588011e-05,
      "loss": 0.0921,
      "step": 1193
    },
    {
      "epoch": 1.6948190205819729,
      "grad_norm": 0.15198980271816254,
      "learning_rate": 8.73453853472883e-05,
      "loss": 0.0782,
      "step": 1194
    },
    {
      "epoch": 1.6962384669978707,
      "grad_norm": 0.12379363179206848,
      "learning_rate": 8.725023786869648e-05,
      "loss": 0.0743,
      "step": 1195
    },
    {
      "epoch": 1.6976579134137686,
      "grad_norm": 0.15231451392173767,
      "learning_rate": 8.715509039010466e-05,
      "loss": 0.0785,
      "step": 1196
    },
    {
      "epoch": 1.6990773598296665,
      "grad_norm": 0.1123708188533783,
      "learning_rate": 8.705994291151285e-05,
      "loss": 0.0698,
      "step": 1197
    },
    {
      "epoch": 1.7004968062455643,
      "grad_norm": 0.1545679122209549,
      "learning_rate": 8.696479543292103e-05,
      "loss": 0.0898,
      "step": 1198
    },
    {
      "epoch": 1.7019162526614622,
      "grad_norm": 0.13263759016990662,
      "learning_rate": 8.686964795432921e-05,
      "loss": 0.0787,
      "step": 1199
    },
    {
      "epoch": 1.7033356990773598,
      "grad_norm": 0.20946283638477325,
      "learning_rate": 8.67745004757374e-05,
      "loss": 0.0959,
      "step": 1200
    },
    {
      "epoch": 1.7047551454932577,
      "grad_norm": 0.22734473645687103,
      "learning_rate": 8.667935299714558e-05,
      "loss": 0.068,
      "step": 1201
    },
    {
      "epoch": 1.7061745919091553,
      "grad_norm": 0.0966707393527031,
      "learning_rate": 8.658420551855376e-05,
      "loss": 0.0553,
      "step": 1202
    },
    {
      "epoch": 1.7075940383250532,
      "grad_norm": 0.10346301645040512,
      "learning_rate": 8.648905803996195e-05,
      "loss": 0.063,
      "step": 1203
    },
    {
      "epoch": 1.709013484740951,
      "grad_norm": 0.09416425973176956,
      "learning_rate": 8.639391056137013e-05,
      "loss": 0.0636,
      "step": 1204
    },
    {
      "epoch": 1.710432931156849,
      "grad_norm": 0.36687150597572327,
      "learning_rate": 8.629876308277831e-05,
      "loss": 0.252,
      "step": 1205
    },
    {
      "epoch": 1.7118523775727468,
      "grad_norm": 0.47827157378196716,
      "learning_rate": 8.62036156041865e-05,
      "loss": 0.3832,
      "step": 1206
    },
    {
      "epoch": 1.7132718239886444,
      "grad_norm": 0.4485202133655548,
      "learning_rate": 8.610846812559468e-05,
      "loss": 0.3625,
      "step": 1207
    },
    {
      "epoch": 1.7146912704045423,
      "grad_norm": 0.34707123041152954,
      "learning_rate": 8.601332064700286e-05,
      "loss": 0.2127,
      "step": 1208
    },
    {
      "epoch": 1.71611071682044,
      "grad_norm": 0.35719022154808044,
      "learning_rate": 8.591817316841105e-05,
      "loss": 0.2603,
      "step": 1209
    },
    {
      "epoch": 1.7175301632363378,
      "grad_norm": 0.33392223715782166,
      "learning_rate": 8.582302568981923e-05,
      "loss": 0.2544,
      "step": 1210
    },
    {
      "epoch": 1.7189496096522356,
      "grad_norm": 0.36374911665916443,
      "learning_rate": 8.572787821122741e-05,
      "loss": 0.2681,
      "step": 1211
    },
    {
      "epoch": 1.7203690560681335,
      "grad_norm": 0.2963283061981201,
      "learning_rate": 8.56327307326356e-05,
      "loss": 0.2438,
      "step": 1212
    },
    {
      "epoch": 1.7217885024840314,
      "grad_norm": 0.3044280707836151,
      "learning_rate": 8.553758325404377e-05,
      "loss": 0.2262,
      "step": 1213
    },
    {
      "epoch": 1.723207948899929,
      "grad_norm": 0.33561423420906067,
      "learning_rate": 8.544243577545195e-05,
      "loss": 0.2027,
      "step": 1214
    },
    {
      "epoch": 1.7246273953158269,
      "grad_norm": 0.3509998619556427,
      "learning_rate": 8.534728829686013e-05,
      "loss": 0.2235,
      "step": 1215
    },
    {
      "epoch": 1.7260468417317245,
      "grad_norm": 0.25298362970352173,
      "learning_rate": 8.525214081826832e-05,
      "loss": 0.2173,
      "step": 1216
    },
    {
      "epoch": 1.7274662881476224,
      "grad_norm": 0.2825247347354889,
      "learning_rate": 8.51569933396765e-05,
      "loss": 0.1807,
      "step": 1217
    },
    {
      "epoch": 1.7288857345635202,
      "grad_norm": 0.2706267833709717,
      "learning_rate": 8.506184586108468e-05,
      "loss": 0.2,
      "step": 1218
    },
    {
      "epoch": 1.730305180979418,
      "grad_norm": 0.2260177731513977,
      "learning_rate": 8.496669838249287e-05,
      "loss": 0.1761,
      "step": 1219
    },
    {
      "epoch": 1.731724627395316,
      "grad_norm": 0.23699478805065155,
      "learning_rate": 8.487155090390105e-05,
      "loss": 0.1657,
      "step": 1220
    },
    {
      "epoch": 1.7331440738112136,
      "grad_norm": 0.2108049988746643,
      "learning_rate": 8.477640342530923e-05,
      "loss": 0.1513,
      "step": 1221
    },
    {
      "epoch": 1.7345635202271115,
      "grad_norm": 0.24796020984649658,
      "learning_rate": 8.468125594671742e-05,
      "loss": 0.1638,
      "step": 1222
    },
    {
      "epoch": 1.735982966643009,
      "grad_norm": 0.21968974173069,
      "learning_rate": 8.45861084681256e-05,
      "loss": 0.1624,
      "step": 1223
    },
    {
      "epoch": 1.737402413058907,
      "grad_norm": 0.21006545424461365,
      "learning_rate": 8.449096098953378e-05,
      "loss": 0.1606,
      "step": 1224
    },
    {
      "epoch": 1.7388218594748048,
      "grad_norm": 0.1888023316860199,
      "learning_rate": 8.439581351094197e-05,
      "loss": 0.1295,
      "step": 1225
    },
    {
      "epoch": 1.7402413058907027,
      "grad_norm": 0.24864818155765533,
      "learning_rate": 8.430066603235015e-05,
      "loss": 0.1817,
      "step": 1226
    },
    {
      "epoch": 1.7416607523066006,
      "grad_norm": 0.18869194388389587,
      "learning_rate": 8.420551855375833e-05,
      "loss": 0.1284,
      "step": 1227
    },
    {
      "epoch": 1.7430801987224982,
      "grad_norm": 0.19849030673503876,
      "learning_rate": 8.411037107516652e-05,
      "loss": 0.1561,
      "step": 1228
    },
    {
      "epoch": 1.744499645138396,
      "grad_norm": 0.18956497311592102,
      "learning_rate": 8.40152235965747e-05,
      "loss": 0.1181,
      "step": 1229
    },
    {
      "epoch": 1.7459190915542937,
      "grad_norm": 0.19686366617679596,
      "learning_rate": 8.392007611798288e-05,
      "loss": 0.1598,
      "step": 1230
    },
    {
      "epoch": 1.7473385379701916,
      "grad_norm": 0.19989241659641266,
      "learning_rate": 8.382492863939107e-05,
      "loss": 0.129,
      "step": 1231
    },
    {
      "epoch": 1.7487579843860894,
      "grad_norm": 0.18743924796581268,
      "learning_rate": 8.372978116079925e-05,
      "loss": 0.1177,
      "step": 1232
    },
    {
      "epoch": 1.7501774308019873,
      "grad_norm": 0.14968110620975494,
      "learning_rate": 8.363463368220743e-05,
      "loss": 0.0976,
      "step": 1233
    },
    {
      "epoch": 1.7515968772178852,
      "grad_norm": 0.14528466761112213,
      "learning_rate": 8.353948620361562e-05,
      "loss": 0.1055,
      "step": 1234
    },
    {
      "epoch": 1.7530163236337828,
      "grad_norm": 0.16507534682750702,
      "learning_rate": 8.34443387250238e-05,
      "loss": 0.1251,
      "step": 1235
    },
    {
      "epoch": 1.7544357700496807,
      "grad_norm": 0.14156639575958252,
      "learning_rate": 8.334919124643198e-05,
      "loss": 0.0896,
      "step": 1236
    },
    {
      "epoch": 1.7558552164655783,
      "grad_norm": 0.1429094821214676,
      "learning_rate": 8.325404376784015e-05,
      "loss": 0.0864,
      "step": 1237
    },
    {
      "epoch": 1.7572746628814762,
      "grad_norm": 0.16865478456020355,
      "learning_rate": 8.315889628924834e-05,
      "loss": 0.1017,
      "step": 1238
    },
    {
      "epoch": 1.758694109297374,
      "grad_norm": 0.16003815829753876,
      "learning_rate": 8.306374881065652e-05,
      "loss": 0.108,
      "step": 1239
    },
    {
      "epoch": 1.7601135557132719,
      "grad_norm": 0.18251800537109375,
      "learning_rate": 8.29686013320647e-05,
      "loss": 0.0946,
      "step": 1240
    },
    {
      "epoch": 1.7615330021291697,
      "grad_norm": 0.19885434210300446,
      "learning_rate": 8.287345385347289e-05,
      "loss": 0.1228,
      "step": 1241
    },
    {
      "epoch": 1.7629524485450674,
      "grad_norm": 0.17238536477088928,
      "learning_rate": 8.277830637488107e-05,
      "loss": 0.0933,
      "step": 1242
    },
    {
      "epoch": 1.7643718949609652,
      "grad_norm": 0.14751304686069489,
      "learning_rate": 8.268315889628924e-05,
      "loss": 0.097,
      "step": 1243
    },
    {
      "epoch": 1.7657913413768629,
      "grad_norm": 0.16485515236854553,
      "learning_rate": 8.258801141769742e-05,
      "loss": 0.0879,
      "step": 1244
    },
    {
      "epoch": 1.7672107877927608,
      "grad_norm": 0.16255003213882446,
      "learning_rate": 8.249286393910561e-05,
      "loss": 0.0889,
      "step": 1245
    },
    {
      "epoch": 1.7686302342086586,
      "grad_norm": 0.13616591691970825,
      "learning_rate": 8.239771646051379e-05,
      "loss": 0.0802,
      "step": 1246
    },
    {
      "epoch": 1.7700496806245565,
      "grad_norm": 0.13940514624118805,
      "learning_rate": 8.230256898192197e-05,
      "loss": 0.0855,
      "step": 1247
    },
    {
      "epoch": 1.7714691270404543,
      "grad_norm": 0.10046036541461945,
      "learning_rate": 8.220742150333016e-05,
      "loss": 0.0574,
      "step": 1248
    },
    {
      "epoch": 1.772888573456352,
      "grad_norm": 0.12781922519207,
      "learning_rate": 8.211227402473834e-05,
      "loss": 0.0728,
      "step": 1249
    },
    {
      "epoch": 1.7743080198722498,
      "grad_norm": 0.11606588959693909,
      "learning_rate": 8.201712654614652e-05,
      "loss": 0.0684,
      "step": 1250
    },
    {
      "epoch": 1.7757274662881475,
      "grad_norm": 0.14370952546596527,
      "learning_rate": 8.192197906755471e-05,
      "loss": 0.0684,
      "step": 1251
    },
    {
      "epoch": 1.7771469127040453,
      "grad_norm": 0.11052340269088745,
      "learning_rate": 8.182683158896289e-05,
      "loss": 0.0729,
      "step": 1252
    },
    {
      "epoch": 1.7785663591199432,
      "grad_norm": 0.14494384825229645,
      "learning_rate": 8.173168411037107e-05,
      "loss": 0.0769,
      "step": 1253
    },
    {
      "epoch": 1.779985805535841,
      "grad_norm": 0.1029040515422821,
      "learning_rate": 8.163653663177926e-05,
      "loss": 0.057,
      "step": 1254
    },
    {
      "epoch": 1.781405251951739,
      "grad_norm": 0.3844671845436096,
      "learning_rate": 8.154138915318744e-05,
      "loss": 0.2694,
      "step": 1255
    },
    {
      "epoch": 1.7828246983676366,
      "grad_norm": 0.4187108278274536,
      "learning_rate": 8.144624167459562e-05,
      "loss": 0.3221,
      "step": 1256
    },
    {
      "epoch": 1.7842441447835344,
      "grad_norm": 0.42832568287849426,
      "learning_rate": 8.135109419600381e-05,
      "loss": 0.3026,
      "step": 1257
    },
    {
      "epoch": 1.785663591199432,
      "grad_norm": 0.40359407663345337,
      "learning_rate": 8.125594671741199e-05,
      "loss": 0.2891,
      "step": 1258
    },
    {
      "epoch": 1.78708303761533,
      "grad_norm": 0.3673126995563507,
      "learning_rate": 8.116079923882017e-05,
      "loss": 0.2876,
      "step": 1259
    },
    {
      "epoch": 1.7885024840312278,
      "grad_norm": 0.3517145812511444,
      "learning_rate": 8.106565176022836e-05,
      "loss": 0.2617,
      "step": 1260
    },
    {
      "epoch": 1.7899219304471257,
      "grad_norm": 0.3282889425754547,
      "learning_rate": 8.097050428163654e-05,
      "loss": 0.2472,
      "step": 1261
    },
    {
      "epoch": 1.7913413768630235,
      "grad_norm": 0.31093570590019226,
      "learning_rate": 8.087535680304473e-05,
      "loss": 0.2435,
      "step": 1262
    },
    {
      "epoch": 1.7927608232789212,
      "grad_norm": 0.36776265501976013,
      "learning_rate": 8.078020932445291e-05,
      "loss": 0.2688,
      "step": 1263
    },
    {
      "epoch": 1.794180269694819,
      "grad_norm": 0.36984625458717346,
      "learning_rate": 8.068506184586109e-05,
      "loss": 0.2476,
      "step": 1264
    },
    {
      "epoch": 1.7955997161107167,
      "grad_norm": 0.2515927851200104,
      "learning_rate": 8.058991436726928e-05,
      "loss": 0.2239,
      "step": 1265
    },
    {
      "epoch": 1.7970191625266145,
      "grad_norm": 0.29788750410079956,
      "learning_rate": 8.049476688867746e-05,
      "loss": 0.2277,
      "step": 1266
    },
    {
      "epoch": 1.7984386089425124,
      "grad_norm": 0.2846829891204834,
      "learning_rate": 8.039961941008564e-05,
      "loss": 0.2255,
      "step": 1267
    },
    {
      "epoch": 1.7998580553584103,
      "grad_norm": 0.2695087194442749,
      "learning_rate": 8.030447193149383e-05,
      "loss": 0.1879,
      "step": 1268
    },
    {
      "epoch": 1.8012775017743081,
      "grad_norm": 0.27484196424484253,
      "learning_rate": 8.0209324452902e-05,
      "loss": 0.1856,
      "step": 1269
    },
    {
      "epoch": 1.8012775017743081,
      "eval_loss": 0.18090929090976715,
      "eval_runtime": 349.6509,
      "eval_samples_per_second": 3.023,
      "eval_steps_per_second": 1.01,
      "step": 1269
    },
    {
      "epoch": 1.802696948190206,
      "grad_norm": 0.25335758924484253,
      "learning_rate": 8.011417697431018e-05,
      "loss": 0.2025,
      "step": 1270
    },
    {
      "epoch": 1.8041163946061036,
      "grad_norm": 0.2552163004875183,
      "learning_rate": 8.001902949571836e-05,
      "loss": 0.1664,
      "step": 1271
    },
    {
      "epoch": 1.8055358410220013,
      "grad_norm": 0.25595882534980774,
      "learning_rate": 7.992388201712655e-05,
      "loss": 0.1847,
      "step": 1272
    },
    {
      "epoch": 1.8069552874378991,
      "grad_norm": 0.2730717957019806,
      "learning_rate": 7.982873453853473e-05,
      "loss": 0.1746,
      "step": 1273
    },
    {
      "epoch": 1.808374733853797,
      "grad_norm": 0.2757052183151245,
      "learning_rate": 7.973358705994291e-05,
      "loss": 0.181,
      "step": 1274
    },
    {
      "epoch": 1.8097941802696949,
      "grad_norm": 0.23661845922470093,
      "learning_rate": 7.96384395813511e-05,
      "loss": 0.1663,
      "step": 1275
    },
    {
      "epoch": 1.8112136266855927,
      "grad_norm": 0.19213782250881195,
      "learning_rate": 7.954329210275928e-05,
      "loss": 0.127,
      "step": 1276
    },
    {
      "epoch": 1.8126330731014906,
      "grad_norm": 0.2580130398273468,
      "learning_rate": 7.944814462416746e-05,
      "loss": 0.1792,
      "step": 1277
    },
    {
      "epoch": 1.8140525195173882,
      "grad_norm": 0.21718113124370575,
      "learning_rate": 7.935299714557565e-05,
      "loss": 0.1523,
      "step": 1278
    },
    {
      "epoch": 1.8154719659332859,
      "grad_norm": 0.18339183926582336,
      "learning_rate": 7.925784966698383e-05,
      "loss": 0.1295,
      "step": 1279
    },
    {
      "epoch": 1.8168914123491837,
      "grad_norm": 0.20249682664871216,
      "learning_rate": 7.916270218839201e-05,
      "loss": 0.1416,
      "step": 1280
    },
    {
      "epoch": 1.8183108587650816,
      "grad_norm": 0.19188742339611053,
      "learning_rate": 7.90675547098002e-05,
      "loss": 0.1376,
      "step": 1281
    },
    {
      "epoch": 1.8197303051809794,
      "grad_norm": 0.20053653419017792,
      "learning_rate": 7.897240723120838e-05,
      "loss": 0.147,
      "step": 1282
    },
    {
      "epoch": 1.8211497515968773,
      "grad_norm": 0.19863907992839813,
      "learning_rate": 7.887725975261656e-05,
      "loss": 0.1261,
      "step": 1283
    },
    {
      "epoch": 1.8225691980127752,
      "grad_norm": 0.20023630559444427,
      "learning_rate": 7.878211227402475e-05,
      "loss": 0.12,
      "step": 1284
    },
    {
      "epoch": 1.8239886444286728,
      "grad_norm": 0.21761375665664673,
      "learning_rate": 7.868696479543293e-05,
      "loss": 0.1672,
      "step": 1285
    },
    {
      "epoch": 1.8254080908445705,
      "grad_norm": 0.16622762382030487,
      "learning_rate": 7.859181731684111e-05,
      "loss": 0.1082,
      "step": 1286
    },
    {
      "epoch": 1.8268275372604683,
      "grad_norm": 0.1923506110906601,
      "learning_rate": 7.84966698382493e-05,
      "loss": 0.1118,
      "step": 1287
    },
    {
      "epoch": 1.8282469836763662,
      "grad_norm": 0.17379166185855865,
      "learning_rate": 7.840152235965748e-05,
      "loss": 0.1074,
      "step": 1288
    },
    {
      "epoch": 1.829666430092264,
      "grad_norm": 0.13147275149822235,
      "learning_rate": 7.830637488106566e-05,
      "loss": 0.0886,
      "step": 1289
    },
    {
      "epoch": 1.831085876508162,
      "grad_norm": 0.16819316148757935,
      "learning_rate": 7.821122740247385e-05,
      "loss": 0.1113,
      "step": 1290
    },
    {
      "epoch": 1.8325053229240598,
      "grad_norm": 0.1362367421388626,
      "learning_rate": 7.811607992388203e-05,
      "loss": 0.0956,
      "step": 1291
    },
    {
      "epoch": 1.8339247693399574,
      "grad_norm": 0.17351478338241577,
      "learning_rate": 7.802093244529021e-05,
      "loss": 0.1004,
      "step": 1292
    },
    {
      "epoch": 1.8353442157558553,
      "grad_norm": 0.1439971625804901,
      "learning_rate": 7.79257849666984e-05,
      "loss": 0.0894,
      "step": 1293
    },
    {
      "epoch": 1.836763662171753,
      "grad_norm": 0.15592727065086365,
      "learning_rate": 7.783063748810658e-05,
      "loss": 0.0919,
      "step": 1294
    },
    {
      "epoch": 1.8381831085876508,
      "grad_norm": 0.13377462327480316,
      "learning_rate": 7.773549000951475e-05,
      "loss": 0.0876,
      "step": 1295
    },
    {
      "epoch": 1.8396025550035486,
      "grad_norm": 0.16579319536685944,
      "learning_rate": 7.764034253092293e-05,
      "loss": 0.0917,
      "step": 1296
    },
    {
      "epoch": 1.8410220014194465,
      "grad_norm": 0.12457873672246933,
      "learning_rate": 7.754519505233112e-05,
      "loss": 0.0705,
      "step": 1297
    },
    {
      "epoch": 1.8424414478353444,
      "grad_norm": 0.13177235424518585,
      "learning_rate": 7.74500475737393e-05,
      "loss": 0.0715,
      "step": 1298
    },
    {
      "epoch": 1.843860894251242,
      "grad_norm": 0.1891987919807434,
      "learning_rate": 7.735490009514748e-05,
      "loss": 0.0881,
      "step": 1299
    },
    {
      "epoch": 1.8452803406671399,
      "grad_norm": 0.11441599577665329,
      "learning_rate": 7.725975261655567e-05,
      "loss": 0.0646,
      "step": 1300
    },
    {
      "epoch": 1.8466997870830375,
      "grad_norm": 0.10397599637508392,
      "learning_rate": 7.716460513796385e-05,
      "loss": 0.0628,
      "step": 1301
    },
    {
      "epoch": 1.8481192334989354,
      "grad_norm": 0.12083600461483002,
      "learning_rate": 7.706945765937203e-05,
      "loss": 0.0623,
      "step": 1302
    },
    {
      "epoch": 1.8495386799148332,
      "grad_norm": 0.1213122308254242,
      "learning_rate": 7.697431018078022e-05,
      "loss": 0.0793,
      "step": 1303
    },
    {
      "epoch": 1.850958126330731,
      "grad_norm": 0.1086667999625206,
      "learning_rate": 7.68791627021884e-05,
      "loss": 0.0513,
      "step": 1304
    },
    {
      "epoch": 1.852377572746629,
      "grad_norm": 0.3027424216270447,
      "learning_rate": 7.678401522359658e-05,
      "loss": 0.2101,
      "step": 1305
    },
    {
      "epoch": 1.8537970191625266,
      "grad_norm": 0.42018696665763855,
      "learning_rate": 7.668886774500477e-05,
      "loss": 0.3162,
      "step": 1306
    },
    {
      "epoch": 1.8552164655784245,
      "grad_norm": 0.4469018876552582,
      "learning_rate": 7.659372026641294e-05,
      "loss": 0.3247,
      "step": 1307
    },
    {
      "epoch": 1.856635911994322,
      "grad_norm": 0.4820120334625244,
      "learning_rate": 7.649857278782112e-05,
      "loss": 0.3073,
      "step": 1308
    },
    {
      "epoch": 1.85805535841022,
      "grad_norm": 0.3769824802875519,
      "learning_rate": 7.64034253092293e-05,
      "loss": 0.2783,
      "step": 1309
    },
    {
      "epoch": 1.8594748048261178,
      "grad_norm": 0.44576773047447205,
      "learning_rate": 7.630827783063749e-05,
      "loss": 0.2875,
      "step": 1310
    },
    {
      "epoch": 1.8608942512420157,
      "grad_norm": 0.31521594524383545,
      "learning_rate": 7.621313035204567e-05,
      "loss": 0.2274,
      "step": 1311
    },
    {
      "epoch": 1.8623136976579135,
      "grad_norm": 0.3480857014656067,
      "learning_rate": 7.611798287345385e-05,
      "loss": 0.257,
      "step": 1312
    },
    {
      "epoch": 1.8637331440738112,
      "grad_norm": 0.33912190794944763,
      "learning_rate": 7.602283539486204e-05,
      "loss": 0.261,
      "step": 1313
    },
    {
      "epoch": 1.865152590489709,
      "grad_norm": 0.2908989489078522,
      "learning_rate": 7.592768791627022e-05,
      "loss": 0.2026,
      "step": 1314
    },
    {
      "epoch": 1.8665720369056067,
      "grad_norm": 0.32905712723731995,
      "learning_rate": 7.58325404376784e-05,
      "loss": 0.2133,
      "step": 1315
    },
    {
      "epoch": 1.8679914833215046,
      "grad_norm": 0.30387499928474426,
      "learning_rate": 7.573739295908659e-05,
      "loss": 0.2189,
      "step": 1316
    },
    {
      "epoch": 1.8694109297374024,
      "grad_norm": 0.2636260688304901,
      "learning_rate": 7.564224548049477e-05,
      "loss": 0.1583,
      "step": 1317
    },
    {
      "epoch": 1.8708303761533003,
      "grad_norm": 0.3009813129901886,
      "learning_rate": 7.554709800190295e-05,
      "loss": 0.2077,
      "step": 1318
    },
    {
      "epoch": 1.8722498225691981,
      "grad_norm": 0.2588810622692108,
      "learning_rate": 7.545195052331114e-05,
      "loss": 0.1714,
      "step": 1319
    },
    {
      "epoch": 1.8736692689850958,
      "grad_norm": 0.2576379179954529,
      "learning_rate": 7.535680304471932e-05,
      "loss": 0.1722,
      "step": 1320
    },
    {
      "epoch": 1.8750887154009936,
      "grad_norm": 0.20477041602134705,
      "learning_rate": 7.52616555661275e-05,
      "loss": 0.1551,
      "step": 1321
    },
    {
      "epoch": 1.8765081618168913,
      "grad_norm": 0.22964976727962494,
      "learning_rate": 7.516650808753569e-05,
      "loss": 0.1721,
      "step": 1322
    },
    {
      "epoch": 1.8779276082327891,
      "grad_norm": 0.27522972226142883,
      "learning_rate": 7.507136060894387e-05,
      "loss": 0.1695,
      "step": 1323
    },
    {
      "epoch": 1.879347054648687,
      "grad_norm": 0.21543565392494202,
      "learning_rate": 7.497621313035205e-05,
      "loss": 0.15,
      "step": 1324
    },
    {
      "epoch": 1.8807665010645849,
      "grad_norm": 0.17688831686973572,
      "learning_rate": 7.488106565176022e-05,
      "loss": 0.1107,
      "step": 1325
    },
    {
      "epoch": 1.8821859474804827,
      "grad_norm": 0.19660411775112152,
      "learning_rate": 7.47859181731684e-05,
      "loss": 0.1412,
      "step": 1326
    },
    {
      "epoch": 1.8836053938963804,
      "grad_norm": 0.2547849118709564,
      "learning_rate": 7.469077069457659e-05,
      "loss": 0.1658,
      "step": 1327
    },
    {
      "epoch": 1.8850248403122782,
      "grad_norm": 0.19344809651374817,
      "learning_rate": 7.459562321598477e-05,
      "loss": 0.1472,
      "step": 1328
    },
    {
      "epoch": 1.8864442867281759,
      "grad_norm": 0.18720605969429016,
      "learning_rate": 7.450047573739296e-05,
      "loss": 0.1361,
      "step": 1329
    },
    {
      "epoch": 1.8878637331440737,
      "grad_norm": 0.19794823229312897,
      "learning_rate": 7.440532825880114e-05,
      "loss": 0.1322,
      "step": 1330
    },
    {
      "epoch": 1.8892831795599716,
      "grad_norm": 0.20531140267848969,
      "learning_rate": 7.431018078020932e-05,
      "loss": 0.1117,
      "step": 1331
    },
    {
      "epoch": 1.8907026259758695,
      "grad_norm": 0.17470048367977142,
      "learning_rate": 7.42150333016175e-05,
      "loss": 0.1075,
      "step": 1332
    },
    {
      "epoch": 1.8921220723917673,
      "grad_norm": 0.30380842089653015,
      "learning_rate": 7.411988582302569e-05,
      "loss": 0.1508,
      "step": 1333
    },
    {
      "epoch": 1.893541518807665,
      "grad_norm": 0.22932332754135132,
      "learning_rate": 7.402473834443387e-05,
      "loss": 0.1281,
      "step": 1334
    },
    {
      "epoch": 1.8949609652235628,
      "grad_norm": 0.1846543848514557,
      "learning_rate": 7.392959086584206e-05,
      "loss": 0.118,
      "step": 1335
    },
    {
      "epoch": 1.8963804116394605,
      "grad_norm": 0.1890941709280014,
      "learning_rate": 7.383444338725024e-05,
      "loss": 0.1183,
      "step": 1336
    },
    {
      "epoch": 1.8977998580553583,
      "grad_norm": 0.20653335750102997,
      "learning_rate": 7.373929590865842e-05,
      "loss": 0.1224,
      "step": 1337
    },
    {
      "epoch": 1.8992193044712562,
      "grad_norm": 0.16145874559879303,
      "learning_rate": 7.36441484300666e-05,
      "loss": 0.1079,
      "step": 1338
    },
    {
      "epoch": 1.900638750887154,
      "grad_norm": 0.24476800858974457,
      "learning_rate": 7.354900095147479e-05,
      "loss": 0.1355,
      "step": 1339
    },
    {
      "epoch": 1.902058197303052,
      "grad_norm": 0.17826874554157257,
      "learning_rate": 7.345385347288297e-05,
      "loss": 0.1075,
      "step": 1340
    },
    {
      "epoch": 1.9034776437189496,
      "grad_norm": 0.18040966987609863,
      "learning_rate": 7.335870599429116e-05,
      "loss": 0.1022,
      "step": 1341
    },
    {
      "epoch": 1.9048970901348474,
      "grad_norm": 0.1272583305835724,
      "learning_rate": 7.326355851569934e-05,
      "loss": 0.0799,
      "step": 1342
    },
    {
      "epoch": 1.906316536550745,
      "grad_norm": 0.17084930837154388,
      "learning_rate": 7.316841103710752e-05,
      "loss": 0.1016,
      "step": 1343
    },
    {
      "epoch": 1.907735982966643,
      "grad_norm": 0.1692785918712616,
      "learning_rate": 7.30732635585157e-05,
      "loss": 0.1075,
      "step": 1344
    },
    {
      "epoch": 1.9091554293825408,
      "grad_norm": 0.1244860365986824,
      "learning_rate": 7.297811607992389e-05,
      "loss": 0.0798,
      "step": 1345
    },
    {
      "epoch": 1.9105748757984387,
      "grad_norm": 0.11862550675868988,
      "learning_rate": 7.288296860133207e-05,
      "loss": 0.0778,
      "step": 1346
    },
    {
      "epoch": 1.9119943222143365,
      "grad_norm": 0.13470906019210815,
      "learning_rate": 7.278782112274026e-05,
      "loss": 0.0795,
      "step": 1347
    },
    {
      "epoch": 1.9134137686302342,
      "grad_norm": 0.14190131425857544,
      "learning_rate": 7.269267364414844e-05,
      "loss": 0.0988,
      "step": 1348
    },
    {
      "epoch": 1.914833215046132,
      "grad_norm": 0.14439800381660461,
      "learning_rate": 7.259752616555662e-05,
      "loss": 0.0844,
      "step": 1349
    },
    {
      "epoch": 1.9162526614620297,
      "grad_norm": 0.11650283634662628,
      "learning_rate": 7.25023786869648e-05,
      "loss": 0.0684,
      "step": 1350
    },
    {
      "epoch": 1.9176721078779275,
      "grad_norm": 0.12047100067138672,
      "learning_rate": 7.240723120837298e-05,
      "loss": 0.0776,
      "step": 1351
    },
    {
      "epoch": 1.9190915542938254,
      "grad_norm": 0.10717561095952988,
      "learning_rate": 7.231208372978116e-05,
      "loss": 0.0709,
      "step": 1352
    },
    {
      "epoch": 1.9205110007097232,
      "grad_norm": 0.12695957720279694,
      "learning_rate": 7.221693625118934e-05,
      "loss": 0.0749,
      "step": 1353
    },
    {
      "epoch": 1.921930447125621,
      "grad_norm": 0.10081259906291962,
      "learning_rate": 7.212178877259753e-05,
      "loss": 0.0544,
      "step": 1354
    },
    {
      "epoch": 1.923349893541519,
      "grad_norm": 0.27839958667755127,
      "learning_rate": 7.202664129400571e-05,
      "loss": 0.2091,
      "step": 1355
    },
    {
      "epoch": 1.9247693399574166,
      "grad_norm": 0.3965684473514557,
      "learning_rate": 7.19314938154139e-05,
      "loss": 0.3482,
      "step": 1356
    },
    {
      "epoch": 1.9261887863733143,
      "grad_norm": 0.4239073693752289,
      "learning_rate": 7.183634633682208e-05,
      "loss": 0.3322,
      "step": 1357
    },
    {
      "epoch": 1.9276082327892121,
      "grad_norm": 0.32631608843803406,
      "learning_rate": 7.174119885823026e-05,
      "loss": 0.2818,
      "step": 1358
    },
    {
      "epoch": 1.92902767920511,
      "grad_norm": 0.3974720239639282,
      "learning_rate": 7.164605137963844e-05,
      "loss": 0.2598,
      "step": 1359
    },
    {
      "epoch": 1.9304471256210078,
      "grad_norm": 0.3308234214782715,
      "learning_rate": 7.155090390104663e-05,
      "loss": 0.2605,
      "step": 1360
    },
    {
      "epoch": 1.9318665720369057,
      "grad_norm": 0.2973816692829132,
      "learning_rate": 7.145575642245481e-05,
      "loss": 0.2155,
      "step": 1361
    },
    {
      "epoch": 1.9332860184528036,
      "grad_norm": 0.31278061866760254,
      "learning_rate": 7.1360608943863e-05,
      "loss": 0.254,
      "step": 1362
    },
    {
      "epoch": 1.9347054648687012,
      "grad_norm": 0.26543116569519043,
      "learning_rate": 7.126546146527118e-05,
      "loss": 0.1909,
      "step": 1363
    },
    {
      "epoch": 1.9361249112845988,
      "grad_norm": 0.2994113266468048,
      "learning_rate": 7.117031398667936e-05,
      "loss": 0.2502,
      "step": 1364
    },
    {
      "epoch": 1.9375443577004967,
      "grad_norm": 0.2846647799015045,
      "learning_rate": 7.107516650808754e-05,
      "loss": 0.1883,
      "step": 1365
    },
    {
      "epoch": 1.9389638041163946,
      "grad_norm": 0.3091801106929779,
      "learning_rate": 7.098001902949573e-05,
      "loss": 0.2038,
      "step": 1366
    },
    {
      "epoch": 1.9403832505322924,
      "grad_norm": 0.28901082277297974,
      "learning_rate": 7.088487155090391e-05,
      "loss": 0.2239,
      "step": 1367
    },
    {
      "epoch": 1.9418026969481903,
      "grad_norm": 0.34473344683647156,
      "learning_rate": 7.07897240723121e-05,
      "loss": 0.1707,
      "step": 1368
    },
    {
      "epoch": 1.9432221433640882,
      "grad_norm": 0.3073323369026184,
      "learning_rate": 7.069457659372028e-05,
      "loss": 0.2133,
      "step": 1369
    },
    {
      "epoch": 1.9446415897799858,
      "grad_norm": 0.2564159035682678,
      "learning_rate": 7.059942911512846e-05,
      "loss": 0.1732,
      "step": 1370
    },
    {
      "epoch": 1.9460610361958834,
      "grad_norm": 0.26522156596183777,
      "learning_rate": 7.050428163653664e-05,
      "loss": 0.1845,
      "step": 1371
    },
    {
      "epoch": 1.9474804826117813,
      "grad_norm": 0.22504465281963348,
      "learning_rate": 7.040913415794483e-05,
      "loss": 0.1512,
      "step": 1372
    },
    {
      "epoch": 1.9488999290276792,
      "grad_norm": 0.2289402037858963,
      "learning_rate": 7.031398667935301e-05,
      "loss": 0.156,
      "step": 1373
    },
    {
      "epoch": 1.950319375443577,
      "grad_norm": 0.2010125368833542,
      "learning_rate": 7.02188392007612e-05,
      "loss": 0.1391,
      "step": 1374
    },
    {
      "epoch": 1.951738821859475,
      "grad_norm": 0.20268549025058746,
      "learning_rate": 7.012369172216938e-05,
      "loss": 0.1447,
      "step": 1375
    },
    {
      "epoch": 1.9531582682753728,
      "grad_norm": 0.22981080412864685,
      "learning_rate": 7.002854424357756e-05,
      "loss": 0.1267,
      "step": 1376
    },
    {
      "epoch": 1.9545777146912704,
      "grad_norm": 0.18656842410564423,
      "learning_rate": 6.993339676498573e-05,
      "loss": 0.1288,
      "step": 1377
    },
    {
      "epoch": 1.9559971611071683,
      "grad_norm": 0.2018098086118698,
      "learning_rate": 6.983824928639391e-05,
      "loss": 0.1239,
      "step": 1378
    },
    {
      "epoch": 1.957416607523066,
      "grad_norm": 0.21206457912921906,
      "learning_rate": 6.97431018078021e-05,
      "loss": 0.136,
      "step": 1379
    },
    {
      "epoch": 1.9588360539389638,
      "grad_norm": 0.1858462691307068,
      "learning_rate": 6.964795432921028e-05,
      "loss": 0.1203,
      "step": 1380
    },
    {
      "epoch": 1.9602555003548616,
      "grad_norm": 0.20797447860240936,
      "learning_rate": 6.955280685061846e-05,
      "loss": 0.1195,
      "step": 1381
    },
    {
      "epoch": 1.9616749467707595,
      "grad_norm": 0.1620146632194519,
      "learning_rate": 6.945765937202663e-05,
      "loss": 0.1154,
      "step": 1382
    },
    {
      "epoch": 1.9630943931866573,
      "grad_norm": 0.194803848862648,
      "learning_rate": 6.936251189343482e-05,
      "loss": 0.1145,
      "step": 1383
    },
    {
      "epoch": 1.964513839602555,
      "grad_norm": 0.16643476486206055,
      "learning_rate": 6.9267364414843e-05,
      "loss": 0.1015,
      "step": 1384
    },
    {
      "epoch": 1.9659332860184529,
      "grad_norm": 0.20933948457241058,
      "learning_rate": 6.917221693625118e-05,
      "loss": 0.1295,
      "step": 1385
    },
    {
      "epoch": 1.9673527324343505,
      "grad_norm": 0.19498176872730255,
      "learning_rate": 6.907706945765937e-05,
      "loss": 0.1203,
      "step": 1386
    },
    {
      "epoch": 1.9687721788502484,
      "grad_norm": 0.19805462658405304,
      "learning_rate": 6.898192197906755e-05,
      "loss": 0.106,
      "step": 1387
    },
    {
      "epoch": 1.9701916252661462,
      "grad_norm": 0.21661534905433655,
      "learning_rate": 6.888677450047573e-05,
      "loss": 0.1112,
      "step": 1388
    },
    {
      "epoch": 1.971611071682044,
      "grad_norm": 0.17225193977355957,
      "learning_rate": 6.879162702188392e-05,
      "loss": 0.102,
      "step": 1389
    },
    {
      "epoch": 1.973030518097942,
      "grad_norm": 0.16768790781497955,
      "learning_rate": 6.86964795432921e-05,
      "loss": 0.0862,
      "step": 1390
    },
    {
      "epoch": 1.9744499645138396,
      "grad_norm": 0.1329689621925354,
      "learning_rate": 6.860133206470028e-05,
      "loss": 0.0801,
      "step": 1391
    },
    {
      "epoch": 1.9758694109297374,
      "grad_norm": 0.13191333413124084,
      "learning_rate": 6.850618458610847e-05,
      "loss": 0.0764,
      "step": 1392
    },
    {
      "epoch": 1.977288857345635,
      "grad_norm": 0.15144743025302887,
      "learning_rate": 6.841103710751665e-05,
      "loss": 0.0892,
      "step": 1393
    },
    {
      "epoch": 1.978708303761533,
      "grad_norm": 0.17563456296920776,
      "learning_rate": 6.831588962892483e-05,
      "loss": 0.0862,
      "step": 1394
    },
    {
      "epoch": 1.9801277501774308,
      "grad_norm": 0.11935578286647797,
      "learning_rate": 6.822074215033302e-05,
      "loss": 0.0707,
      "step": 1395
    },
    {
      "epoch": 1.9815471965933287,
      "grad_norm": 0.14608460664749146,
      "learning_rate": 6.81255946717412e-05,
      "loss": 0.077,
      "step": 1396
    },
    {
      "epoch": 1.9829666430092265,
      "grad_norm": 0.14016255736351013,
      "learning_rate": 6.803044719314938e-05,
      "loss": 0.0695,
      "step": 1397
    },
    {
      "epoch": 1.9843860894251242,
      "grad_norm": 0.13223499059677124,
      "learning_rate": 6.793529971455757e-05,
      "loss": 0.0694,
      "step": 1398
    },
    {
      "epoch": 1.985805535841022,
      "grad_norm": 0.14191453158855438,
      "learning_rate": 6.784015223596575e-05,
      "loss": 0.0696,
      "step": 1399
    },
    {
      "epoch": 1.9872249822569197,
      "grad_norm": 0.13430748879909515,
      "learning_rate": 6.774500475737393e-05,
      "loss": 0.0722,
      "step": 1400
    },
    {
      "epoch": 1.9886444286728175,
      "grad_norm": 0.13002027571201324,
      "learning_rate": 6.764985727878212e-05,
      "loss": 0.0773,
      "step": 1401
    },
    {
      "epoch": 1.9900638750887154,
      "grad_norm": 0.09762473404407501,
      "learning_rate": 6.75547098001903e-05,
      "loss": 0.067,
      "step": 1402
    },
    {
      "epoch": 1.9914833215046133,
      "grad_norm": 0.08847501873970032,
      "learning_rate": 6.745956232159848e-05,
      "loss": 0.0559,
      "step": 1403
    },
    {
      "epoch": 1.9929027679205111,
      "grad_norm": 0.0938560739159584,
      "learning_rate": 6.736441484300667e-05,
      "loss": 0.0486,
      "step": 1404
    },
    {
      "epoch": 1.9943222143364088,
      "grad_norm": 0.22236356139183044,
      "learning_rate": 6.726926736441485e-05,
      "loss": 0.1494,
      "step": 1405
    },
    {
      "epoch": 1.9957416607523066,
      "grad_norm": 0.2791394293308258,
      "learning_rate": 6.717411988582303e-05,
      "loss": 0.1452,
      "step": 1406
    },
    {
      "epoch": 1.9971611071682043,
      "grad_norm": 0.2048097550868988,
      "learning_rate": 6.707897240723122e-05,
      "loss": 0.1144,
      "step": 1407
    },
    {
      "epoch": 1.9985805535841021,
      "grad_norm": 0.12779255211353302,
      "learning_rate": 6.698382492863939e-05,
      "loss": 0.0732,
      "step": 1408
    },
    {
      "epoch": 2.0,
      "grad_norm": 0.13225574791431427,
      "learning_rate": 6.688867745004757e-05,
      "loss": 0.0541,
      "step": 1409
    },
    {
      "epoch": 2.001419446415898,
      "grad_norm": 0.3866910934448242,
      "learning_rate": 6.679352997145575e-05,
      "loss": 0.2905,
      "step": 1410
    },
    {
      "epoch": 2.0028388928317957,
      "grad_norm": 0.3406664729118347,
      "learning_rate": 6.669838249286394e-05,
      "loss": 0.2261,
      "step": 1411
    },
    {
      "epoch": 2.0042583392476936,
      "grad_norm": 0.37865588068962097,
      "learning_rate": 6.660323501427212e-05,
      "loss": 0.1968,
      "step": 1412
    },
    {
      "epoch": 2.005677785663591,
      "grad_norm": 0.31208136677742004,
      "learning_rate": 6.65080875356803e-05,
      "loss": 0.1922,
      "step": 1413
    },
    {
      "epoch": 2.007097232079489,
      "grad_norm": 0.32101890444755554,
      "learning_rate": 6.641294005708849e-05,
      "loss": 0.1495,
      "step": 1414
    },
    {
      "epoch": 2.0085166784953867,
      "grad_norm": 0.5393568277359009,
      "learning_rate": 6.631779257849667e-05,
      "loss": 0.1641,
      "step": 1415
    },
    {
      "epoch": 2.0099361249112846,
      "grad_norm": 0.3828626573085785,
      "learning_rate": 6.622264509990485e-05,
      "loss": 0.1501,
      "step": 1416
    },
    {
      "epoch": 2.0113555713271825,
      "grad_norm": 0.46871960163116455,
      "learning_rate": 6.612749762131304e-05,
      "loss": 0.1922,
      "step": 1417
    },
    {
      "epoch": 2.0127750177430803,
      "grad_norm": 0.6728284955024719,
      "learning_rate": 6.603235014272122e-05,
      "loss": 0.1654,
      "step": 1418
    },
    {
      "epoch": 2.014194464158978,
      "grad_norm": 0.3925950825214386,
      "learning_rate": 6.59372026641294e-05,
      "loss": 0.1323,
      "step": 1419
    },
    {
      "epoch": 2.0156139105748756,
      "grad_norm": 0.3322359323501587,
      "learning_rate": 6.584205518553759e-05,
      "loss": 0.1175,
      "step": 1420
    },
    {
      "epoch": 2.0170333569907735,
      "grad_norm": 0.3908604085445404,
      "learning_rate": 6.574690770694577e-05,
      "loss": 0.1299,
      "step": 1421
    },
    {
      "epoch": 2.0184528034066713,
      "grad_norm": 0.27828076481819153,
      "learning_rate": 6.565176022835395e-05,
      "loss": 0.1299,
      "step": 1422
    },
    {
      "epoch": 2.019872249822569,
      "grad_norm": 0.2852488160133362,
      "learning_rate": 6.555661274976214e-05,
      "loss": 0.1226,
      "step": 1423
    },
    {
      "epoch": 2.021291696238467,
      "grad_norm": 0.3594944477081299,
      "learning_rate": 6.546146527117032e-05,
      "loss": 0.1377,
      "step": 1424
    },
    {
      "epoch": 2.022711142654365,
      "grad_norm": 0.25570937991142273,
      "learning_rate": 6.53663177925785e-05,
      "loss": 0.1072,
      "step": 1425
    },
    {
      "epoch": 2.0241305890702628,
      "grad_norm": 0.2888139486312866,
      "learning_rate": 6.527117031398669e-05,
      "loss": 0.1103,
      "step": 1426
    },
    {
      "epoch": 2.02555003548616,
      "grad_norm": 0.263323575258255,
      "learning_rate": 6.517602283539487e-05,
      "loss": 0.1001,
      "step": 1427
    },
    {
      "epoch": 2.026969481902058,
      "grad_norm": 0.30023112893104553,
      "learning_rate": 6.508087535680305e-05,
      "loss": 0.1296,
      "step": 1428
    },
    {
      "epoch": 2.028388928317956,
      "grad_norm": 0.23492954671382904,
      "learning_rate": 6.498572787821124e-05,
      "loss": 0.1105,
      "step": 1429
    },
    {
      "epoch": 2.029808374733854,
      "grad_norm": 0.22969822585582733,
      "learning_rate": 6.489058039961942e-05,
      "loss": 0.0858,
      "step": 1430
    },
    {
      "epoch": 2.0312278211497516,
      "grad_norm": 0.21866445243358612,
      "learning_rate": 6.47954329210276e-05,
      "loss": 0.0928,
      "step": 1431
    },
    {
      "epoch": 2.0326472675656495,
      "grad_norm": 0.25528761744499207,
      "learning_rate": 6.470028544243579e-05,
      "loss": 0.1064,
      "step": 1432
    },
    {
      "epoch": 2.0340667139815474,
      "grad_norm": 0.1804562360048294,
      "learning_rate": 6.460513796384396e-05,
      "loss": 0.098,
      "step": 1433
    },
    {
      "epoch": 2.035486160397445,
      "grad_norm": 0.23018378019332886,
      "learning_rate": 6.450999048525214e-05,
      "loss": 0.0955,
      "step": 1434
    },
    {
      "epoch": 2.0369056068133427,
      "grad_norm": 0.18864093720912933,
      "learning_rate": 6.441484300666032e-05,
      "loss": 0.0766,
      "step": 1435
    },
    {
      "epoch": 2.0383250532292405,
      "grad_norm": 0.2762451767921448,
      "learning_rate": 6.431969552806851e-05,
      "loss": 0.0941,
      "step": 1436
    },
    {
      "epoch": 2.0397444996451384,
      "grad_norm": 0.23403623700141907,
      "learning_rate": 6.422454804947669e-05,
      "loss": 0.0857,
      "step": 1437
    },
    {
      "epoch": 2.0411639460610362,
      "grad_norm": 0.17354239523410797,
      "learning_rate": 6.412940057088487e-05,
      "loss": 0.076,
      "step": 1438
    },
    {
      "epoch": 2.042583392476934,
      "grad_norm": 0.2016811966896057,
      "learning_rate": 6.403425309229306e-05,
      "loss": 0.0817,
      "step": 1439
    },
    {
      "epoch": 2.044002838892832,
      "grad_norm": 0.2101217359304428,
      "learning_rate": 6.393910561370124e-05,
      "loss": 0.0827,
      "step": 1440
    },
    {
      "epoch": 2.0454222853087294,
      "grad_norm": 0.1535053253173828,
      "learning_rate": 6.384395813510942e-05,
      "loss": 0.0738,
      "step": 1441
    },
    {
      "epoch": 2.0468417317246272,
      "grad_norm": 0.16173303127288818,
      "learning_rate": 6.374881065651761e-05,
      "loss": 0.0766,
      "step": 1442
    },
    {
      "epoch": 2.048261178140525,
      "grad_norm": 0.23462478816509247,
      "learning_rate": 6.365366317792579e-05,
      "loss": 0.0859,
      "step": 1443
    },
    {
      "epoch": 2.049680624556423,
      "grad_norm": 0.18252162635326385,
      "learning_rate": 6.355851569933397e-05,
      "loss": 0.0847,
      "step": 1444
    },
    {
      "epoch": 2.051100070972321,
      "grad_norm": 0.126511812210083,
      "learning_rate": 6.346336822074216e-05,
      "loss": 0.0621,
      "step": 1445
    },
    {
      "epoch": 2.0525195173882187,
      "grad_norm": 0.20081031322479248,
      "learning_rate": 6.336822074215034e-05,
      "loss": 0.0732,
      "step": 1446
    },
    {
      "epoch": 2.0539389638041166,
      "grad_norm": 0.16578300297260284,
      "learning_rate": 6.327307326355851e-05,
      "loss": 0.0666,
      "step": 1447
    },
    {
      "epoch": 2.055358410220014,
      "grad_norm": 0.14992602169513702,
      "learning_rate": 6.31779257849667e-05,
      "loss": 0.0645,
      "step": 1448
    },
    {
      "epoch": 2.056777856635912,
      "grad_norm": 0.22030863165855408,
      "learning_rate": 6.308277830637488e-05,
      "loss": 0.069,
      "step": 1449
    },
    {
      "epoch": 2.0581973030518097,
      "grad_norm": 0.17958520352840424,
      "learning_rate": 6.298763082778306e-05,
      "loss": 0.065,
      "step": 1450
    },
    {
      "epoch": 2.0596167494677076,
      "grad_norm": 0.2740776240825653,
      "learning_rate": 6.289248334919124e-05,
      "loss": 0.0686,
      "step": 1451
    },
    {
      "epoch": 2.0610361958836054,
      "grad_norm": 0.16436436772346497,
      "learning_rate": 6.279733587059943e-05,
      "loss": 0.0804,
      "step": 1452
    },
    {
      "epoch": 2.0624556422995033,
      "grad_norm": 0.14773011207580566,
      "learning_rate": 6.270218839200761e-05,
      "loss": 0.0585,
      "step": 1453
    },
    {
      "epoch": 2.063875088715401,
      "grad_norm": 0.20657142996788025,
      "learning_rate": 6.26070409134158e-05,
      "loss": 0.0738,
      "step": 1454
    },
    {
      "epoch": 2.065294535131299,
      "grad_norm": 0.1689532846212387,
      "learning_rate": 6.251189343482398e-05,
      "loss": 0.0721,
      "step": 1455
    },
    {
      "epoch": 2.0667139815471964,
      "grad_norm": 0.14074765145778656,
      "learning_rate": 6.241674595623216e-05,
      "loss": 0.0631,
      "step": 1456
    },
    {
      "epoch": 2.0681334279630943,
      "grad_norm": 0.1300768107175827,
      "learning_rate": 6.232159847764035e-05,
      "loss": 0.0574,
      "step": 1457
    },
    {
      "epoch": 2.069552874378992,
      "grad_norm": 0.10497155785560608,
      "learning_rate": 6.222645099904853e-05,
      "loss": 0.0501,
      "step": 1458
    },
    {
      "epoch": 2.07097232079489,
      "grad_norm": 0.1143781989812851,
      "learning_rate": 6.213130352045671e-05,
      "loss": 0.0493,
      "step": 1459
    },
    {
      "epoch": 2.072391767210788,
      "grad_norm": 0.5024269819259644,
      "learning_rate": 6.20361560418649e-05,
      "loss": 0.257,
      "step": 1460
    },
    {
      "epoch": 2.0738112136266857,
      "grad_norm": 0.5130751729011536,
      "learning_rate": 6.194100856327308e-05,
      "loss": 0.2024,
      "step": 1461
    },
    {
      "epoch": 2.0752306600425836,
      "grad_norm": 0.3886708915233612,
      "learning_rate": 6.184586108468126e-05,
      "loss": 0.143,
      "step": 1462
    },
    {
      "epoch": 2.076650106458481,
      "grad_norm": 0.3901277780532837,
      "learning_rate": 6.175071360608945e-05,
      "loss": 0.1508,
      "step": 1463
    },
    {
      "epoch": 2.078069552874379,
      "grad_norm": 0.4594157636165619,
      "learning_rate": 6.165556612749762e-05,
      "loss": 0.1812,
      "step": 1464
    },
    {
      "epoch": 2.0794889992902768,
      "grad_norm": 0.6469210386276245,
      "learning_rate": 6.15604186489058e-05,
      "loss": 0.1675,
      "step": 1465
    },
    {
      "epoch": 2.0809084457061746,
      "grad_norm": 0.4650428295135498,
      "learning_rate": 6.146527117031398e-05,
      "loss": 0.1685,
      "step": 1466
    },
    {
      "epoch": 2.0823278921220725,
      "grad_norm": 0.400772362947464,
      "learning_rate": 6.137012369172217e-05,
      "loss": 0.1483,
      "step": 1467
    },
    {
      "epoch": 2.0837473385379703,
      "grad_norm": 0.6909211874008179,
      "learning_rate": 6.127497621313035e-05,
      "loss": 0.1626,
      "step": 1468
    },
    {
      "epoch": 2.085166784953868,
      "grad_norm": 0.45928695797920227,
      "learning_rate": 6.117982873453853e-05,
      "loss": 0.1328,
      "step": 1469
    },
    {
      "epoch": 2.0865862313697656,
      "grad_norm": 0.3342830240726471,
      "learning_rate": 6.108468125594672e-05,
      "loss": 0.136,
      "step": 1470
    },
    {
      "epoch": 2.0880056777856635,
      "grad_norm": 0.43438857793807983,
      "learning_rate": 6.09895337773549e-05,
      "loss": 0.1558,
      "step": 1471
    },
    {
      "epoch": 2.0894251242015613,
      "grad_norm": 0.26334458589553833,
      "learning_rate": 6.089438629876308e-05,
      "loss": 0.1253,
      "step": 1472
    },
    {
      "epoch": 2.090844570617459,
      "grad_norm": 0.2950877249240875,
      "learning_rate": 6.0799238820171265e-05,
      "loss": 0.1158,
      "step": 1473
    },
    {
      "epoch": 2.092264017033357,
      "grad_norm": 0.37877851724624634,
      "learning_rate": 6.070409134157945e-05,
      "loss": 0.105,
      "step": 1474
    },
    {
      "epoch": 2.093683463449255,
      "grad_norm": 0.3390577733516693,
      "learning_rate": 6.060894386298763e-05,
      "loss": 0.1228,
      "step": 1475
    },
    {
      "epoch": 2.095102909865153,
      "grad_norm": 0.26333940029144287,
      "learning_rate": 6.0513796384395815e-05,
      "loss": 0.1104,
      "step": 1476
    },
    {
      "epoch": 2.09652235628105,
      "grad_norm": 0.2828284502029419,
      "learning_rate": 6.0418648905804e-05,
      "loss": 0.0903,
      "step": 1477
    },
    {
      "epoch": 2.097941802696948,
      "grad_norm": 0.2868214547634125,
      "learning_rate": 6.032350142721218e-05,
      "loss": 0.1147,
      "step": 1478
    },
    {
      "epoch": 2.099361249112846,
      "grad_norm": 0.31874406337738037,
      "learning_rate": 6.0228353948620366e-05,
      "loss": 0.0999,
      "step": 1479
    },
    {
      "epoch": 2.100780695528744,
      "grad_norm": 0.40906596183776855,
      "learning_rate": 6.013320647002855e-05,
      "loss": 0.1093,
      "step": 1480
    },
    {
      "epoch": 2.1022001419446417,
      "grad_norm": 0.3417215943336487,
      "learning_rate": 6.003805899143673e-05,
      "loss": 0.1045,
      "step": 1481
    },
    {
      "epoch": 2.1036195883605395,
      "grad_norm": 0.3793365955352783,
      "learning_rate": 5.9942911512844916e-05,
      "loss": 0.1076,
      "step": 1482
    },
    {
      "epoch": 2.1050390347764374,
      "grad_norm": 0.31575435400009155,
      "learning_rate": 5.984776403425309e-05,
      "loss": 0.0935,
      "step": 1483
    },
    {
      "epoch": 2.106458481192335,
      "grad_norm": 0.20128384232521057,
      "learning_rate": 5.9752616555661276e-05,
      "loss": 0.0912,
      "step": 1484
    },
    {
      "epoch": 2.1078779276082327,
      "grad_norm": 0.3357791006565094,
      "learning_rate": 5.965746907706946e-05,
      "loss": 0.1113,
      "step": 1485
    },
    {
      "epoch": 2.1092973740241305,
      "grad_norm": 0.2465868890285492,
      "learning_rate": 5.956232159847764e-05,
      "loss": 0.0828,
      "step": 1486
    },
    {
      "epoch": 2.1107168204400284,
      "grad_norm": 0.23431675136089325,
      "learning_rate": 5.9467174119885826e-05,
      "loss": 0.0887,
      "step": 1487
    },
    {
      "epoch": 2.1121362668559263,
      "grad_norm": 0.24793821573257446,
      "learning_rate": 5.937202664129401e-05,
      "loss": 0.0965,
      "step": 1488
    },
    {
      "epoch": 2.113555713271824,
      "grad_norm": 0.6035594344139099,
      "learning_rate": 5.927687916270219e-05,
      "loss": 0.0924,
      "step": 1489
    },
    {
      "epoch": 2.114975159687722,
      "grad_norm": 0.21020185947418213,
      "learning_rate": 5.9181731684110376e-05,
      "loss": 0.0842,
      "step": 1490
    },
    {
      "epoch": 2.1163946061036194,
      "grad_norm": 0.21727967262268066,
      "learning_rate": 5.908658420551856e-05,
      "loss": 0.0922,
      "step": 1491
    },
    {
      "epoch": 2.1178140525195173,
      "grad_norm": 0.19486652314662933,
      "learning_rate": 5.899143672692674e-05,
      "loss": 0.0768,
      "step": 1492
    },
    {
      "epoch": 2.119233498935415,
      "grad_norm": 0.18109843134880066,
      "learning_rate": 5.8896289248334926e-05,
      "loss": 0.0716,
      "step": 1493
    },
    {
      "epoch": 2.120652945351313,
      "grad_norm": 0.16911660134792328,
      "learning_rate": 5.880114176974311e-05,
      "loss": 0.0737,
      "step": 1494
    },
    {
      "epoch": 2.122072391767211,
      "grad_norm": 0.1580275148153305,
      "learning_rate": 5.870599429115129e-05,
      "loss": 0.0639,
      "step": 1495
    },
    {
      "epoch": 2.1234918381831087,
      "grad_norm": 0.1769164651632309,
      "learning_rate": 5.861084681255947e-05,
      "loss": 0.0659,
      "step": 1496
    },
    {
      "epoch": 2.1249112845990066,
      "grad_norm": 0.15815311670303345,
      "learning_rate": 5.851569933396765e-05,
      "loss": 0.068,
      "step": 1497
    },
    {
      "epoch": 2.126330731014904,
      "grad_norm": 0.1202741339802742,
      "learning_rate": 5.8420551855375836e-05,
      "loss": 0.0561,
      "step": 1498
    },
    {
      "epoch": 2.127750177430802,
      "grad_norm": 0.17753197252750397,
      "learning_rate": 5.832540437678402e-05,
      "loss": 0.0707,
      "step": 1499
    },
    {
      "epoch": 2.1291696238466997,
      "grad_norm": 0.14884261786937714,
      "learning_rate": 5.82302568981922e-05,
      "loss": 0.0624,
      "step": 1500
    },
    {
      "epoch": 2.1305890702625976,
      "grad_norm": 0.16520223021507263,
      "learning_rate": 5.8135109419600386e-05,
      "loss": 0.072,
      "step": 1501
    },
    {
      "epoch": 2.1320085166784954,
      "grad_norm": 0.14692355692386627,
      "learning_rate": 5.803996194100857e-05,
      "loss": 0.068,
      "step": 1502
    },
    {
      "epoch": 2.1334279630943933,
      "grad_norm": 0.1469438672065735,
      "learning_rate": 5.794481446241675e-05,
      "loss": 0.0656,
      "step": 1503
    },
    {
      "epoch": 2.134847409510291,
      "grad_norm": 0.12021106481552124,
      "learning_rate": 5.7849666983824936e-05,
      "loss": 0.0553,
      "step": 1504
    },
    {
      "epoch": 2.1362668559261886,
      "grad_norm": 0.13258452713489532,
      "learning_rate": 5.775451950523312e-05,
      "loss": 0.0599,
      "step": 1505
    },
    {
      "epoch": 2.1376863023420865,
      "grad_norm": 0.11290636658668518,
      "learning_rate": 5.76593720266413e-05,
      "loss": 0.0529,
      "step": 1506
    },
    {
      "epoch": 2.1391057487579843,
      "grad_norm": 0.14991694688796997,
      "learning_rate": 5.7564224548049486e-05,
      "loss": 0.0664,
      "step": 1507
    },
    {
      "epoch": 2.140525195173882,
      "grad_norm": 0.14096342027187347,
      "learning_rate": 5.746907706945767e-05,
      "loss": 0.0691,
      "step": 1508
    },
    {
      "epoch": 2.14194464158978,
      "grad_norm": 0.11452538520097733,
      "learning_rate": 5.7373929590865846e-05,
      "loss": 0.05,
      "step": 1509
    },
    {
      "epoch": 2.143364088005678,
      "grad_norm": 0.6343727707862854,
      "learning_rate": 5.727878211227403e-05,
      "loss": 0.2659,
      "step": 1510
    },
    {
      "epoch": 2.1447835344215758,
      "grad_norm": 0.5417853593826294,
      "learning_rate": 5.718363463368221e-05,
      "loss": 0.1905,
      "step": 1511
    },
    {
      "epoch": 2.146202980837473,
      "grad_norm": 0.614004373550415,
      "learning_rate": 5.7088487155090396e-05,
      "loss": 0.1933,
      "step": 1512
    },
    {
      "epoch": 2.147622427253371,
      "grad_norm": 0.481158971786499,
      "learning_rate": 5.699333967649858e-05,
      "loss": 0.1944,
      "step": 1513
    },
    {
      "epoch": 2.149041873669269,
      "grad_norm": 0.45506924390792847,
      "learning_rate": 5.689819219790676e-05,
      "loss": 0.178,
      "step": 1514
    },
    {
      "epoch": 2.1504613200851668,
      "grad_norm": 0.46817296743392944,
      "learning_rate": 5.6803044719314946e-05,
      "loss": 0.1898,
      "step": 1515
    },
    {
      "epoch": 2.1518807665010646,
      "grad_norm": 0.44854775071144104,
      "learning_rate": 5.670789724072313e-05,
      "loss": 0.1895,
      "step": 1516
    },
    {
      "epoch": 2.1533002129169625,
      "grad_norm": 0.42101776599884033,
      "learning_rate": 5.66127497621313e-05,
      "loss": 0.1588,
      "step": 1517
    },
    {
      "epoch": 2.1547196593328604,
      "grad_norm": 0.33724406361579895,
      "learning_rate": 5.651760228353948e-05,
      "loss": 0.1412,
      "step": 1518
    },
    {
      "epoch": 2.156139105748758,
      "grad_norm": 0.3441827893257141,
      "learning_rate": 5.6422454804947666e-05,
      "loss": 0.1514,
      "step": 1519
    },
    {
      "epoch": 2.1575585521646556,
      "grad_norm": 0.29800960421562195,
      "learning_rate": 5.632730732635585e-05,
      "loss": 0.1216,
      "step": 1520
    },
    {
      "epoch": 2.1589779985805535,
      "grad_norm": 0.27453649044036865,
      "learning_rate": 5.623215984776403e-05,
      "loss": 0.1376,
      "step": 1521
    },
    {
      "epoch": 2.1603974449964514,
      "grad_norm": 0.338130384683609,
      "learning_rate": 5.6137012369172216e-05,
      "loss": 0.1208,
      "step": 1522
    },
    {
      "epoch": 2.1618168914123492,
      "grad_norm": 0.3001154959201813,
      "learning_rate": 5.60418648905804e-05,
      "loss": 0.1141,
      "step": 1523
    },
    {
      "epoch": 2.163236337828247,
      "grad_norm": 0.27465197443962097,
      "learning_rate": 5.594671741198858e-05,
      "loss": 0.1283,
      "step": 1524
    },
    {
      "epoch": 2.164655784244145,
      "grad_norm": 0.3017851412296295,
      "learning_rate": 5.5851569933396766e-05,
      "loss": 0.109,
      "step": 1525
    },
    {
      "epoch": 2.1660752306600424,
      "grad_norm": 0.25549665093421936,
      "learning_rate": 5.575642245480495e-05,
      "loss": 0.1122,
      "step": 1526
    },
    {
      "epoch": 2.1674946770759402,
      "grad_norm": 0.2627529799938202,
      "learning_rate": 5.5661274976213126e-05,
      "loss": 0.108,
      "step": 1527
    },
    {
      "epoch": 2.168914123491838,
      "grad_norm": 0.26205170154571533,
      "learning_rate": 5.556612749762131e-05,
      "loss": 0.1273,
      "step": 1528
    },
    {
      "epoch": 2.170333569907736,
      "grad_norm": 0.24484708905220032,
      "learning_rate": 5.547098001902949e-05,
      "loss": 0.1047,
      "step": 1529
    },
    {
      "epoch": 2.171753016323634,
      "grad_norm": 0.25989964604377747,
      "learning_rate": 5.5375832540437676e-05,
      "loss": 0.1144,
      "step": 1530
    },
    {
      "epoch": 2.1731724627395317,
      "grad_norm": 0.23393355309963226,
      "learning_rate": 5.528068506184586e-05,
      "loss": 0.099,
      "step": 1531
    },
    {
      "epoch": 2.1745919091554295,
      "grad_norm": 0.2543446123600006,
      "learning_rate": 5.518553758325404e-05,
      "loss": 0.1025,
      "step": 1532
    },
    {
      "epoch": 2.176011355571327,
      "grad_norm": 0.23105141520500183,
      "learning_rate": 5.5090390104662226e-05,
      "loss": 0.0979,
      "step": 1533
    },
    {
      "epoch": 2.177430801987225,
      "grad_norm": 0.2660439908504486,
      "learning_rate": 5.499524262607041e-05,
      "loss": 0.0975,
      "step": 1534
    },
    {
      "epoch": 2.1788502484031227,
      "grad_norm": 0.22402983903884888,
      "learning_rate": 5.490009514747859e-05,
      "loss": 0.0898,
      "step": 1535
    },
    {
      "epoch": 2.1802696948190206,
      "grad_norm": 0.24268898367881775,
      "learning_rate": 5.4804947668886776e-05,
      "loss": 0.075,
      "step": 1536
    },
    {
      "epoch": 2.1816891412349184,
      "grad_norm": 0.2506079375743866,
      "learning_rate": 5.470980019029496e-05,
      "loss": 0.099,
      "step": 1537
    },
    {
      "epoch": 2.1831085876508163,
      "grad_norm": 0.19745281338691711,
      "learning_rate": 5.461465271170314e-05,
      "loss": 0.0837,
      "step": 1538
    },
    {
      "epoch": 2.184528034066714,
      "grad_norm": 0.19891826808452606,
      "learning_rate": 5.451950523311132e-05,
      "loss": 0.0822,
      "step": 1539
    },
    {
      "epoch": 2.1859474804826116,
      "grad_norm": 0.22622480988502502,
      "learning_rate": 5.44243577545195e-05,
      "loss": 0.0896,
      "step": 1540
    },
    {
      "epoch": 2.1873669268985094,
      "grad_norm": 0.19163843989372253,
      "learning_rate": 5.4329210275927687e-05,
      "loss": 0.0831,
      "step": 1541
    },
    {
      "epoch": 2.1887863733144073,
      "grad_norm": 0.2028207778930664,
      "learning_rate": 5.423406279733587e-05,
      "loss": 0.0759,
      "step": 1542
    },
    {
      "epoch": 2.190205819730305,
      "grad_norm": 0.19864995777606964,
      "learning_rate": 5.413891531874405e-05,
      "loss": 0.0754,
      "step": 1543
    },
    {
      "epoch": 2.191625266146203,
      "grad_norm": 0.20050279796123505,
      "learning_rate": 5.404376784015224e-05,
      "loss": 0.0875,
      "step": 1544
    },
    {
      "epoch": 2.193044712562101,
      "grad_norm": 0.23595067858695984,
      "learning_rate": 5.394862036156042e-05,
      "loss": 0.0761,
      "step": 1545
    },
    {
      "epoch": 2.1944641589779987,
      "grad_norm": 0.19147136807441711,
      "learning_rate": 5.38534728829686e-05,
      "loss": 0.08,
      "step": 1546
    },
    {
      "epoch": 2.195883605393896,
      "grad_norm": 0.18176569044589996,
      "learning_rate": 5.375832540437679e-05,
      "loss": 0.0801,
      "step": 1547
    },
    {
      "epoch": 2.197303051809794,
      "grad_norm": 0.1513565331697464,
      "learning_rate": 5.366317792578497e-05,
      "loss": 0.0707,
      "step": 1548
    },
    {
      "epoch": 2.198722498225692,
      "grad_norm": 0.14546626806259155,
      "learning_rate": 5.3568030447193153e-05,
      "loss": 0.0669,
      "step": 1549
    },
    {
      "epoch": 2.2001419446415897,
      "grad_norm": 0.1528206765651703,
      "learning_rate": 5.347288296860134e-05,
      "loss": 0.0781,
      "step": 1550
    },
    {
      "epoch": 2.2015613910574876,
      "grad_norm": 0.18386796116828918,
      "learning_rate": 5.337773549000952e-05,
      "loss": 0.0777,
      "step": 1551
    },
    {
      "epoch": 2.2029808374733855,
      "grad_norm": 0.15282951295375824,
      "learning_rate": 5.32825880114177e-05,
      "loss": 0.0693,
      "step": 1552
    },
    {
      "epoch": 2.2044002838892833,
      "grad_norm": 0.1520063728094101,
      "learning_rate": 5.318744053282588e-05,
      "loss": 0.0613,
      "step": 1553
    },
    {
      "epoch": 2.2058197303051807,
      "grad_norm": 0.1768648475408554,
      "learning_rate": 5.3092293054234063e-05,
      "loss": 0.0655,
      "step": 1554
    },
    {
      "epoch": 2.2072391767210786,
      "grad_norm": 0.1228799894452095,
      "learning_rate": 5.299714557564225e-05,
      "loss": 0.0624,
      "step": 1555
    },
    {
      "epoch": 2.2086586231369765,
      "grad_norm": 0.1470033824443817,
      "learning_rate": 5.290199809705043e-05,
      "loss": 0.0682,
      "step": 1556
    },
    {
      "epoch": 2.2100780695528743,
      "grad_norm": 0.14107364416122437,
      "learning_rate": 5.2806850618458614e-05,
      "loss": 0.0602,
      "step": 1557
    },
    {
      "epoch": 2.211497515968772,
      "grad_norm": 0.1307341605424881,
      "learning_rate": 5.27117031398668e-05,
      "loss": 0.0641,
      "step": 1558
    },
    {
      "epoch": 2.21291696238467,
      "grad_norm": 0.11123425513505936,
      "learning_rate": 5.261655566127498e-05,
      "loss": 0.0563,
      "step": 1559
    },
    {
      "epoch": 2.214336408800568,
      "grad_norm": 0.6185559034347534,
      "learning_rate": 5.2521408182683164e-05,
      "loss": 0.2,
      "step": 1560
    },
    {
      "epoch": 2.215755855216466,
      "grad_norm": 0.5600557923316956,
      "learning_rate": 5.242626070409135e-05,
      "loss": 0.1939,
      "step": 1561
    },
    {
      "epoch": 2.217175301632363,
      "grad_norm": 0.6036210656166077,
      "learning_rate": 5.233111322549953e-05,
      "loss": 0.1692,
      "step": 1562
    },
    {
      "epoch": 2.218594748048261,
      "grad_norm": 0.5486056208610535,
      "learning_rate": 5.2235965746907714e-05,
      "loss": 0.1751,
      "step": 1563
    },
    {
      "epoch": 2.220014194464159,
      "grad_norm": 0.49377888441085815,
      "learning_rate": 5.21408182683159e-05,
      "loss": 0.1695,
      "step": 1564
    },
    {
      "epoch": 2.221433640880057,
      "grad_norm": 0.5517827868461609,
      "learning_rate": 5.2045670789724074e-05,
      "loss": 0.1898,
      "step": 1565
    },
    {
      "epoch": 2.2228530872959547,
      "grad_norm": 0.4934732913970947,
      "learning_rate": 5.195052331113226e-05,
      "loss": 0.1371,
      "step": 1566
    },
    {
      "epoch": 2.2242725337118525,
      "grad_norm": 0.5168126225471497,
      "learning_rate": 5.185537583254044e-05,
      "loss": 0.1666,
      "step": 1567
    },
    {
      "epoch": 2.2256919801277504,
      "grad_norm": 0.539000928401947,
      "learning_rate": 5.1760228353948624e-05,
      "loss": 0.1399,
      "step": 1568
    },
    {
      "epoch": 2.227111426543648,
      "grad_norm": 0.40716490149497986,
      "learning_rate": 5.166508087535681e-05,
      "loss": 0.1588,
      "step": 1569
    },
    {
      "epoch": 2.2285308729595457,
      "grad_norm": 0.40496841073036194,
      "learning_rate": 5.156993339676499e-05,
      "loss": 0.1698,
      "step": 1570
    },
    {
      "epoch": 2.2299503193754435,
      "grad_norm": 0.39922451972961426,
      "learning_rate": 5.1474785918173174e-05,
      "loss": 0.1561,
      "step": 1571
    },
    {
      "epoch": 2.2313697657913414,
      "grad_norm": 0.37973153591156006,
      "learning_rate": 5.137963843958136e-05,
      "loss": 0.1272,
      "step": 1572
    },
    {
      "epoch": 2.2327892122072392,
      "grad_norm": 0.4420809745788574,
      "learning_rate": 5.128449096098954e-05,
      "loss": 0.134,
      "step": 1573
    },
    {
      "epoch": 2.234208658623137,
      "grad_norm": 0.31322282552719116,
      "learning_rate": 5.1189343482397724e-05,
      "loss": 0.1187,
      "step": 1574
    },
    {
      "epoch": 2.235628105039035,
      "grad_norm": 0.32824674248695374,
      "learning_rate": 5.109419600380591e-05,
      "loss": 0.1152,
      "step": 1575
    },
    {
      "epoch": 2.2370475514549324,
      "grad_norm": 0.3399249315261841,
      "learning_rate": 5.099904852521409e-05,
      "loss": 0.1173,
      "step": 1576
    },
    {
      "epoch": 2.2384669978708303,
      "grad_norm": 0.28839772939682007,
      "learning_rate": 5.0903901046622274e-05,
      "loss": 0.1091,
      "step": 1577
    },
    {
      "epoch": 2.239886444286728,
      "grad_norm": 0.3695591688156128,
      "learning_rate": 5.080875356803045e-05,
      "loss": 0.1168,
      "step": 1578
    },
    {
      "epoch": 2.241305890702626,
      "grad_norm": 0.2855490446090698,
      "learning_rate": 5.0713606089438634e-05,
      "loss": 0.1084,
      "step": 1579
    },
    {
      "epoch": 2.242725337118524,
      "grad_norm": 0.2592602074146271,
      "learning_rate": 5.061845861084682e-05,
      "loss": 0.1028,
      "step": 1580
    },
    {
      "epoch": 2.2441447835344217,
      "grad_norm": 0.32652342319488525,
      "learning_rate": 5.0523311132255e-05,
      "loss": 0.1048,
      "step": 1581
    },
    {
      "epoch": 2.2455642299503196,
      "grad_norm": 0.27464383840560913,
      "learning_rate": 5.0428163653663184e-05,
      "loss": 0.0907,
      "step": 1582
    },
    {
      "epoch": 2.246983676366217,
      "grad_norm": 0.279872864484787,
      "learning_rate": 5.033301617507137e-05,
      "loss": 0.122,
      "step": 1583
    },
    {
      "epoch": 2.248403122782115,
      "grad_norm": 0.2881994843482971,
      "learning_rate": 5.023786869647955e-05,
      "loss": 0.0917,
      "step": 1584
    },
    {
      "epoch": 2.2498225691980127,
      "grad_norm": 0.2830841541290283,
      "learning_rate": 5.0142721217887734e-05,
      "loss": 0.1119,
      "step": 1585
    },
    {
      "epoch": 2.2512420156139106,
      "grad_norm": 0.22750380635261536,
      "learning_rate": 5.004757373929592e-05,
      "loss": 0.0774,
      "step": 1586
    },
    {
      "epoch": 2.2526614620298084,
      "grad_norm": 0.27525395154953003,
      "learning_rate": 4.9952426260704094e-05,
      "loss": 0.0872,
      "step": 1587
    },
    {
      "epoch": 2.2540809084457063,
      "grad_norm": 0.2866874635219574,
      "learning_rate": 4.985727878211228e-05,
      "loss": 0.0922,
      "step": 1588
    },
    {
      "epoch": 2.255500354861604,
      "grad_norm": 0.25327765941619873,
      "learning_rate": 4.976213130352046e-05,
      "loss": 0.0849,
      "step": 1589
    },
    {
      "epoch": 2.2569198012775016,
      "grad_norm": 0.41026830673217773,
      "learning_rate": 4.9666983824928644e-05,
      "loss": 0.0957,
      "step": 1590
    },
    {
      "epoch": 2.2583392476933994,
      "grad_norm": 0.24985714256763458,
      "learning_rate": 4.957183634633683e-05,
      "loss": 0.0923,
      "step": 1591
    },
    {
      "epoch": 2.2597586941092973,
      "grad_norm": 0.22608542442321777,
      "learning_rate": 4.947668886774501e-05,
      "loss": 0.08,
      "step": 1592
    },
    {
      "epoch": 2.261178140525195,
      "grad_norm": 0.20788998901844025,
      "learning_rate": 4.938154138915319e-05,
      "loss": 0.0861,
      "step": 1593
    },
    {
      "epoch": 2.262597586941093,
      "grad_norm": 0.18425752222537994,
      "learning_rate": 4.928639391056137e-05,
      "loss": 0.0759,
      "step": 1594
    },
    {
      "epoch": 2.264017033356991,
      "grad_norm": 0.27933061122894287,
      "learning_rate": 4.9191246431969554e-05,
      "loss": 0.0907,
      "step": 1595
    },
    {
      "epoch": 2.2654364797728888,
      "grad_norm": 0.163210928440094,
      "learning_rate": 4.909609895337774e-05,
      "loss": 0.0765,
      "step": 1596
    },
    {
      "epoch": 2.2668559261887866,
      "grad_norm": 0.19876892864704132,
      "learning_rate": 4.900095147478592e-05,
      "loss": 0.0686,
      "step": 1597
    },
    {
      "epoch": 2.268275372604684,
      "grad_norm": 0.18849346041679382,
      "learning_rate": 4.8905803996194104e-05,
      "loss": 0.0734,
      "step": 1598
    },
    {
      "epoch": 2.269694819020582,
      "grad_norm": 0.22756102681159973,
      "learning_rate": 4.881065651760229e-05,
      "loss": 0.0738,
      "step": 1599
    },
    {
      "epoch": 2.2711142654364798,
      "grad_norm": 0.19165663421154022,
      "learning_rate": 4.871550903901047e-05,
      "loss": 0.0658,
      "step": 1600
    },
    {
      "epoch": 2.2725337118523776,
      "grad_norm": 0.17454519867897034,
      "learning_rate": 4.8620361560418654e-05,
      "loss": 0.0737,
      "step": 1601
    },
    {
      "epoch": 2.2739531582682755,
      "grad_norm": 0.13529585301876068,
      "learning_rate": 4.852521408182684e-05,
      "loss": 0.0617,
      "step": 1602
    },
    {
      "epoch": 2.2753726046841733,
      "grad_norm": 0.15160545706748962,
      "learning_rate": 4.843006660323502e-05,
      "loss": 0.0643,
      "step": 1603
    },
    {
      "epoch": 2.276792051100071,
      "grad_norm": 0.15028516948223114,
      "learning_rate": 4.8334919124643204e-05,
      "loss": 0.0565,
      "step": 1604
    },
    {
      "epoch": 2.2782114975159686,
      "grad_norm": 0.1641780287027359,
      "learning_rate": 4.823977164605138e-05,
      "loss": 0.0721,
      "step": 1605
    },
    {
      "epoch": 2.2796309439318665,
      "grad_norm": 0.16615141928195953,
      "learning_rate": 4.8144624167459564e-05,
      "loss": 0.0647,
      "step": 1606
    },
    {
      "epoch": 2.2810503903477644,
      "grad_norm": 0.11363261938095093,
      "learning_rate": 4.804947668886775e-05,
      "loss": 0.0548,
      "step": 1607
    },
    {
      "epoch": 2.282469836763662,
      "grad_norm": 0.0980365127325058,
      "learning_rate": 4.795432921027593e-05,
      "loss": 0.0605,
      "step": 1608
    },
    {
      "epoch": 2.28388928317956,
      "grad_norm": 0.10916942358016968,
      "learning_rate": 4.785918173168411e-05,
      "loss": 0.0664,
      "step": 1609
    },
    {
      "epoch": 2.285308729595458,
      "grad_norm": 0.5204362869262695,
      "learning_rate": 4.776403425309229e-05,
      "loss": 0.2026,
      "step": 1610
    },
    {
      "epoch": 2.286728176011356,
      "grad_norm": 0.4924947917461395,
      "learning_rate": 4.7668886774500474e-05,
      "loss": 0.166,
      "step": 1611
    },
    {
      "epoch": 2.2881476224272532,
      "grad_norm": 0.4985105097293854,
      "learning_rate": 4.757373929590866e-05,
      "loss": 0.1793,
      "step": 1612
    },
    {
      "epoch": 2.289567068843151,
      "grad_norm": 0.46619531512260437,
      "learning_rate": 4.747859181731684e-05,
      "loss": 0.1849,
      "step": 1613
    },
    {
      "epoch": 2.290986515259049,
      "grad_norm": 0.5034202337265015,
      "learning_rate": 4.7383444338725025e-05,
      "loss": 0.1603,
      "step": 1614
    },
    {
      "epoch": 2.292405961674947,
      "grad_norm": 0.4738958179950714,
      "learning_rate": 4.728829686013321e-05,
      "loss": 0.1638,
      "step": 1615
    },
    {
      "epoch": 2.2938254080908447,
      "grad_norm": 0.47847580909729004,
      "learning_rate": 4.719314938154139e-05,
      "loss": 0.1359,
      "step": 1616
    },
    {
      "epoch": 2.2952448545067425,
      "grad_norm": 0.3544195294380188,
      "learning_rate": 4.7098001902949575e-05,
      "loss": 0.1183,
      "step": 1617
    },
    {
      "epoch": 2.2966643009226404,
      "grad_norm": 0.4492340683937073,
      "learning_rate": 4.700285442435776e-05,
      "loss": 0.1631,
      "step": 1618
    },
    {
      "epoch": 2.298083747338538,
      "grad_norm": 0.422797292470932,
      "learning_rate": 4.690770694576594e-05,
      "loss": 0.1714,
      "step": 1619
    },
    {
      "epoch": 2.2995031937544357,
      "grad_norm": 0.44202935695648193,
      "learning_rate": 4.6812559467174125e-05,
      "loss": 0.1552,
      "step": 1620
    },
    {
      "epoch": 2.3009226401703335,
      "grad_norm": 0.38717570900917053,
      "learning_rate": 4.671741198858231e-05,
      "loss": 0.1251,
      "step": 1621
    },
    {
      "epoch": 2.3023420865862314,
      "grad_norm": 0.3259645402431488,
      "learning_rate": 4.6622264509990485e-05,
      "loss": 0.1137,
      "step": 1622
    },
    {
      "epoch": 2.3037615330021293,
      "grad_norm": 0.37970268726348877,
      "learning_rate": 4.652711703139867e-05,
      "loss": 0.1107,
      "step": 1623
    },
    {
      "epoch": 2.305180979418027,
      "grad_norm": 0.34293460845947266,
      "learning_rate": 4.643196955280685e-05,
      "loss": 0.1242,
      "step": 1624
    },
    {
      "epoch": 2.306600425833925,
      "grad_norm": 0.32296255230903625,
      "learning_rate": 4.6336822074215035e-05,
      "loss": 0.1143,
      "step": 1625
    },
    {
      "epoch": 2.3080198722498224,
      "grad_norm": 0.2929137647151947,
      "learning_rate": 4.624167459562322e-05,
      "loss": 0.1113,
      "step": 1626
    },
    {
      "epoch": 2.3094393186657203,
      "grad_norm": 0.2790016829967499,
      "learning_rate": 4.61465271170314e-05,
      "loss": 0.1087,
      "step": 1627
    },
    {
      "epoch": 2.310858765081618,
      "grad_norm": 0.3062804043292999,
      "learning_rate": 4.6051379638439585e-05,
      "loss": 0.1098,
      "step": 1628
    },
    {
      "epoch": 2.312278211497516,
      "grad_norm": 0.29506462812423706,
      "learning_rate": 4.595623215984777e-05,
      "loss": 0.0919,
      "step": 1629
    },
    {
      "epoch": 2.313697657913414,
      "grad_norm": 0.2793644666671753,
      "learning_rate": 4.586108468125595e-05,
      "loss": 0.1108,
      "step": 1630
    },
    {
      "epoch": 2.3151171043293117,
      "grad_norm": 0.31525465846061707,
      "learning_rate": 4.5765937202664135e-05,
      "loss": 0.1268,
      "step": 1631
    },
    {
      "epoch": 2.3165365507452096,
      "grad_norm": 0.306108295917511,
      "learning_rate": 4.567078972407232e-05,
      "loss": 0.099,
      "step": 1632
    },
    {
      "epoch": 2.317955997161107,
      "grad_norm": 0.21604038774967194,
      "learning_rate": 4.55756422454805e-05,
      "loss": 0.0875,
      "step": 1633
    },
    {
      "epoch": 2.319375443577005,
      "grad_norm": 0.2152591049671173,
      "learning_rate": 4.5480494766888685e-05,
      "loss": 0.0756,
      "step": 1634
    },
    {
      "epoch": 2.3207948899929027,
      "grad_norm": 0.23634043335914612,
      "learning_rate": 4.538534728829686e-05,
      "loss": 0.0841,
      "step": 1635
    },
    {
      "epoch": 2.3222143364088006,
      "grad_norm": 0.22665266692638397,
      "learning_rate": 4.5290199809705045e-05,
      "loss": 0.0867,
      "step": 1636
    },
    {
      "epoch": 2.3236337828246985,
      "grad_norm": 0.2879273593425751,
      "learning_rate": 4.519505233111323e-05,
      "loss": 0.1232,
      "step": 1637
    },
    {
      "epoch": 2.3250532292405963,
      "grad_norm": 0.29449066519737244,
      "learning_rate": 4.509990485252141e-05,
      "loss": 0.0909,
      "step": 1638
    },
    {
      "epoch": 2.326472675656494,
      "grad_norm": 0.23596374690532684,
      "learning_rate": 4.5004757373929595e-05,
      "loss": 0.0811,
      "step": 1639
    },
    {
      "epoch": 2.3278921220723916,
      "grad_norm": 0.238552063703537,
      "learning_rate": 4.490960989533777e-05,
      "loss": 0.0756,
      "step": 1640
    },
    {
      "epoch": 2.3293115684882895,
      "grad_norm": 0.34356656670570374,
      "learning_rate": 4.4814462416745955e-05,
      "loss": 0.0883,
      "step": 1641
    },
    {
      "epoch": 2.3307310149041873,
      "grad_norm": 0.1686023473739624,
      "learning_rate": 4.471931493815414e-05,
      "loss": 0.0734,
      "step": 1642
    },
    {
      "epoch": 2.332150461320085,
      "grad_norm": 0.1648300737142563,
      "learning_rate": 4.462416745956232e-05,
      "loss": 0.0767,
      "step": 1643
    },
    {
      "epoch": 2.333569907735983,
      "grad_norm": 0.15329912304878235,
      "learning_rate": 4.4529019980970505e-05,
      "loss": 0.0638,
      "step": 1644
    },
    {
      "epoch": 2.334989354151881,
      "grad_norm": 0.188451886177063,
      "learning_rate": 4.443387250237869e-05,
      "loss": 0.074,
      "step": 1645
    },
    {
      "epoch": 2.3364088005677788,
      "grad_norm": 0.20418265461921692,
      "learning_rate": 4.433872502378687e-05,
      "loss": 0.0706,
      "step": 1646
    },
    {
      "epoch": 2.337828246983676,
      "grad_norm": 0.16717682778835297,
      "learning_rate": 4.4243577545195055e-05,
      "loss": 0.0714,
      "step": 1647
    },
    {
      "epoch": 2.339247693399574,
      "grad_norm": 0.13157010078430176,
      "learning_rate": 4.414843006660324e-05,
      "loss": 0.0625,
      "step": 1648
    },
    {
      "epoch": 2.340667139815472,
      "grad_norm": 0.1485060155391693,
      "learning_rate": 4.405328258801142e-05,
      "loss": 0.0558,
      "step": 1649
    },
    {
      "epoch": 2.34208658623137,
      "grad_norm": 0.17275385558605194,
      "learning_rate": 4.39581351094196e-05,
      "loss": 0.0666,
      "step": 1650
    },
    {
      "epoch": 2.3435060326472676,
      "grad_norm": 0.23959404230117798,
      "learning_rate": 4.386298763082778e-05,
      "loss": 0.0662,
      "step": 1651
    },
    {
      "epoch": 2.3449254790631655,
      "grad_norm": 0.20797523856163025,
      "learning_rate": 4.3767840152235965e-05,
      "loss": 0.0618,
      "step": 1652
    },
    {
      "epoch": 2.3463449254790634,
      "grad_norm": 0.24150420725345612,
      "learning_rate": 4.367269267364415e-05,
      "loss": 0.0717,
      "step": 1653
    },
    {
      "epoch": 2.347764371894961,
      "grad_norm": 0.1462114304304123,
      "learning_rate": 4.357754519505233e-05,
      "loss": 0.0563,
      "step": 1654
    },
    {
      "epoch": 2.3491838183108587,
      "grad_norm": 0.16900897026062012,
      "learning_rate": 4.3482397716460515e-05,
      "loss": 0.0671,
      "step": 1655
    },
    {
      "epoch": 2.3506032647267565,
      "grad_norm": 0.14209772646427155,
      "learning_rate": 4.33872502378687e-05,
      "loss": 0.0654,
      "step": 1656
    },
    {
      "epoch": 2.3520227111426544,
      "grad_norm": 0.2246704250574112,
      "learning_rate": 4.329210275927688e-05,
      "loss": 0.067,
      "step": 1657
    },
    {
      "epoch": 2.3534421575585522,
      "grad_norm": 0.10681483149528503,
      "learning_rate": 4.3196955280685065e-05,
      "loss": 0.0484,
      "step": 1658
    },
    {
      "epoch": 2.35486160397445,
      "grad_norm": 0.10475751012563705,
      "learning_rate": 4.310180780209325e-05,
      "loss": 0.0472,
      "step": 1659
    },
    {
      "epoch": 2.356281050390348,
      "grad_norm": 0.7095282077789307,
      "learning_rate": 4.300666032350143e-05,
      "loss": 0.1996,
      "step": 1660
    },
    {
      "epoch": 2.3577004968062454,
      "grad_norm": 0.5396527051925659,
      "learning_rate": 4.2911512844909615e-05,
      "loss": 0.1822,
      "step": 1661
    },
    {
      "epoch": 2.3591199432221432,
      "grad_norm": 0.5762582421302795,
      "learning_rate": 4.28163653663178e-05,
      "loss": 0.1887,
      "step": 1662
    },
    {
      "epoch": 2.360539389638041,
      "grad_norm": 0.5015017986297607,
      "learning_rate": 4.2721217887725975e-05,
      "loss": 0.1449,
      "step": 1663
    },
    {
      "epoch": 2.361958836053939,
      "grad_norm": 0.5713378190994263,
      "learning_rate": 4.262607040913416e-05,
      "loss": 0.1777,
      "step": 1664
    },
    {
      "epoch": 2.363378282469837,
      "grad_norm": 0.5493751764297485,
      "learning_rate": 4.253092293054234e-05,
      "loss": 0.1841,
      "step": 1665
    },
    {
      "epoch": 2.3647977288857347,
      "grad_norm": 0.48893994092941284,
      "learning_rate": 4.2435775451950525e-05,
      "loss": 0.1329,
      "step": 1666
    },
    {
      "epoch": 2.3662171753016326,
      "grad_norm": 0.44671526551246643,
      "learning_rate": 4.234062797335871e-05,
      "loss": 0.1439,
      "step": 1667
    },
    {
      "epoch": 2.36763662171753,
      "grad_norm": 0.45982763171195984,
      "learning_rate": 4.224548049476689e-05,
      "loss": 0.1613,
      "step": 1668
    },
    {
      "epoch": 2.369056068133428,
      "grad_norm": 0.40598589181900024,
      "learning_rate": 4.2150333016175076e-05,
      "loss": 0.1404,
      "step": 1669
    },
    {
      "epoch": 2.3704755145493257,
      "grad_norm": 0.35256317257881165,
      "learning_rate": 4.205518553758326e-05,
      "loss": 0.1323,
      "step": 1670
    },
    {
      "epoch": 2.3718949609652236,
      "grad_norm": 0.3524557948112488,
      "learning_rate": 4.196003805899144e-05,
      "loss": 0.1148,
      "step": 1671
    },
    {
      "epoch": 2.3733144073811214,
      "grad_norm": 0.37074416875839233,
      "learning_rate": 4.1864890580399626e-05,
      "loss": 0.1271,
      "step": 1672
    },
    {
      "epoch": 2.3747338537970193,
      "grad_norm": 0.4601385295391083,
      "learning_rate": 4.176974310180781e-05,
      "loss": 0.141,
      "step": 1673
    },
    {
      "epoch": 2.376153300212917,
      "grad_norm": 0.4161722660064697,
      "learning_rate": 4.167459562321599e-05,
      "loss": 0.1274,
      "step": 1674
    },
    {
      "epoch": 2.3775727466288146,
      "grad_norm": 0.34640610218048096,
      "learning_rate": 4.157944814462417e-05,
      "loss": 0.1182,
      "step": 1675
    },
    {
      "epoch": 2.3789921930447124,
      "grad_norm": 0.2984077036380768,
      "learning_rate": 4.148430066603235e-05,
      "loss": 0.1225,
      "step": 1676
    },
    {
      "epoch": 2.3804116394606103,
      "grad_norm": 0.2916264832019806,
      "learning_rate": 4.1389153187440536e-05,
      "loss": 0.1187,
      "step": 1677
    },
    {
      "epoch": 2.381831085876508,
      "grad_norm": 0.28284019231796265,
      "learning_rate": 4.129400570884871e-05,
      "loss": 0.1095,
      "step": 1678
    },
    {
      "epoch": 2.383250532292406,
      "grad_norm": 0.30725744366645813,
      "learning_rate": 4.1198858230256896e-05,
      "loss": 0.11,
      "step": 1679
    },
    {
      "epoch": 2.384669978708304,
      "grad_norm": 0.2839166224002838,
      "learning_rate": 4.110371075166508e-05,
      "loss": 0.1052,
      "step": 1680
    },
    {
      "epoch": 2.3860894251242017,
      "grad_norm": 0.2612336575984955,
      "learning_rate": 4.100856327307326e-05,
      "loss": 0.0995,
      "step": 1681
    },
    {
      "epoch": 2.387508871540099,
      "grad_norm": 0.2566988468170166,
      "learning_rate": 4.0913415794481446e-05,
      "loss": 0.0838,
      "step": 1682
    },
    {
      "epoch": 2.388928317955997,
      "grad_norm": 0.25363069772720337,
      "learning_rate": 4.081826831588963e-05,
      "loss": 0.0906,
      "step": 1683
    },
    {
      "epoch": 2.390347764371895,
      "grad_norm": 0.26987236738204956,
      "learning_rate": 4.072312083729781e-05,
      "loss": 0.0987,
      "step": 1684
    },
    {
      "epoch": 2.3917672107877928,
      "grad_norm": 0.27083539962768555,
      "learning_rate": 4.0627973358705996e-05,
      "loss": 0.0983,
      "step": 1685
    },
    {
      "epoch": 2.3931866572036906,
      "grad_norm": 0.21648462116718292,
      "learning_rate": 4.053282588011418e-05,
      "loss": 0.0926,
      "step": 1686
    },
    {
      "epoch": 2.3946061036195885,
      "grad_norm": 0.3596116304397583,
      "learning_rate": 4.043767840152236e-05,
      "loss": 0.0971,
      "step": 1687
    },
    {
      "epoch": 2.3960255500354863,
      "grad_norm": 0.22761599719524384,
      "learning_rate": 4.0342530922930546e-05,
      "loss": 0.0952,
      "step": 1688
    },
    {
      "epoch": 2.3974449964513838,
      "grad_norm": 0.2238224744796753,
      "learning_rate": 4.024738344433873e-05,
      "loss": 0.09,
      "step": 1689
    },
    {
      "epoch": 2.3988644428672816,
      "grad_norm": 0.23355059325695038,
      "learning_rate": 4.015223596574691e-05,
      "loss": 0.0917,
      "step": 1690
    },
    {
      "epoch": 2.4002838892831795,
      "grad_norm": 0.24018386006355286,
      "learning_rate": 4.005708848715509e-05,
      "loss": 0.0922,
      "step": 1691
    },
    {
      "epoch": 2.4017033356990773,
      "grad_norm": 0.20059040188789368,
      "learning_rate": 3.996194100856327e-05,
      "loss": 0.0861,
      "step": 1692
    },
    {
      "epoch": 2.4017033356990773,
      "eval_loss": 0.20274049043655396,
      "eval_runtime": 349.0719,
      "eval_samples_per_second": 3.028,
      "eval_steps_per_second": 1.011,
      "step": 1692
    },
    {
      "epoch": 2.403122782114975,
      "grad_norm": 0.23115693032741547,
      "learning_rate": 3.9866793529971456e-05,
      "loss": 0.0859,
      "step": 1693
    },
    {
      "epoch": 2.404542228530873,
      "grad_norm": 0.18284037709236145,
      "learning_rate": 3.977164605137964e-05,
      "loss": 0.0843,
      "step": 1694
    },
    {
      "epoch": 2.405961674946771,
      "grad_norm": 0.2863667905330658,
      "learning_rate": 3.967649857278782e-05,
      "loss": 0.0918,
      "step": 1695
    },
    {
      "epoch": 2.4073811213626684,
      "grad_norm": 0.2227601408958435,
      "learning_rate": 3.9581351094196006e-05,
      "loss": 0.084,
      "step": 1696
    },
    {
      "epoch": 2.408800567778566,
      "grad_norm": 0.18289464712142944,
      "learning_rate": 3.948620361560419e-05,
      "loss": 0.0729,
      "step": 1697
    },
    {
      "epoch": 2.410220014194464,
      "grad_norm": 0.16665226221084595,
      "learning_rate": 3.939105613701237e-05,
      "loss": 0.073,
      "step": 1698
    },
    {
      "epoch": 2.411639460610362,
      "grad_norm": 0.2103600949048996,
      "learning_rate": 3.9295908658420556e-05,
      "loss": 0.084,
      "step": 1699
    },
    {
      "epoch": 2.41305890702626,
      "grad_norm": 0.22063235938549042,
      "learning_rate": 3.920076117982874e-05,
      "loss": 0.0683,
      "step": 1700
    },
    {
      "epoch": 2.4144783534421577,
      "grad_norm": 0.18380844593048096,
      "learning_rate": 3.910561370123692e-05,
      "loss": 0.0798,
      "step": 1701
    },
    {
      "epoch": 2.4158977998580555,
      "grad_norm": 0.14841853082180023,
      "learning_rate": 3.9010466222645106e-05,
      "loss": 0.0645,
      "step": 1702
    },
    {
      "epoch": 2.417317246273953,
      "grad_norm": 0.21471500396728516,
      "learning_rate": 3.891531874405329e-05,
      "loss": 0.0667,
      "step": 1703
    },
    {
      "epoch": 2.418736692689851,
      "grad_norm": 0.12759453058242798,
      "learning_rate": 3.8820171265461466e-05,
      "loss": 0.0656,
      "step": 1704
    },
    {
      "epoch": 2.4201561391057487,
      "grad_norm": 0.14957265555858612,
      "learning_rate": 3.872502378686965e-05,
      "loss": 0.0606,
      "step": 1705
    },
    {
      "epoch": 2.4215755855216465,
      "grad_norm": 0.13496854901313782,
      "learning_rate": 3.862987630827783e-05,
      "loss": 0.0599,
      "step": 1706
    },
    {
      "epoch": 2.4229950319375444,
      "grad_norm": 0.10677335411310196,
      "learning_rate": 3.8534728829686016e-05,
      "loss": 0.0565,
      "step": 1707
    },
    {
      "epoch": 2.4244144783534423,
      "grad_norm": 0.1290086805820465,
      "learning_rate": 3.84395813510942e-05,
      "loss": 0.059,
      "step": 1708
    },
    {
      "epoch": 2.42583392476934,
      "grad_norm": 0.103047676384449,
      "learning_rate": 3.834443387250238e-05,
      "loss": 0.0494,
      "step": 1709
    },
    {
      "epoch": 2.4272533711852375,
      "grad_norm": 0.5690036416053772,
      "learning_rate": 3.824928639391056e-05,
      "loss": 0.2792,
      "step": 1710
    },
    {
      "epoch": 2.4286728176011354,
      "grad_norm": 0.677786648273468,
      "learning_rate": 3.815413891531874e-05,
      "loss": 0.2157,
      "step": 1711
    },
    {
      "epoch": 2.4300922640170333,
      "grad_norm": 0.6138237118721008,
      "learning_rate": 3.8058991436726926e-05,
      "loss": 0.2,
      "step": 1712
    },
    {
      "epoch": 2.431511710432931,
      "grad_norm": 0.3625549376010895,
      "learning_rate": 3.796384395813511e-05,
      "loss": 0.1393,
      "step": 1713
    },
    {
      "epoch": 2.432931156848829,
      "grad_norm": 0.4831855595111847,
      "learning_rate": 3.786869647954329e-05,
      "loss": 0.178,
      "step": 1714
    },
    {
      "epoch": 2.434350603264727,
      "grad_norm": 0.42887628078460693,
      "learning_rate": 3.7773549000951476e-05,
      "loss": 0.1768,
      "step": 1715
    },
    {
      "epoch": 2.4357700496806247,
      "grad_norm": 0.5142930150032043,
      "learning_rate": 3.767840152235966e-05,
      "loss": 0.1753,
      "step": 1716
    },
    {
      "epoch": 2.437189496096522,
      "grad_norm": 0.47690409421920776,
      "learning_rate": 3.758325404376784e-05,
      "loss": 0.1652,
      "step": 1717
    },
    {
      "epoch": 2.43860894251242,
      "grad_norm": 0.4087882339954376,
      "learning_rate": 3.7488106565176026e-05,
      "loss": 0.1569,
      "step": 1718
    },
    {
      "epoch": 2.440028388928318,
      "grad_norm": 0.32161054015159607,
      "learning_rate": 3.73929590865842e-05,
      "loss": 0.1091,
      "step": 1719
    },
    {
      "epoch": 2.4414478353442157,
      "grad_norm": 0.3565692603588104,
      "learning_rate": 3.7297811607992386e-05,
      "loss": 0.1294,
      "step": 1720
    },
    {
      "epoch": 2.4428672817601136,
      "grad_norm": 0.2977777123451233,
      "learning_rate": 3.720266412940057e-05,
      "loss": 0.1118,
      "step": 1721
    },
    {
      "epoch": 2.4442867281760114,
      "grad_norm": 0.36344656348228455,
      "learning_rate": 3.710751665080875e-05,
      "loss": 0.1136,
      "step": 1722
    },
    {
      "epoch": 2.4457061745919093,
      "grad_norm": 0.3550844192504883,
      "learning_rate": 3.7012369172216936e-05,
      "loss": 0.1324,
      "step": 1723
    },
    {
      "epoch": 2.4471256210078067,
      "grad_norm": 0.30436238646507263,
      "learning_rate": 3.691722169362512e-05,
      "loss": 0.1201,
      "step": 1724
    },
    {
      "epoch": 2.4485450674237046,
      "grad_norm": 0.28720661997795105,
      "learning_rate": 3.68220742150333e-05,
      "loss": 0.114,
      "step": 1725
    },
    {
      "epoch": 2.4499645138396025,
      "grad_norm": 0.31380924582481384,
      "learning_rate": 3.6726926736441487e-05,
      "loss": 0.1095,
      "step": 1726
    },
    {
      "epoch": 2.4513839602555003,
      "grad_norm": 0.27836254239082336,
      "learning_rate": 3.663177925784967e-05,
      "loss": 0.1075,
      "step": 1727
    },
    {
      "epoch": 2.452803406671398,
      "grad_norm": 0.2555030882358551,
      "learning_rate": 3.653663177925785e-05,
      "loss": 0.0992,
      "step": 1728
    },
    {
      "epoch": 2.454222853087296,
      "grad_norm": 0.3026980757713318,
      "learning_rate": 3.644148430066604e-05,
      "loss": 0.1091,
      "step": 1729
    },
    {
      "epoch": 2.455642299503194,
      "grad_norm": 0.32878902554512024,
      "learning_rate": 3.634633682207422e-05,
      "loss": 0.101,
      "step": 1730
    },
    {
      "epoch": 2.4570617459190913,
      "grad_norm": 0.25537875294685364,
      "learning_rate": 3.62511893434824e-05,
      "loss": 0.0938,
      "step": 1731
    },
    {
      "epoch": 2.458481192334989,
      "grad_norm": 0.28320929408073425,
      "learning_rate": 3.615604186489058e-05,
      "loss": 0.1106,
      "step": 1732
    },
    {
      "epoch": 2.459900638750887,
      "grad_norm": 0.257485955953598,
      "learning_rate": 3.606089438629876e-05,
      "loss": 0.0885,
      "step": 1733
    },
    {
      "epoch": 2.461320085166785,
      "grad_norm": 0.22057867050170898,
      "learning_rate": 3.596574690770695e-05,
      "loss": 0.0768,
      "step": 1734
    },
    {
      "epoch": 2.4627395315826828,
      "grad_norm": 0.24505066871643066,
      "learning_rate": 3.587059942911513e-05,
      "loss": 0.0791,
      "step": 1735
    },
    {
      "epoch": 2.4641589779985806,
      "grad_norm": 0.23252467811107635,
      "learning_rate": 3.577545195052331e-05,
      "loss": 0.0786,
      "step": 1736
    },
    {
      "epoch": 2.4655784244144785,
      "grad_norm": 0.22888454794883728,
      "learning_rate": 3.56803044719315e-05,
      "loss": 0.0835,
      "step": 1737
    },
    {
      "epoch": 2.466997870830376,
      "grad_norm": 0.1980009824037552,
      "learning_rate": 3.558515699333968e-05,
      "loss": 0.0768,
      "step": 1738
    },
    {
      "epoch": 2.468417317246274,
      "grad_norm": 0.282001256942749,
      "learning_rate": 3.5490009514747863e-05,
      "loss": 0.0965,
      "step": 1739
    },
    {
      "epoch": 2.4698367636621716,
      "grad_norm": 0.34063518047332764,
      "learning_rate": 3.539486203615605e-05,
      "loss": 0.0911,
      "step": 1740
    },
    {
      "epoch": 2.4712562100780695,
      "grad_norm": 0.24878622591495514,
      "learning_rate": 3.529971455756423e-05,
      "loss": 0.0764,
      "step": 1741
    },
    {
      "epoch": 2.4726756564939674,
      "grad_norm": 0.29924023151397705,
      "learning_rate": 3.5204567078972414e-05,
      "loss": 0.081,
      "step": 1742
    },
    {
      "epoch": 2.4740951029098652,
      "grad_norm": 0.23366902768611908,
      "learning_rate": 3.51094196003806e-05,
      "loss": 0.073,
      "step": 1743
    },
    {
      "epoch": 2.475514549325763,
      "grad_norm": 0.23735205829143524,
      "learning_rate": 3.501427212178878e-05,
      "loss": 0.0764,
      "step": 1744
    },
    {
      "epoch": 2.4769339957416605,
      "grad_norm": 0.23180271685123444,
      "learning_rate": 3.491912464319696e-05,
      "loss": 0.0815,
      "step": 1745
    },
    {
      "epoch": 2.4783534421575584,
      "grad_norm": 0.18846595287322998,
      "learning_rate": 3.482397716460514e-05,
      "loss": 0.0692,
      "step": 1746
    },
    {
      "epoch": 2.4797728885734562,
      "grad_norm": 0.19519153237342834,
      "learning_rate": 3.472882968601332e-05,
      "loss": 0.0789,
      "step": 1747
    },
    {
      "epoch": 2.481192334989354,
      "grad_norm": 0.16162128746509552,
      "learning_rate": 3.46336822074215e-05,
      "loss": 0.0698,
      "step": 1748
    },
    {
      "epoch": 2.482611781405252,
      "grad_norm": 0.1800423115491867,
      "learning_rate": 3.4538534728829684e-05,
      "loss": 0.0706,
      "step": 1749
    },
    {
      "epoch": 2.48403122782115,
      "grad_norm": 0.16326811909675598,
      "learning_rate": 3.444338725023787e-05,
      "loss": 0.0697,
      "step": 1750
    },
    {
      "epoch": 2.4854506742370477,
      "grad_norm": 0.19629241526126862,
      "learning_rate": 3.434823977164605e-05,
      "loss": 0.0722,
      "step": 1751
    },
    {
      "epoch": 2.486870120652945,
      "grad_norm": 0.1886446475982666,
      "learning_rate": 3.4253092293054234e-05,
      "loss": 0.0667,
      "step": 1752
    },
    {
      "epoch": 2.488289567068843,
      "grad_norm": 0.15789766609668732,
      "learning_rate": 3.415794481446242e-05,
      "loss": 0.0599,
      "step": 1753
    },
    {
      "epoch": 2.489709013484741,
      "grad_norm": 0.19379641115665436,
      "learning_rate": 3.40627973358706e-05,
      "loss": 0.0672,
      "step": 1754
    },
    {
      "epoch": 2.4911284599006387,
      "grad_norm": 0.1458674967288971,
      "learning_rate": 3.3967649857278784e-05,
      "loss": 0.0674,
      "step": 1755
    },
    {
      "epoch": 2.4925479063165366,
      "grad_norm": 0.11463010311126709,
      "learning_rate": 3.387250237868697e-05,
      "loss": 0.0607,
      "step": 1756
    },
    {
      "epoch": 2.4939673527324344,
      "grad_norm": 0.14746470749378204,
      "learning_rate": 3.377735490009515e-05,
      "loss": 0.0667,
      "step": 1757
    },
    {
      "epoch": 2.4953867991483323,
      "grad_norm": 0.1176338940858841,
      "learning_rate": 3.3682207421503334e-05,
      "loss": 0.0559,
      "step": 1758
    },
    {
      "epoch": 2.49680624556423,
      "grad_norm": 0.1335037797689438,
      "learning_rate": 3.358705994291152e-05,
      "loss": 0.0586,
      "step": 1759
    },
    {
      "epoch": 2.4982256919801276,
      "grad_norm": 0.5044611692428589,
      "learning_rate": 3.3491912464319694e-05,
      "loss": 0.1964,
      "step": 1760
    },
    {
      "epoch": 2.4996451383960254,
      "grad_norm": 0.5485973358154297,
      "learning_rate": 3.339676498572788e-05,
      "loss": 0.1738,
      "step": 1761
    },
    {
      "epoch": 2.5010645848119233,
      "grad_norm": 0.5278415679931641,
      "learning_rate": 3.330161750713606e-05,
      "loss": 0.1756,
      "step": 1762
    },
    {
      "epoch": 2.502484031227821,
      "grad_norm": 0.4525172710418701,
      "learning_rate": 3.3206470028544244e-05,
      "loss": 0.1644,
      "step": 1763
    },
    {
      "epoch": 2.503903477643719,
      "grad_norm": 0.47665244340896606,
      "learning_rate": 3.311132254995243e-05,
      "loss": 0.1425,
      "step": 1764
    },
    {
      "epoch": 2.505322924059617,
      "grad_norm": 0.5072240829467773,
      "learning_rate": 3.301617507136061e-05,
      "loss": 0.1636,
      "step": 1765
    },
    {
      "epoch": 2.5067423704755143,
      "grad_norm": 0.43444398045539856,
      "learning_rate": 3.2921027592768794e-05,
      "loss": 0.1428,
      "step": 1766
    },
    {
      "epoch": 2.5081618168914126,
      "grad_norm": 0.4865790009498596,
      "learning_rate": 3.282588011417698e-05,
      "loss": 0.1659,
      "step": 1767
    },
    {
      "epoch": 2.50958126330731,
      "grad_norm": 0.4580123722553253,
      "learning_rate": 3.273073263558516e-05,
      "loss": 0.1327,
      "step": 1768
    },
    {
      "epoch": 2.511000709723208,
      "grad_norm": 0.47466230392456055,
      "learning_rate": 3.2635585156993344e-05,
      "loss": 0.1554,
      "step": 1769
    },
    {
      "epoch": 2.5124201561391057,
      "grad_norm": 0.4142892062664032,
      "learning_rate": 3.254043767840153e-05,
      "loss": 0.1241,
      "step": 1770
    },
    {
      "epoch": 2.5138396025550036,
      "grad_norm": 0.4665525555610657,
      "learning_rate": 3.244529019980971e-05,
      "loss": 0.1435,
      "step": 1771
    },
    {
      "epoch": 2.5152590489709015,
      "grad_norm": 0.3934594988822937,
      "learning_rate": 3.2350142721217894e-05,
      "loss": 0.1297,
      "step": 1772
    },
    {
      "epoch": 2.516678495386799,
      "grad_norm": 0.4494738280773163,
      "learning_rate": 3.225499524262607e-05,
      "loss": 0.1351,
      "step": 1773
    },
    {
      "epoch": 2.518097941802697,
      "grad_norm": 0.33796700835227966,
      "learning_rate": 3.2159847764034254e-05,
      "loss": 0.1266,
      "step": 1774
    },
    {
      "epoch": 2.5195173882185946,
      "grad_norm": 0.3310290277004242,
      "learning_rate": 3.206470028544244e-05,
      "loss": 0.1127,
      "step": 1775
    },
    {
      "epoch": 2.5209368346344925,
      "grad_norm": 0.3812306821346283,
      "learning_rate": 3.196955280685062e-05,
      "loss": 0.1328,
      "step": 1776
    },
    {
      "epoch": 2.5223562810503903,
      "grad_norm": 0.3127322793006897,
      "learning_rate": 3.1874405328258804e-05,
      "loss": 0.1058,
      "step": 1777
    },
    {
      "epoch": 2.523775727466288,
      "grad_norm": 0.2853889763355255,
      "learning_rate": 3.177925784966699e-05,
      "loss": 0.0897,
      "step": 1778
    },
    {
      "epoch": 2.525195173882186,
      "grad_norm": 0.286628782749176,
      "learning_rate": 3.168411037107517e-05,
      "loss": 0.0993,
      "step": 1779
    },
    {
      "epoch": 2.5266146202980835,
      "grad_norm": 0.2769748270511627,
      "learning_rate": 3.158896289248335e-05,
      "loss": 0.0906,
      "step": 1780
    },
    {
      "epoch": 2.528034066713982,
      "grad_norm": 0.29913511872291565,
      "learning_rate": 3.149381541389153e-05,
      "loss": 0.1031,
      "step": 1781
    },
    {
      "epoch": 2.529453513129879,
      "grad_norm": 0.27662521600723267,
      "learning_rate": 3.1398667935299714e-05,
      "loss": 0.1027,
      "step": 1782
    },
    {
      "epoch": 2.530872959545777,
      "grad_norm": 0.30976253747940063,
      "learning_rate": 3.13035204567079e-05,
      "loss": 0.0877,
      "step": 1783
    },
    {
      "epoch": 2.532292405961675,
      "grad_norm": 0.3400900065898895,
      "learning_rate": 3.120837297811608e-05,
      "loss": 0.0908,
      "step": 1784
    },
    {
      "epoch": 2.533711852377573,
      "grad_norm": 0.3058347702026367,
      "learning_rate": 3.1113225499524264e-05,
      "loss": 0.0857,
      "step": 1785
    },
    {
      "epoch": 2.5351312987934707,
      "grad_norm": 0.31037116050720215,
      "learning_rate": 3.101807802093245e-05,
      "loss": 0.1062,
      "step": 1786
    },
    {
      "epoch": 2.536550745209368,
      "grad_norm": 0.24405819177627563,
      "learning_rate": 3.092293054234063e-05,
      "loss": 0.0994,
      "step": 1787
    },
    {
      "epoch": 2.5379701916252664,
      "grad_norm": 0.21504291892051697,
      "learning_rate": 3.082778306374881e-05,
      "loss": 0.0799,
      "step": 1788
    },
    {
      "epoch": 2.539389638041164,
      "grad_norm": 0.2443324774503708,
      "learning_rate": 3.073263558515699e-05,
      "loss": 0.0858,
      "step": 1789
    },
    {
      "epoch": 2.5408090844570617,
      "grad_norm": 0.18979890644550323,
      "learning_rate": 3.0637488106565174e-05,
      "loss": 0.0739,
      "step": 1790
    },
    {
      "epoch": 2.5422285308729595,
      "grad_norm": 0.2272489070892334,
      "learning_rate": 3.054234062797336e-05,
      "loss": 0.0774,
      "step": 1791
    },
    {
      "epoch": 2.5436479772888574,
      "grad_norm": 0.2330632507801056,
      "learning_rate": 3.044719314938154e-05,
      "loss": 0.093,
      "step": 1792
    },
    {
      "epoch": 2.5450674237047552,
      "grad_norm": 0.23068378865718842,
      "learning_rate": 3.0352045670789724e-05,
      "loss": 0.0851,
      "step": 1793
    },
    {
      "epoch": 2.5464868701206527,
      "grad_norm": 0.25296837091445923,
      "learning_rate": 3.0256898192197908e-05,
      "loss": 0.1057,
      "step": 1794
    },
    {
      "epoch": 2.547906316536551,
      "grad_norm": 0.200236514210701,
      "learning_rate": 3.016175071360609e-05,
      "loss": 0.0778,
      "step": 1795
    },
    {
      "epoch": 2.5493257629524484,
      "grad_norm": 0.1910010129213333,
      "learning_rate": 3.0066603235014274e-05,
      "loss": 0.0675,
      "step": 1796
    },
    {
      "epoch": 2.5507452093683463,
      "grad_norm": 0.23147843778133392,
      "learning_rate": 2.9971455756422458e-05,
      "loss": 0.0809,
      "step": 1797
    },
    {
      "epoch": 2.552164655784244,
      "grad_norm": 0.2134430855512619,
      "learning_rate": 2.9876308277830638e-05,
      "loss": 0.0853,
      "step": 1798
    },
    {
      "epoch": 2.553584102200142,
      "grad_norm": 0.16603246331214905,
      "learning_rate": 2.978116079923882e-05,
      "loss": 0.0633,
      "step": 1799
    },
    {
      "epoch": 2.55500354861604,
      "grad_norm": 0.1694834679365158,
      "learning_rate": 2.9686013320647004e-05,
      "loss": 0.0725,
      "step": 1800
    },
    {
      "epoch": 2.5564229950319377,
      "grad_norm": 0.14255475997924805,
      "learning_rate": 2.9590865842055188e-05,
      "loss": 0.0648,
      "step": 1801
    },
    {
      "epoch": 2.5578424414478356,
      "grad_norm": 0.1503046154975891,
      "learning_rate": 2.949571836346337e-05,
      "loss": 0.0676,
      "step": 1802
    },
    {
      "epoch": 2.559261887863733,
      "grad_norm": 0.14296875894069672,
      "learning_rate": 2.9400570884871555e-05,
      "loss": 0.0603,
      "step": 1803
    },
    {
      "epoch": 2.560681334279631,
      "grad_norm": 0.1779826432466507,
      "learning_rate": 2.9305423406279735e-05,
      "loss": 0.0637,
      "step": 1804
    },
    {
      "epoch": 2.5621007806955287,
      "grad_norm": 0.17864440381526947,
      "learning_rate": 2.9210275927687918e-05,
      "loss": 0.0628,
      "step": 1805
    },
    {
      "epoch": 2.5635202271114266,
      "grad_norm": 0.15080642700195312,
      "learning_rate": 2.91151284490961e-05,
      "loss": 0.0617,
      "step": 1806
    },
    {
      "epoch": 2.5649396735273244,
      "grad_norm": 0.1555405855178833,
      "learning_rate": 2.9019980970504285e-05,
      "loss": 0.0647,
      "step": 1807
    },
    {
      "epoch": 2.5663591199432223,
      "grad_norm": 0.12484290450811386,
      "learning_rate": 2.8924833491912468e-05,
      "loss": 0.0533,
      "step": 1808
    },
    {
      "epoch": 2.56777856635912,
      "grad_norm": 0.10782703012228012,
      "learning_rate": 2.882968601332065e-05,
      "loss": 0.052,
      "step": 1809
    },
    {
      "epoch": 2.5691980127750176,
      "grad_norm": 0.6566162705421448,
      "learning_rate": 2.8734538534728835e-05,
      "loss": 0.2767,
      "step": 1810
    },
    {
      "epoch": 2.5706174591909154,
      "grad_norm": 0.5200997591018677,
      "learning_rate": 2.8639391056137015e-05,
      "loss": 0.1902,
      "step": 1811
    },
    {
      "epoch": 2.5720369056068133,
      "grad_norm": 0.47225528955459595,
      "learning_rate": 2.8544243577545198e-05,
      "loss": 0.206,
      "step": 1812
    },
    {
      "epoch": 2.573456352022711,
      "grad_norm": 0.46030327677726746,
      "learning_rate": 2.844909609895338e-05,
      "loss": 0.1568,
      "step": 1813
    },
    {
      "epoch": 2.574875798438609,
      "grad_norm": 0.48979952931404114,
      "learning_rate": 2.8353948620361565e-05,
      "loss": 0.1701,
      "step": 1814
    },
    {
      "epoch": 2.576295244854507,
      "grad_norm": 0.39725199341773987,
      "learning_rate": 2.825880114176974e-05,
      "loss": 0.1449,
      "step": 1815
    },
    {
      "epoch": 2.5777146912704048,
      "grad_norm": 0.38224470615386963,
      "learning_rate": 2.8163653663177925e-05,
      "loss": 0.133,
      "step": 1816
    },
    {
      "epoch": 2.579134137686302,
      "grad_norm": 0.34103095531463623,
      "learning_rate": 2.8068506184586108e-05,
      "loss": 0.1345,
      "step": 1817
    },
    {
      "epoch": 2.5805535841022,
      "grad_norm": 0.45782333612442017,
      "learning_rate": 2.797335870599429e-05,
      "loss": 0.1524,
      "step": 1818
    },
    {
      "epoch": 2.581973030518098,
      "grad_norm": 0.39575597643852234,
      "learning_rate": 2.7878211227402475e-05,
      "loss": 0.1291,
      "step": 1819
    },
    {
      "epoch": 2.5833924769339958,
      "grad_norm": 0.3432583808898926,
      "learning_rate": 2.7783063748810655e-05,
      "loss": 0.1171,
      "step": 1820
    },
    {
      "epoch": 2.5848119233498936,
      "grad_norm": 0.3438637852668762,
      "learning_rate": 2.7687916270218838e-05,
      "loss": 0.1148,
      "step": 1821
    },
    {
      "epoch": 2.5862313697657915,
      "grad_norm": 0.37088778614997864,
      "learning_rate": 2.759276879162702e-05,
      "loss": 0.1331,
      "step": 1822
    },
    {
      "epoch": 2.5876508161816894,
      "grad_norm": 0.36243563890457153,
      "learning_rate": 2.7497621313035205e-05,
      "loss": 0.1094,
      "step": 1823
    },
    {
      "epoch": 2.5890702625975868,
      "grad_norm": 0.35172227025032043,
      "learning_rate": 2.7402473834443388e-05,
      "loss": 0.125,
      "step": 1824
    },
    {
      "epoch": 2.5904897090134846,
      "grad_norm": 0.33114078640937805,
      "learning_rate": 2.730732635585157e-05,
      "loss": 0.1113,
      "step": 1825
    },
    {
      "epoch": 2.5919091554293825,
      "grad_norm": 0.36347055435180664,
      "learning_rate": 2.721217887725975e-05,
      "loss": 0.1233,
      "step": 1826
    },
    {
      "epoch": 2.5933286018452804,
      "grad_norm": 0.3998251259326935,
      "learning_rate": 2.7117031398667935e-05,
      "loss": 0.1189,
      "step": 1827
    },
    {
      "epoch": 2.594748048261178,
      "grad_norm": 0.32658395171165466,
      "learning_rate": 2.702188392007612e-05,
      "loss": 0.1022,
      "step": 1828
    },
    {
      "epoch": 2.596167494677076,
      "grad_norm": 0.2813437283039093,
      "learning_rate": 2.69267364414843e-05,
      "loss": 0.0923,
      "step": 1829
    },
    {
      "epoch": 2.597586941092974,
      "grad_norm": 0.3459652066230774,
      "learning_rate": 2.6831588962892485e-05,
      "loss": 0.0999,
      "step": 1830
    },
    {
      "epoch": 2.5990063875088714,
      "grad_norm": 0.2742632031440735,
      "learning_rate": 2.673644148430067e-05,
      "loss": 0.1011,
      "step": 1831
    },
    {
      "epoch": 2.6004258339247692,
      "grad_norm": 0.2906953990459442,
      "learning_rate": 2.664129400570885e-05,
      "loss": 0.1087,
      "step": 1832
    },
    {
      "epoch": 2.601845280340667,
      "grad_norm": 0.2798442542552948,
      "learning_rate": 2.6546146527117032e-05,
      "loss": 0.1068,
      "step": 1833
    },
    {
      "epoch": 2.603264726756565,
      "grad_norm": 0.2569821774959564,
      "learning_rate": 2.6450999048525215e-05,
      "loss": 0.0871,
      "step": 1834
    },
    {
      "epoch": 2.604684173172463,
      "grad_norm": 0.28922972083091736,
      "learning_rate": 2.63558515699334e-05,
      "loss": 0.1025,
      "step": 1835
    },
    {
      "epoch": 2.6061036195883607,
      "grad_norm": 0.19782812893390656,
      "learning_rate": 2.6260704091341582e-05,
      "loss": 0.088,
      "step": 1836
    },
    {
      "epoch": 2.6075230660042585,
      "grad_norm": 0.23173148930072784,
      "learning_rate": 2.6165556612749765e-05,
      "loss": 0.0829,
      "step": 1837
    },
    {
      "epoch": 2.608942512420156,
      "grad_norm": 0.21946054697036743,
      "learning_rate": 2.607040913415795e-05,
      "loss": 0.0794,
      "step": 1838
    },
    {
      "epoch": 2.610361958836054,
      "grad_norm": 0.37050920724868774,
      "learning_rate": 2.597526165556613e-05,
      "loss": 0.0914,
      "step": 1839
    },
    {
      "epoch": 2.6117814052519517,
      "grad_norm": 0.25368526577949524,
      "learning_rate": 2.5880114176974312e-05,
      "loss": 0.0853,
      "step": 1840
    },
    {
      "epoch": 2.6132008516678495,
      "grad_norm": 0.23388749361038208,
      "learning_rate": 2.5784966698382495e-05,
      "loss": 0.0949,
      "step": 1841
    },
    {
      "epoch": 2.6146202980837474,
      "grad_norm": 0.30577707290649414,
      "learning_rate": 2.568981921979068e-05,
      "loss": 0.0718,
      "step": 1842
    },
    {
      "epoch": 2.6160397444996453,
      "grad_norm": 0.2959517240524292,
      "learning_rate": 2.5594671741198862e-05,
      "loss": 0.0735,
      "step": 1843
    },
    {
      "epoch": 2.617459190915543,
      "grad_norm": 0.20159479975700378,
      "learning_rate": 2.5499524262607045e-05,
      "loss": 0.0737,
      "step": 1844
    },
    {
      "epoch": 2.6188786373314406,
      "grad_norm": 0.23493513464927673,
      "learning_rate": 2.5404376784015225e-05,
      "loss": 0.0847,
      "step": 1845
    },
    {
      "epoch": 2.6202980837473384,
      "grad_norm": 0.16484084725379944,
      "learning_rate": 2.530922930542341e-05,
      "loss": 0.0686,
      "step": 1846
    },
    {
      "epoch": 2.6217175301632363,
      "grad_norm": 0.19852887094020844,
      "learning_rate": 2.5214081826831592e-05,
      "loss": 0.0779,
      "step": 1847
    },
    {
      "epoch": 2.623136976579134,
      "grad_norm": 0.19594256579875946,
      "learning_rate": 2.5118934348239775e-05,
      "loss": 0.0694,
      "step": 1848
    },
    {
      "epoch": 2.624556422995032,
      "grad_norm": 0.19094648957252502,
      "learning_rate": 2.502378686964796e-05,
      "loss": 0.0743,
      "step": 1849
    },
    {
      "epoch": 2.62597586941093,
      "grad_norm": 0.19857542216777802,
      "learning_rate": 2.492863939105614e-05,
      "loss": 0.0547,
      "step": 1850
    },
    {
      "epoch": 2.6273953158268277,
      "grad_norm": 0.15865221619606018,
      "learning_rate": 2.4833491912464322e-05,
      "loss": 0.0699,
      "step": 1851
    },
    {
      "epoch": 2.628814762242725,
      "grad_norm": 0.13326357305049896,
      "learning_rate": 2.4738344433872505e-05,
      "loss": 0.0584,
      "step": 1852
    },
    {
      "epoch": 2.630234208658623,
      "grad_norm": 0.15245528519153595,
      "learning_rate": 2.4643196955280685e-05,
      "loss": 0.062,
      "step": 1853
    },
    {
      "epoch": 2.631653655074521,
      "grad_norm": 0.1800384372472763,
      "learning_rate": 2.454804947668887e-05,
      "loss": 0.0636,
      "step": 1854
    },
    {
      "epoch": 2.6330731014904187,
      "grad_norm": 0.1853189617395401,
      "learning_rate": 2.4452901998097052e-05,
      "loss": 0.0586,
      "step": 1855
    },
    {
      "epoch": 2.6344925479063166,
      "grad_norm": 0.156202033162117,
      "learning_rate": 2.4357754519505236e-05,
      "loss": 0.0578,
      "step": 1856
    },
    {
      "epoch": 2.6359119943222145,
      "grad_norm": 0.13628332316875458,
      "learning_rate": 2.426260704091342e-05,
      "loss": 0.0599,
      "step": 1857
    },
    {
      "epoch": 2.6373314407381123,
      "grad_norm": 0.14445681869983673,
      "learning_rate": 2.4167459562321602e-05,
      "loss": 0.0597,
      "step": 1858
    },
    {
      "epoch": 2.6387508871540097,
      "grad_norm": 0.10394003987312317,
      "learning_rate": 2.4072312083729782e-05,
      "loss": 0.0501,
      "step": 1859
    },
    {
      "epoch": 2.6401703335699076,
      "grad_norm": 0.7152101397514343,
      "learning_rate": 2.3977164605137966e-05,
      "loss": 0.2455,
      "step": 1860
    },
    {
      "epoch": 2.6415897799858055,
      "grad_norm": 0.46047019958496094,
      "learning_rate": 2.3882017126546146e-05,
      "loss": 0.1526,
      "step": 1861
    },
    {
      "epoch": 2.6430092264017033,
      "grad_norm": 0.5490464568138123,
      "learning_rate": 2.378686964795433e-05,
      "loss": 0.1916,
      "step": 1862
    },
    {
      "epoch": 2.644428672817601,
      "grad_norm": 0.507875919342041,
      "learning_rate": 2.3691722169362512e-05,
      "loss": 0.1403,
      "step": 1863
    },
    {
      "epoch": 2.645848119233499,
      "grad_norm": 0.5330614447593689,
      "learning_rate": 2.3596574690770696e-05,
      "loss": 0.1777,
      "step": 1864
    },
    {
      "epoch": 2.647267565649397,
      "grad_norm": 0.4258630573749542,
      "learning_rate": 2.350142721217888e-05,
      "loss": 0.1542,
      "step": 1865
    },
    {
      "epoch": 2.6486870120652943,
      "grad_norm": 0.39363572001457214,
      "learning_rate": 2.3406279733587062e-05,
      "loss": 0.1547,
      "step": 1866
    },
    {
      "epoch": 2.650106458481192,
      "grad_norm": 0.5148694515228271,
      "learning_rate": 2.3311132254995242e-05,
      "loss": 0.1479,
      "step": 1867
    },
    {
      "epoch": 2.65152590489709,
      "grad_norm": 0.3737536072731018,
      "learning_rate": 2.3215984776403426e-05,
      "loss": 0.1317,
      "step": 1868
    },
    {
      "epoch": 2.652945351312988,
      "grad_norm": 0.45602431893348694,
      "learning_rate": 2.312083729781161e-05,
      "loss": 0.1267,
      "step": 1869
    },
    {
      "epoch": 2.654364797728886,
      "grad_norm": 0.47048768401145935,
      "learning_rate": 2.3025689819219792e-05,
      "loss": 0.1146,
      "step": 1870
    },
    {
      "epoch": 2.6557842441447836,
      "grad_norm": 0.3945799171924591,
      "learning_rate": 2.2930542340627976e-05,
      "loss": 0.1223,
      "step": 1871
    },
    {
      "epoch": 2.6572036905606815,
      "grad_norm": 0.4044477045536041,
      "learning_rate": 2.283539486203616e-05,
      "loss": 0.1225,
      "step": 1872
    },
    {
      "epoch": 2.658623136976579,
      "grad_norm": 0.3672584593296051,
      "learning_rate": 2.2740247383444342e-05,
      "loss": 0.1127,
      "step": 1873
    },
    {
      "epoch": 2.660042583392477,
      "grad_norm": 0.3329125642776489,
      "learning_rate": 2.2645099904852522e-05,
      "loss": 0.1216,
      "step": 1874
    },
    {
      "epoch": 2.6614620298083747,
      "grad_norm": 0.2998928129673004,
      "learning_rate": 2.2549952426260706e-05,
      "loss": 0.1068,
      "step": 1875
    },
    {
      "epoch": 2.6628814762242725,
      "grad_norm": 0.35347655415534973,
      "learning_rate": 2.2454804947668886e-05,
      "loss": 0.1229,
      "step": 1876
    },
    {
      "epoch": 2.6643009226401704,
      "grad_norm": 0.26327359676361084,
      "learning_rate": 2.235965746907707e-05,
      "loss": 0.1022,
      "step": 1877
    },
    {
      "epoch": 2.6657203690560682,
      "grad_norm": 0.3880177140235901,
      "learning_rate": 2.2264509990485253e-05,
      "loss": 0.1139,
      "step": 1878
    },
    {
      "epoch": 2.667139815471966,
      "grad_norm": 0.3152746558189392,
      "learning_rate": 2.2169362511893436e-05,
      "loss": 0.1062,
      "step": 1879
    },
    {
      "epoch": 2.6685592618878635,
      "grad_norm": 0.2833639681339264,
      "learning_rate": 2.207421503330162e-05,
      "loss": 0.0772,
      "step": 1880
    },
    {
      "epoch": 2.6699787083037614,
      "grad_norm": 0.3240460157394409,
      "learning_rate": 2.19790675547098e-05,
      "loss": 0.0985,
      "step": 1881
    },
    {
      "epoch": 2.6713981547196592,
      "grad_norm": 0.23543372750282288,
      "learning_rate": 2.1883920076117983e-05,
      "loss": 0.0954,
      "step": 1882
    },
    {
      "epoch": 2.672817601135557,
      "grad_norm": 0.22943076491355896,
      "learning_rate": 2.1788772597526166e-05,
      "loss": 0.0722,
      "step": 1883
    },
    {
      "epoch": 2.674237047551455,
      "grad_norm": 0.17962437868118286,
      "learning_rate": 2.169362511893435e-05,
      "loss": 0.0682,
      "step": 1884
    },
    {
      "epoch": 2.675656493967353,
      "grad_norm": 0.28107067942619324,
      "learning_rate": 2.1598477640342533e-05,
      "loss": 0.0995,
      "step": 1885
    },
    {
      "epoch": 2.6770759403832507,
      "grad_norm": 0.24755075573921204,
      "learning_rate": 2.1503330161750716e-05,
      "loss": 0.0976,
      "step": 1886
    },
    {
      "epoch": 2.678495386799148,
      "grad_norm": 0.2767440676689148,
      "learning_rate": 2.14081826831589e-05,
      "loss": 0.0979,
      "step": 1887
    },
    {
      "epoch": 2.679914833215046,
      "grad_norm": 0.21749790012836456,
      "learning_rate": 2.131303520456708e-05,
      "loss": 0.0818,
      "step": 1888
    },
    {
      "epoch": 2.681334279630944,
      "grad_norm": 0.23402513563632965,
      "learning_rate": 2.1217887725975263e-05,
      "loss": 0.08,
      "step": 1889
    },
    {
      "epoch": 2.6827537260468417,
      "grad_norm": 0.25522488355636597,
      "learning_rate": 2.1122740247383446e-05,
      "loss": 0.0853,
      "step": 1890
    },
    {
      "epoch": 2.6841731724627396,
      "grad_norm": 0.19264133274555206,
      "learning_rate": 2.102759276879163e-05,
      "loss": 0.0818,
      "step": 1891
    },
    {
      "epoch": 2.6855926188786374,
      "grad_norm": 0.1900152862071991,
      "learning_rate": 2.0932445290199813e-05,
      "loss": 0.0783,
      "step": 1892
    },
    {
      "epoch": 2.6870120652945353,
      "grad_norm": 0.18146057426929474,
      "learning_rate": 2.0837297811607996e-05,
      "loss": 0.0652,
      "step": 1893
    },
    {
      "epoch": 2.6884315117104327,
      "grad_norm": 0.1845375895500183,
      "learning_rate": 2.0742150333016176e-05,
      "loss": 0.077,
      "step": 1894
    },
    {
      "epoch": 2.6898509581263306,
      "grad_norm": 0.17781497538089752,
      "learning_rate": 2.0647002854424356e-05,
      "loss": 0.0727,
      "step": 1895
    },
    {
      "epoch": 2.6912704045422284,
      "grad_norm": 0.18774807453155518,
      "learning_rate": 2.055185537583254e-05,
      "loss": 0.0652,
      "step": 1896
    },
    {
      "epoch": 2.6926898509581263,
      "grad_norm": 0.18283909559249878,
      "learning_rate": 2.0456707897240723e-05,
      "loss": 0.0687,
      "step": 1897
    },
    {
      "epoch": 2.694109297374024,
      "grad_norm": 0.17576353251934052,
      "learning_rate": 2.0361560418648906e-05,
      "loss": 0.0643,
      "step": 1898
    },
    {
      "epoch": 2.695528743789922,
      "grad_norm": 0.16973640024662018,
      "learning_rate": 2.026641294005709e-05,
      "loss": 0.0627,
      "step": 1899
    },
    {
      "epoch": 2.69694819020582,
      "grad_norm": 0.18243123590946198,
      "learning_rate": 2.0171265461465273e-05,
      "loss": 0.0701,
      "step": 1900
    },
    {
      "epoch": 2.6983676366217173,
      "grad_norm": 0.16910439729690552,
      "learning_rate": 2.0076117982873456e-05,
      "loss": 0.0712,
      "step": 1901
    },
    {
      "epoch": 2.699787083037615,
      "grad_norm": 0.1811225861310959,
      "learning_rate": 1.9980970504281636e-05,
      "loss": 0.0627,
      "step": 1902
    },
    {
      "epoch": 2.701206529453513,
      "grad_norm": 0.20206838846206665,
      "learning_rate": 1.988582302568982e-05,
      "loss": 0.0657,
      "step": 1903
    },
    {
      "epoch": 2.702625975869411,
      "grad_norm": 0.1359485387802124,
      "learning_rate": 1.9790675547098003e-05,
      "loss": 0.0526,
      "step": 1904
    },
    {
      "epoch": 2.7040454222853088,
      "grad_norm": 0.18353040516376495,
      "learning_rate": 1.9695528068506186e-05,
      "loss": 0.0739,
      "step": 1905
    },
    {
      "epoch": 2.7054648687012066,
      "grad_norm": 0.15110459923744202,
      "learning_rate": 1.960038058991437e-05,
      "loss": 0.0591,
      "step": 1906
    },
    {
      "epoch": 2.7068843151171045,
      "grad_norm": 0.1583547443151474,
      "learning_rate": 1.9505233111322553e-05,
      "loss": 0.0597,
      "step": 1907
    },
    {
      "epoch": 2.708303761533002,
      "grad_norm": 0.16944874823093414,
      "learning_rate": 1.9410085632730733e-05,
      "loss": 0.0583,
      "step": 1908
    },
    {
      "epoch": 2.7097232079489,
      "grad_norm": 0.13128142058849335,
      "learning_rate": 1.9314938154138916e-05,
      "loss": 0.0515,
      "step": 1909
    },
    {
      "epoch": 2.7111426543647976,
      "grad_norm": 0.6503589153289795,
      "learning_rate": 1.92197906755471e-05,
      "loss": 0.2512,
      "step": 1910
    },
    {
      "epoch": 2.7125621007806955,
      "grad_norm": 0.4686568081378937,
      "learning_rate": 1.912464319695528e-05,
      "loss": 0.1747,
      "step": 1911
    },
    {
      "epoch": 2.7139815471965933,
      "grad_norm": 0.5118332505226135,
      "learning_rate": 1.9029495718363463e-05,
      "loss": 0.2097,
      "step": 1912
    },
    {
      "epoch": 2.715400993612491,
      "grad_norm": 0.4730061888694763,
      "learning_rate": 1.8934348239771646e-05,
      "loss": 0.1547,
      "step": 1913
    },
    {
      "epoch": 2.716820440028389,
      "grad_norm": 0.4699457883834839,
      "learning_rate": 1.883920076117983e-05,
      "loss": 0.1805,
      "step": 1914
    },
    {
      "epoch": 2.7182398864442865,
      "grad_norm": 0.48229309916496277,
      "learning_rate": 1.8744053282588013e-05,
      "loss": 0.1735,
      "step": 1915
    },
    {
      "epoch": 2.719659332860185,
      "grad_norm": 0.46072664856910706,
      "learning_rate": 1.8648905803996193e-05,
      "loss": 0.1348,
      "step": 1916
    },
    {
      "epoch": 2.721078779276082,
      "grad_norm": 0.47450873255729675,
      "learning_rate": 1.8553758325404377e-05,
      "loss": 0.16,
      "step": 1917
    },
    {
      "epoch": 2.72249822569198,
      "grad_norm": 0.5334250330924988,
      "learning_rate": 1.845861084681256e-05,
      "loss": 0.1345,
      "step": 1918
    },
    {
      "epoch": 2.723917672107878,
      "grad_norm": 0.4897802174091339,
      "learning_rate": 1.8363463368220743e-05,
      "loss": 0.1416,
      "step": 1919
    },
    {
      "epoch": 2.725337118523776,
      "grad_norm": 0.36599671840667725,
      "learning_rate": 1.8268315889628927e-05,
      "loss": 0.1183,
      "step": 1920
    },
    {
      "epoch": 2.7267565649396737,
      "grad_norm": 0.3934321403503418,
      "learning_rate": 1.817316841103711e-05,
      "loss": 0.1209,
      "step": 1921
    },
    {
      "epoch": 2.728176011355571,
      "grad_norm": 0.4165361523628235,
      "learning_rate": 1.807802093244529e-05,
      "loss": 0.1424,
      "step": 1922
    },
    {
      "epoch": 2.7295954577714694,
      "grad_norm": 0.3523315191268921,
      "learning_rate": 1.7982873453853473e-05,
      "loss": 0.1238,
      "step": 1923
    },
    {
      "epoch": 2.731014904187367,
      "grad_norm": 0.3793753683567047,
      "learning_rate": 1.7887725975261657e-05,
      "loss": 0.1161,
      "step": 1924
    },
    {
      "epoch": 2.7324343506032647,
      "grad_norm": 0.3970918655395508,
      "learning_rate": 1.779257849666984e-05,
      "loss": 0.1318,
      "step": 1925
    },
    {
      "epoch": 2.7338537970191625,
      "grad_norm": 0.32695138454437256,
      "learning_rate": 1.7697431018078023e-05,
      "loss": 0.1073,
      "step": 1926
    },
    {
      "epoch": 2.7352732434350604,
      "grad_norm": 0.36186403036117554,
      "learning_rate": 1.7602283539486207e-05,
      "loss": 0.1034,
      "step": 1927
    },
    {
      "epoch": 2.7366926898509583,
      "grad_norm": 0.2719131410121918,
      "learning_rate": 1.750713606089439e-05,
      "loss": 0.1031,
      "step": 1928
    },
    {
      "epoch": 2.7381121362668557,
      "grad_norm": 0.3073371648788452,
      "learning_rate": 1.741198858230257e-05,
      "loss": 0.0988,
      "step": 1929
    },
    {
      "epoch": 2.739531582682754,
      "grad_norm": 0.3703919053077698,
      "learning_rate": 1.731684110371075e-05,
      "loss": 0.0917,
      "step": 1930
    },
    {
      "epoch": 2.7409510290986514,
      "grad_norm": 0.268734872341156,
      "learning_rate": 1.7221693625118933e-05,
      "loss": 0.1012,
      "step": 1931
    },
    {
      "epoch": 2.7423704755145493,
      "grad_norm": 0.3012807071208954,
      "learning_rate": 1.7126546146527117e-05,
      "loss": 0.1045,
      "step": 1932
    },
    {
      "epoch": 2.743789921930447,
      "grad_norm": 0.2811996638774872,
      "learning_rate": 1.70313986679353e-05,
      "loss": 0.084,
      "step": 1933
    },
    {
      "epoch": 2.745209368346345,
      "grad_norm": 0.2715948820114136,
      "learning_rate": 1.6936251189343484e-05,
      "loss": 0.0913,
      "step": 1934
    },
    {
      "epoch": 2.746628814762243,
      "grad_norm": 0.2598288953304291,
      "learning_rate": 1.6841103710751667e-05,
      "loss": 0.0848,
      "step": 1935
    },
    {
      "epoch": 2.7480482611781403,
      "grad_norm": 0.24040916562080383,
      "learning_rate": 1.6745956232159847e-05,
      "loss": 0.0867,
      "step": 1936
    },
    {
      "epoch": 2.7494677075940386,
      "grad_norm": 0.2292431890964508,
      "learning_rate": 1.665080875356803e-05,
      "loss": 0.0813,
      "step": 1937
    },
    {
      "epoch": 2.750887154009936,
      "grad_norm": 0.30252838134765625,
      "learning_rate": 1.6555661274976214e-05,
      "loss": 0.0955,
      "step": 1938
    },
    {
      "epoch": 2.752306600425834,
      "grad_norm": 0.3142598867416382,
      "learning_rate": 1.6460513796384397e-05,
      "loss": 0.0948,
      "step": 1939
    },
    {
      "epoch": 2.7537260468417317,
      "grad_norm": 0.24730628728866577,
      "learning_rate": 1.636536631779258e-05,
      "loss": 0.0844,
      "step": 1940
    },
    {
      "epoch": 2.7551454932576296,
      "grad_norm": 0.2503422200679779,
      "learning_rate": 1.6270218839200764e-05,
      "loss": 0.0778,
      "step": 1941
    },
    {
      "epoch": 2.7565649396735274,
      "grad_norm": 0.273527055978775,
      "learning_rate": 1.6175071360608947e-05,
      "loss": 0.0858,
      "step": 1942
    },
    {
      "epoch": 2.757984386089425,
      "grad_norm": 0.28141796588897705,
      "learning_rate": 1.6079923882017127e-05,
      "loss": 0.0876,
      "step": 1943
    },
    {
      "epoch": 2.759403832505323,
      "grad_norm": 0.2354303002357483,
      "learning_rate": 1.598477640342531e-05,
      "loss": 0.0896,
      "step": 1944
    },
    {
      "epoch": 2.7608232789212206,
      "grad_norm": 0.23151646554470062,
      "learning_rate": 1.5889628924833494e-05,
      "loss": 0.0757,
      "step": 1945
    },
    {
      "epoch": 2.7622427253371185,
      "grad_norm": 0.21738365292549133,
      "learning_rate": 1.5794481446241674e-05,
      "loss": 0.0669,
      "step": 1946
    },
    {
      "epoch": 2.7636621717530163,
      "grad_norm": 0.20137126743793488,
      "learning_rate": 1.5699333967649857e-05,
      "loss": 0.0717,
      "step": 1947
    },
    {
      "epoch": 2.765081618168914,
      "grad_norm": 0.20311547815799713,
      "learning_rate": 1.560418648905804e-05,
      "loss": 0.0706,
      "step": 1948
    },
    {
      "epoch": 2.766501064584812,
      "grad_norm": 0.19374048709869385,
      "learning_rate": 1.5509039010466224e-05,
      "loss": 0.0684,
      "step": 1949
    },
    {
      "epoch": 2.7679205110007095,
      "grad_norm": 0.1529543399810791,
      "learning_rate": 1.5413891531874404e-05,
      "loss": 0.0608,
      "step": 1950
    },
    {
      "epoch": 2.7693399574166078,
      "grad_norm": 0.16964974999427795,
      "learning_rate": 1.5318744053282587e-05,
      "loss": 0.0602,
      "step": 1951
    },
    {
      "epoch": 2.770759403832505,
      "grad_norm": 0.16012424230575562,
      "learning_rate": 1.522359657469077e-05,
      "loss": 0.061,
      "step": 1952
    },
    {
      "epoch": 2.772178850248403,
      "grad_norm": 0.18353797495365143,
      "learning_rate": 1.5128449096098954e-05,
      "loss": 0.0602,
      "step": 1953
    },
    {
      "epoch": 2.773598296664301,
      "grad_norm": 0.1431272029876709,
      "learning_rate": 1.5033301617507137e-05,
      "loss": 0.0585,
      "step": 1954
    },
    {
      "epoch": 2.7750177430801988,
      "grad_norm": 0.15288320183753967,
      "learning_rate": 1.4938154138915319e-05,
      "loss": 0.0671,
      "step": 1955
    },
    {
      "epoch": 2.7764371894960966,
      "grad_norm": 0.1810208112001419,
      "learning_rate": 1.4843006660323502e-05,
      "loss": 0.0635,
      "step": 1956
    },
    {
      "epoch": 2.777856635911994,
      "grad_norm": 0.1415603905916214,
      "learning_rate": 1.4747859181731686e-05,
      "loss": 0.0563,
      "step": 1957
    },
    {
      "epoch": 2.7792760823278924,
      "grad_norm": 0.15445151925086975,
      "learning_rate": 1.4652711703139867e-05,
      "loss": 0.0626,
      "step": 1958
    },
    {
      "epoch": 2.78069552874379,
      "grad_norm": 0.1414465457201004,
      "learning_rate": 1.455756422454805e-05,
      "loss": 0.0499,
      "step": 1959
    },
    {
      "epoch": 2.7821149751596876,
      "grad_norm": 0.5781146287918091,
      "learning_rate": 1.4462416745956234e-05,
      "loss": 0.2053,
      "step": 1960
    },
    {
      "epoch": 2.7835344215755855,
      "grad_norm": 0.5816090106964111,
      "learning_rate": 1.4367269267364417e-05,
      "loss": 0.1977,
      "step": 1961
    },
    {
      "epoch": 2.7849538679914834,
      "grad_norm": 0.5768044590950012,
      "learning_rate": 1.4272121788772599e-05,
      "loss": 0.192,
      "step": 1962
    },
    {
      "epoch": 2.7863733144073812,
      "grad_norm": 0.44712451100349426,
      "learning_rate": 1.4176974310180782e-05,
      "loss": 0.1515,
      "step": 1963
    },
    {
      "epoch": 2.7877927608232786,
      "grad_norm": 0.5414344072341919,
      "learning_rate": 1.4081826831588962e-05,
      "loss": 0.1466,
      "step": 1964
    },
    {
      "epoch": 2.789212207239177,
      "grad_norm": 0.5132465362548828,
      "learning_rate": 1.3986679352997146e-05,
      "loss": 0.1349,
      "step": 1965
    },
    {
      "epoch": 2.7906316536550744,
      "grad_norm": 0.5994132161140442,
      "learning_rate": 1.3891531874405327e-05,
      "loss": 0.1537,
      "step": 1966
    },
    {
      "epoch": 2.7920511000709722,
      "grad_norm": 0.5213509202003479,
      "learning_rate": 1.379638439581351e-05,
      "loss": 0.1511,
      "step": 1967
    },
    {
      "epoch": 2.79347054648687,
      "grad_norm": 0.4583136737346649,
      "learning_rate": 1.3701236917221694e-05,
      "loss": 0.1336,
      "step": 1968
    },
    {
      "epoch": 2.794889992902768,
      "grad_norm": 0.43030935525894165,
      "learning_rate": 1.3606089438629876e-05,
      "loss": 0.1311,
      "step": 1969
    },
    {
      "epoch": 2.796309439318666,
      "grad_norm": 0.42564767599105835,
      "learning_rate": 1.351094196003806e-05,
      "loss": 0.1457,
      "step": 1970
    },
    {
      "epoch": 2.7977288857345637,
      "grad_norm": 0.5245090126991272,
      "learning_rate": 1.3415794481446243e-05,
      "loss": 0.1386,
      "step": 1971
    },
    {
      "epoch": 2.7991483321504615,
      "grad_norm": 0.4005574584007263,
      "learning_rate": 1.3320647002854424e-05,
      "loss": 0.1187,
      "step": 1972
    },
    {
      "epoch": 2.800567778566359,
      "grad_norm": 0.40005940198898315,
      "learning_rate": 1.3225499524262608e-05,
      "loss": 0.135,
      "step": 1973
    },
    {
      "epoch": 2.801987224982257,
      "grad_norm": 0.3529641330242157,
      "learning_rate": 1.3130352045670791e-05,
      "loss": 0.1128,
      "step": 1974
    },
    {
      "epoch": 2.8034066713981547,
      "grad_norm": 0.38755378127098083,
      "learning_rate": 1.3035204567078974e-05,
      "loss": 0.104,
      "step": 1975
    },
    {
      "epoch": 2.8048261178140526,
      "grad_norm": 0.35170379281044006,
      "learning_rate": 1.2940057088487156e-05,
      "loss": 0.1057,
      "step": 1976
    },
    {
      "epoch": 2.8062455642299504,
      "grad_norm": 0.3373531699180603,
      "learning_rate": 1.284490960989534e-05,
      "loss": 0.1108,
      "step": 1977
    },
    {
      "epoch": 2.8076650106458483,
      "grad_norm": 0.3387095034122467,
      "learning_rate": 1.2749762131303523e-05,
      "loss": 0.1031,
      "step": 1978
    },
    {
      "epoch": 2.809084457061746,
      "grad_norm": 0.3989664316177368,
      "learning_rate": 1.2654614652711704e-05,
      "loss": 0.1227,
      "step": 1979
    },
    {
      "epoch": 2.8105039034776436,
      "grad_norm": 0.2481224238872528,
      "learning_rate": 1.2559467174119888e-05,
      "loss": 0.0793,
      "step": 1980
    },
    {
      "epoch": 2.8119233498935414,
      "grad_norm": 0.21809916198253632,
      "learning_rate": 1.246431969552807e-05,
      "loss": 0.0752,
      "step": 1981
    },
    {
      "epoch": 2.8133427963094393,
      "grad_norm": 0.25266480445861816,
      "learning_rate": 1.2369172216936253e-05,
      "loss": 0.0864,
      "step": 1982
    },
    {
      "epoch": 2.814762242725337,
      "grad_norm": 0.29678404331207275,
      "learning_rate": 1.2274024738344434e-05,
      "loss": 0.1092,
      "step": 1983
    },
    {
      "epoch": 2.816181689141235,
      "grad_norm": 0.29585087299346924,
      "learning_rate": 1.2178877259752618e-05,
      "loss": 0.1145,
      "step": 1984
    },
    {
      "epoch": 2.817601135557133,
      "grad_norm": 0.34492170810699463,
      "learning_rate": 1.2083729781160801e-05,
      "loss": 0.09,
      "step": 1985
    },
    {
      "epoch": 2.8190205819730307,
      "grad_norm": 0.23931469023227692,
      "learning_rate": 1.1988582302568983e-05,
      "loss": 0.0983,
      "step": 1986
    },
    {
      "epoch": 2.820440028388928,
      "grad_norm": 0.23630143702030182,
      "learning_rate": 1.1893434823977164e-05,
      "loss": 0.0823,
      "step": 1987
    },
    {
      "epoch": 2.821859474804826,
      "grad_norm": 0.21431031823158264,
      "learning_rate": 1.1798287345385348e-05,
      "loss": 0.0797,
      "step": 1988
    },
    {
      "epoch": 2.823278921220724,
      "grad_norm": 0.276755154132843,
      "learning_rate": 1.1703139866793531e-05,
      "loss": 0.0844,
      "step": 1989
    },
    {
      "epoch": 2.8246983676366217,
      "grad_norm": 0.20050965249538422,
      "learning_rate": 1.1607992388201713e-05,
      "loss": 0.0717,
      "step": 1990
    },
    {
      "epoch": 2.8261178140525196,
      "grad_norm": 0.18103650212287903,
      "learning_rate": 1.1512844909609896e-05,
      "loss": 0.0666,
      "step": 1991
    },
    {
      "epoch": 2.8275372604684175,
      "grad_norm": 0.19498181343078613,
      "learning_rate": 1.141769743101808e-05,
      "loss": 0.0812,
      "step": 1992
    },
    {
      "epoch": 2.8289567068843153,
      "grad_norm": 0.17638099193572998,
      "learning_rate": 1.1322549952426261e-05,
      "loss": 0.0681,
      "step": 1993
    },
    {
      "epoch": 2.8303761533002127,
      "grad_norm": 0.18531982600688934,
      "learning_rate": 1.1227402473834443e-05,
      "loss": 0.0745,
      "step": 1994
    },
    {
      "epoch": 2.8317955997161106,
      "grad_norm": 0.21908044815063477,
      "learning_rate": 1.1132254995242626e-05,
      "loss": 0.0715,
      "step": 1995
    },
    {
      "epoch": 2.8332150461320085,
      "grad_norm": 0.18277457356452942,
      "learning_rate": 1.103710751665081e-05,
      "loss": 0.0785,
      "step": 1996
    },
    {
      "epoch": 2.8346344925479063,
      "grad_norm": 0.20543172955513,
      "learning_rate": 1.0941960038058991e-05,
      "loss": 0.073,
      "step": 1997
    },
    {
      "epoch": 2.836053938963804,
      "grad_norm": 0.19596345722675323,
      "learning_rate": 1.0846812559467175e-05,
      "loss": 0.0657,
      "step": 1998
    },
    {
      "epoch": 2.837473385379702,
      "grad_norm": 0.17131707072257996,
      "learning_rate": 1.0751665080875358e-05,
      "loss": 0.0657,
      "step": 1999
    },
    {
      "epoch": 2.8388928317956,
      "grad_norm": 0.1895187646150589,
      "learning_rate": 1.065651760228354e-05,
      "loss": 0.0743,
      "step": 2000
    }
  ],
  "logging_steps": 1,
  "max_steps": 2112,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1.3818965227175117e+17,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
