{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.4194464158978,
  "eval_steps": 423,
  "global_step": 1000,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0014194464158978,
      "grad_norm": 3.8753199577331543,
      "learning_rate": 2e-05,
      "loss": 2.6848,
      "step": 1
    },
    {
      "epoch": 0.0028388928317956,
      "grad_norm": 4.06846809387207,
      "learning_rate": 4e-05,
      "loss": 2.6312,
      "step": 2
    },
    {
      "epoch": 0.0042583392476933995,
      "grad_norm": 3.860471248626709,
      "learning_rate": 6e-05,
      "loss": 2.6378,
      "step": 3
    },
    {
      "epoch": 0.0056777856635912,
      "grad_norm": 2.591989517211914,
      "learning_rate": 8e-05,
      "loss": 2.4154,
      "step": 4
    },
    {
      "epoch": 0.007097232079488999,
      "grad_norm": 2.2637760639190674,
      "learning_rate": 0.0001,
      "loss": 2.2543,
      "step": 5
    },
    {
      "epoch": 0.008516678495386799,
      "grad_norm": 1.7446980476379395,
      "learning_rate": 0.00012,
      "loss": 2.0235,
      "step": 6
    },
    {
      "epoch": 0.0099361249112846,
      "grad_norm": 1.61598801612854,
      "learning_rate": 0.00014,
      "loss": 1.8347,
      "step": 7
    },
    {
      "epoch": 0.0113555713271824,
      "grad_norm": 1.692824363708496,
      "learning_rate": 0.00016,
      "loss": 1.5201,
      "step": 8
    },
    {
      "epoch": 0.0127750177430802,
      "grad_norm": 1.7318449020385742,
      "learning_rate": 0.00018,
      "loss": 1.1865,
      "step": 9
    },
    {
      "epoch": 0.014194464158977998,
      "grad_norm": 2.3941969871520996,
      "learning_rate": 0.0002,
      "loss": 0.8478,
      "step": 10
    },
    {
      "epoch": 0.015613910574875798,
      "grad_norm": 2.488072395324707,
      "learning_rate": 0.0001999048525214082,
      "loss": 0.5818,
      "step": 11
    },
    {
      "epoch": 0.017033356990773598,
      "grad_norm": 5.200738906860352,
      "learning_rate": 0.00019980970504281638,
      "loss": 0.4402,
      "step": 12
    },
    {
      "epoch": 0.018452803406671398,
      "grad_norm": 1.2015495300292969,
      "learning_rate": 0.00019971455756422456,
      "loss": 0.363,
      "step": 13
    },
    {
      "epoch": 0.0198722498225692,
      "grad_norm": 0.5281029939651489,
      "learning_rate": 0.00019961941008563274,
      "loss": 0.329,
      "step": 14
    },
    {
      "epoch": 0.021291696238467,
      "grad_norm": 0.5371572375297546,
      "learning_rate": 0.0001995242626070409,
      "loss": 0.2941,
      "step": 15
    },
    {
      "epoch": 0.0227111426543648,
      "grad_norm": 0.6878210306167603,
      "learning_rate": 0.0001994291151284491,
      "loss": 0.3151,
      "step": 16
    },
    {
      "epoch": 0.0241305890702626,
      "grad_norm": 0.7888450026512146,
      "learning_rate": 0.00019933396764985727,
      "loss": 0.2811,
      "step": 17
    },
    {
      "epoch": 0.0255500354861604,
      "grad_norm": 0.9834377765655518,
      "learning_rate": 0.00019923882017126548,
      "loss": 0.2909,
      "step": 18
    },
    {
      "epoch": 0.0269694819020582,
      "grad_norm": 1.111398696899414,
      "learning_rate": 0.00019914367269267363,
      "loss": 0.2877,
      "step": 19
    },
    {
      "epoch": 0.028388928317955996,
      "grad_norm": 1.3554736375808716,
      "learning_rate": 0.00019904852521408184,
      "loss": 0.2168,
      "step": 20
    },
    {
      "epoch": 0.029808374733853796,
      "grad_norm": 2.863246202468872,
      "learning_rate": 0.00019895337773549,
      "loss": 0.2247,
      "step": 21
    },
    {
      "epoch": 0.031227821149751596,
      "grad_norm": 0.4769139587879181,
      "learning_rate": 0.0001988582302568982,
      "loss": 0.2451,
      "step": 22
    },
    {
      "epoch": 0.032647267565649396,
      "grad_norm": 0.3291550576686859,
      "learning_rate": 0.00019876308277830637,
      "loss": 0.1798,
      "step": 23
    },
    {
      "epoch": 0.034066713981547196,
      "grad_norm": 0.3495163321495056,
      "learning_rate": 0.00019866793529971458,
      "loss": 0.1882,
      "step": 24
    },
    {
      "epoch": 0.035486160397444996,
      "grad_norm": 0.4018864035606384,
      "learning_rate": 0.00019857278782112273,
      "loss": 0.1911,
      "step": 25
    },
    {
      "epoch": 0.036905606813342796,
      "grad_norm": 0.5376371145248413,
      "learning_rate": 0.00019847764034253094,
      "loss": 0.156,
      "step": 26
    },
    {
      "epoch": 0.0383250532292406,
      "grad_norm": 0.8540717959403992,
      "learning_rate": 0.0001983824928639391,
      "loss": 0.1568,
      "step": 27
    },
    {
      "epoch": 0.0397444996451384,
      "grad_norm": 0.9595521092414856,
      "learning_rate": 0.0001982873453853473,
      "loss": 0.1599,
      "step": 28
    },
    {
      "epoch": 0.0411639460610362,
      "grad_norm": 0.7133545875549316,
      "learning_rate": 0.00019819219790675547,
      "loss": 0.1678,
      "step": 29
    },
    {
      "epoch": 0.042583392476934,
      "grad_norm": 0.43954452872276306,
      "learning_rate": 0.00019809705042816368,
      "loss": 0.1581,
      "step": 30
    },
    {
      "epoch": 0.0440028388928318,
      "grad_norm": 0.51581871509552,
      "learning_rate": 0.00019800190294957183,
      "loss": 0.1458,
      "step": 31
    },
    {
      "epoch": 0.0454222853087296,
      "grad_norm": 0.5623205900192261,
      "learning_rate": 0.00019790675547098004,
      "loss": 0.1473,
      "step": 32
    },
    {
      "epoch": 0.0468417317246274,
      "grad_norm": 0.6040207147598267,
      "learning_rate": 0.0001978116079923882,
      "loss": 0.1368,
      "step": 33
    },
    {
      "epoch": 0.0482611781405252,
      "grad_norm": 0.556030809879303,
      "learning_rate": 0.0001977164605137964,
      "loss": 0.1352,
      "step": 34
    },
    {
      "epoch": 0.049680624556423,
      "grad_norm": 0.49861204624176025,
      "learning_rate": 0.00019762131303520457,
      "loss": 0.1348,
      "step": 35
    },
    {
      "epoch": 0.0511000709723208,
      "grad_norm": 0.27800312638282776,
      "learning_rate": 0.00019752616555661275,
      "loss": 0.109,
      "step": 36
    },
    {
      "epoch": 0.0525195173882186,
      "grad_norm": 0.2570524215698242,
      "learning_rate": 0.00019743101807802093,
      "loss": 0.1141,
      "step": 37
    },
    {
      "epoch": 0.0539389638041164,
      "grad_norm": 0.3502003848552704,
      "learning_rate": 0.00019733587059942912,
      "loss": 0.1413,
      "step": 38
    },
    {
      "epoch": 0.05535841022001419,
      "grad_norm": 0.3913830518722534,
      "learning_rate": 0.0001972407231208373,
      "loss": 0.0961,
      "step": 39
    },
    {
      "epoch": 0.05677785663591199,
      "grad_norm": 0.4328113794326782,
      "learning_rate": 0.00019714557564224548,
      "loss": 0.097,
      "step": 40
    },
    {
      "epoch": 0.05819730305180979,
      "grad_norm": 0.38531455397605896,
      "learning_rate": 0.00019705042816365367,
      "loss": 0.1106,
      "step": 41
    },
    {
      "epoch": 0.05961674946770759,
      "grad_norm": 0.35614219307899475,
      "learning_rate": 0.00019695528068506185,
      "loss": 0.0956,
      "step": 42
    },
    {
      "epoch": 0.06103619588360539,
      "grad_norm": 0.26061660051345825,
      "learning_rate": 0.00019686013320647003,
      "loss": 0.0981,
      "step": 43
    },
    {
      "epoch": 0.06245564229950319,
      "grad_norm": 0.16854628920555115,
      "learning_rate": 0.00019676498572787822,
      "loss": 0.0877,
      "step": 44
    },
    {
      "epoch": 0.063875088715401,
      "grad_norm": 0.359539657831192,
      "learning_rate": 0.0001966698382492864,
      "loss": 0.0816,
      "step": 45
    },
    {
      "epoch": 0.06529453513129879,
      "grad_norm": 0.2330690622329712,
      "learning_rate": 0.00019657469077069458,
      "loss": 0.101,
      "step": 46
    },
    {
      "epoch": 0.0667139815471966,
      "grad_norm": 0.25437015295028687,
      "learning_rate": 0.00019647954329210277,
      "loss": 0.1031,
      "step": 47
    },
    {
      "epoch": 0.06813342796309439,
      "grad_norm": 0.2970932722091675,
      "learning_rate": 0.00019638439581351095,
      "loss": 0.0731,
      "step": 48
    },
    {
      "epoch": 0.0695528743789922,
      "grad_norm": 0.2961924374103546,
      "learning_rate": 0.00019628924833491913,
      "loss": 0.0669,
      "step": 49
    },
    {
      "epoch": 0.07097232079488999,
      "grad_norm": 0.288361519575119,
      "learning_rate": 0.00019619410085632732,
      "loss": 0.0664,
      "step": 50
    },
    {
      "epoch": 0.0723917672107878,
      "grad_norm": 0.8922231197357178,
      "learning_rate": 0.0001960989533777355,
      "loss": 0.5873,
      "step": 51
    },
    {
      "epoch": 0.07381121362668559,
      "grad_norm": 0.5401262044906616,
      "learning_rate": 0.00019600380589914368,
      "loss": 0.447,
      "step": 52
    },
    {
      "epoch": 0.07523066004258339,
      "grad_norm": 1.9218655824661255,
      "learning_rate": 0.00019590865842055187,
      "loss": 0.3904,
      "step": 53
    },
    {
      "epoch": 0.0766501064584812,
      "grad_norm": 0.5285252928733826,
      "learning_rate": 0.00019581351094196005,
      "loss": 0.3745,
      "step": 54
    },
    {
      "epoch": 0.07806955287437899,
      "grad_norm": 0.31439849734306335,
      "learning_rate": 0.00019571836346336823,
      "loss": 0.3307,
      "step": 55
    },
    {
      "epoch": 0.0794889992902768,
      "grad_norm": 0.4290097951889038,
      "learning_rate": 0.00019562321598477642,
      "loss": 0.3252,
      "step": 56
    },
    {
      "epoch": 0.08090844570617459,
      "grad_norm": 0.3310398459434509,
      "learning_rate": 0.0001955280685061846,
      "loss": 0.3135,
      "step": 57
    },
    {
      "epoch": 0.0823278921220724,
      "grad_norm": 0.32844847440719604,
      "learning_rate": 0.00019543292102759278,
      "loss": 0.2843,
      "step": 58
    },
    {
      "epoch": 0.08374733853797019,
      "grad_norm": 0.3329128324985504,
      "learning_rate": 0.00019533777354900097,
      "loss": 0.271,
      "step": 59
    },
    {
      "epoch": 0.085166784953868,
      "grad_norm": 0.2432798445224762,
      "learning_rate": 0.00019524262607040915,
      "loss": 0.2519,
      "step": 60
    },
    {
      "epoch": 0.08658623136976579,
      "grad_norm": 0.2554329037666321,
      "learning_rate": 0.00019514747859181733,
      "loss": 0.2459,
      "step": 61
    },
    {
      "epoch": 0.0880056777856636,
      "grad_norm": 0.2721186876296997,
      "learning_rate": 0.00019505233111322552,
      "loss": 0.2464,
      "step": 62
    },
    {
      "epoch": 0.08942512420156139,
      "grad_norm": 0.22954344749450684,
      "learning_rate": 0.0001949571836346337,
      "loss": 0.2196,
      "step": 63
    },
    {
      "epoch": 0.0908445706174592,
      "grad_norm": 0.26764383912086487,
      "learning_rate": 0.00019486203615604188,
      "loss": 0.228,
      "step": 64
    },
    {
      "epoch": 0.09226401703335699,
      "grad_norm": 0.22909002006053925,
      "learning_rate": 0.00019476688867745007,
      "loss": 0.2039,
      "step": 65
    },
    {
      "epoch": 0.0936834634492548,
      "grad_norm": 0.2602919340133667,
      "learning_rate": 0.00019467174119885825,
      "loss": 0.2007,
      "step": 66
    },
    {
      "epoch": 0.09510290986515259,
      "grad_norm": 0.22124329209327698,
      "learning_rate": 0.0001945765937202664,
      "loss": 0.1886,
      "step": 67
    },
    {
      "epoch": 0.0965223562810504,
      "grad_norm": 0.21159298717975616,
      "learning_rate": 0.00019448144624167462,
      "loss": 0.2003,
      "step": 68
    },
    {
      "epoch": 0.09794180269694819,
      "grad_norm": 0.2761073112487793,
      "learning_rate": 0.00019438629876308277,
      "loss": 0.2093,
      "step": 69
    },
    {
      "epoch": 0.099361249112846,
      "grad_norm": 0.20292288064956665,
      "learning_rate": 0.00019429115128449098,
      "loss": 0.1712,
      "step": 70
    },
    {
      "epoch": 0.10078069552874379,
      "grad_norm": 0.19448138773441315,
      "learning_rate": 0.00019419600380589914,
      "loss": 0.1434,
      "step": 71
    },
    {
      "epoch": 0.1022001419446416,
      "grad_norm": 0.20379109680652618,
      "learning_rate": 0.00019410085632730735,
      "loss": 0.1644,
      "step": 72
    },
    {
      "epoch": 0.10361958836053939,
      "grad_norm": 0.223442941904068,
      "learning_rate": 0.0001940057088487155,
      "loss": 0.1879,
      "step": 73
    },
    {
      "epoch": 0.1050390347764372,
      "grad_norm": 0.1932620406150818,
      "learning_rate": 0.00019391056137012372,
      "loss": 0.1413,
      "step": 74
    },
    {
      "epoch": 0.10645848119233499,
      "grad_norm": 0.2164372056722641,
      "learning_rate": 0.00019381541389153187,
      "loss": 0.1703,
      "step": 75
    },
    {
      "epoch": 0.1078779276082328,
      "grad_norm": 0.1846214234828949,
      "learning_rate": 0.00019372026641294008,
      "loss": 0.1742,
      "step": 76
    },
    {
      "epoch": 0.10929737402413059,
      "grad_norm": 0.16839642822742462,
      "learning_rate": 0.00019362511893434824,
      "loss": 0.1471,
      "step": 77
    },
    {
      "epoch": 0.11071682044002838,
      "grad_norm": 0.2075975090265274,
      "learning_rate": 0.00019352997145575645,
      "loss": 0.1939,
      "step": 78
    },
    {
      "epoch": 0.11213626685592619,
      "grad_norm": 0.21337558329105377,
      "learning_rate": 0.0001934348239771646,
      "loss": 0.1677,
      "step": 79
    },
    {
      "epoch": 0.11355571327182398,
      "grad_norm": 0.1997154951095581,
      "learning_rate": 0.00019333967649857282,
      "loss": 0.1525,
      "step": 80
    },
    {
      "epoch": 0.11497515968772179,
      "grad_norm": 0.20924781262874603,
      "learning_rate": 0.00019324452901998097,
      "loss": 0.1234,
      "step": 81
    },
    {
      "epoch": 0.11639460610361958,
      "grad_norm": 0.20264357328414917,
      "learning_rate": 0.00019314938154138916,
      "loss": 0.1459,
      "step": 82
    },
    {
      "epoch": 0.11781405251951739,
      "grad_norm": 0.15568575263023376,
      "learning_rate": 0.00019305423406279734,
      "loss": 0.1262,
      "step": 83
    },
    {
      "epoch": 0.11923349893541518,
      "grad_norm": 0.18003098666667938,
      "learning_rate": 0.00019295908658420552,
      "loss": 0.1073,
      "step": 84
    },
    {
      "epoch": 0.12065294535131299,
      "grad_norm": 0.18673446774482727,
      "learning_rate": 0.0001928639391056137,
      "loss": 0.1248,
      "step": 85
    },
    {
      "epoch": 0.12207239176721078,
      "grad_norm": 0.16824555397033691,
      "learning_rate": 0.0001927687916270219,
      "loss": 0.1181,
      "step": 86
    },
    {
      "epoch": 0.12349183818310859,
      "grad_norm": 0.15063299238681793,
      "learning_rate": 0.00019267364414843007,
      "loss": 0.1117,
      "step": 87
    },
    {
      "epoch": 0.12491128459900638,
      "grad_norm": 0.16303810477256775,
      "learning_rate": 0.00019257849666983826,
      "loss": 0.105,
      "step": 88
    },
    {
      "epoch": 0.1263307310149042,
      "grad_norm": 0.16362158954143524,
      "learning_rate": 0.00019248334919124644,
      "loss": 0.0931,
      "step": 89
    },
    {
      "epoch": 0.127750177430802,
      "grad_norm": 0.15042024850845337,
      "learning_rate": 0.00019238820171265462,
      "loss": 0.0981,
      "step": 90
    },
    {
      "epoch": 0.12916962384669978,
      "grad_norm": 0.14741793274879456,
      "learning_rate": 0.0001922930542340628,
      "loss": 0.0986,
      "step": 91
    },
    {
      "epoch": 0.13058907026259758,
      "grad_norm": 0.158548966050148,
      "learning_rate": 0.000192197906755471,
      "loss": 0.0884,
      "step": 92
    },
    {
      "epoch": 0.1320085166784954,
      "grad_norm": 0.14750920236110687,
      "learning_rate": 0.00019210275927687917,
      "loss": 0.0886,
      "step": 93
    },
    {
      "epoch": 0.1334279630943932,
      "grad_norm": 0.12116905301809311,
      "learning_rate": 0.00019200761179828736,
      "loss": 0.0841,
      "step": 94
    },
    {
      "epoch": 0.13484740951029098,
      "grad_norm": 0.12684527039527893,
      "learning_rate": 0.00019191246431969554,
      "loss": 0.0836,
      "step": 95
    },
    {
      "epoch": 0.13626685592618878,
      "grad_norm": 0.13355328142642975,
      "learning_rate": 0.00019181731684110372,
      "loss": 0.0852,
      "step": 96
    },
    {
      "epoch": 0.1376863023420866,
      "grad_norm": 0.11786916851997375,
      "learning_rate": 0.0001917221693625119,
      "loss": 0.066,
      "step": 97
    },
    {
      "epoch": 0.1391057487579844,
      "grad_norm": 0.1535901427268982,
      "learning_rate": 0.00019162702188392006,
      "loss": 0.0697,
      "step": 98
    },
    {
      "epoch": 0.14052519517388218,
      "grad_norm": 0.10121750086545944,
      "learning_rate": 0.00019153187440532827,
      "loss": 0.0604,
      "step": 99
    },
    {
      "epoch": 0.14194464158977999,
      "grad_norm": 0.09791870415210724,
      "learning_rate": 0.00019143672692673643,
      "loss": 0.0422,
      "step": 100
    },
    {
      "epoch": 0.1433640880056778,
      "grad_norm": 0.8991482853889465,
      "learning_rate": 0.00019134157944814464,
      "loss": 0.6245,
      "step": 101
    },
    {
      "epoch": 0.1447835344215756,
      "grad_norm": 0.536517858505249,
      "learning_rate": 0.0001912464319695528,
      "loss": 0.4928,
      "step": 102
    },
    {
      "epoch": 0.14620298083747338,
      "grad_norm": 0.3712337911128998,
      "learning_rate": 0.000191151284490961,
      "loss": 0.4136,
      "step": 103
    },
    {
      "epoch": 0.14762242725337119,
      "grad_norm": 0.29865872859954834,
      "learning_rate": 0.00019105613701236916,
      "loss": 0.3265,
      "step": 104
    },
    {
      "epoch": 0.149041873669269,
      "grad_norm": 0.3236014246940613,
      "learning_rate": 0.00019096098953377737,
      "loss": 0.3386,
      "step": 105
    },
    {
      "epoch": 0.15046132008516677,
      "grad_norm": 0.31175604462623596,
      "learning_rate": 0.00019086584205518553,
      "loss": 0.3567,
      "step": 106
    },
    {
      "epoch": 0.15188076650106458,
      "grad_norm": 0.3100660741329193,
      "learning_rate": 0.00019077069457659374,
      "loss": 0.3257,
      "step": 107
    },
    {
      "epoch": 0.1533002129169624,
      "grad_norm": 0.30780166387557983,
      "learning_rate": 0.0001906755470980019,
      "loss": 0.3294,
      "step": 108
    },
    {
      "epoch": 0.1547196593328602,
      "grad_norm": 0.2866743505001068,
      "learning_rate": 0.0001905803996194101,
      "loss": 0.3086,
      "step": 109
    },
    {
      "epoch": 0.15613910574875797,
      "grad_norm": 0.25725001096725464,
      "learning_rate": 0.00019048525214081826,
      "loss": 0.2569,
      "step": 110
    },
    {
      "epoch": 0.15755855216465578,
      "grad_norm": 0.2841548025608063,
      "learning_rate": 0.00019039010466222647,
      "loss": 0.2833,
      "step": 111
    },
    {
      "epoch": 0.1589779985805536,
      "grad_norm": 0.21969172358512878,
      "learning_rate": 0.00019029495718363463,
      "loss": 0.213,
      "step": 112
    },
    {
      "epoch": 0.1603974449964514,
      "grad_norm": 0.2883078455924988,
      "learning_rate": 0.00019019980970504284,
      "loss": 0.2762,
      "step": 113
    },
    {
      "epoch": 0.16181689141234917,
      "grad_norm": 0.21333856880664825,
      "learning_rate": 0.000190104662226451,
      "loss": 0.2362,
      "step": 114
    },
    {
      "epoch": 0.16323633782824698,
      "grad_norm": 0.2552758753299713,
      "learning_rate": 0.0001900095147478592,
      "loss": 0.2223,
      "step": 115
    },
    {
      "epoch": 0.1646557842441448,
      "grad_norm": 0.23104964196681976,
      "learning_rate": 0.00018991436726926736,
      "loss": 0.1887,
      "step": 116
    },
    {
      "epoch": 0.1660752306600426,
      "grad_norm": 0.27122604846954346,
      "learning_rate": 0.00018981921979067558,
      "loss": 0.2177,
      "step": 117
    },
    {
      "epoch": 0.16749467707594037,
      "grad_norm": 0.2582598626613617,
      "learning_rate": 0.00018972407231208373,
      "loss": 0.2375,
      "step": 118
    },
    {
      "epoch": 0.16891412349183818,
      "grad_norm": 0.2079077512025833,
      "learning_rate": 0.00018962892483349191,
      "loss": 0.1653,
      "step": 119
    },
    {
      "epoch": 0.170333569907736,
      "grad_norm": 0.2133801132440567,
      "learning_rate": 0.0001895337773549001,
      "loss": 0.1939,
      "step": 120
    },
    {
      "epoch": 0.1717530163236338,
      "grad_norm": 0.25759196281433105,
      "learning_rate": 0.00018943862987630828,
      "loss": 0.2134,
      "step": 121
    },
    {
      "epoch": 0.17317246273953157,
      "grad_norm": 0.1881779581308365,
      "learning_rate": 0.00018934348239771646,
      "loss": 0.1756,
      "step": 122
    },
    {
      "epoch": 0.17459190915542938,
      "grad_norm": 0.227374866604805,
      "learning_rate": 0.00018924833491912465,
      "loss": 0.1788,
      "step": 123
    },
    {
      "epoch": 0.1760113555713272,
      "grad_norm": 0.21692273020744324,
      "learning_rate": 0.00018915318744053283,
      "loss": 0.1817,
      "step": 124
    },
    {
      "epoch": 0.177430801987225,
      "grad_norm": 0.22101150453090668,
      "learning_rate": 0.00018905803996194101,
      "loss": 0.176,
      "step": 125
    },
    {
      "epoch": 0.17885024840312277,
      "grad_norm": 0.1902722418308258,
      "learning_rate": 0.0001889628924833492,
      "loss": 0.1228,
      "step": 126
    },
    {
      "epoch": 0.18026969481902058,
      "grad_norm": 0.20391739904880524,
      "learning_rate": 0.00018886774500475738,
      "loss": 0.1454,
      "step": 127
    },
    {
      "epoch": 0.1816891412349184,
      "grad_norm": 0.16759422421455383,
      "learning_rate": 0.00018877259752616556,
      "loss": 0.115,
      "step": 128
    },
    {
      "epoch": 0.18310858765081617,
      "grad_norm": 0.17888155579566956,
      "learning_rate": 0.00018867745004757375,
      "loss": 0.1211,
      "step": 129
    },
    {
      "epoch": 0.18452803406671398,
      "grad_norm": 0.1745491474866867,
      "learning_rate": 0.00018858230256898193,
      "loss": 0.133,
      "step": 130
    },
    {
      "epoch": 0.18594748048261178,
      "grad_norm": 0.168021097779274,
      "learning_rate": 0.00018848715509039012,
      "loss": 0.137,
      "step": 131
    },
    {
      "epoch": 0.1873669268985096,
      "grad_norm": 0.2074587345123291,
      "learning_rate": 0.0001883920076117983,
      "loss": 0.1185,
      "step": 132
    },
    {
      "epoch": 0.18878637331440737,
      "grad_norm": 0.18290935456752777,
      "learning_rate": 0.00018829686013320648,
      "loss": 0.1491,
      "step": 133
    },
    {
      "epoch": 0.19020581973030518,
      "grad_norm": 0.20074568688869476,
      "learning_rate": 0.00018820171265461467,
      "loss": 0.1425,
      "step": 134
    },
    {
      "epoch": 0.19162526614620298,
      "grad_norm": 0.16316284239292145,
      "learning_rate": 0.00018810656517602285,
      "loss": 0.1285,
      "step": 135
    },
    {
      "epoch": 0.1930447125621008,
      "grad_norm": 0.13204751908779144,
      "learning_rate": 0.00018801141769743103,
      "loss": 0.1088,
      "step": 136
    },
    {
      "epoch": 0.19446415897799857,
      "grad_norm": 0.17812924087047577,
      "learning_rate": 0.00018791627021883922,
      "loss": 0.1283,
      "step": 137
    },
    {
      "epoch": 0.19588360539389638,
      "grad_norm": 0.17065832018852234,
      "learning_rate": 0.0001878211227402474,
      "loss": 0.1153,
      "step": 138
    },
    {
      "epoch": 0.19730305180979418,
      "grad_norm": 0.14613445103168488,
      "learning_rate": 0.00018772597526165558,
      "loss": 0.1048,
      "step": 139
    },
    {
      "epoch": 0.198722498225692,
      "grad_norm": 0.1457911729812622,
      "learning_rate": 0.00018763082778306377,
      "loss": 0.0918,
      "step": 140
    },
    {
      "epoch": 0.20014194464158977,
      "grad_norm": 0.13844063878059387,
      "learning_rate": 0.00018753568030447195,
      "loss": 0.0909,
      "step": 141
    },
    {
      "epoch": 0.20156139105748758,
      "grad_norm": 0.14067940413951874,
      "learning_rate": 0.00018744053282588013,
      "loss": 0.1034,
      "step": 142
    },
    {
      "epoch": 0.20298083747338538,
      "grad_norm": 0.12681680917739868,
      "learning_rate": 0.00018734538534728832,
      "loss": 0.0947,
      "step": 143
    },
    {
      "epoch": 0.2044002838892832,
      "grad_norm": 0.14871253073215485,
      "learning_rate": 0.0001872502378686965,
      "loss": 0.108,
      "step": 144
    },
    {
      "epoch": 0.20581973030518097,
      "grad_norm": 0.20518921315670013,
      "learning_rate": 0.00018715509039010468,
      "loss": 0.1038,
      "step": 145
    },
    {
      "epoch": 0.20723917672107878,
      "grad_norm": 0.17616136372089386,
      "learning_rate": 0.00018705994291151287,
      "loss": 0.0715,
      "step": 146
    },
    {
      "epoch": 0.20865862313697658,
      "grad_norm": 0.11235202103853226,
      "learning_rate": 0.00018696479543292105,
      "loss": 0.0786,
      "step": 147
    },
    {
      "epoch": 0.2100780695528744,
      "grad_norm": 0.12890516221523285,
      "learning_rate": 0.00018686964795432923,
      "loss": 0.0701,
      "step": 148
    },
    {
      "epoch": 0.21149751596877217,
      "grad_norm": 0.09201624244451523,
      "learning_rate": 0.00018677450047573742,
      "loss": 0.0552,
      "step": 149
    },
    {
      "epoch": 0.21291696238466998,
      "grad_norm": 0.1286080777645111,
      "learning_rate": 0.00018667935299714557,
      "loss": 0.0494,
      "step": 150
    },
    {
      "epoch": 0.21433640880056778,
      "grad_norm": 0.6585749387741089,
      "learning_rate": 0.00018658420551855376,
      "loss": 0.7252,
      "step": 151
    },
    {
      "epoch": 0.2157558552164656,
      "grad_norm": 0.5025274157524109,
      "learning_rate": 0.00018648905803996194,
      "loss": 0.5642,
      "step": 152
    },
    {
      "epoch": 0.21717530163236337,
      "grad_norm": 0.3372633755207062,
      "learning_rate": 0.00018639391056137012,
      "loss": 0.4086,
      "step": 153
    },
    {
      "epoch": 0.21859474804826118,
      "grad_norm": 0.298308789730072,
      "learning_rate": 0.0001862987630827783,
      "loss": 0.3602,
      "step": 154
    },
    {
      "epoch": 0.22001419446415899,
      "grad_norm": 0.254146933555603,
      "learning_rate": 0.0001862036156041865,
      "loss": 0.3183,
      "step": 155
    },
    {
      "epoch": 0.22143364088005676,
      "grad_norm": 0.2620442509651184,
      "learning_rate": 0.00018610846812559467,
      "loss": 0.3252,
      "step": 156
    },
    {
      "epoch": 0.22285308729595457,
      "grad_norm": 0.2677079737186432,
      "learning_rate": 0.00018601332064700286,
      "loss": 0.3069,
      "step": 157
    },
    {
      "epoch": 0.22427253371185238,
      "grad_norm": 0.26175227761268616,
      "learning_rate": 0.00018591817316841104,
      "loss": 0.3178,
      "step": 158
    },
    {
      "epoch": 0.22569198012775019,
      "grad_norm": 0.2651664614677429,
      "learning_rate": 0.00018582302568981922,
      "loss": 0.3102,
      "step": 159
    },
    {
      "epoch": 0.22711142654364797,
      "grad_norm": 0.24599607288837433,
      "learning_rate": 0.0001857278782112274,
      "loss": 0.2539,
      "step": 160
    },
    {
      "epoch": 0.22853087295954577,
      "grad_norm": 0.28982409834861755,
      "learning_rate": 0.0001856327307326356,
      "loss": 0.236,
      "step": 161
    },
    {
      "epoch": 0.22995031937544358,
      "grad_norm": 0.24877092242240906,
      "learning_rate": 0.00018553758325404377,
      "loss": 0.2827,
      "step": 162
    },
    {
      "epoch": 0.2313697657913414,
      "grad_norm": 0.31241002678871155,
      "learning_rate": 0.00018544243577545196,
      "loss": 0.2639,
      "step": 163
    },
    {
      "epoch": 0.23278921220723917,
      "grad_norm": 0.23131708800792694,
      "learning_rate": 0.00018534728829686014,
      "loss": 0.2099,
      "step": 164
    },
    {
      "epoch": 0.23420865862313697,
      "grad_norm": 0.24522876739501953,
      "learning_rate": 0.00018525214081826832,
      "loss": 0.2242,
      "step": 165
    },
    {
      "epoch": 0.23562810503903478,
      "grad_norm": 0.23746757209300995,
      "learning_rate": 0.0001851569933396765,
      "loss": 0.2173,
      "step": 166
    },
    {
      "epoch": 0.2370475514549326,
      "grad_norm": 0.19376133382320404,
      "learning_rate": 0.0001850618458610847,
      "loss": 0.1677,
      "step": 167
    },
    {
      "epoch": 0.23846699787083037,
      "grad_norm": 0.24670350551605225,
      "learning_rate": 0.00018496669838249287,
      "loss": 0.1954,
      "step": 168
    },
    {
      "epoch": 0.23988644428672817,
      "grad_norm": 0.2263229489326477,
      "learning_rate": 0.00018487155090390106,
      "loss": 0.1983,
      "step": 169
    },
    {
      "epoch": 0.24130589070262598,
      "grad_norm": 0.2091400921344757,
      "learning_rate": 0.00018477640342530924,
      "loss": 0.186,
      "step": 170
    },
    {
      "epoch": 0.2427253371185238,
      "grad_norm": 0.20422160625457764,
      "learning_rate": 0.00018468125594671742,
      "loss": 0.1713,
      "step": 171
    },
    {
      "epoch": 0.24414478353442157,
      "grad_norm": 0.21125274896621704,
      "learning_rate": 0.0001845861084681256,
      "loss": 0.1888,
      "step": 172
    },
    {
      "epoch": 0.24556422995031937,
      "grad_norm": 0.2694367468357086,
      "learning_rate": 0.0001844909609895338,
      "loss": 0.2226,
      "step": 173
    },
    {
      "epoch": 0.24698367636621718,
      "grad_norm": 0.18443642556667328,
      "learning_rate": 0.00018439581351094197,
      "loss": 0.1524,
      "step": 174
    },
    {
      "epoch": 0.248403122782115,
      "grad_norm": 0.20904751121997833,
      "learning_rate": 0.00018430066603235016,
      "loss": 0.1829,
      "step": 175
    },
    {
      "epoch": 0.24982256919801277,
      "grad_norm": 0.19625532627105713,
      "learning_rate": 0.00018420551855375834,
      "loss": 0.2097,
      "step": 176
    },
    {
      "epoch": 0.2512420156139106,
      "grad_norm": 0.19554728269577026,
      "learning_rate": 0.00018411037107516652,
      "loss": 0.1498,
      "step": 177
    },
    {
      "epoch": 0.2526614620298084,
      "grad_norm": 0.19032320380210876,
      "learning_rate": 0.0001840152235965747,
      "loss": 0.1684,
      "step": 178
    },
    {
      "epoch": 0.2540809084457062,
      "grad_norm": 0.1574752926826477,
      "learning_rate": 0.00018392007611798286,
      "loss": 0.1511,
      "step": 179
    },
    {
      "epoch": 0.255500354861604,
      "grad_norm": 0.15841525793075562,
      "learning_rate": 0.00018382492863939107,
      "loss": 0.1521,
      "step": 180
    },
    {
      "epoch": 0.25691980127750175,
      "grad_norm": 0.22403736412525177,
      "learning_rate": 0.00018372978116079923,
      "loss": 0.1683,
      "step": 181
    },
    {
      "epoch": 0.25833924769339955,
      "grad_norm": 0.1463969349861145,
      "learning_rate": 0.00018363463368220744,
      "loss": 0.1201,
      "step": 182
    },
    {
      "epoch": 0.25975869410929736,
      "grad_norm": 0.182897686958313,
      "learning_rate": 0.0001835394862036156,
      "loss": 0.1254,
      "step": 183
    },
    {
      "epoch": 0.26117814052519517,
      "grad_norm": 0.16555581986904144,
      "learning_rate": 0.0001834443387250238,
      "loss": 0.1435,
      "step": 184
    },
    {
      "epoch": 0.262597586941093,
      "grad_norm": 0.16409210860729218,
      "learning_rate": 0.00018334919124643196,
      "loss": 0.1144,
      "step": 185
    },
    {
      "epoch": 0.2640170333569908,
      "grad_norm": 0.15717332065105438,
      "learning_rate": 0.00018325404376784017,
      "loss": 0.1227,
      "step": 186
    },
    {
      "epoch": 0.2654364797728886,
      "grad_norm": 0.14730651676654816,
      "learning_rate": 0.00018315889628924833,
      "loss": 0.1139,
      "step": 187
    },
    {
      "epoch": 0.2668559261887864,
      "grad_norm": 0.15004900097846985,
      "learning_rate": 0.00018306374881065654,
      "loss": 0.1141,
      "step": 188
    },
    {
      "epoch": 0.26827537260468415,
      "grad_norm": 0.14527259767055511,
      "learning_rate": 0.0001829686013320647,
      "loss": 0.1227,
      "step": 189
    },
    {
      "epoch": 0.26969481902058196,
      "grad_norm": 0.12058791518211365,
      "learning_rate": 0.0001828734538534729,
      "loss": 0.0967,
      "step": 190
    },
    {
      "epoch": 0.27111426543647976,
      "grad_norm": 0.15104159712791443,
      "learning_rate": 0.00018277830637488106,
      "loss": 0.1133,
      "step": 191
    },
    {
      "epoch": 0.27253371185237757,
      "grad_norm": 0.1208389550447464,
      "learning_rate": 0.00018268315889628927,
      "loss": 0.0909,
      "step": 192
    },
    {
      "epoch": 0.2739531582682754,
      "grad_norm": 0.13636882603168488,
      "learning_rate": 0.00018258801141769743,
      "loss": 0.1006,
      "step": 193
    },
    {
      "epoch": 0.2753726046841732,
      "grad_norm": 0.14653749763965607,
      "learning_rate": 0.00018249286393910564,
      "loss": 0.1028,
      "step": 194
    },
    {
      "epoch": 0.276792051100071,
      "grad_norm": 0.13390621542930603,
      "learning_rate": 0.0001823977164605138,
      "loss": 0.1013,
      "step": 195
    },
    {
      "epoch": 0.2782114975159688,
      "grad_norm": 0.17225858569145203,
      "learning_rate": 0.000182302568981922,
      "loss": 0.0982,
      "step": 196
    },
    {
      "epoch": 0.27963094393186655,
      "grad_norm": 0.1274978071451187,
      "learning_rate": 0.00018220742150333016,
      "loss": 0.0923,
      "step": 197
    },
    {
      "epoch": 0.28105039034776436,
      "grad_norm": 0.1318695843219757,
      "learning_rate": 0.00018211227402473837,
      "loss": 0.0639,
      "step": 198
    },
    {
      "epoch": 0.28246983676366216,
      "grad_norm": 0.13581371307373047,
      "learning_rate": 0.00018201712654614653,
      "loss": 0.0774,
      "step": 199
    },
    {
      "epoch": 0.28388928317955997,
      "grad_norm": 0.10221122205257416,
      "learning_rate": 0.00018192197906755474,
      "loss": 0.0662,
      "step": 200
    },
    {
      "epoch": 0.2853087295954578,
      "grad_norm": 0.5923755764961243,
      "learning_rate": 0.0001818268315889629,
      "loss": 0.5019,
      "step": 201
    },
    {
      "epoch": 0.2867281760113556,
      "grad_norm": 0.47801506519317627,
      "learning_rate": 0.00018173168411037108,
      "loss": 0.4882,
      "step": 202
    },
    {
      "epoch": 0.2881476224272534,
      "grad_norm": 0.416513055562973,
      "learning_rate": 0.00018163653663177926,
      "loss": 0.4185,
      "step": 203
    },
    {
      "epoch": 0.2895670688431512,
      "grad_norm": 0.30151981115341187,
      "learning_rate": 0.00018154138915318745,
      "loss": 0.3634,
      "step": 204
    },
    {
      "epoch": 0.29098651525904895,
      "grad_norm": 0.3366330564022064,
      "learning_rate": 0.00018144624167459563,
      "loss": 0.3766,
      "step": 205
    },
    {
      "epoch": 0.29240596167494676,
      "grad_norm": 0.3388964831829071,
      "learning_rate": 0.0001813510941960038,
      "loss": 0.3425,
      "step": 206
    },
    {
      "epoch": 0.29382540809084456,
      "grad_norm": 0.28328460454940796,
      "learning_rate": 0.000181255946717412,
      "loss": 0.3147,
      "step": 207
    },
    {
      "epoch": 0.29524485450674237,
      "grad_norm": 0.27872422337532043,
      "learning_rate": 0.00018116079923882018,
      "loss": 0.3018,
      "step": 208
    },
    {
      "epoch": 0.2966643009226402,
      "grad_norm": 0.26885318756103516,
      "learning_rate": 0.00018106565176022836,
      "loss": 0.3222,
      "step": 209
    },
    {
      "epoch": 0.298083747338538,
      "grad_norm": 0.26485034823417664,
      "learning_rate": 0.00018097050428163655,
      "loss": 0.3397,
      "step": 210
    },
    {
      "epoch": 0.2995031937544358,
      "grad_norm": 0.2604244351387024,
      "learning_rate": 0.00018087535680304473,
      "loss": 0.2642,
      "step": 211
    },
    {
      "epoch": 0.30092264017033354,
      "grad_norm": 0.22035960853099823,
      "learning_rate": 0.0001807802093244529,
      "loss": 0.2307,
      "step": 212
    },
    {
      "epoch": 0.30234208658623135,
      "grad_norm": 0.21684938669204712,
      "learning_rate": 0.0001806850618458611,
      "loss": 0.2531,
      "step": 213
    },
    {
      "epoch": 0.30376153300212916,
      "grad_norm": 0.24349994957447052,
      "learning_rate": 0.00018058991436726928,
      "loss": 0.2496,
      "step": 214
    },
    {
      "epoch": 0.30518097941802697,
      "grad_norm": 0.23300093412399292,
      "learning_rate": 0.00018049476688867746,
      "loss": 0.2167,
      "step": 215
    },
    {
      "epoch": 0.3066004258339248,
      "grad_norm": 0.20153824985027313,
      "learning_rate": 0.00018039961941008565,
      "loss": 0.1941,
      "step": 216
    },
    {
      "epoch": 0.3080198722498226,
      "grad_norm": 0.22291982173919678,
      "learning_rate": 0.00018030447193149383,
      "loss": 0.2212,
      "step": 217
    },
    {
      "epoch": 0.3094393186657204,
      "grad_norm": 0.2710703909397125,
      "learning_rate": 0.000180209324452902,
      "loss": 0.2324,
      "step": 218
    },
    {
      "epoch": 0.3108587650816182,
      "grad_norm": 0.22228169441223145,
      "learning_rate": 0.0001801141769743102,
      "loss": 0.2219,
      "step": 219
    },
    {
      "epoch": 0.31227821149751595,
      "grad_norm": 0.19645242393016815,
      "learning_rate": 0.00018001902949571838,
      "loss": 0.1829,
      "step": 220
    },
    {
      "epoch": 0.31369765791341375,
      "grad_norm": 0.2148899883031845,
      "learning_rate": 0.00017992388201712656,
      "loss": 0.1846,
      "step": 221
    },
    {
      "epoch": 0.31511710432931156,
      "grad_norm": 0.21384695172309875,
      "learning_rate": 0.00017982873453853472,
      "loss": 0.1629,
      "step": 222
    },
    {
      "epoch": 0.31653655074520937,
      "grad_norm": 0.20600435137748718,
      "learning_rate": 0.00017973358705994293,
      "loss": 0.1472,
      "step": 223
    },
    {
      "epoch": 0.3179559971611072,
      "grad_norm": 0.15907453000545502,
      "learning_rate": 0.00017963843958135109,
      "loss": 0.1376,
      "step": 224
    },
    {
      "epoch": 0.319375443577005,
      "grad_norm": 0.2737395763397217,
      "learning_rate": 0.0001795432921027593,
      "loss": 0.1338,
      "step": 225
    },
    {
      "epoch": 0.3207948899929028,
      "grad_norm": 0.18210864067077637,
      "learning_rate": 0.00017944814462416745,
      "loss": 0.1591,
      "step": 226
    },
    {
      "epoch": 0.3222143364088006,
      "grad_norm": 0.18131813406944275,
      "learning_rate": 0.00017935299714557566,
      "loss": 0.1454,
      "step": 227
    },
    {
      "epoch": 0.32363378282469835,
      "grad_norm": 0.1633884608745575,
      "learning_rate": 0.00017925784966698382,
      "loss": 0.1188,
      "step": 228
    },
    {
      "epoch": 0.32505322924059615,
      "grad_norm": 0.1833418607711792,
      "learning_rate": 0.00017916270218839203,
      "loss": 0.1387,
      "step": 229
    },
    {
      "epoch": 0.32647267565649396,
      "grad_norm": 0.20457161962985992,
      "learning_rate": 0.0001790675547098002,
      "loss": 0.1397,
      "step": 230
    },
    {
      "epoch": 0.32789212207239177,
      "grad_norm": 0.16873478889465332,
      "learning_rate": 0.00017897240723120837,
      "loss": 0.1374,
      "step": 231
    },
    {
      "epoch": 0.3293115684882896,
      "grad_norm": 0.16101014614105225,
      "learning_rate": 0.00017887725975261655,
      "loss": 0.1483,
      "step": 232
    },
    {
      "epoch": 0.3307310149041874,
      "grad_norm": 0.18449218571186066,
      "learning_rate": 0.00017878211227402474,
      "loss": 0.1686,
      "step": 233
    },
    {
      "epoch": 0.3321504613200852,
      "grad_norm": 0.154401496052742,
      "learning_rate": 0.00017868696479543292,
      "loss": 0.1242,
      "step": 234
    },
    {
      "epoch": 0.33356990773598294,
      "grad_norm": 0.1769985854625702,
      "learning_rate": 0.0001785918173168411,
      "loss": 0.1254,
      "step": 235
    },
    {
      "epoch": 0.33498935415188075,
      "grad_norm": 0.1648237556219101,
      "learning_rate": 0.0001784966698382493,
      "loss": 0.1232,
      "step": 236
    },
    {
      "epoch": 0.33640880056777855,
      "grad_norm": 0.1461450308561325,
      "learning_rate": 0.00017840152235965747,
      "loss": 0.1055,
      "step": 237
    },
    {
      "epoch": 0.33782824698367636,
      "grad_norm": 0.15033933520317078,
      "learning_rate": 0.00017830637488106565,
      "loss": 0.1054,
      "step": 238
    },
    {
      "epoch": 0.33924769339957417,
      "grad_norm": 0.11923829466104507,
      "learning_rate": 0.00017821122740247384,
      "loss": 0.1106,
      "step": 239
    },
    {
      "epoch": 0.340667139815472,
      "grad_norm": 0.13053034245967865,
      "learning_rate": 0.00017811607992388202,
      "loss": 0.1153,
      "step": 240
    },
    {
      "epoch": 0.3420865862313698,
      "grad_norm": 0.12194915115833282,
      "learning_rate": 0.0001780209324452902,
      "loss": 0.0878,
      "step": 241
    },
    {
      "epoch": 0.3435060326472676,
      "grad_norm": 0.12302149087190628,
      "learning_rate": 0.0001779257849666984,
      "loss": 0.1003,
      "step": 242
    },
    {
      "epoch": 0.34492547906316534,
      "grad_norm": 0.10759289562702179,
      "learning_rate": 0.00017783063748810657,
      "loss": 0.0948,
      "step": 243
    },
    {
      "epoch": 0.34634492547906315,
      "grad_norm": 0.11200319230556488,
      "learning_rate": 0.00017773549000951475,
      "loss": 0.0835,
      "step": 244
    },
    {
      "epoch": 0.34776437189496096,
      "grad_norm": 0.12429293245077133,
      "learning_rate": 0.00017764034253092294,
      "loss": 0.0768,
      "step": 245
    },
    {
      "epoch": 0.34918381831085876,
      "grad_norm": 0.08911320567131042,
      "learning_rate": 0.00017754519505233112,
      "loss": 0.0817,
      "step": 246
    },
    {
      "epoch": 0.35060326472675657,
      "grad_norm": 0.12302954494953156,
      "learning_rate": 0.0001774500475737393,
      "loss": 0.0935,
      "step": 247
    },
    {
      "epoch": 0.3520227111426544,
      "grad_norm": 0.09873057901859283,
      "learning_rate": 0.0001773549000951475,
      "loss": 0.0701,
      "step": 248
    },
    {
      "epoch": 0.3534421575585522,
      "grad_norm": 0.0786275565624237,
      "learning_rate": 0.00017725975261655567,
      "loss": 0.0564,
      "step": 249
    },
    {
      "epoch": 0.35486160397445,
      "grad_norm": 0.09582041203975677,
      "learning_rate": 0.00017716460513796385,
      "loss": 0.0691,
      "step": 250
    },
    {
      "epoch": 0.35628105039034774,
      "grad_norm": 0.5196399688720703,
      "learning_rate": 0.00017706945765937204,
      "loss": 0.5333,
      "step": 251
    },
    {
      "epoch": 0.35770049680624555,
      "grad_norm": 0.444915771484375,
      "learning_rate": 0.00017697431018078022,
      "loss": 0.4228,
      "step": 252
    },
    {
      "epoch": 0.35911994322214336,
      "grad_norm": 0.3279583752155304,
      "learning_rate": 0.0001768791627021884,
      "loss": 0.3702,
      "step": 253
    },
    {
      "epoch": 0.36053938963804116,
      "grad_norm": 0.295723021030426,
      "learning_rate": 0.0001767840152235966,
      "loss": 0.3464,
      "step": 254
    },
    {
      "epoch": 0.36195883605393897,
      "grad_norm": 0.32640308141708374,
      "learning_rate": 0.00017668886774500477,
      "loss": 0.3714,
      "step": 255
    },
    {
      "epoch": 0.3633782824698368,
      "grad_norm": 0.23948761820793152,
      "learning_rate": 0.00017659372026641295,
      "loss": 0.3377,
      "step": 256
    },
    {
      "epoch": 0.3647977288857346,
      "grad_norm": 0.2686794102191925,
      "learning_rate": 0.00017649857278782114,
      "loss": 0.3323,
      "step": 257
    },
    {
      "epoch": 0.36621717530163234,
      "grad_norm": 0.2624031603336334,
      "learning_rate": 0.00017640342530922932,
      "loss": 0.2537,
      "step": 258
    },
    {
      "epoch": 0.36763662171753014,
      "grad_norm": 0.2933129668235779,
      "learning_rate": 0.0001763082778306375,
      "loss": 0.3,
      "step": 259
    },
    {
      "epoch": 0.36905606813342795,
      "grad_norm": 0.23483724892139435,
      "learning_rate": 0.0001762131303520457,
      "loss": 0.2649,
      "step": 260
    },
    {
      "epoch": 0.37047551454932576,
      "grad_norm": 0.23871733248233795,
      "learning_rate": 0.00017611798287345387,
      "loss": 0.2487,
      "step": 261
    },
    {
      "epoch": 0.37189496096522356,
      "grad_norm": 0.2093488723039627,
      "learning_rate": 0.00017602283539486203,
      "loss": 0.2416,
      "step": 262
    },
    {
      "epoch": 0.37331440738112137,
      "grad_norm": 0.33524712920188904,
      "learning_rate": 0.00017592768791627024,
      "loss": 0.2572,
      "step": 263
    },
    {
      "epoch": 0.3747338537970192,
      "grad_norm": 0.2671527564525604,
      "learning_rate": 0.0001758325404376784,
      "loss": 0.2473,
      "step": 264
    },
    {
      "epoch": 0.376153300212917,
      "grad_norm": 0.21593506634235382,
      "learning_rate": 0.0001757373929590866,
      "loss": 0.2438,
      "step": 265
    },
    {
      "epoch": 0.37757274662881474,
      "grad_norm": 0.245122030377388,
      "learning_rate": 0.00017564224548049476,
      "loss": 0.2661,
      "step": 266
    },
    {
      "epoch": 0.37899219304471254,
      "grad_norm": 0.2078303098678589,
      "learning_rate": 0.00017554709800190297,
      "loss": 0.2199,
      "step": 267
    },
    {
      "epoch": 0.38041163946061035,
      "grad_norm": 0.22816424071788788,
      "learning_rate": 0.00017545195052331113,
      "loss": 0.2178,
      "step": 268
    },
    {
      "epoch": 0.38183108587650816,
      "grad_norm": 0.19368530809879303,
      "learning_rate": 0.00017535680304471934,
      "loss": 0.2057,
      "step": 269
    },
    {
      "epoch": 0.38325053229240597,
      "grad_norm": 0.25312456488609314,
      "learning_rate": 0.0001752616555661275,
      "loss": 0.1906,
      "step": 270
    },
    {
      "epoch": 0.3846699787083038,
      "grad_norm": 0.20021282136440277,
      "learning_rate": 0.0001751665080875357,
      "loss": 0.1888,
      "step": 271
    },
    {
      "epoch": 0.3860894251242016,
      "grad_norm": 0.19169561564922333,
      "learning_rate": 0.00017507136060894386,
      "loss": 0.2014,
      "step": 272
    },
    {
      "epoch": 0.3875088715400994,
      "grad_norm": 0.23282867670059204,
      "learning_rate": 0.00017497621313035207,
      "loss": 0.2081,
      "step": 273
    },
    {
      "epoch": 0.38892831795599714,
      "grad_norm": 0.15645594894886017,
      "learning_rate": 0.00017488106565176023,
      "loss": 0.14,
      "step": 274
    },
    {
      "epoch": 0.39034776437189495,
      "grad_norm": 0.20146551728248596,
      "learning_rate": 0.00017478591817316844,
      "loss": 0.1751,
      "step": 275
    },
    {
      "epoch": 0.39176721078779275,
      "grad_norm": 0.15790389478206635,
      "learning_rate": 0.0001746907706945766,
      "loss": 0.1428,
      "step": 276
    },
    {
      "epoch": 0.39318665720369056,
      "grad_norm": 0.13402463495731354,
      "learning_rate": 0.0001745956232159848,
      "loss": 0.1508,
      "step": 277
    },
    {
      "epoch": 0.39460610361958837,
      "grad_norm": 0.19556523859500885,
      "learning_rate": 0.00017450047573739296,
      "loss": 0.1774,
      "step": 278
    },
    {
      "epoch": 0.3960255500354862,
      "grad_norm": 0.26072943210601807,
      "learning_rate": 0.00017440532825880117,
      "loss": 0.1569,
      "step": 279
    },
    {
      "epoch": 0.397444996451384,
      "grad_norm": 0.14136475324630737,
      "learning_rate": 0.00017431018078020933,
      "loss": 0.1252,
      "step": 280
    },
    {
      "epoch": 0.3988644428672818,
      "grad_norm": 0.17014621198177338,
      "learning_rate": 0.00017421503330161754,
      "loss": 0.1561,
      "step": 281
    },
    {
      "epoch": 0.40028388928317954,
      "grad_norm": 0.16529017686843872,
      "learning_rate": 0.0001741198858230257,
      "loss": 0.1521,
      "step": 282
    },
    {
      "epoch": 0.40170333569907735,
      "grad_norm": 0.17142519354820251,
      "learning_rate": 0.00017402473834443388,
      "loss": 0.1061,
      "step": 283
    },
    {
      "epoch": 0.40312278211497515,
      "grad_norm": 0.1425708681344986,
      "learning_rate": 0.00017392959086584206,
      "loss": 0.1285,
      "step": 284
    },
    {
      "epoch": 0.40454222853087296,
      "grad_norm": 0.18765318393707275,
      "learning_rate": 0.00017383444338725024,
      "loss": 0.1233,
      "step": 285
    },
    {
      "epoch": 0.40596167494677077,
      "grad_norm": 0.14780974388122559,
      "learning_rate": 0.00017373929590865843,
      "loss": 0.1043,
      "step": 286
    },
    {
      "epoch": 0.4073811213626686,
      "grad_norm": 0.14468996226787567,
      "learning_rate": 0.0001736441484300666,
      "loss": 0.112,
      "step": 287
    },
    {
      "epoch": 0.4088005677785664,
      "grad_norm": 0.12419953942298889,
      "learning_rate": 0.0001735490009514748,
      "loss": 0.0896,
      "step": 288
    },
    {
      "epoch": 0.41022001419446413,
      "grad_norm": 0.1314602941274643,
      "learning_rate": 0.00017345385347288298,
      "loss": 0.1008,
      "step": 289
    },
    {
      "epoch": 0.41163946061036194,
      "grad_norm": 0.15486060082912445,
      "learning_rate": 0.00017335870599429116,
      "loss": 0.0982,
      "step": 290
    },
    {
      "epoch": 0.41305890702625975,
      "grad_norm": 0.12233155220746994,
      "learning_rate": 0.00017326355851569934,
      "loss": 0.0964,
      "step": 291
    },
    {
      "epoch": 0.41447835344215755,
      "grad_norm": 0.1104344055056572,
      "learning_rate": 0.00017316841103710753,
      "loss": 0.0955,
      "step": 292
    },
    {
      "epoch": 0.41589779985805536,
      "grad_norm": 0.0978473499417305,
      "learning_rate": 0.00017307326355851568,
      "loss": 0.0746,
      "step": 293
    },
    {
      "epoch": 0.41731724627395317,
      "grad_norm": 0.10647564381361008,
      "learning_rate": 0.0001729781160799239,
      "loss": 0.0688,
      "step": 294
    },
    {
      "epoch": 0.418736692689851,
      "grad_norm": 0.1035827174782753,
      "learning_rate": 0.00017288296860133205,
      "loss": 0.0785,
      "step": 295
    },
    {
      "epoch": 0.4201561391057488,
      "grad_norm": 0.12469320744276047,
      "learning_rate": 0.00017278782112274026,
      "loss": 0.0889,
      "step": 296
    },
    {
      "epoch": 0.42157558552164653,
      "grad_norm": 0.11159732937812805,
      "learning_rate": 0.00017269267364414842,
      "loss": 0.0786,
      "step": 297
    },
    {
      "epoch": 0.42299503193754434,
      "grad_norm": 0.09567723423242569,
      "learning_rate": 0.00017259752616555663,
      "loss": 0.0618,
      "step": 298
    },
    {
      "epoch": 0.42441447835344215,
      "grad_norm": 0.11323487013578415,
      "learning_rate": 0.00017250237868696478,
      "loss": 0.0735,
      "step": 299
    },
    {
      "epoch": 0.42583392476933996,
      "grad_norm": 0.36350539326667786,
      "learning_rate": 0.000172407231208373,
      "loss": 0.0587,
      "step": 300
    },
    {
      "epoch": 0.42725337118523776,
      "grad_norm": 0.4785170257091522,
      "learning_rate": 0.00017231208372978115,
      "loss": 0.518,
      "step": 301
    },
    {
      "epoch": 0.42867281760113557,
      "grad_norm": 0.4595721960067749,
      "learning_rate": 0.00017221693625118936,
      "loss": 0.4631,
      "step": 302
    },
    {
      "epoch": 0.4300922640170334,
      "grad_norm": 0.3478117287158966,
      "learning_rate": 0.00017212178877259752,
      "loss": 0.4587,
      "step": 303
    },
    {
      "epoch": 0.4315117104329312,
      "grad_norm": 0.33338478207588196,
      "learning_rate": 0.00017202664129400573,
      "loss": 0.3858,
      "step": 304
    },
    {
      "epoch": 0.43293115684882894,
      "grad_norm": 0.27139970660209656,
      "learning_rate": 0.00017193149381541388,
      "loss": 0.3406,
      "step": 305
    },
    {
      "epoch": 0.43435060326472674,
      "grad_norm": 0.24543721973896027,
      "learning_rate": 0.0001718363463368221,
      "loss": 0.3331,
      "step": 306
    },
    {
      "epoch": 0.43577004968062455,
      "grad_norm": 0.2112877368927002,
      "learning_rate": 0.00017174119885823025,
      "loss": 0.2871,
      "step": 307
    },
    {
      "epoch": 0.43718949609652236,
      "grad_norm": 0.304798424243927,
      "learning_rate": 0.00017164605137963846,
      "loss": 0.3329,
      "step": 308
    },
    {
      "epoch": 0.43860894251242016,
      "grad_norm": 0.21619813144207,
      "learning_rate": 0.00017155090390104662,
      "loss": 0.2867,
      "step": 309
    },
    {
      "epoch": 0.44002838892831797,
      "grad_norm": 0.24975015223026276,
      "learning_rate": 0.00017145575642245483,
      "loss": 0.3055,
      "step": 310
    },
    {
      "epoch": 0.4414478353442158,
      "grad_norm": 0.21540270745754242,
      "learning_rate": 0.00017136060894386298,
      "loss": 0.2333,
      "step": 311
    },
    {
      "epoch": 0.44286728176011353,
      "grad_norm": 0.18148262798786163,
      "learning_rate": 0.0001712654614652712,
      "loss": 0.2232,
      "step": 312
    },
    {
      "epoch": 0.44428672817601134,
      "grad_norm": 0.20767313241958618,
      "learning_rate": 0.00017117031398667935,
      "loss": 0.204,
      "step": 313
    },
    {
      "epoch": 0.44570617459190914,
      "grad_norm": 0.22825096547603607,
      "learning_rate": 0.00017107516650808753,
      "loss": 0.2421,
      "step": 314
    },
    {
      "epoch": 0.44712562100780695,
      "grad_norm": 0.23413901031017303,
      "learning_rate": 0.00017098001902949572,
      "loss": 0.2224,
      "step": 315
    },
    {
      "epoch": 0.44854506742370476,
      "grad_norm": 0.20314759016036987,
      "learning_rate": 0.0001708848715509039,
      "loss": 0.1989,
      "step": 316
    },
    {
      "epoch": 0.44996451383960256,
      "grad_norm": 0.19911476969718933,
      "learning_rate": 0.00017078972407231208,
      "loss": 0.2239,
      "step": 317
    },
    {
      "epoch": 0.45138396025550037,
      "grad_norm": 0.20485487580299377,
      "learning_rate": 0.00017069457659372027,
      "loss": 0.21,
      "step": 318
    },
    {
      "epoch": 0.4528034066713982,
      "grad_norm": 0.16163139045238495,
      "learning_rate": 0.00017059942911512845,
      "loss": 0.154,
      "step": 319
    },
    {
      "epoch": 0.45422285308729593,
      "grad_norm": 0.17442378401756287,
      "learning_rate": 0.00017050428163653663,
      "loss": 0.1623,
      "step": 320
    },
    {
      "epoch": 0.45564229950319374,
      "grad_norm": 0.1901973932981491,
      "learning_rate": 0.00017040913415794482,
      "loss": 0.1975,
      "step": 321
    },
    {
      "epoch": 0.45706174591909154,
      "grad_norm": 0.17201147973537445,
      "learning_rate": 0.000170313986679353,
      "loss": 0.1577,
      "step": 322
    },
    {
      "epoch": 0.45848119233498935,
      "grad_norm": 0.24799390137195587,
      "learning_rate": 0.00017021883920076119,
      "loss": 0.1743,
      "step": 323
    },
    {
      "epoch": 0.45990063875088716,
      "grad_norm": 0.1515909880399704,
      "learning_rate": 0.00017012369172216937,
      "loss": 0.1395,
      "step": 324
    },
    {
      "epoch": 0.46132008516678497,
      "grad_norm": 0.3801301419734955,
      "learning_rate": 0.00017002854424357755,
      "loss": 0.2097,
      "step": 325
    },
    {
      "epoch": 0.4627395315826828,
      "grad_norm": 0.2507898807525635,
      "learning_rate": 0.00016993339676498574,
      "loss": 0.1767,
      "step": 326
    },
    {
      "epoch": 0.4641589779985806,
      "grad_norm": 0.1987512707710266,
      "learning_rate": 0.00016983824928639392,
      "loss": 0.1428,
      "step": 327
    },
    {
      "epoch": 0.46557842441447833,
      "grad_norm": 0.1773303896188736,
      "learning_rate": 0.0001697431018078021,
      "loss": 0.167,
      "step": 328
    },
    {
      "epoch": 0.46699787083037614,
      "grad_norm": 0.15946342051029205,
      "learning_rate": 0.00016964795432921029,
      "loss": 0.1494,
      "step": 329
    },
    {
      "epoch": 0.46841731724627395,
      "grad_norm": 0.1415938287973404,
      "learning_rate": 0.00016955280685061847,
      "loss": 0.1197,
      "step": 330
    },
    {
      "epoch": 0.46983676366217175,
      "grad_norm": 0.1696440577507019,
      "learning_rate": 0.00016945765937202665,
      "loss": 0.1348,
      "step": 331
    },
    {
      "epoch": 0.47125621007806956,
      "grad_norm": 0.17328503727912903,
      "learning_rate": 0.00016936251189343484,
      "loss": 0.1301,
      "step": 332
    },
    {
      "epoch": 0.47267565649396737,
      "grad_norm": 0.14782625436782837,
      "learning_rate": 0.00016926736441484302,
      "loss": 0.1398,
      "step": 333
    },
    {
      "epoch": 0.4740951029098652,
      "grad_norm": 0.1644575446844101,
      "learning_rate": 0.0001691722169362512,
      "loss": 0.1488,
      "step": 334
    },
    {
      "epoch": 0.4755145493257629,
      "grad_norm": 0.13526898622512817,
      "learning_rate": 0.00016907706945765939,
      "loss": 0.1213,
      "step": 335
    },
    {
      "epoch": 0.47693399574166073,
      "grad_norm": 0.14072123169898987,
      "learning_rate": 0.00016898192197906757,
      "loss": 0.1249,
      "step": 336
    },
    {
      "epoch": 0.47835344215755854,
      "grad_norm": 0.12543220818042755,
      "learning_rate": 0.00016888677450047575,
      "loss": 0.1191,
      "step": 337
    },
    {
      "epoch": 0.47977288857345635,
      "grad_norm": 0.13075068593025208,
      "learning_rate": 0.00016879162702188394,
      "loss": 0.113,
      "step": 338
    },
    {
      "epoch": 0.48119233498935415,
      "grad_norm": 0.1628408282995224,
      "learning_rate": 0.00016869647954329212,
      "loss": 0.1083,
      "step": 339
    },
    {
      "epoch": 0.48261178140525196,
      "grad_norm": 0.12735773622989655,
      "learning_rate": 0.0001686013320647003,
      "loss": 0.0759,
      "step": 340
    },
    {
      "epoch": 0.48403122782114977,
      "grad_norm": 0.13596266508102417,
      "learning_rate": 0.00016850618458610849,
      "loss": 0.1073,
      "step": 341
    },
    {
      "epoch": 0.4854506742370476,
      "grad_norm": 0.10148149728775024,
      "learning_rate": 0.00016841103710751667,
      "loss": 0.0714,
      "step": 342
    },
    {
      "epoch": 0.4868701206529453,
      "grad_norm": 0.10629739612340927,
      "learning_rate": 0.00016831588962892485,
      "loss": 0.083,
      "step": 343
    },
    {
      "epoch": 0.48828956706884313,
      "grad_norm": 0.16768154501914978,
      "learning_rate": 0.00016822074215033304,
      "loss": 0.1239,
      "step": 344
    },
    {
      "epoch": 0.48970901348474094,
      "grad_norm": 0.15854743123054504,
      "learning_rate": 0.0001681255946717412,
      "loss": 0.0868,
      "step": 345
    },
    {
      "epoch": 0.49112845990063875,
      "grad_norm": 0.126465305685997,
      "learning_rate": 0.0001680304471931494,
      "loss": 0.0918,
      "step": 346
    },
    {
      "epoch": 0.49254790631653655,
      "grad_norm": 0.14185777306556702,
      "learning_rate": 0.00016793529971455756,
      "loss": 0.0713,
      "step": 347
    },
    {
      "epoch": 0.49396735273243436,
      "grad_norm": 0.10624835640192032,
      "learning_rate": 0.00016784015223596577,
      "loss": 0.0697,
      "step": 348
    },
    {
      "epoch": 0.49538679914833217,
      "grad_norm": 0.11770059168338776,
      "learning_rate": 0.00016774500475737393,
      "loss": 0.0699,
      "step": 349
    },
    {
      "epoch": 0.49680624556423,
      "grad_norm": 0.0840187519788742,
      "learning_rate": 0.00016764985727878214,
      "loss": 0.0478,
      "step": 350
    },
    {
      "epoch": 0.4982256919801277,
      "grad_norm": 0.6331724524497986,
      "learning_rate": 0.0001675547098001903,
      "loss": 0.5741,
      "step": 351
    },
    {
      "epoch": 0.49964513839602553,
      "grad_norm": 0.488374799489975,
      "learning_rate": 0.0001674595623215985,
      "loss": 0.446,
      "step": 352
    },
    {
      "epoch": 0.5010645848119234,
      "grad_norm": 0.38088110089302063,
      "learning_rate": 0.00016736441484300666,
      "loss": 0.4208,
      "step": 353
    },
    {
      "epoch": 0.5024840312278211,
      "grad_norm": 0.3756221830844879,
      "learning_rate": 0.00016726926736441487,
      "loss": 0.3902,
      "step": 354
    },
    {
      "epoch": 0.5039034776437189,
      "grad_norm": 0.2471628040075302,
      "learning_rate": 0.00016717411988582303,
      "loss": 0.3258,
      "step": 355
    },
    {
      "epoch": 0.5053229240596168,
      "grad_norm": 0.23875264823436737,
      "learning_rate": 0.00016707897240723124,
      "loss": 0.32,
      "step": 356
    },
    {
      "epoch": 0.5067423704755145,
      "grad_norm": 0.2846216559410095,
      "learning_rate": 0.0001669838249286394,
      "loss": 0.3069,
      "step": 357
    },
    {
      "epoch": 0.5081618168914124,
      "grad_norm": 0.2903178036212921,
      "learning_rate": 0.0001668886774500476,
      "loss": 0.2584,
      "step": 358
    },
    {
      "epoch": 0.5095812633073101,
      "grad_norm": 0.21420243382453918,
      "learning_rate": 0.00016679352997145576,
      "loss": 0.2632,
      "step": 359
    },
    {
      "epoch": 0.511000709723208,
      "grad_norm": 0.2651553452014923,
      "learning_rate": 0.00016669838249286397,
      "loss": 0.3097,
      "step": 360
    },
    {
      "epoch": 0.5124201561391057,
      "grad_norm": 0.22691239416599274,
      "learning_rate": 0.00016660323501427213,
      "loss": 0.258,
      "step": 361
    },
    {
      "epoch": 0.5138396025550035,
      "grad_norm": 0.2461789846420288,
      "learning_rate": 0.0001665080875356803,
      "loss": 0.2406,
      "step": 362
    },
    {
      "epoch": 0.5152590489709014,
      "grad_norm": 0.21322080492973328,
      "learning_rate": 0.0001664129400570885,
      "loss": 0.2157,
      "step": 363
    },
    {
      "epoch": 0.5166784953867991,
      "grad_norm": 0.22189216315746307,
      "learning_rate": 0.00016631779257849668,
      "loss": 0.245,
      "step": 364
    },
    {
      "epoch": 0.518097941802697,
      "grad_norm": 0.21649399399757385,
      "learning_rate": 0.00016622264509990486,
      "loss": 0.2343,
      "step": 365
    },
    {
      "epoch": 0.5195173882185947,
      "grad_norm": 0.2525191307067871,
      "learning_rate": 0.00016612749762131304,
      "loss": 0.212,
      "step": 366
    },
    {
      "epoch": 0.5209368346344926,
      "grad_norm": 0.2230282723903656,
      "learning_rate": 0.00016603235014272123,
      "loss": 0.2404,
      "step": 367
    },
    {
      "epoch": 0.5223562810503903,
      "grad_norm": 0.19831006228923798,
      "learning_rate": 0.0001659372026641294,
      "loss": 0.183,
      "step": 368
    },
    {
      "epoch": 0.5237757274662882,
      "grad_norm": 0.2026294767856598,
      "learning_rate": 0.0001658420551855376,
      "loss": 0.1909,
      "step": 369
    },
    {
      "epoch": 0.525195173882186,
      "grad_norm": 0.18979088962078094,
      "learning_rate": 0.00016574690770694578,
      "loss": 0.1757,
      "step": 370
    },
    {
      "epoch": 0.5266146202980837,
      "grad_norm": 0.21828339993953705,
      "learning_rate": 0.00016565176022835396,
      "loss": 0.1836,
      "step": 371
    },
    {
      "epoch": 0.5280340667139816,
      "grad_norm": 0.17623811960220337,
      "learning_rate": 0.00016555661274976214,
      "loss": 0.1601,
      "step": 372
    },
    {
      "epoch": 0.5294535131298793,
      "grad_norm": 0.14808376133441925,
      "learning_rate": 0.00016546146527117033,
      "loss": 0.1293,
      "step": 373
    },
    {
      "epoch": 0.5308729595457772,
      "grad_norm": 0.17691995203495026,
      "learning_rate": 0.00016536631779257848,
      "loss": 0.1526,
      "step": 374
    },
    {
      "epoch": 0.5322924059616749,
      "grad_norm": 0.17905530333518982,
      "learning_rate": 0.0001652711703139867,
      "loss": 0.1266,
      "step": 375
    },
    {
      "epoch": 0.5337118523775728,
      "grad_norm": 0.16115914285182953,
      "learning_rate": 0.00016517602283539485,
      "loss": 0.1249,
      "step": 376
    },
    {
      "epoch": 0.5351312987934705,
      "grad_norm": 0.1475640833377838,
      "learning_rate": 0.00016508087535680306,
      "loss": 0.1325,
      "step": 377
    },
    {
      "epoch": 0.5365507452093683,
      "grad_norm": 0.1882176697254181,
      "learning_rate": 0.00016498572787821122,
      "loss": 0.158,
      "step": 378
    },
    {
      "epoch": 0.5379701916252662,
      "grad_norm": 0.16310134530067444,
      "learning_rate": 0.00016489058039961943,
      "loss": 0.1441,
      "step": 379
    },
    {
      "epoch": 0.5393896380411639,
      "grad_norm": 0.1749742180109024,
      "learning_rate": 0.00016479543292102758,
      "loss": 0.1537,
      "step": 380
    },
    {
      "epoch": 0.5408090844570618,
      "grad_norm": 0.18974344432353973,
      "learning_rate": 0.0001647002854424358,
      "loss": 0.1483,
      "step": 381
    },
    {
      "epoch": 0.5422285308729595,
      "grad_norm": 0.15348966419696808,
      "learning_rate": 0.00016460513796384395,
      "loss": 0.1261,
      "step": 382
    },
    {
      "epoch": 0.5436479772888574,
      "grad_norm": 0.13731154799461365,
      "learning_rate": 0.00016450999048525216,
      "loss": 0.0929,
      "step": 383
    },
    {
      "epoch": 0.5450674237047551,
      "grad_norm": 0.1556616723537445,
      "learning_rate": 0.00016441484300666032,
      "loss": 0.1203,
      "step": 384
    },
    {
      "epoch": 0.5464868701206529,
      "grad_norm": 0.13559553027153015,
      "learning_rate": 0.00016431969552806853,
      "loss": 0.1004,
      "step": 385
    },
    {
      "epoch": 0.5479063165365508,
      "grad_norm": 0.13521873950958252,
      "learning_rate": 0.00016422454804947668,
      "loss": 0.122,
      "step": 386
    },
    {
      "epoch": 0.5493257629524485,
      "grad_norm": 0.13034307956695557,
      "learning_rate": 0.0001641294005708849,
      "loss": 0.1071,
      "step": 387
    },
    {
      "epoch": 0.5507452093683464,
      "grad_norm": 0.1721305400133133,
      "learning_rate": 0.00016403425309229305,
      "loss": 0.1119,
      "step": 388
    },
    {
      "epoch": 0.5521646557842441,
      "grad_norm": 0.15545590221881866,
      "learning_rate": 0.00016393910561370126,
      "loss": 0.0976,
      "step": 389
    },
    {
      "epoch": 0.553584102200142,
      "grad_norm": 0.1454247236251831,
      "learning_rate": 0.00016384395813510942,
      "loss": 0.0913,
      "step": 390
    },
    {
      "epoch": 0.5550035486160397,
      "grad_norm": 0.13429966568946838,
      "learning_rate": 0.00016374881065651763,
      "loss": 0.0924,
      "step": 391
    },
    {
      "epoch": 0.5564229950319376,
      "grad_norm": 0.1830882728099823,
      "learning_rate": 0.00016365366317792578,
      "loss": 0.0892,
      "step": 392
    },
    {
      "epoch": 0.5578424414478353,
      "grad_norm": 0.12371549755334854,
      "learning_rate": 0.000163558515699334,
      "loss": 0.0632,
      "step": 393
    },
    {
      "epoch": 0.5592618878637331,
      "grad_norm": 0.11389799416065216,
      "learning_rate": 0.00016346336822074215,
      "loss": 0.0821,
      "step": 394
    },
    {
      "epoch": 0.560681334279631,
      "grad_norm": 0.1552814394235611,
      "learning_rate": 0.00016336822074215033,
      "loss": 0.0961,
      "step": 395
    },
    {
      "epoch": 0.5621007806955287,
      "grad_norm": 0.1489168405532837,
      "learning_rate": 0.00016327307326355852,
      "loss": 0.0927,
      "step": 396
    },
    {
      "epoch": 0.5635202271114266,
      "grad_norm": 0.147613987326622,
      "learning_rate": 0.0001631779257849667,
      "loss": 0.091,
      "step": 397
    },
    {
      "epoch": 0.5649396735273243,
      "grad_norm": 0.10550914704799652,
      "learning_rate": 0.00016308277830637488,
      "loss": 0.0645,
      "step": 398
    },
    {
      "epoch": 0.5663591199432222,
      "grad_norm": 0.11544385552406311,
      "learning_rate": 0.00016298763082778307,
      "loss": 0.0705,
      "step": 399
    },
    {
      "epoch": 0.5677785663591199,
      "grad_norm": 0.09835031628608704,
      "learning_rate": 0.00016289248334919125,
      "loss": 0.0551,
      "step": 400
    },
    {
      "epoch": 0.5691980127750177,
      "grad_norm": 0.5734143257141113,
      "learning_rate": 0.00016279733587059943,
      "loss": 0.5311,
      "step": 401
    },
    {
      "epoch": 0.5706174591909156,
      "grad_norm": 0.3967488706111908,
      "learning_rate": 0.00016270218839200762,
      "loss": 0.42,
      "step": 402
    },
    {
      "epoch": 0.5720369056068133,
      "grad_norm": 0.3979634940624237,
      "learning_rate": 0.0001626070409134158,
      "loss": 0.4227,
      "step": 403
    },
    {
      "epoch": 0.5734563520227112,
      "grad_norm": 0.3215809464454651,
      "learning_rate": 0.00016251189343482398,
      "loss": 0.3438,
      "step": 404
    },
    {
      "epoch": 0.5748757984386089,
      "grad_norm": 0.28150179982185364,
      "learning_rate": 0.00016241674595623217,
      "loss": 0.3409,
      "step": 405
    },
    {
      "epoch": 0.5762952448545068,
      "grad_norm": 0.23088285326957703,
      "learning_rate": 0.00016232159847764035,
      "loss": 0.2856,
      "step": 406
    },
    {
      "epoch": 0.5777146912704045,
      "grad_norm": 0.24941927194595337,
      "learning_rate": 0.00016222645099904853,
      "loss": 0.3215,
      "step": 407
    },
    {
      "epoch": 0.5791341376863024,
      "grad_norm": 0.2608039975166321,
      "learning_rate": 0.00016213130352045672,
      "loss": 0.2984,
      "step": 408
    },
    {
      "epoch": 0.5805535841022001,
      "grad_norm": 0.2782004177570343,
      "learning_rate": 0.0001620361560418649,
      "loss": 0.3123,
      "step": 409
    },
    {
      "epoch": 0.5819730305180979,
      "grad_norm": 0.29299309849739075,
      "learning_rate": 0.00016194100856327308,
      "loss": 0.3224,
      "step": 410
    },
    {
      "epoch": 0.5833924769339958,
      "grad_norm": 0.2324187159538269,
      "learning_rate": 0.00016184586108468127,
      "loss": 0.2626,
      "step": 411
    },
    {
      "epoch": 0.5848119233498935,
      "grad_norm": 0.31871795654296875,
      "learning_rate": 0.00016175071360608945,
      "loss": 0.3211,
      "step": 412
    },
    {
      "epoch": 0.5862313697657914,
      "grad_norm": 0.2733614146709442,
      "learning_rate": 0.00016165556612749763,
      "loss": 0.288,
      "step": 413
    },
    {
      "epoch": 0.5876508161816891,
      "grad_norm": 0.2656559348106384,
      "learning_rate": 0.00016156041864890582,
      "loss": 0.2593,
      "step": 414
    },
    {
      "epoch": 0.589070262597587,
      "grad_norm": 0.2410850077867508,
      "learning_rate": 0.000161465271170314,
      "loss": 0.2606,
      "step": 415
    },
    {
      "epoch": 0.5904897090134847,
      "grad_norm": 0.2182936817407608,
      "learning_rate": 0.00016137012369172218,
      "loss": 0.2177,
      "step": 416
    },
    {
      "epoch": 0.5919091554293825,
      "grad_norm": 0.2415715456008911,
      "learning_rate": 0.00016127497621313037,
      "loss": 0.2419,
      "step": 417
    },
    {
      "epoch": 0.5933286018452804,
      "grad_norm": 0.18864968419075012,
      "learning_rate": 0.00016117982873453855,
      "loss": 0.1927,
      "step": 418
    },
    {
      "epoch": 0.5947480482611781,
      "grad_norm": 0.19137367606163025,
      "learning_rate": 0.00016108468125594673,
      "loss": 0.1817,
      "step": 419
    },
    {
      "epoch": 0.596167494677076,
      "grad_norm": 0.222140371799469,
      "learning_rate": 0.00016098953377735492,
      "loss": 0.2109,
      "step": 420
    },
    {
      "epoch": 0.5975869410929737,
      "grad_norm": 0.23214884102344513,
      "learning_rate": 0.0001608943862987631,
      "loss": 0.2105,
      "step": 421
    },
    {
      "epoch": 0.5990063875088716,
      "grad_norm": 0.19018523395061493,
      "learning_rate": 0.00016079923882017128,
      "loss": 0.1821,
      "step": 422
    },
    {
      "epoch": 0.6004258339247693,
      "grad_norm": 0.19725508987903595,
      "learning_rate": 0.00016070409134157947,
      "loss": 0.1668,
      "step": 423
    },
    {
      "epoch": 0.6004258339247693,
      "eval_loss": 0.18132758140563965,
      "eval_runtime": 350.2051,
      "eval_samples_per_second": 3.018,
      "eval_steps_per_second": 1.008,
      "step": 423
    },
    {
      "epoch": 0.6018452803406671,
      "grad_norm": 0.20987409353256226,
      "learning_rate": 0.00016060894386298765,
      "loss": 0.1645,
      "step": 424
    },
    {
      "epoch": 0.603264726756565,
      "grad_norm": 0.19481807947158813,
      "learning_rate": 0.00016051379638439583,
      "loss": 0.1913,
      "step": 425
    },
    {
      "epoch": 0.6046841731724627,
      "grad_norm": 0.2456100434064865,
      "learning_rate": 0.000160418648905804,
      "loss": 0.2019,
      "step": 426
    },
    {
      "epoch": 0.6061036195883606,
      "grad_norm": 0.21857067942619324,
      "learning_rate": 0.0001603235014272122,
      "loss": 0.198,
      "step": 427
    },
    {
      "epoch": 0.6075230660042583,
      "grad_norm": 0.1517402082681656,
      "learning_rate": 0.00016022835394862036,
      "loss": 0.1194,
      "step": 428
    },
    {
      "epoch": 0.6089425124201562,
      "grad_norm": 0.1431589126586914,
      "learning_rate": 0.00016013320647002857,
      "loss": 0.1407,
      "step": 429
    },
    {
      "epoch": 0.6103619588360539,
      "grad_norm": 0.1423029750585556,
      "learning_rate": 0.00016003805899143672,
      "loss": 0.1311,
      "step": 430
    },
    {
      "epoch": 0.6117814052519518,
      "grad_norm": 0.1388556808233261,
      "learning_rate": 0.0001599429115128449,
      "loss": 0.127,
      "step": 431
    },
    {
      "epoch": 0.6132008516678495,
      "grad_norm": 0.15656495094299316,
      "learning_rate": 0.0001598477640342531,
      "loss": 0.1219,
      "step": 432
    },
    {
      "epoch": 0.6146202980837473,
      "grad_norm": 0.1460929661989212,
      "learning_rate": 0.00015975261655566127,
      "loss": 0.1175,
      "step": 433
    },
    {
      "epoch": 0.6160397444996452,
      "grad_norm": 0.14177191257476807,
      "learning_rate": 0.00015965746907706946,
      "loss": 0.122,
      "step": 434
    },
    {
      "epoch": 0.6174591909155429,
      "grad_norm": 0.163153275847435,
      "learning_rate": 0.00015956232159847764,
      "loss": 0.1422,
      "step": 435
    },
    {
      "epoch": 0.6188786373314408,
      "grad_norm": 0.14655143022537231,
      "learning_rate": 0.00015946717411988582,
      "loss": 0.1119,
      "step": 436
    },
    {
      "epoch": 0.6202980837473385,
      "grad_norm": 0.1259157657623291,
      "learning_rate": 0.000159372026641294,
      "loss": 0.0865,
      "step": 437
    },
    {
      "epoch": 0.6217175301632364,
      "grad_norm": 0.14728648960590363,
      "learning_rate": 0.0001592768791627022,
      "loss": 0.1048,
      "step": 438
    },
    {
      "epoch": 0.6231369765791341,
      "grad_norm": 0.1266816407442093,
      "learning_rate": 0.00015918173168411037,
      "loss": 0.1025,
      "step": 439
    },
    {
      "epoch": 0.6245564229950319,
      "grad_norm": 0.12114408612251282,
      "learning_rate": 0.00015908658420551856,
      "loss": 0.1071,
      "step": 440
    },
    {
      "epoch": 0.6259758694109298,
      "grad_norm": 0.14592917263507843,
      "learning_rate": 0.00015899143672692674,
      "loss": 0.1268,
      "step": 441
    },
    {
      "epoch": 0.6273953158268275,
      "grad_norm": 0.12582072615623474,
      "learning_rate": 0.00015889628924833492,
      "loss": 0.0941,
      "step": 442
    },
    {
      "epoch": 0.6288147622427254,
      "grad_norm": 0.12897424399852753,
      "learning_rate": 0.0001588011417697431,
      "loss": 0.086,
      "step": 443
    },
    {
      "epoch": 0.6302342086586231,
      "grad_norm": 0.11238119751214981,
      "learning_rate": 0.0001587059942911513,
      "loss": 0.0952,
      "step": 444
    },
    {
      "epoch": 0.631653655074521,
      "grad_norm": 0.1291157603263855,
      "learning_rate": 0.00015861084681255947,
      "loss": 0.1033,
      "step": 445
    },
    {
      "epoch": 0.6330731014904187,
      "grad_norm": 0.0873965173959732,
      "learning_rate": 0.00015851569933396766,
      "loss": 0.0675,
      "step": 446
    },
    {
      "epoch": 0.6344925479063165,
      "grad_norm": 0.13512372970581055,
      "learning_rate": 0.00015842055185537584,
      "loss": 0.0967,
      "step": 447
    },
    {
      "epoch": 0.6359119943222143,
      "grad_norm": 0.14364920556545258,
      "learning_rate": 0.00015832540437678402,
      "loss": 0.0781,
      "step": 448
    },
    {
      "epoch": 0.6373314407381121,
      "grad_norm": 0.0919160544872284,
      "learning_rate": 0.0001582302568981922,
      "loss": 0.0646,
      "step": 449
    },
    {
      "epoch": 0.63875088715401,
      "grad_norm": 0.09288521856069565,
      "learning_rate": 0.0001581351094196004,
      "loss": 0.0578,
      "step": 450
    },
    {
      "epoch": 0.6401703335699077,
      "grad_norm": 0.5773417949676514,
      "learning_rate": 0.00015803996194100857,
      "loss": 0.6209,
      "step": 451
    },
    {
      "epoch": 0.6415897799858056,
      "grad_norm": 0.39017078280448914,
      "learning_rate": 0.00015794481446241676,
      "loss": 0.393,
      "step": 452
    },
    {
      "epoch": 0.6430092264017033,
      "grad_norm": 0.29224568605422974,
      "learning_rate": 0.00015784966698382494,
      "loss": 0.3267,
      "step": 453
    },
    {
      "epoch": 0.6444286728176012,
      "grad_norm": 0.3001405596733093,
      "learning_rate": 0.00015775451950523312,
      "loss": 0.3313,
      "step": 454
    },
    {
      "epoch": 0.6458481192334989,
      "grad_norm": 0.2834407389163971,
      "learning_rate": 0.0001576593720266413,
      "loss": 0.342,
      "step": 455
    },
    {
      "epoch": 0.6472675656493967,
      "grad_norm": 0.23928727209568024,
      "learning_rate": 0.0001575642245480495,
      "loss": 0.2847,
      "step": 456
    },
    {
      "epoch": 0.6486870120652946,
      "grad_norm": 0.2410030961036682,
      "learning_rate": 0.00015746907706945765,
      "loss": 0.3095,
      "step": 457
    },
    {
      "epoch": 0.6501064584811923,
      "grad_norm": 0.22346051037311554,
      "learning_rate": 0.00015737392959086586,
      "loss": 0.288,
      "step": 458
    },
    {
      "epoch": 0.6515259048970902,
      "grad_norm": 0.23195426166057587,
      "learning_rate": 0.00015727878211227401,
      "loss": 0.2825,
      "step": 459
    },
    {
      "epoch": 0.6529453513129879,
      "grad_norm": 0.20546920597553253,
      "learning_rate": 0.00015718363463368222,
      "loss": 0.251,
      "step": 460
    },
    {
      "epoch": 0.6543647977288858,
      "grad_norm": 0.24309487640857697,
      "learning_rate": 0.00015708848715509038,
      "loss": 0.2642,
      "step": 461
    },
    {
      "epoch": 0.6557842441447835,
      "grad_norm": 0.2406681925058365,
      "learning_rate": 0.0001569933396764986,
      "loss": 0.2356,
      "step": 462
    },
    {
      "epoch": 0.6572036905606813,
      "grad_norm": 0.21421076357364655,
      "learning_rate": 0.00015689819219790675,
      "loss": 0.2481,
      "step": 463
    },
    {
      "epoch": 0.6586231369765791,
      "grad_norm": 0.24608848989009857,
      "learning_rate": 0.00015680304471931496,
      "loss": 0.2426,
      "step": 464
    },
    {
      "epoch": 0.6600425833924769,
      "grad_norm": 0.22647297382354736,
      "learning_rate": 0.00015670789724072311,
      "loss": 0.2414,
      "step": 465
    },
    {
      "epoch": 0.6614620298083748,
      "grad_norm": 0.20979394018650055,
      "learning_rate": 0.00015661274976213132,
      "loss": 0.2067,
      "step": 466
    },
    {
      "epoch": 0.6628814762242725,
      "grad_norm": 0.1916702538728714,
      "learning_rate": 0.00015651760228353948,
      "loss": 0.1637,
      "step": 467
    },
    {
      "epoch": 0.6643009226401704,
      "grad_norm": 0.21306970715522766,
      "learning_rate": 0.0001564224548049477,
      "loss": 0.2083,
      "step": 468
    },
    {
      "epoch": 0.6657203690560681,
      "grad_norm": 0.18934592604637146,
      "learning_rate": 0.00015632730732635585,
      "loss": 0.1861,
      "step": 469
    },
    {
      "epoch": 0.6671398154719659,
      "grad_norm": 0.16308657824993134,
      "learning_rate": 0.00015623215984776406,
      "loss": 0.146,
      "step": 470
    },
    {
      "epoch": 0.6685592618878637,
      "grad_norm": 0.18044470250606537,
      "learning_rate": 0.00015613701236917221,
      "loss": 0.176,
      "step": 471
    },
    {
      "epoch": 0.6699787083037615,
      "grad_norm": 0.1645916849374771,
      "learning_rate": 0.00015604186489058042,
      "loss": 0.1327,
      "step": 472
    },
    {
      "epoch": 0.6713981547196594,
      "grad_norm": 0.15306499600410461,
      "learning_rate": 0.00015594671741198858,
      "loss": 0.1419,
      "step": 473
    },
    {
      "epoch": 0.6728176011355571,
      "grad_norm": 0.17582587897777557,
      "learning_rate": 0.0001558515699333968,
      "loss": 0.1426,
      "step": 474
    },
    {
      "epoch": 0.674237047551455,
      "grad_norm": 0.16724251210689545,
      "learning_rate": 0.00015575642245480495,
      "loss": 0.1479,
      "step": 475
    },
    {
      "epoch": 0.6756564939673527,
      "grad_norm": 0.14700044691562653,
      "learning_rate": 0.00015566127497621316,
      "loss": 0.136,
      "step": 476
    },
    {
      "epoch": 0.6770759403832506,
      "grad_norm": 0.1487903892993927,
      "learning_rate": 0.00015556612749762131,
      "loss": 0.1351,
      "step": 477
    },
    {
      "epoch": 0.6784953867991483,
      "grad_norm": 0.20699326694011688,
      "learning_rate": 0.0001554709800190295,
      "loss": 0.1403,
      "step": 478
    },
    {
      "epoch": 0.6799148332150461,
      "grad_norm": 0.15087048709392548,
      "learning_rate": 0.00015537583254043768,
      "loss": 0.1224,
      "step": 479
    },
    {
      "epoch": 0.681334279630944,
      "grad_norm": 0.1315482258796692,
      "learning_rate": 0.00015528068506184586,
      "loss": 0.1058,
      "step": 480
    },
    {
      "epoch": 0.6827537260468417,
      "grad_norm": 0.18645484745502472,
      "learning_rate": 0.00015518553758325405,
      "loss": 0.1342,
      "step": 481
    },
    {
      "epoch": 0.6841731724627396,
      "grad_norm": 0.19662021100521088,
      "learning_rate": 0.00015509039010466223,
      "loss": 0.1463,
      "step": 482
    },
    {
      "epoch": 0.6855926188786373,
      "grad_norm": 0.12735840678215027,
      "learning_rate": 0.00015499524262607041,
      "loss": 0.0963,
      "step": 483
    },
    {
      "epoch": 0.6870120652945352,
      "grad_norm": 0.1387183964252472,
      "learning_rate": 0.0001549000951474786,
      "loss": 0.0957,
      "step": 484
    },
    {
      "epoch": 0.6884315117104329,
      "grad_norm": 0.13882651925086975,
      "learning_rate": 0.00015480494766888678,
      "loss": 0.1095,
      "step": 485
    },
    {
      "epoch": 0.6898509581263307,
      "grad_norm": 0.17697463929653168,
      "learning_rate": 0.00015470980019029496,
      "loss": 0.0947,
      "step": 486
    },
    {
      "epoch": 0.6912704045422285,
      "grad_norm": 0.12020111083984375,
      "learning_rate": 0.00015461465271170315,
      "loss": 0.0928,
      "step": 487
    },
    {
      "epoch": 0.6926898509581263,
      "grad_norm": 0.11944457143545151,
      "learning_rate": 0.00015451950523311133,
      "loss": 0.0909,
      "step": 488
    },
    {
      "epoch": 0.6941092973740242,
      "grad_norm": 0.12041766196489334,
      "learning_rate": 0.00015442435775451951,
      "loss": 0.0907,
      "step": 489
    },
    {
      "epoch": 0.6955287437899219,
      "grad_norm": 0.11162368953227997,
      "learning_rate": 0.0001543292102759277,
      "loss": 0.084,
      "step": 490
    },
    {
      "epoch": 0.6969481902058198,
      "grad_norm": 0.11690417677164078,
      "learning_rate": 0.00015423406279733588,
      "loss": 0.0778,
      "step": 491
    },
    {
      "epoch": 0.6983676366217175,
      "grad_norm": 0.13275593519210815,
      "learning_rate": 0.00015413891531874406,
      "loss": 0.1012,
      "step": 492
    },
    {
      "epoch": 0.6997870830376153,
      "grad_norm": 0.12870202958583832,
      "learning_rate": 0.00015404376784015225,
      "loss": 0.0807,
      "step": 493
    },
    {
      "epoch": 0.7012065294535131,
      "grad_norm": 0.13579553365707397,
      "learning_rate": 0.00015394862036156043,
      "loss": 0.0824,
      "step": 494
    },
    {
      "epoch": 0.7026259758694109,
      "grad_norm": 0.1251567006111145,
      "learning_rate": 0.00015385347288296861,
      "loss": 0.0808,
      "step": 495
    },
    {
      "epoch": 0.7040454222853088,
      "grad_norm": 0.10862187296152115,
      "learning_rate": 0.0001537583254043768,
      "loss": 0.0766,
      "step": 496
    },
    {
      "epoch": 0.7054648687012065,
      "grad_norm": 0.10223617404699326,
      "learning_rate": 0.00015366317792578498,
      "loss": 0.0595,
      "step": 497
    },
    {
      "epoch": 0.7068843151171044,
      "grad_norm": 0.125504732131958,
      "learning_rate": 0.00015356803044719316,
      "loss": 0.0711,
      "step": 498
    },
    {
      "epoch": 0.7083037615330021,
      "grad_norm": 0.09378299862146378,
      "learning_rate": 0.00015347288296860135,
      "loss": 0.0569,
      "step": 499
    },
    {
      "epoch": 0.7097232079489,
      "grad_norm": 0.09513435512781143,
      "learning_rate": 0.00015337773549000953,
      "loss": 0.0519,
      "step": 500
    },
    {
      "epoch": 0.7111426543647977,
      "grad_norm": 0.5318410396575928,
      "learning_rate": 0.00015328258801141772,
      "loss": 0.5911,
      "step": 501
    },
    {
      "epoch": 0.7125621007806955,
      "grad_norm": 0.4041525423526764,
      "learning_rate": 0.00015318744053282587,
      "loss": 0.4444,
      "step": 502
    },
    {
      "epoch": 0.7139815471965933,
      "grad_norm": 0.36350980401039124,
      "learning_rate": 0.00015309229305423408,
      "loss": 0.4043,
      "step": 503
    },
    {
      "epoch": 0.7154009936124911,
      "grad_norm": 0.3149220049381256,
      "learning_rate": 0.00015299714557564224,
      "loss": 0.388,
      "step": 504
    },
    {
      "epoch": 0.716820440028389,
      "grad_norm": 0.3172186315059662,
      "learning_rate": 0.00015290199809705045,
      "loss": 0.3572,
      "step": 505
    },
    {
      "epoch": 0.7182398864442867,
      "grad_norm": 0.21996641159057617,
      "learning_rate": 0.0001528068506184586,
      "loss": 0.3169,
      "step": 506
    },
    {
      "epoch": 0.7196593328601846,
      "grad_norm": 0.2433776557445526,
      "learning_rate": 0.00015271170313986682,
      "loss": 0.2814,
      "step": 507
    },
    {
      "epoch": 0.7210787792760823,
      "grad_norm": 0.22343401610851288,
      "learning_rate": 0.00015261655566127497,
      "loss": 0.2988,
      "step": 508
    },
    {
      "epoch": 0.7224982256919801,
      "grad_norm": 0.3074162006378174,
      "learning_rate": 0.00015252140818268315,
      "loss": 0.2837,
      "step": 509
    },
    {
      "epoch": 0.7239176721078779,
      "grad_norm": 0.3100600242614746,
      "learning_rate": 0.00015242626070409134,
      "loss": 0.3222,
      "step": 510
    },
    {
      "epoch": 0.7253371185237757,
      "grad_norm": 0.2587689757347107,
      "learning_rate": 0.00015233111322549952,
      "loss": 0.3424,
      "step": 511
    },
    {
      "epoch": 0.7267565649396736,
      "grad_norm": 0.32157301902770996,
      "learning_rate": 0.0001522359657469077,
      "loss": 0.2827,
      "step": 512
    },
    {
      "epoch": 0.7281760113555713,
      "grad_norm": 0.1988394856452942,
      "learning_rate": 0.0001521408182683159,
      "loss": 0.2079,
      "step": 513
    },
    {
      "epoch": 0.7295954577714692,
      "grad_norm": 0.19347591698169708,
      "learning_rate": 0.00015204567078972407,
      "loss": 0.2008,
      "step": 514
    },
    {
      "epoch": 0.7310149041873669,
      "grad_norm": 0.26390835642814636,
      "learning_rate": 0.00015195052331113226,
      "loss": 0.2376,
      "step": 515
    },
    {
      "epoch": 0.7324343506032647,
      "grad_norm": 0.2353726327419281,
      "learning_rate": 0.00015185537583254044,
      "loss": 0.2092,
      "step": 516
    },
    {
      "epoch": 0.7338537970191625,
      "grad_norm": 0.19849388301372528,
      "learning_rate": 0.00015176022835394862,
      "loss": 0.2198,
      "step": 517
    },
    {
      "epoch": 0.7352732434350603,
      "grad_norm": 0.18831156194210052,
      "learning_rate": 0.0001516650808753568,
      "loss": 0.1801,
      "step": 518
    },
    {
      "epoch": 0.7366926898509581,
      "grad_norm": 0.20285390317440033,
      "learning_rate": 0.000151569933396765,
      "loss": 0.2012,
      "step": 519
    },
    {
      "epoch": 0.7381121362668559,
      "grad_norm": 0.19947929680347443,
      "learning_rate": 0.00015147478591817317,
      "loss": 0.168,
      "step": 520
    },
    {
      "epoch": 0.7395315826827538,
      "grad_norm": 0.18636251986026764,
      "learning_rate": 0.00015137963843958136,
      "loss": 0.1753,
      "step": 521
    },
    {
      "epoch": 0.7409510290986515,
      "grad_norm": 0.19068703055381775,
      "learning_rate": 0.00015128449096098954,
      "loss": 0.175,
      "step": 522
    },
    {
      "epoch": 0.7423704755145494,
      "grad_norm": 0.14717668294906616,
      "learning_rate": 0.00015118934348239772,
      "loss": 0.1273,
      "step": 523
    },
    {
      "epoch": 0.7437899219304471,
      "grad_norm": 0.2050025761127472,
      "learning_rate": 0.0001510941960038059,
      "loss": 0.1684,
      "step": 524
    },
    {
      "epoch": 0.7452093683463449,
      "grad_norm": 0.1962311714887619,
      "learning_rate": 0.0001509990485252141,
      "loss": 0.1756,
      "step": 525
    },
    {
      "epoch": 0.7466288147622427,
      "grad_norm": 0.19705097377300262,
      "learning_rate": 0.00015090390104662227,
      "loss": 0.1744,
      "step": 526
    },
    {
      "epoch": 0.7480482611781405,
      "grad_norm": 0.16563889384269714,
      "learning_rate": 0.00015080875356803046,
      "loss": 0.1331,
      "step": 527
    },
    {
      "epoch": 0.7494677075940384,
      "grad_norm": 0.15262381732463837,
      "learning_rate": 0.00015071360608943864,
      "loss": 0.1118,
      "step": 528
    },
    {
      "epoch": 0.7508871540099361,
      "grad_norm": 0.16130661964416504,
      "learning_rate": 0.00015061845861084682,
      "loss": 0.1514,
      "step": 529
    },
    {
      "epoch": 0.752306600425834,
      "grad_norm": 0.1357499212026596,
      "learning_rate": 0.000150523311132255,
      "loss": 0.1327,
      "step": 530
    },
    {
      "epoch": 0.7537260468417317,
      "grad_norm": 0.171394482254982,
      "learning_rate": 0.0001504281636536632,
      "loss": 0.139,
      "step": 531
    },
    {
      "epoch": 0.7551454932576295,
      "grad_norm": 0.1561088263988495,
      "learning_rate": 0.00015033301617507137,
      "loss": 0.1381,
      "step": 532
    },
    {
      "epoch": 0.7565649396735273,
      "grad_norm": 0.14966213703155518,
      "learning_rate": 0.00015023786869647956,
      "loss": 0.143,
      "step": 533
    },
    {
      "epoch": 0.7579843860894251,
      "grad_norm": 0.14861424267292023,
      "learning_rate": 0.00015014272121788774,
      "loss": 0.1309,
      "step": 534
    },
    {
      "epoch": 0.759403832505323,
      "grad_norm": 0.12363504618406296,
      "learning_rate": 0.00015004757373929592,
      "loss": 0.1117,
      "step": 535
    },
    {
      "epoch": 0.7608232789212207,
      "grad_norm": 0.1706092208623886,
      "learning_rate": 0.0001499524262607041,
      "loss": 0.1281,
      "step": 536
    },
    {
      "epoch": 0.7622427253371186,
      "grad_norm": 0.27571582794189453,
      "learning_rate": 0.0001498572787821123,
      "loss": 0.1283,
      "step": 537
    },
    {
      "epoch": 0.7636621717530163,
      "grad_norm": 0.12227420508861542,
      "learning_rate": 0.00014976213130352045,
      "loss": 0.1099,
      "step": 538
    },
    {
      "epoch": 0.7650816181689141,
      "grad_norm": 0.1284242570400238,
      "learning_rate": 0.00014966698382492866,
      "loss": 0.0966,
      "step": 539
    },
    {
      "epoch": 0.7665010645848119,
      "grad_norm": 0.1298067420721054,
      "learning_rate": 0.0001495718363463368,
      "loss": 0.1141,
      "step": 540
    },
    {
      "epoch": 0.7679205110007097,
      "grad_norm": 0.10451561957597733,
      "learning_rate": 0.00014947668886774502,
      "loss": 0.0854,
      "step": 541
    },
    {
      "epoch": 0.7693399574166075,
      "grad_norm": 0.10955027490854263,
      "learning_rate": 0.00014938154138915318,
      "loss": 0.0872,
      "step": 542
    },
    {
      "epoch": 0.7707594038325053,
      "grad_norm": 0.11931219696998596,
      "learning_rate": 0.0001492863939105614,
      "loss": 0.0724,
      "step": 543
    },
    {
      "epoch": 0.7721788502484032,
      "grad_norm": 0.10852313041687012,
      "learning_rate": 0.00014919124643196955,
      "loss": 0.0737,
      "step": 544
    },
    {
      "epoch": 0.7735982966643009,
      "grad_norm": 0.1705986112356186,
      "learning_rate": 0.00014909609895337776,
      "loss": 0.1093,
      "step": 545
    },
    {
      "epoch": 0.7750177430801988,
      "grad_norm": 0.12562288343906403,
      "learning_rate": 0.0001490009514747859,
      "loss": 0.0799,
      "step": 546
    },
    {
      "epoch": 0.7764371894960965,
      "grad_norm": 0.15277740359306335,
      "learning_rate": 0.00014890580399619412,
      "loss": 0.0744,
      "step": 547
    },
    {
      "epoch": 0.7778566359119943,
      "grad_norm": 0.15108178555965424,
      "learning_rate": 0.00014881065651760228,
      "loss": 0.0825,
      "step": 548
    },
    {
      "epoch": 0.7792760823278921,
      "grad_norm": 0.08579418808221817,
      "learning_rate": 0.0001487155090390105,
      "loss": 0.0609,
      "step": 549
    },
    {
      "epoch": 0.7806955287437899,
      "grad_norm": 0.07670292258262634,
      "learning_rate": 0.00014862036156041865,
      "loss": 0.0521,
      "step": 550
    },
    {
      "epoch": 0.7821149751596878,
      "grad_norm": 0.5727811455726624,
      "learning_rate": 0.00014852521408182686,
      "loss": 0.6004,
      "step": 551
    },
    {
      "epoch": 0.7835344215755855,
      "grad_norm": 0.4237355887889862,
      "learning_rate": 0.000148430066603235,
      "loss": 0.4187,
      "step": 552
    },
    {
      "epoch": 0.7849538679914834,
      "grad_norm": 0.38377857208251953,
      "learning_rate": 0.00014833491912464322,
      "loss": 0.4589,
      "step": 553
    },
    {
      "epoch": 0.7863733144073811,
      "grad_norm": 0.30041319131851196,
      "learning_rate": 0.00014823977164605138,
      "loss": 0.3742,
      "step": 554
    },
    {
      "epoch": 0.7877927608232789,
      "grad_norm": 0.2506842017173767,
      "learning_rate": 0.0001481446241674596,
      "loss": 0.3193,
      "step": 555
    },
    {
      "epoch": 0.7892122072391767,
      "grad_norm": 0.28410252928733826,
      "learning_rate": 0.00014804947668886775,
      "loss": 0.3387,
      "step": 556
    },
    {
      "epoch": 0.7906316536550745,
      "grad_norm": 0.2975349724292755,
      "learning_rate": 0.00014795432921027596,
      "loss": 0.3203,
      "step": 557
    },
    {
      "epoch": 0.7920511000709723,
      "grad_norm": 0.2852706015110016,
      "learning_rate": 0.0001478591817316841,
      "loss": 0.3333,
      "step": 558
    },
    {
      "epoch": 0.7934705464868701,
      "grad_norm": 0.24174071848392487,
      "learning_rate": 0.0001477640342530923,
      "loss": 0.2559,
      "step": 559
    },
    {
      "epoch": 0.794889992902768,
      "grad_norm": 0.24527372419834137,
      "learning_rate": 0.00014766888677450048,
      "loss": 0.2817,
      "step": 560
    },
    {
      "epoch": 0.7963094393186657,
      "grad_norm": 0.24680295586585999,
      "learning_rate": 0.00014757373929590866,
      "loss": 0.259,
      "step": 561
    },
    {
      "epoch": 0.7977288857345636,
      "grad_norm": 0.26462438702583313,
      "learning_rate": 0.00014747859181731685,
      "loss": 0.238,
      "step": 562
    },
    {
      "epoch": 0.7991483321504613,
      "grad_norm": 0.22806233167648315,
      "learning_rate": 0.00014738344433872503,
      "loss": 0.2437,
      "step": 563
    },
    {
      "epoch": 0.8005677785663591,
      "grad_norm": 0.2250288724899292,
      "learning_rate": 0.0001472882968601332,
      "loss": 0.2275,
      "step": 564
    },
    {
      "epoch": 0.8019872249822569,
      "grad_norm": 0.2455858588218689,
      "learning_rate": 0.0001471931493815414,
      "loss": 0.2286,
      "step": 565
    },
    {
      "epoch": 0.8034066713981547,
      "grad_norm": 0.21104341745376587,
      "learning_rate": 0.00014709800190294958,
      "loss": 0.1818,
      "step": 566
    },
    {
      "epoch": 0.8048261178140526,
      "grad_norm": 0.26976826786994934,
      "learning_rate": 0.00014700285442435776,
      "loss": 0.2443,
      "step": 567
    },
    {
      "epoch": 0.8062455642299503,
      "grad_norm": 0.29840242862701416,
      "learning_rate": 0.00014690770694576595,
      "loss": 0.2517,
      "step": 568
    },
    {
      "epoch": 0.8076650106458482,
      "grad_norm": 0.22859202325344086,
      "learning_rate": 0.00014681255946717413,
      "loss": 0.2014,
      "step": 569
    },
    {
      "epoch": 0.8090844570617459,
      "grad_norm": 0.1852816492319107,
      "learning_rate": 0.0001467174119885823,
      "loss": 0.1858,
      "step": 570
    },
    {
      "epoch": 0.8105039034776437,
      "grad_norm": 0.18705476820468903,
      "learning_rate": 0.00014662226450999047,
      "loss": 0.1809,
      "step": 571
    },
    {
      "epoch": 0.8119233498935415,
      "grad_norm": 0.17783915996551514,
      "learning_rate": 0.00014652711703139868,
      "loss": 0.1493,
      "step": 572
    },
    {
      "epoch": 0.8133427963094393,
      "grad_norm": 0.2490440160036087,
      "learning_rate": 0.00014643196955280684,
      "loss": 0.2033,
      "step": 573
    },
    {
      "epoch": 0.8147622427253371,
      "grad_norm": 0.19925494492053986,
      "learning_rate": 0.00014633682207421505,
      "loss": 0.1792,
      "step": 574
    },
    {
      "epoch": 0.8161816891412349,
      "grad_norm": 0.15136615931987762,
      "learning_rate": 0.0001462416745956232,
      "loss": 0.146,
      "step": 575
    },
    {
      "epoch": 0.8176011355571328,
      "grad_norm": 0.16686761379241943,
      "learning_rate": 0.0001461465271170314,
      "loss": 0.1605,
      "step": 576
    },
    {
      "epoch": 0.8190205819730305,
      "grad_norm": 0.17302149534225464,
      "learning_rate": 0.00014605137963843957,
      "loss": 0.1574,
      "step": 577
    },
    {
      "epoch": 0.8204400283889283,
      "grad_norm": 0.1727643460035324,
      "learning_rate": 0.00014595623215984778,
      "loss": 0.1601,
      "step": 578
    },
    {
      "epoch": 0.8218594748048261,
      "grad_norm": 0.19917789101600647,
      "learning_rate": 0.00014586108468125594,
      "loss": 0.1617,
      "step": 579
    },
    {
      "epoch": 0.8232789212207239,
      "grad_norm": 0.15862904489040375,
      "learning_rate": 0.00014576593720266415,
      "loss": 0.127,
      "step": 580
    },
    {
      "epoch": 0.8246983676366217,
      "grad_norm": 0.19296683371067047,
      "learning_rate": 0.0001456707897240723,
      "loss": 0.169,
      "step": 581
    },
    {
      "epoch": 0.8261178140525195,
      "grad_norm": 0.1642456352710724,
      "learning_rate": 0.0001455756422454805,
      "loss": 0.1349,
      "step": 582
    },
    {
      "epoch": 0.8275372604684174,
      "grad_norm": 0.15335328876972198,
      "learning_rate": 0.00014548049476688867,
      "loss": 0.1295,
      "step": 583
    },
    {
      "epoch": 0.8289567068843151,
      "grad_norm": 0.18904566764831543,
      "learning_rate": 0.00014538534728829688,
      "loss": 0.1352,
      "step": 584
    },
    {
      "epoch": 0.830376153300213,
      "grad_norm": 0.17180368304252625,
      "learning_rate": 0.00014529019980970504,
      "loss": 0.1422,
      "step": 585
    },
    {
      "epoch": 0.8317955997161107,
      "grad_norm": 0.17974838614463806,
      "learning_rate": 0.00014519505233111325,
      "loss": 0.1232,
      "step": 586
    },
    {
      "epoch": 0.8332150461320085,
      "grad_norm": 0.16072316467761993,
      "learning_rate": 0.0001450999048525214,
      "loss": 0.1128,
      "step": 587
    },
    {
      "epoch": 0.8346344925479063,
      "grad_norm": 0.14107412099838257,
      "learning_rate": 0.0001450047573739296,
      "loss": 0.096,
      "step": 588
    },
    {
      "epoch": 0.8360539389638041,
      "grad_norm": 0.13276265561580658,
      "learning_rate": 0.00014490960989533777,
      "loss": 0.1049,
      "step": 589
    },
    {
      "epoch": 0.837473385379702,
      "grad_norm": 0.15781976282596588,
      "learning_rate": 0.00014481446241674595,
      "loss": 0.1152,
      "step": 590
    },
    {
      "epoch": 0.8388928317955997,
      "grad_norm": 0.1590309888124466,
      "learning_rate": 0.00014471931493815414,
      "loss": 0.1101,
      "step": 591
    },
    {
      "epoch": 0.8403122782114976,
      "grad_norm": 0.1210738867521286,
      "learning_rate": 0.00014462416745956232,
      "loss": 0.0638,
      "step": 592
    },
    {
      "epoch": 0.8417317246273953,
      "grad_norm": 0.14234381914138794,
      "learning_rate": 0.0001445290199809705,
      "loss": 0.1131,
      "step": 593
    },
    {
      "epoch": 0.8431511710432931,
      "grad_norm": 0.11053589731454849,
      "learning_rate": 0.00014443387250237869,
      "loss": 0.0849,
      "step": 594
    },
    {
      "epoch": 0.8445706174591909,
      "grad_norm": 0.11865406483411789,
      "learning_rate": 0.00014433872502378687,
      "loss": 0.0836,
      "step": 595
    },
    {
      "epoch": 0.8459900638750887,
      "grad_norm": 0.11474697291851044,
      "learning_rate": 0.00014424357754519505,
      "loss": 0.0784,
      "step": 596
    },
    {
      "epoch": 0.8474095102909865,
      "grad_norm": 0.11239312589168549,
      "learning_rate": 0.00014414843006660324,
      "loss": 0.0788,
      "step": 597
    },
    {
      "epoch": 0.8488289567068843,
      "grad_norm": 0.093875452876091,
      "learning_rate": 0.00014405328258801142,
      "loss": 0.0711,
      "step": 598
    },
    {
      "epoch": 0.8502484031227822,
      "grad_norm": 0.09124719351530075,
      "learning_rate": 0.0001439581351094196,
      "loss": 0.0503,
      "step": 599
    },
    {
      "epoch": 0.8516678495386799,
      "grad_norm": 0.09440264850854874,
      "learning_rate": 0.0001438629876308278,
      "loss": 0.0669,
      "step": 600
    },
    {
      "epoch": 0.8530872959545777,
      "grad_norm": 0.6293944120407104,
      "learning_rate": 0.00014376784015223597,
      "loss": 0.5435,
      "step": 601
    },
    {
      "epoch": 0.8545067423704755,
      "grad_norm": 0.4291839897632599,
      "learning_rate": 0.00014367269267364415,
      "loss": 0.5023,
      "step": 602
    },
    {
      "epoch": 0.8559261887863733,
      "grad_norm": 0.38119930028915405,
      "learning_rate": 0.00014357754519505234,
      "loss": 0.4098,
      "step": 603
    },
    {
      "epoch": 0.8573456352022711,
      "grad_norm": 0.3160819411277771,
      "learning_rate": 0.00014348239771646052,
      "loss": 0.3627,
      "step": 604
    },
    {
      "epoch": 0.8587650816181689,
      "grad_norm": 0.3257262706756592,
      "learning_rate": 0.0001433872502378687,
      "loss": 0.3849,
      "step": 605
    },
    {
      "epoch": 0.8601845280340668,
      "grad_norm": 0.313341349363327,
      "learning_rate": 0.0001432921027592769,
      "loss": 0.3333,
      "step": 606
    },
    {
      "epoch": 0.8616039744499645,
      "grad_norm": 0.24657322466373444,
      "learning_rate": 0.00014319695528068507,
      "loss": 0.3,
      "step": 607
    },
    {
      "epoch": 0.8630234208658624,
      "grad_norm": 0.23522303998470306,
      "learning_rate": 0.00014310180780209325,
      "loss": 0.2958,
      "step": 608
    },
    {
      "epoch": 0.8644428672817601,
      "grad_norm": 0.24196945130825043,
      "learning_rate": 0.00014300666032350144,
      "loss": 0.2626,
      "step": 609
    },
    {
      "epoch": 0.8658623136976579,
      "grad_norm": 0.25085851550102234,
      "learning_rate": 0.00014291151284490962,
      "loss": 0.2921,
      "step": 610
    },
    {
      "epoch": 0.8672817601135557,
      "grad_norm": 0.27037549018859863,
      "learning_rate": 0.0001428163653663178,
      "loss": 0.2503,
      "step": 611
    },
    {
      "epoch": 0.8687012065294535,
      "grad_norm": 0.22163833677768707,
      "learning_rate": 0.000142721217887726,
      "loss": 0.2544,
      "step": 612
    },
    {
      "epoch": 0.8701206529453513,
      "grad_norm": 0.22180965542793274,
      "learning_rate": 0.00014262607040913417,
      "loss": 0.2629,
      "step": 613
    },
    {
      "epoch": 0.8715400993612491,
      "grad_norm": 0.21035288274288177,
      "learning_rate": 0.00014253092293054235,
      "loss": 0.2249,
      "step": 614
    },
    {
      "epoch": 0.872959545777147,
      "grad_norm": 0.23706668615341187,
      "learning_rate": 0.00014243577545195054,
      "loss": 0.237,
      "step": 615
    },
    {
      "epoch": 0.8743789921930447,
      "grad_norm": 0.21941930055618286,
      "learning_rate": 0.00014234062797335872,
      "loss": 0.2174,
      "step": 616
    },
    {
      "epoch": 0.8757984386089425,
      "grad_norm": 0.19976352155208588,
      "learning_rate": 0.0001422454804947669,
      "loss": 0.2056,
      "step": 617
    },
    {
      "epoch": 0.8772178850248403,
      "grad_norm": 0.21921448409557343,
      "learning_rate": 0.0001421503330161751,
      "loss": 0.2323,
      "step": 618
    },
    {
      "epoch": 0.8786373314407381,
      "grad_norm": 0.20175445079803467,
      "learning_rate": 0.00014205518553758327,
      "loss": 0.19,
      "step": 619
    },
    {
      "epoch": 0.8800567778566359,
      "grad_norm": 0.16209647059440613,
      "learning_rate": 0.00014196003805899145,
      "loss": 0.1979,
      "step": 620
    },
    {
      "epoch": 0.8814762242725337,
      "grad_norm": 0.17713458836078644,
      "learning_rate": 0.0001418648905803996,
      "loss": 0.1737,
      "step": 621
    },
    {
      "epoch": 0.8828956706884316,
      "grad_norm": 0.18487538397312164,
      "learning_rate": 0.00014176974310180782,
      "loss": 0.1931,
      "step": 622
    },
    {
      "epoch": 0.8843151171043293,
      "grad_norm": 0.15035980939865112,
      "learning_rate": 0.00014167459562321598,
      "loss": 0.1376,
      "step": 623
    },
    {
      "epoch": 0.8857345635202271,
      "grad_norm": 0.158620685338974,
      "learning_rate": 0.0001415794481446242,
      "loss": 0.1449,
      "step": 624
    },
    {
      "epoch": 0.8871540099361249,
      "grad_norm": 0.18606463074684143,
      "learning_rate": 0.00014148430066603234,
      "loss": 0.1711,
      "step": 625
    },
    {
      "epoch": 0.8885734563520227,
      "grad_norm": 0.17129795253276825,
      "learning_rate": 0.00014138915318744055,
      "loss": 0.1586,
      "step": 626
    },
    {
      "epoch": 0.8899929027679205,
      "grad_norm": 0.15806519985198975,
      "learning_rate": 0.0001412940057088487,
      "loss": 0.1509,
      "step": 627
    },
    {
      "epoch": 0.8914123491838183,
      "grad_norm": 0.18260568380355835,
      "learning_rate": 0.00014119885823025692,
      "loss": 0.178,
      "step": 628
    },
    {
      "epoch": 0.8928317955997161,
      "grad_norm": 0.1893806755542755,
      "learning_rate": 0.00014110371075166508,
      "loss": 0.1496,
      "step": 629
    },
    {
      "epoch": 0.8942512420156139,
      "grad_norm": 0.16832983493804932,
      "learning_rate": 0.0001410085632730733,
      "loss": 0.1352,
      "step": 630
    },
    {
      "epoch": 0.8956706884315118,
      "grad_norm": 0.1850036233663559,
      "learning_rate": 0.00014091341579448144,
      "loss": 0.1513,
      "step": 631
    },
    {
      "epoch": 0.8970901348474095,
      "grad_norm": 0.14322754740715027,
      "learning_rate": 0.00014081826831588965,
      "loss": 0.1216,
      "step": 632
    },
    {
      "epoch": 0.8985095812633073,
      "grad_norm": 0.1574215441942215,
      "learning_rate": 0.0001407231208372978,
      "loss": 0.1369,
      "step": 633
    },
    {
      "epoch": 0.8999290276792051,
      "grad_norm": 0.1363058090209961,
      "learning_rate": 0.00014062797335870602,
      "loss": 0.1099,
      "step": 634
    },
    {
      "epoch": 0.9013484740951029,
      "grad_norm": 0.14761459827423096,
      "learning_rate": 0.00014053282588011418,
      "loss": 0.1092,
      "step": 635
    },
    {
      "epoch": 0.9027679205110007,
      "grad_norm": 0.1275966316461563,
      "learning_rate": 0.0001404376784015224,
      "loss": 0.0984,
      "step": 636
    },
    {
      "epoch": 0.9041873669268985,
      "grad_norm": 0.1448143869638443,
      "learning_rate": 0.00014034253092293054,
      "loss": 0.1154,
      "step": 637
    },
    {
      "epoch": 0.9056068133427964,
      "grad_norm": 0.15729297697544098,
      "learning_rate": 0.00014024738344433875,
      "loss": 0.1186,
      "step": 638
    },
    {
      "epoch": 0.9070262597586941,
      "grad_norm": 0.12504489719867706,
      "learning_rate": 0.0001401522359657469,
      "loss": 0.0992,
      "step": 639
    },
    {
      "epoch": 0.9084457061745919,
      "grad_norm": 0.12685270607471466,
      "learning_rate": 0.00014005708848715512,
      "loss": 0.0977,
      "step": 640
    },
    {
      "epoch": 0.9098651525904897,
      "grad_norm": 0.1121777817606926,
      "learning_rate": 0.00013996194100856328,
      "loss": 0.0849,
      "step": 641
    },
    {
      "epoch": 0.9112845990063875,
      "grad_norm": 0.11588460206985474,
      "learning_rate": 0.00013986679352997146,
      "loss": 0.0805,
      "step": 642
    },
    {
      "epoch": 0.9127040454222853,
      "grad_norm": 0.12288933247327805,
      "learning_rate": 0.00013977164605137964,
      "loss": 0.0942,
      "step": 643
    },
    {
      "epoch": 0.9141234918381831,
      "grad_norm": 0.11066070199012756,
      "learning_rate": 0.00013967649857278783,
      "loss": 0.0844,
      "step": 644
    },
    {
      "epoch": 0.915542938254081,
      "grad_norm": 0.11849730461835861,
      "learning_rate": 0.000139581351094196,
      "loss": 0.0903,
      "step": 645
    },
    {
      "epoch": 0.9169623846699787,
      "grad_norm": 0.14358580112457275,
      "learning_rate": 0.0001394862036156042,
      "loss": 0.0851,
      "step": 646
    },
    {
      "epoch": 0.9183818310858765,
      "grad_norm": 0.11204153299331665,
      "learning_rate": 0.00013939105613701238,
      "loss": 0.0682,
      "step": 647
    },
    {
      "epoch": 0.9198012775017743,
      "grad_norm": 0.10966405272483826,
      "learning_rate": 0.00013929590865842056,
      "loss": 0.0794,
      "step": 648
    },
    {
      "epoch": 0.9212207239176721,
      "grad_norm": 0.09689820557832718,
      "learning_rate": 0.00013920076117982874,
      "loss": 0.0618,
      "step": 649
    },
    {
      "epoch": 0.9226401703335699,
      "grad_norm": 0.1057770848274231,
      "learning_rate": 0.00013910561370123693,
      "loss": 0.0544,
      "step": 650
    },
    {
      "epoch": 0.9240596167494677,
      "grad_norm": 0.43264076113700867,
      "learning_rate": 0.0001390104662226451,
      "loss": 0.5093,
      "step": 651
    },
    {
      "epoch": 0.9254790631653655,
      "grad_norm": 0.37290966510772705,
      "learning_rate": 0.00013891531874405327,
      "loss": 0.3851,
      "step": 652
    },
    {
      "epoch": 0.9268985095812633,
      "grad_norm": 0.35502201318740845,
      "learning_rate": 0.00013882017126546148,
      "loss": 0.3743,
      "step": 653
    },
    {
      "epoch": 0.9283179559971612,
      "grad_norm": 0.36546796560287476,
      "learning_rate": 0.00013872502378686963,
      "loss": 0.4008,
      "step": 654
    },
    {
      "epoch": 0.9297374024130589,
      "grad_norm": 0.3241652846336365,
      "learning_rate": 0.00013862987630827784,
      "loss": 0.3426,
      "step": 655
    },
    {
      "epoch": 0.9311568488289567,
      "grad_norm": 0.27071768045425415,
      "learning_rate": 0.000138534728829686,
      "loss": 0.3561,
      "step": 656
    },
    {
      "epoch": 0.9325762952448545,
      "grad_norm": 0.24140791594982147,
      "learning_rate": 0.0001384395813510942,
      "loss": 0.3154,
      "step": 657
    },
    {
      "epoch": 0.9339957416607523,
      "grad_norm": 0.2794343829154968,
      "learning_rate": 0.00013834443387250237,
      "loss": 0.3443,
      "step": 658
    },
    {
      "epoch": 0.9354151880766501,
      "grad_norm": 0.2668094336986542,
      "learning_rate": 0.00013824928639391058,
      "loss": 0.2793,
      "step": 659
    },
    {
      "epoch": 0.9368346344925479,
      "grad_norm": 0.23154842853546143,
      "learning_rate": 0.00013815413891531873,
      "loss": 0.2511,
      "step": 660
    },
    {
      "epoch": 0.9382540809084458,
      "grad_norm": 0.2238822877407074,
      "learning_rate": 0.00013805899143672694,
      "loss": 0.2304,
      "step": 661
    },
    {
      "epoch": 0.9396735273243435,
      "grad_norm": 0.23827619850635529,
      "learning_rate": 0.0001379638439581351,
      "loss": 0.232,
      "step": 662
    },
    {
      "epoch": 0.9410929737402413,
      "grad_norm": 0.22854574024677277,
      "learning_rate": 0.0001378686964795433,
      "loss": 0.2643,
      "step": 663
    },
    {
      "epoch": 0.9425124201561391,
      "grad_norm": 0.26202166080474854,
      "learning_rate": 0.00013777354900095147,
      "loss": 0.2451,
      "step": 664
    },
    {
      "epoch": 0.9439318665720369,
      "grad_norm": 0.2784346044063568,
      "learning_rate": 0.00013767840152235968,
      "loss": 0.2481,
      "step": 665
    },
    {
      "epoch": 0.9453513129879347,
      "grad_norm": 0.24547907710075378,
      "learning_rate": 0.00013758325404376783,
      "loss": 0.2185,
      "step": 666
    },
    {
      "epoch": 0.9467707594038325,
      "grad_norm": 0.17956671118736267,
      "learning_rate": 0.00013748810656517604,
      "loss": 0.1563,
      "step": 667
    },
    {
      "epoch": 0.9481902058197303,
      "grad_norm": 0.19529953598976135,
      "learning_rate": 0.0001373929590865842,
      "loss": 0.2116,
      "step": 668
    },
    {
      "epoch": 0.9496096522356281,
      "grad_norm": 0.18870826065540314,
      "learning_rate": 0.0001372978116079924,
      "loss": 0.1932,
      "step": 669
    },
    {
      "epoch": 0.9510290986515259,
      "grad_norm": 0.1614811271429062,
      "learning_rate": 0.00013720266412940057,
      "loss": 0.1405,
      "step": 670
    },
    {
      "epoch": 0.9524485450674237,
      "grad_norm": 0.17701715230941772,
      "learning_rate": 0.00013710751665080878,
      "loss": 0.1635,
      "step": 671
    },
    {
      "epoch": 0.9538679914833215,
      "grad_norm": 0.25134775042533875,
      "learning_rate": 0.00013701236917221693,
      "loss": 0.2015,
      "step": 672
    },
    {
      "epoch": 0.9552874378992193,
      "grad_norm": 0.16321654617786407,
      "learning_rate": 0.00013691722169362512,
      "loss": 0.1383,
      "step": 673
    },
    {
      "epoch": 0.9567068843151171,
      "grad_norm": 0.26167479157447815,
      "learning_rate": 0.0001368220742150333,
      "loss": 0.1783,
      "step": 674
    },
    {
      "epoch": 0.9581263307310149,
      "grad_norm": 0.18638291954994202,
      "learning_rate": 0.00013672692673644148,
      "loss": 0.161,
      "step": 675
    },
    {
      "epoch": 0.9595457771469127,
      "grad_norm": 0.1568879634141922,
      "learning_rate": 0.00013663177925784967,
      "loss": 0.1449,
      "step": 676
    },
    {
      "epoch": 0.9609652235628106,
      "grad_norm": 0.21420665085315704,
      "learning_rate": 0.00013653663177925785,
      "loss": 0.1969,
      "step": 677
    },
    {
      "epoch": 0.9623846699787083,
      "grad_norm": 0.14903709292411804,
      "learning_rate": 0.00013644148430066603,
      "loss": 0.1236,
      "step": 678
    },
    {
      "epoch": 0.9638041163946061,
      "grad_norm": 0.1530379056930542,
      "learning_rate": 0.00013634633682207422,
      "loss": 0.1267,
      "step": 679
    },
    {
      "epoch": 0.9652235628105039,
      "grad_norm": 0.15803420543670654,
      "learning_rate": 0.0001362511893434824,
      "loss": 0.133,
      "step": 680
    },
    {
      "epoch": 0.9666430092264017,
      "grad_norm": 0.14294297993183136,
      "learning_rate": 0.00013615604186489058,
      "loss": 0.1071,
      "step": 681
    },
    {
      "epoch": 0.9680624556422995,
      "grad_norm": 0.2188594788312912,
      "learning_rate": 0.00013606089438629877,
      "loss": 0.1488,
      "step": 682
    },
    {
      "epoch": 0.9694819020581973,
      "grad_norm": 0.2593432068824768,
      "learning_rate": 0.00013596574690770695,
      "loss": 0.1371,
      "step": 683
    },
    {
      "epoch": 0.9709013484740951,
      "grad_norm": 0.1615634709596634,
      "learning_rate": 0.00013587059942911513,
      "loss": 0.1343,
      "step": 684
    },
    {
      "epoch": 0.9723207948899929,
      "grad_norm": 0.14472191035747528,
      "learning_rate": 0.00013577545195052332,
      "loss": 0.104,
      "step": 685
    },
    {
      "epoch": 0.9737402413058907,
      "grad_norm": 0.11434387415647507,
      "learning_rate": 0.0001356803044719315,
      "loss": 0.0982,
      "step": 686
    },
    {
      "epoch": 0.9751596877217885,
      "grad_norm": 0.14229045808315277,
      "learning_rate": 0.00013558515699333968,
      "loss": 0.0915,
      "step": 687
    },
    {
      "epoch": 0.9765791341376863,
      "grad_norm": 0.14187514781951904,
      "learning_rate": 0.00013549000951474787,
      "loss": 0.1274,
      "step": 688
    },
    {
      "epoch": 0.9779985805535841,
      "grad_norm": 0.1579059362411499,
      "learning_rate": 0.00013539486203615605,
      "loss": 0.1116,
      "step": 689
    },
    {
      "epoch": 0.9794180269694819,
      "grad_norm": 0.10884373635053635,
      "learning_rate": 0.00013529971455756423,
      "loss": 0.0702,
      "step": 690
    },
    {
      "epoch": 0.9808374733853797,
      "grad_norm": 0.11126106232404709,
      "learning_rate": 0.00013520456707897242,
      "loss": 0.0892,
      "step": 691
    },
    {
      "epoch": 0.9822569198012775,
      "grad_norm": 0.10765853524208069,
      "learning_rate": 0.0001351094196003806,
      "loss": 0.0815,
      "step": 692
    },
    {
      "epoch": 0.9836763662171752,
      "grad_norm": 0.09569776058197021,
      "learning_rate": 0.00013501427212178879,
      "loss": 0.0667,
      "step": 693
    },
    {
      "epoch": 0.9850958126330731,
      "grad_norm": 0.12836676836013794,
      "learning_rate": 0.00013491912464319697,
      "loss": 0.0831,
      "step": 694
    },
    {
      "epoch": 0.9865152590489709,
      "grad_norm": 0.1558128148317337,
      "learning_rate": 0.00013482397716460515,
      "loss": 0.0659,
      "step": 695
    },
    {
      "epoch": 0.9879347054648687,
      "grad_norm": 0.10963049530982971,
      "learning_rate": 0.00013472882968601334,
      "loss": 0.0729,
      "step": 696
    },
    {
      "epoch": 0.9893541518807665,
      "grad_norm": 0.10026916861534119,
      "learning_rate": 0.00013463368220742152,
      "loss": 0.0732,
      "step": 697
    },
    {
      "epoch": 0.9907735982966643,
      "grad_norm": 0.09855934232473373,
      "learning_rate": 0.0001345385347288297,
      "loss": 0.0822,
      "step": 698
    },
    {
      "epoch": 0.9921930447125621,
      "grad_norm": 0.11414436995983124,
      "learning_rate": 0.00013444338725023789,
      "loss": 0.0719,
      "step": 699
    },
    {
      "epoch": 0.99361249112846,
      "grad_norm": 0.10073580592870712,
      "learning_rate": 0.00013434823977164607,
      "loss": 0.0562,
      "step": 700
    },
    {
      "epoch": 0.9950319375443577,
      "grad_norm": 0.38939520716667175,
      "learning_rate": 0.00013425309229305425,
      "loss": 0.396,
      "step": 701
    },
    {
      "epoch": 0.9964513839602555,
      "grad_norm": 0.24623413383960724,
      "learning_rate": 0.00013415794481446244,
      "loss": 0.2127,
      "step": 702
    },
    {
      "epoch": 0.9978708303761533,
      "grad_norm": 0.16796725988388062,
      "learning_rate": 0.00013406279733587062,
      "loss": 0.1338,
      "step": 703
    },
    {
      "epoch": 0.9992902767920511,
      "grad_norm": 0.11252884566783905,
      "learning_rate": 0.00013396764985727877,
      "loss": 0.0768,
      "step": 704
    },
    {
      "epoch": 1.000709723207949,
      "grad_norm": 0.34894123673439026,
      "learning_rate": 0.00013387250237868699,
      "loss": 0.3404,
      "step": 705
    },
    {
      "epoch": 1.0021291696238468,
      "grad_norm": 0.3218008577823639,
      "learning_rate": 0.00013377735490009514,
      "loss": 0.409,
      "step": 706
    },
    {
      "epoch": 1.0035486160397444,
      "grad_norm": 0.32862189412117004,
      "learning_rate": 0.00013368220742150335,
      "loss": 0.3043,
      "step": 707
    },
    {
      "epoch": 1.0049680624556423,
      "grad_norm": 0.24026300013065338,
      "learning_rate": 0.0001335870599429115,
      "loss": 0.2644,
      "step": 708
    },
    {
      "epoch": 1.0063875088715402,
      "grad_norm": 0.5634042024612427,
      "learning_rate": 0.00013349191246431972,
      "loss": 0.279,
      "step": 709
    },
    {
      "epoch": 1.0078069552874378,
      "grad_norm": 0.20807743072509766,
      "learning_rate": 0.00013339676498572788,
      "loss": 0.233,
      "step": 710
    },
    {
      "epoch": 1.0092264017033357,
      "grad_norm": 0.20175105333328247,
      "learning_rate": 0.00013330161750713606,
      "loss": 0.2394,
      "step": 711
    },
    {
      "epoch": 1.0106458481192335,
      "grad_norm": 0.25610262155532837,
      "learning_rate": 0.00013320647002854424,
      "loss": 0.2639,
      "step": 712
    },
    {
      "epoch": 1.0120652945351314,
      "grad_norm": 0.28106600046157837,
      "learning_rate": 0.00013311132254995243,
      "loss": 0.3007,
      "step": 713
    },
    {
      "epoch": 1.013484740951029,
      "grad_norm": 0.2338162213563919,
      "learning_rate": 0.0001330161750713606,
      "loss": 0.225,
      "step": 714
    },
    {
      "epoch": 1.014904187366927,
      "grad_norm": 0.24868424236774445,
      "learning_rate": 0.0001329210275927688,
      "loss": 0.2076,
      "step": 715
    },
    {
      "epoch": 1.0163236337828248,
      "grad_norm": 0.2331075370311737,
      "learning_rate": 0.00013282588011417698,
      "loss": 0.1801,
      "step": 716
    },
    {
      "epoch": 1.0177430801987224,
      "grad_norm": 0.2903284430503845,
      "learning_rate": 0.00013273073263558516,
      "loss": 0.2158,
      "step": 717
    },
    {
      "epoch": 1.0191625266146203,
      "grad_norm": 0.2172362208366394,
      "learning_rate": 0.00013263558515699334,
      "loss": 0.1947,
      "step": 718
    },
    {
      "epoch": 1.0205819730305181,
      "grad_norm": 0.20478737354278564,
      "learning_rate": 0.00013254043767840153,
      "loss": 0.1783,
      "step": 719
    },
    {
      "epoch": 1.022001419446416,
      "grad_norm": 0.24831555783748627,
      "learning_rate": 0.0001324452901998097,
      "loss": 0.1681,
      "step": 720
    },
    {
      "epoch": 1.0234208658623136,
      "grad_norm": 0.1960899829864502,
      "learning_rate": 0.0001323501427212179,
      "loss": 0.1545,
      "step": 721
    },
    {
      "epoch": 1.0248403122782115,
      "grad_norm": 0.20280173420906067,
      "learning_rate": 0.00013225499524262608,
      "loss": 0.1574,
      "step": 722
    },
    {
      "epoch": 1.0262597586941093,
      "grad_norm": 0.19267137348651886,
      "learning_rate": 0.00013215984776403426,
      "loss": 0.159,
      "step": 723
    },
    {
      "epoch": 1.027679205110007,
      "grad_norm": 0.26803693175315857,
      "learning_rate": 0.00013206470028544244,
      "loss": 0.1873,
      "step": 724
    },
    {
      "epoch": 1.0290986515259049,
      "grad_norm": 0.18435527384281158,
      "learning_rate": 0.00013196955280685063,
      "loss": 0.1352,
      "step": 725
    },
    {
      "epoch": 1.0305180979418027,
      "grad_norm": 0.25576648116111755,
      "learning_rate": 0.0001318744053282588,
      "loss": 0.1614,
      "step": 726
    },
    {
      "epoch": 1.0319375443577006,
      "grad_norm": 0.17966328561306,
      "learning_rate": 0.000131779257849667,
      "loss": 0.1374,
      "step": 727
    },
    {
      "epoch": 1.0333569907735982,
      "grad_norm": 0.20831584930419922,
      "learning_rate": 0.00013168411037107518,
      "loss": 0.1387,
      "step": 728
    },
    {
      "epoch": 1.034776437189496,
      "grad_norm": 0.2099025398492813,
      "learning_rate": 0.00013158896289248336,
      "loss": 0.1407,
      "step": 729
    },
    {
      "epoch": 1.036195883605394,
      "grad_norm": 0.18164142966270447,
      "learning_rate": 0.00013149381541389154,
      "loss": 0.1244,
      "step": 730
    },
    {
      "epoch": 1.0376153300212918,
      "grad_norm": 0.19698351621627808,
      "learning_rate": 0.00013139866793529973,
      "loss": 0.1299,
      "step": 731
    },
    {
      "epoch": 1.0390347764371894,
      "grad_norm": 0.1799570471048355,
      "learning_rate": 0.0001313035204567079,
      "loss": 0.144,
      "step": 732
    },
    {
      "epoch": 1.0404542228530873,
      "grad_norm": 0.19182667136192322,
      "learning_rate": 0.00013120837297811607,
      "loss": 0.1299,
      "step": 733
    },
    {
      "epoch": 1.0418736692689852,
      "grad_norm": 0.16167569160461426,
      "learning_rate": 0.00013111322549952428,
      "loss": 0.1237,
      "step": 734
    },
    {
      "epoch": 1.0432931156848828,
      "grad_norm": 0.15219692885875702,
      "learning_rate": 0.00013101807802093243,
      "loss": 0.1093,
      "step": 735
    },
    {
      "epoch": 1.0447125621007807,
      "grad_norm": 0.21530640125274658,
      "learning_rate": 0.00013092293054234064,
      "loss": 0.1192,
      "step": 736
    },
    {
      "epoch": 1.0461320085166785,
      "grad_norm": 0.1552867889404297,
      "learning_rate": 0.0001308277830637488,
      "loss": 0.0846,
      "step": 737
    },
    {
      "epoch": 1.0475514549325764,
      "grad_norm": 0.2005973905324936,
      "learning_rate": 0.000130732635585157,
      "loss": 0.1068,
      "step": 738
    },
    {
      "epoch": 1.048970901348474,
      "grad_norm": 0.18921338021755219,
      "learning_rate": 0.00013063748810656517,
      "loss": 0.1038,
      "step": 739
    },
    {
      "epoch": 1.050390347764372,
      "grad_norm": 0.11821583658456802,
      "learning_rate": 0.00013054234062797338,
      "loss": 0.0803,
      "step": 740
    },
    {
      "epoch": 1.0518097941802698,
      "grad_norm": 0.15257775783538818,
      "learning_rate": 0.00013044719314938153,
      "loss": 0.0961,
      "step": 741
    },
    {
      "epoch": 1.0532292405961674,
      "grad_norm": 0.17699576914310455,
      "learning_rate": 0.00013035204567078974,
      "loss": 0.1068,
      "step": 742
    },
    {
      "epoch": 1.0546486870120653,
      "grad_norm": 0.14737801253795624,
      "learning_rate": 0.0001302568981921979,
      "loss": 0.0924,
      "step": 743
    },
    {
      "epoch": 1.0560681334279631,
      "grad_norm": 0.12181035429239273,
      "learning_rate": 0.0001301617507136061,
      "loss": 0.0738,
      "step": 744
    },
    {
      "epoch": 1.057487579843861,
      "grad_norm": 0.13820628821849823,
      "learning_rate": 0.00013006660323501427,
      "loss": 0.0939,
      "step": 745
    },
    {
      "epoch": 1.0589070262597586,
      "grad_norm": 0.15334512293338776,
      "learning_rate": 0.00012997145575642248,
      "loss": 0.0737,
      "step": 746
    },
    {
      "epoch": 1.0603264726756565,
      "grad_norm": 0.11891650408506393,
      "learning_rate": 0.00012987630827783063,
      "loss": 0.0696,
      "step": 747
    },
    {
      "epoch": 1.0617459190915544,
      "grad_norm": 0.12140782922506332,
      "learning_rate": 0.00012978116079923884,
      "loss": 0.0674,
      "step": 748
    },
    {
      "epoch": 1.063165365507452,
      "grad_norm": 0.1472795605659485,
      "learning_rate": 0.000129686013320647,
      "loss": 0.0733,
      "step": 749
    },
    {
      "epoch": 1.0645848119233499,
      "grad_norm": 0.13995087146759033,
      "learning_rate": 0.0001295908658420552,
      "loss": 0.065,
      "step": 750
    },
    {
      "epoch": 1.0660042583392477,
      "grad_norm": 0.11231423914432526,
      "learning_rate": 0.00012949571836346337,
      "loss": 0.0616,
      "step": 751
    },
    {
      "epoch": 1.0674237047551456,
      "grad_norm": 0.15325985848903656,
      "learning_rate": 0.00012940057088487158,
      "loss": 0.0594,
      "step": 752
    },
    {
      "epoch": 1.0688431511710432,
      "grad_norm": 0.13390852510929108,
      "learning_rate": 0.00012930542340627973,
      "loss": 0.058,
      "step": 753
    },
    {
      "epoch": 1.070262597586941,
      "grad_norm": 0.10027240961790085,
      "learning_rate": 0.00012921027592768792,
      "loss": 0.0473,
      "step": 754
    },
    {
      "epoch": 1.071682044002839,
      "grad_norm": 0.4430636465549469,
      "learning_rate": 0.0001291151284490961,
      "loss": 0.2616,
      "step": 755
    },
    {
      "epoch": 1.0731014904187366,
      "grad_norm": 0.5103023052215576,
      "learning_rate": 0.00012901998097050428,
      "loss": 0.3646,
      "step": 756
    },
    {
      "epoch": 1.0745209368346345,
      "grad_norm": 0.5317808389663696,
      "learning_rate": 0.00012892483349191247,
      "loss": 0.2895,
      "step": 757
    },
    {
      "epoch": 1.0759403832505323,
      "grad_norm": 0.3523619472980499,
      "learning_rate": 0.00012882968601332065,
      "loss": 0.2638,
      "step": 758
    },
    {
      "epoch": 1.0773598296664302,
      "grad_norm": 0.32921886444091797,
      "learning_rate": 0.00012873453853472883,
      "loss": 0.2775,
      "step": 759
    },
    {
      "epoch": 1.0787792760823278,
      "grad_norm": 0.3308810591697693,
      "learning_rate": 0.00012863939105613702,
      "loss": 0.2494,
      "step": 760
    },
    {
      "epoch": 1.0801987224982257,
      "grad_norm": 0.26525869965553284,
      "learning_rate": 0.0001285442435775452,
      "loss": 0.225,
      "step": 761
    },
    {
      "epoch": 1.0816181689141235,
      "grad_norm": 0.22003328800201416,
      "learning_rate": 0.00012844909609895338,
      "loss": 0.2023,
      "step": 762
    },
    {
      "epoch": 1.0830376153300212,
      "grad_norm": 0.27551794052124023,
      "learning_rate": 0.00012835394862036157,
      "loss": 0.2594,
      "step": 763
    },
    {
      "epoch": 1.084457061745919,
      "grad_norm": 0.23424625396728516,
      "learning_rate": 0.00012825880114176975,
      "loss": 0.194,
      "step": 764
    },
    {
      "epoch": 1.085876508161817,
      "grad_norm": 0.19375629723072052,
      "learning_rate": 0.00012816365366317793,
      "loss": 0.1736,
      "step": 765
    },
    {
      "epoch": 1.0872959545777148,
      "grad_norm": 0.21587787568569183,
      "learning_rate": 0.00012806850618458612,
      "loss": 0.2051,
      "step": 766
    },
    {
      "epoch": 1.0887154009936124,
      "grad_norm": 0.17300263047218323,
      "learning_rate": 0.0001279733587059943,
      "loss": 0.1539,
      "step": 767
    },
    {
      "epoch": 1.0901348474095103,
      "grad_norm": 0.21971891820430756,
      "learning_rate": 0.00012787821122740248,
      "loss": 0.1885,
      "step": 768
    },
    {
      "epoch": 1.0915542938254081,
      "grad_norm": 0.2105615884065628,
      "learning_rate": 0.00012778306374881067,
      "loss": 0.1787,
      "step": 769
    },
    {
      "epoch": 1.0929737402413058,
      "grad_norm": 0.1858362853527069,
      "learning_rate": 0.00012768791627021885,
      "loss": 0.1558,
      "step": 770
    },
    {
      "epoch": 1.0943931866572036,
      "grad_norm": 0.21708060801029205,
      "learning_rate": 0.00012759276879162703,
      "loss": 0.1839,
      "step": 771
    },
    {
      "epoch": 1.0958126330731015,
      "grad_norm": 0.24080990254878998,
      "learning_rate": 0.00012749762131303522,
      "loss": 0.1655,
      "step": 772
    },
    {
      "epoch": 1.0972320794889994,
      "grad_norm": 0.20318418741226196,
      "learning_rate": 0.0001274024738344434,
      "loss": 0.1605,
      "step": 773
    },
    {
      "epoch": 1.098651525904897,
      "grad_norm": 0.16543614864349365,
      "learning_rate": 0.00012730732635585158,
      "loss": 0.119,
      "step": 774
    },
    {
      "epoch": 1.1000709723207949,
      "grad_norm": 0.1733444780111313,
      "learning_rate": 0.00012721217887725977,
      "loss": 0.1315,
      "step": 775
    },
    {
      "epoch": 1.1014904187366927,
      "grad_norm": 0.19277739524841309,
      "learning_rate": 0.00012711703139866795,
      "loss": 0.1363,
      "step": 776
    },
    {
      "epoch": 1.1029098651525904,
      "grad_norm": 0.15131960809230804,
      "learning_rate": 0.00012702188392007613,
      "loss": 0.101,
      "step": 777
    },
    {
      "epoch": 1.1043293115684882,
      "grad_norm": 0.1636597216129303,
      "learning_rate": 0.00012692673644148432,
      "loss": 0.1102,
      "step": 778
    },
    {
      "epoch": 1.105748757984386,
      "grad_norm": 0.15759044885635376,
      "learning_rate": 0.0001268315889628925,
      "loss": 0.1049,
      "step": 779
    },
    {
      "epoch": 1.107168204400284,
      "grad_norm": 0.18001241981983185,
      "learning_rate": 0.00012673644148430068,
      "loss": 0.1056,
      "step": 780
    },
    {
      "epoch": 1.1085876508161816,
      "grad_norm": 0.20508702099323273,
      "learning_rate": 0.00012664129400570887,
      "loss": 0.1495,
      "step": 781
    },
    {
      "epoch": 1.1100070972320795,
      "grad_norm": 0.1843666434288025,
      "learning_rate": 0.00012654614652711702,
      "loss": 0.1228,
      "step": 782
    },
    {
      "epoch": 1.1114265436479773,
      "grad_norm": 0.21471849083900452,
      "learning_rate": 0.00012645099904852523,
      "loss": 0.1481,
      "step": 783
    },
    {
      "epoch": 1.1128459900638752,
      "grad_norm": 0.20250990986824036,
      "learning_rate": 0.0001263558515699334,
      "loss": 0.1289,
      "step": 784
    },
    {
      "epoch": 1.1142654364797728,
      "grad_norm": 0.15460456907749176,
      "learning_rate": 0.00012626070409134157,
      "loss": 0.0891,
      "step": 785
    },
    {
      "epoch": 1.1156848828956707,
      "grad_norm": 0.16691207885742188,
      "learning_rate": 0.00012616555661274976,
      "loss": 0.1069,
      "step": 786
    },
    {
      "epoch": 1.1171043293115686,
      "grad_norm": 0.1643488109111786,
      "learning_rate": 0.00012607040913415794,
      "loss": 0.1221,
      "step": 787
    },
    {
      "epoch": 1.1185237757274662,
      "grad_norm": 0.17483361065387726,
      "learning_rate": 0.00012597526165556612,
      "loss": 0.1084,
      "step": 788
    },
    {
      "epoch": 1.119943222143364,
      "grad_norm": 0.1809265911579132,
      "learning_rate": 0.0001258801141769743,
      "loss": 0.1008,
      "step": 789
    },
    {
      "epoch": 1.121362668559262,
      "grad_norm": 0.14745040237903595,
      "learning_rate": 0.0001257849666983825,
      "loss": 0.0913,
      "step": 790
    },
    {
      "epoch": 1.1227821149751598,
      "grad_norm": 0.17484422028064728,
      "learning_rate": 0.00012568981921979067,
      "loss": 0.1124,
      "step": 791
    },
    {
      "epoch": 1.1242015613910574,
      "grad_norm": 0.1568085104227066,
      "learning_rate": 0.00012559467174119886,
      "loss": 0.1049,
      "step": 792
    },
    {
      "epoch": 1.1256210078069553,
      "grad_norm": 0.1401190161705017,
      "learning_rate": 0.00012549952426260704,
      "loss": 0.0804,
      "step": 793
    },
    {
      "epoch": 1.1270404542228531,
      "grad_norm": 0.15491287410259247,
      "learning_rate": 0.00012540437678401522,
      "loss": 0.09,
      "step": 794
    },
    {
      "epoch": 1.1284599006387508,
      "grad_norm": 0.12146248668432236,
      "learning_rate": 0.0001253092293054234,
      "loss": 0.0736,
      "step": 795
    },
    {
      "epoch": 1.1298793470546487,
      "grad_norm": 0.13991238176822662,
      "learning_rate": 0.0001252140818268316,
      "loss": 0.092,
      "step": 796
    },
    {
      "epoch": 1.1312987934705465,
      "grad_norm": 0.13818562030792236,
      "learning_rate": 0.00012511893434823977,
      "loss": 0.0625,
      "step": 797
    },
    {
      "epoch": 1.1327182398864444,
      "grad_norm": 0.16054265201091766,
      "learning_rate": 0.00012502378686964796,
      "loss": 0.0788,
      "step": 798
    },
    {
      "epoch": 1.134137686302342,
      "grad_norm": 0.16226358711719513,
      "learning_rate": 0.00012492863939105614,
      "loss": 0.0799,
      "step": 799
    },
    {
      "epoch": 1.1355571327182399,
      "grad_norm": 0.14217671751976013,
      "learning_rate": 0.00012483349191246432,
      "loss": 0.0872,
      "step": 800
    },
    {
      "epoch": 1.1369765791341377,
      "grad_norm": 0.11626420170068741,
      "learning_rate": 0.0001247383444338725,
      "loss": 0.0631,
      "step": 801
    },
    {
      "epoch": 1.1383960255500356,
      "grad_norm": 0.13634441792964935,
      "learning_rate": 0.0001246431969552807,
      "loss": 0.0714,
      "step": 802
    },
    {
      "epoch": 1.1398154719659332,
      "grad_norm": 0.1397586613893509,
      "learning_rate": 0.00012454804947668887,
      "loss": 0.0749,
      "step": 803
    },
    {
      "epoch": 1.141234918381831,
      "grad_norm": 0.1097993329167366,
      "learning_rate": 0.00012445290199809706,
      "loss": 0.0519,
      "step": 804
    },
    {
      "epoch": 1.142654364797729,
      "grad_norm": 0.5238766074180603,
      "learning_rate": 0.00012435775451950524,
      "loss": 0.2286,
      "step": 805
    },
    {
      "epoch": 1.1440738112136266,
      "grad_norm": 4.3247270584106445,
      "learning_rate": 0.00012426260704091342,
      "loss": 0.3534,
      "step": 806
    },
    {
      "epoch": 1.1454932576295245,
      "grad_norm": 0.48436227440834045,
      "learning_rate": 0.0001241674595623216,
      "loss": 0.3187,
      "step": 807
    },
    {
      "epoch": 1.1469127040454223,
      "grad_norm": 1.8418009281158447,
      "learning_rate": 0.0001240723120837298,
      "loss": 0.3589,
      "step": 808
    },
    {
      "epoch": 1.1483321504613202,
      "grad_norm": 1.3584396839141846,
      "learning_rate": 0.00012397716460513797,
      "loss": 0.3722,
      "step": 809
    },
    {
      "epoch": 1.1497515968772178,
      "grad_norm": 3.1571452617645264,
      "learning_rate": 0.00012388201712654616,
      "loss": 0.3251,
      "step": 810
    },
    {
      "epoch": 1.1511710432931157,
      "grad_norm": 2.3034403324127197,
      "learning_rate": 0.00012378686964795434,
      "loss": 0.2542,
      "step": 811
    },
    {
      "epoch": 1.1525904897090136,
      "grad_norm": 2.939605236053467,
      "learning_rate": 0.00012369172216936252,
      "loss": 0.3349,
      "step": 812
    },
    {
      "epoch": 1.1540099361249112,
      "grad_norm": 1.1878662109375,
      "learning_rate": 0.0001235965746907707,
      "loss": 0.3345,
      "step": 813
    },
    {
      "epoch": 1.155429382540809,
      "grad_norm": 2.1705081462860107,
      "learning_rate": 0.0001235014272121789,
      "loss": 0.3751,
      "step": 814
    },
    {
      "epoch": 1.156848828956707,
      "grad_norm": 1.0016138553619385,
      "learning_rate": 0.00012340627973358707,
      "loss": 0.2843,
      "step": 815
    },
    {
      "epoch": 1.1582682753726048,
      "grad_norm": 0.37471213936805725,
      "learning_rate": 0.00012331113225499523,
      "loss": 0.2343,
      "step": 816
    },
    {
      "epoch": 1.1596877217885024,
      "grad_norm": 0.29775741696357727,
      "learning_rate": 0.00012321598477640344,
      "loss": 0.2155,
      "step": 817
    },
    {
      "epoch": 1.1611071682044003,
      "grad_norm": 0.27718648314476013,
      "learning_rate": 0.0001231208372978116,
      "loss": 0.2223,
      "step": 818
    },
    {
      "epoch": 1.1625266146202982,
      "grad_norm": 0.32277005910873413,
      "learning_rate": 0.0001230256898192198,
      "loss": 0.179,
      "step": 819
    },
    {
      "epoch": 1.1639460610361958,
      "grad_norm": 0.28431737422943115,
      "learning_rate": 0.00012293054234062796,
      "loss": 0.2003,
      "step": 820
    },
    {
      "epoch": 1.1653655074520937,
      "grad_norm": 0.302427738904953,
      "learning_rate": 0.00012283539486203617,
      "loss": 0.1774,
      "step": 821
    },
    {
      "epoch": 1.1667849538679915,
      "grad_norm": 0.2704111933708191,
      "learning_rate": 0.00012274024738344433,
      "loss": 0.1794,
      "step": 822
    },
    {
      "epoch": 1.1682044002838894,
      "grad_norm": 0.2798808217048645,
      "learning_rate": 0.00012264509990485254,
      "loss": 0.1905,
      "step": 823
    },
    {
      "epoch": 1.169623846699787,
      "grad_norm": 0.22384904325008392,
      "learning_rate": 0.0001225499524262607,
      "loss": 0.1456,
      "step": 824
    },
    {
      "epoch": 1.171043293115685,
      "grad_norm": 0.2550671398639679,
      "learning_rate": 0.0001224548049476689,
      "loss": 0.157,
      "step": 825
    },
    {
      "epoch": 1.1724627395315828,
      "grad_norm": 0.25126901268959045,
      "learning_rate": 0.00012235965746907706,
      "loss": 0.1754,
      "step": 826
    },
    {
      "epoch": 1.1738821859474804,
      "grad_norm": 0.21364066004753113,
      "learning_rate": 0.00012226450999048527,
      "loss": 0.1414,
      "step": 827
    },
    {
      "epoch": 1.1753016323633783,
      "grad_norm": 0.2681367099285126,
      "learning_rate": 0.00012216936251189343,
      "loss": 0.1462,
      "step": 828
    },
    {
      "epoch": 1.1767210787792761,
      "grad_norm": 0.19708532094955444,
      "learning_rate": 0.00012207421503330164,
      "loss": 0.1355,
      "step": 829
    },
    {
      "epoch": 1.178140525195174,
      "grad_norm": 0.19216667115688324,
      "learning_rate": 0.0001219790675547098,
      "loss": 0.132,
      "step": 830
    },
    {
      "epoch": 1.1795599716110716,
      "grad_norm": 0.19203446805477142,
      "learning_rate": 0.000121883920076118,
      "loss": 0.1141,
      "step": 831
    },
    {
      "epoch": 1.1809794180269695,
      "grad_norm": 0.21067063510417938,
      "learning_rate": 0.00012178877259752616,
      "loss": 0.1384,
      "step": 832
    },
    {
      "epoch": 1.1823988644428673,
      "grad_norm": 0.18489737808704376,
      "learning_rate": 0.00012169362511893436,
      "loss": 0.1209,
      "step": 833
    },
    {
      "epoch": 1.183818310858765,
      "grad_norm": 0.1805708408355713,
      "learning_rate": 0.00012159847764034253,
      "loss": 0.1103,
      "step": 834
    },
    {
      "epoch": 1.1852377572746629,
      "grad_norm": 0.1872464120388031,
      "learning_rate": 0.00012150333016175073,
      "loss": 0.1254,
      "step": 835
    },
    {
      "epoch": 1.1866572036905607,
      "grad_norm": 0.19910162687301636,
      "learning_rate": 0.0001214081826831589,
      "loss": 0.1246,
      "step": 836
    },
    {
      "epoch": 1.1880766501064586,
      "grad_norm": 0.1490432769060135,
      "learning_rate": 0.0001213130352045671,
      "loss": 0.1023,
      "step": 837
    },
    {
      "epoch": 1.1894960965223562,
      "grad_norm": 0.1573842316865921,
      "learning_rate": 0.00012121788772597526,
      "loss": 0.1006,
      "step": 838
    },
    {
      "epoch": 1.190915542938254,
      "grad_norm": 0.17362456023693085,
      "learning_rate": 0.00012112274024738346,
      "loss": 0.112,
      "step": 839
    },
    {
      "epoch": 1.192334989354152,
      "grad_norm": 0.15489695966243744,
      "learning_rate": 0.00012102759276879163,
      "loss": 0.101,
      "step": 840
    },
    {
      "epoch": 1.1937544357700496,
      "grad_norm": 0.198067769408226,
      "learning_rate": 0.00012093244529019983,
      "loss": 0.0953,
      "step": 841
    },
    {
      "epoch": 1.1951738821859474,
      "grad_norm": 0.18136829137802124,
      "learning_rate": 0.000120837297811608,
      "loss": 0.1047,
      "step": 842
    },
    {
      "epoch": 1.1965933286018453,
      "grad_norm": 0.14393557608127594,
      "learning_rate": 0.0001207421503330162,
      "loss": 0.0921,
      "step": 843
    },
    {
      "epoch": 1.1980127750177432,
      "grad_norm": 0.14543643593788147,
      "learning_rate": 0.00012064700285442436,
      "loss": 0.0875,
      "step": 844
    },
    {
      "epoch": 1.1994322214336408,
      "grad_norm": 0.15259906649589539,
      "learning_rate": 0.00012055185537583256,
      "loss": 0.0879,
      "step": 845
    },
    {
      "epoch": 1.2008516678495387,
      "grad_norm": 0.1388278603553772,
      "learning_rate": 0.00012045670789724073,
      "loss": 0.0761,
      "step": 846
    },
    {
      "epoch": 1.2008516678495387,
      "eval_loss": 0.18455936014652252,
      "eval_runtime": 350.5514,
      "eval_samples_per_second": 3.015,
      "eval_steps_per_second": 1.007,
      "step": 846
    },
    {
      "epoch": 1.2022711142654365,
      "grad_norm": 0.1468459814786911,
      "learning_rate": 0.00012036156041864893,
      "loss": 0.0842,
      "step": 847
    },
    {
      "epoch": 1.2036905606813342,
      "grad_norm": 0.14862684905529022,
      "learning_rate": 0.0001202664129400571,
      "loss": 0.0872,
      "step": 848
    },
    {
      "epoch": 1.205110007097232,
      "grad_norm": 0.13126423954963684,
      "learning_rate": 0.00012017126546146528,
      "loss": 0.0787,
      "step": 849
    },
    {
      "epoch": 1.20652945351313,
      "grad_norm": 0.12527677416801453,
      "learning_rate": 0.00012007611798287346,
      "loss": 0.0689,
      "step": 850
    },
    {
      "epoch": 1.2079488999290278,
      "grad_norm": 0.13559743762016296,
      "learning_rate": 0.00011998097050428163,
      "loss": 0.0718,
      "step": 851
    },
    {
      "epoch": 1.2093683463449254,
      "grad_norm": 0.14549559354782104,
      "learning_rate": 0.00011988582302568983,
      "loss": 0.0758,
      "step": 852
    },
    {
      "epoch": 1.2107877927608233,
      "grad_norm": 0.17298828065395355,
      "learning_rate": 0.000119790675547098,
      "loss": 0.0708,
      "step": 853
    },
    {
      "epoch": 1.2122072391767211,
      "grad_norm": 0.09239975363016129,
      "learning_rate": 0.00011969552806850618,
      "loss": 0.0474,
      "step": 854
    },
    {
      "epoch": 1.2136266855926188,
      "grad_norm": 0.3613687753677368,
      "learning_rate": 0.00011960038058991437,
      "loss": 0.2991,
      "step": 855
    },
    {
      "epoch": 1.2150461320085166,
      "grad_norm": 0.41135674715042114,
      "learning_rate": 0.00011950523311132255,
      "loss": 0.3662,
      "step": 856
    },
    {
      "epoch": 1.2164655784244145,
      "grad_norm": 0.414628803730011,
      "learning_rate": 0.00011941008563273073,
      "loss": 0.3604,
      "step": 857
    },
    {
      "epoch": 1.2178850248403124,
      "grad_norm": 0.7705197930335999,
      "learning_rate": 0.00011931493815413892,
      "loss": 0.315,
      "step": 858
    },
    {
      "epoch": 1.21930447125621,
      "grad_norm": 0.32966041564941406,
      "learning_rate": 0.00011921979067554709,
      "loss": 0.285,
      "step": 859
    },
    {
      "epoch": 1.2207239176721079,
      "grad_norm": 0.31775933504104614,
      "learning_rate": 0.00011912464319695528,
      "loss": 0.2339,
      "step": 860
    },
    {
      "epoch": 1.2221433640880057,
      "grad_norm": 0.3154318928718567,
      "learning_rate": 0.00011902949571836345,
      "loss": 0.2703,
      "step": 861
    },
    {
      "epoch": 1.2235628105039034,
      "grad_norm": 0.25369754433631897,
      "learning_rate": 0.00011893434823977165,
      "loss": 0.2151,
      "step": 862
    },
    {
      "epoch": 1.2249822569198012,
      "grad_norm": 0.3355623781681061,
      "learning_rate": 0.00011883920076117982,
      "loss": 0.2431,
      "step": 863
    },
    {
      "epoch": 1.226401703335699,
      "grad_norm": 0.29226014018058777,
      "learning_rate": 0.00011874405328258802,
      "loss": 0.223,
      "step": 864
    },
    {
      "epoch": 1.227821149751597,
      "grad_norm": 0.2907524108886719,
      "learning_rate": 0.00011864890580399619,
      "loss": 0.199,
      "step": 865
    },
    {
      "epoch": 1.2292405961674946,
      "grad_norm": 0.24319379031658173,
      "learning_rate": 0.00011855375832540438,
      "loss": 0.1955,
      "step": 866
    },
    {
      "epoch": 1.2306600425833925,
      "grad_norm": 0.26656320691108704,
      "learning_rate": 0.00011845861084681255,
      "loss": 0.1926,
      "step": 867
    },
    {
      "epoch": 1.2320794889992903,
      "grad_norm": 0.33448919653892517,
      "learning_rate": 0.00011836346336822075,
      "loss": 0.2131,
      "step": 868
    },
    {
      "epoch": 1.233498935415188,
      "grad_norm": 0.24090465903282166,
      "learning_rate": 0.00011826831588962892,
      "loss": 0.1623,
      "step": 869
    },
    {
      "epoch": 1.2349183818310858,
      "grad_norm": 0.21560679376125336,
      "learning_rate": 0.00011817316841103712,
      "loss": 0.1456,
      "step": 870
    },
    {
      "epoch": 1.2363378282469837,
      "grad_norm": 0.21122831106185913,
      "learning_rate": 0.00011807802093244529,
      "loss": 0.1626,
      "step": 871
    },
    {
      "epoch": 1.2377572746628815,
      "grad_norm": 0.20528544485569,
      "learning_rate": 0.00011798287345385348,
      "loss": 0.144,
      "step": 872
    },
    {
      "epoch": 1.2391767210787792,
      "grad_norm": 0.26725128293037415,
      "learning_rate": 0.00011788772597526165,
      "loss": 0.1551,
      "step": 873
    },
    {
      "epoch": 1.240596167494677,
      "grad_norm": 0.23850692808628082,
      "learning_rate": 0.00011779257849666985,
      "loss": 0.1374,
      "step": 874
    },
    {
      "epoch": 1.242015613910575,
      "grad_norm": 0.20298372209072113,
      "learning_rate": 0.00011769743101807802,
      "loss": 0.1562,
      "step": 875
    },
    {
      "epoch": 1.2434350603264726,
      "grad_norm": 0.19317595660686493,
      "learning_rate": 0.00011760228353948622,
      "loss": 0.1305,
      "step": 876
    },
    {
      "epoch": 1.2448545067423704,
      "grad_norm": 0.2484245002269745,
      "learning_rate": 0.00011750713606089439,
      "loss": 0.1654,
      "step": 877
    },
    {
      "epoch": 1.2462739531582683,
      "grad_norm": 0.20022067427635193,
      "learning_rate": 0.00011741198858230259,
      "loss": 0.1336,
      "step": 878
    },
    {
      "epoch": 1.2476933995741661,
      "grad_norm": 0.22455428540706635,
      "learning_rate": 0.00011731684110371075,
      "loss": 0.1362,
      "step": 879
    },
    {
      "epoch": 1.2491128459900638,
      "grad_norm": 0.22041738033294678,
      "learning_rate": 0.00011722169362511894,
      "loss": 0.1561,
      "step": 880
    },
    {
      "epoch": 1.2505322924059616,
      "grad_norm": 0.19736546277999878,
      "learning_rate": 0.00011712654614652712,
      "loss": 0.1307,
      "step": 881
    },
    {
      "epoch": 1.2519517388218595,
      "grad_norm": 0.30246126651763916,
      "learning_rate": 0.0001170313986679353,
      "loss": 0.1394,
      "step": 882
    },
    {
      "epoch": 1.2533711852377571,
      "grad_norm": 0.19718517363071442,
      "learning_rate": 0.00011693625118934349,
      "loss": 0.117,
      "step": 883
    },
    {
      "epoch": 1.254790631653655,
      "grad_norm": 0.15760773420333862,
      "learning_rate": 0.00011684110371075167,
      "loss": 0.1133,
      "step": 884
    },
    {
      "epoch": 1.2562100780695529,
      "grad_norm": 0.16693347692489624,
      "learning_rate": 0.00011674595623215984,
      "loss": 0.1052,
      "step": 885
    },
    {
      "epoch": 1.2576295244854507,
      "grad_norm": 0.1633010059595108,
      "learning_rate": 0.00011665080875356804,
      "loss": 0.0975,
      "step": 886
    },
    {
      "epoch": 1.2590489709013486,
      "grad_norm": 0.19909369945526123,
      "learning_rate": 0.00011655566127497621,
      "loss": 0.0996,
      "step": 887
    },
    {
      "epoch": 1.2604684173172462,
      "grad_norm": 0.15285266935825348,
      "learning_rate": 0.0001164605137963844,
      "loss": 0.1074,
      "step": 888
    },
    {
      "epoch": 1.261887863733144,
      "grad_norm": 0.1329864263534546,
      "learning_rate": 0.00011636536631779257,
      "loss": 0.0821,
      "step": 889
    },
    {
      "epoch": 1.2633073101490417,
      "grad_norm": 0.16709905862808228,
      "learning_rate": 0.00011627021883920077,
      "loss": 0.0971,
      "step": 890
    },
    {
      "epoch": 1.2647267565649396,
      "grad_norm": 0.14971426129341125,
      "learning_rate": 0.00011617507136060894,
      "loss": 0.0882,
      "step": 891
    },
    {
      "epoch": 1.2661462029808375,
      "grad_norm": 0.14370709657669067,
      "learning_rate": 0.00011607992388201714,
      "loss": 0.0927,
      "step": 892
    },
    {
      "epoch": 1.2675656493967353,
      "grad_norm": 0.15775443613529205,
      "learning_rate": 0.00011598477640342531,
      "loss": 0.0914,
      "step": 893
    },
    {
      "epoch": 1.2689850958126332,
      "grad_norm": 0.16949594020843506,
      "learning_rate": 0.0001158896289248335,
      "loss": 0.0863,
      "step": 894
    },
    {
      "epoch": 1.2704045422285308,
      "grad_norm": 0.13893845677375793,
      "learning_rate": 0.00011579448144624168,
      "loss": 0.0671,
      "step": 895
    },
    {
      "epoch": 1.2718239886444287,
      "grad_norm": 0.12404394149780273,
      "learning_rate": 0.00011569933396764987,
      "loss": 0.0703,
      "step": 896
    },
    {
      "epoch": 1.2732434350603263,
      "grad_norm": 0.14517563581466675,
      "learning_rate": 0.00011560418648905804,
      "loss": 0.0767,
      "step": 897
    },
    {
      "epoch": 1.2746628814762242,
      "grad_norm": 0.12821710109710693,
      "learning_rate": 0.00011550903901046624,
      "loss": 0.08,
      "step": 898
    },
    {
      "epoch": 1.276082327892122,
      "grad_norm": 0.11503323912620544,
      "learning_rate": 0.00011541389153187441,
      "loss": 0.0609,
      "step": 899
    },
    {
      "epoch": 1.27750177430802,
      "grad_norm": 0.20002481341362,
      "learning_rate": 0.0001153187440532826,
      "loss": 0.0826,
      "step": 900
    },
    {
      "epoch": 1.2789212207239178,
      "grad_norm": 0.13138751685619354,
      "learning_rate": 0.00011522359657469078,
      "loss": 0.0779,
      "step": 901
    },
    {
      "epoch": 1.2803406671398154,
      "grad_norm": 0.15923921763896942,
      "learning_rate": 0.00011512844909609897,
      "loss": 0.0719,
      "step": 902
    },
    {
      "epoch": 1.2817601135557133,
      "grad_norm": 0.12470070272684097,
      "learning_rate": 0.00011503330161750714,
      "loss": 0.0613,
      "step": 903
    },
    {
      "epoch": 1.2831795599716112,
      "grad_norm": 0.09330444782972336,
      "learning_rate": 0.00011493815413891534,
      "loss": 0.047,
      "step": 904
    },
    {
      "epoch": 1.2845990063875088,
      "grad_norm": 0.41469815373420715,
      "learning_rate": 0.00011484300666032351,
      "loss": 0.3099,
      "step": 905
    },
    {
      "epoch": 1.2860184528034067,
      "grad_norm": 0.5250797867774963,
      "learning_rate": 0.00011474785918173169,
      "loss": 0.4028,
      "step": 906
    },
    {
      "epoch": 1.2874378992193045,
      "grad_norm": 0.43176230788230896,
      "learning_rate": 0.00011465271170313988,
      "loss": 0.3241,
      "step": 907
    },
    {
      "epoch": 1.2888573456352024,
      "grad_norm": 0.36824601888656616,
      "learning_rate": 0.00011455756422454806,
      "loss": 0.2948,
      "step": 908
    },
    {
      "epoch": 1.2902767920511,
      "grad_norm": 0.3636336028575897,
      "learning_rate": 0.00011446241674595624,
      "loss": 0.2767,
      "step": 909
    },
    {
      "epoch": 1.2916962384669979,
      "grad_norm": 0.3332219123840332,
      "learning_rate": 0.00011436726926736443,
      "loss": 0.2558,
      "step": 910
    },
    {
      "epoch": 1.2931156848828957,
      "grad_norm": 0.37777385115623474,
      "learning_rate": 0.0001142721217887726,
      "loss": 0.2921,
      "step": 911
    },
    {
      "epoch": 1.2945351312987934,
      "grad_norm": 0.2618837058544159,
      "learning_rate": 0.00011417697431018079,
      "loss": 0.2338,
      "step": 912
    },
    {
      "epoch": 1.2959545777146912,
      "grad_norm": 0.31428951025009155,
      "learning_rate": 0.00011408182683158896,
      "loss": 0.276,
      "step": 913
    },
    {
      "epoch": 1.297374024130589,
      "grad_norm": 0.3028257191181183,
      "learning_rate": 0.00011398667935299716,
      "loss": 0.221,
      "step": 914
    },
    {
      "epoch": 1.298793470546487,
      "grad_norm": 0.2830982208251953,
      "learning_rate": 0.00011389153187440533,
      "loss": 0.2374,
      "step": 915
    },
    {
      "epoch": 1.3002129169623846,
      "grad_norm": 0.2544098198413849,
      "learning_rate": 0.00011379638439581353,
      "loss": 0.2136,
      "step": 916
    },
    {
      "epoch": 1.3016323633782825,
      "grad_norm": 0.2352762669324875,
      "learning_rate": 0.0001137012369172217,
      "loss": 0.199,
      "step": 917
    },
    {
      "epoch": 1.3030518097941803,
      "grad_norm": 0.27317607402801514,
      "learning_rate": 0.00011360608943862989,
      "loss": 0.225,
      "step": 918
    },
    {
      "epoch": 1.304471256210078,
      "grad_norm": 0.22307567298412323,
      "learning_rate": 0.00011351094196003806,
      "loss": 0.2079,
      "step": 919
    },
    {
      "epoch": 1.3058907026259758,
      "grad_norm": 0.20242930948734283,
      "learning_rate": 0.00011341579448144626,
      "loss": 0.1508,
      "step": 920
    },
    {
      "epoch": 1.3073101490418737,
      "grad_norm": 0.24381127953529358,
      "learning_rate": 0.00011332064700285443,
      "loss": 0.1564,
      "step": 921
    },
    {
      "epoch": 1.3087295954577716,
      "grad_norm": 0.2172638177871704,
      "learning_rate": 0.0001132254995242626,
      "loss": 0.1559,
      "step": 922
    },
    {
      "epoch": 1.3101490418736692,
      "grad_norm": 0.24027672410011292,
      "learning_rate": 0.0001131303520456708,
      "loss": 0.1673,
      "step": 923
    },
    {
      "epoch": 1.311568488289567,
      "grad_norm": 0.22242408990859985,
      "learning_rate": 0.00011303520456707897,
      "loss": 0.1452,
      "step": 924
    },
    {
      "epoch": 1.312987934705465,
      "grad_norm": 0.18609297275543213,
      "learning_rate": 0.00011294005708848716,
      "loss": 0.1344,
      "step": 925
    },
    {
      "epoch": 1.3144073811213626,
      "grad_norm": 0.20028699934482574,
      "learning_rate": 0.00011284490960989533,
      "loss": 0.1419,
      "step": 926
    },
    {
      "epoch": 1.3158268275372604,
      "grad_norm": 0.2508096992969513,
      "learning_rate": 0.00011274976213130353,
      "loss": 0.1573,
      "step": 927
    },
    {
      "epoch": 1.3172462739531583,
      "grad_norm": 0.20852036774158478,
      "learning_rate": 0.0001126546146527117,
      "loss": 0.1254,
      "step": 928
    },
    {
      "epoch": 1.3186657203690562,
      "grad_norm": 0.18415208160877228,
      "learning_rate": 0.0001125594671741199,
      "loss": 0.1218,
      "step": 929
    },
    {
      "epoch": 1.3200851667849538,
      "grad_norm": 0.2075454443693161,
      "learning_rate": 0.00011246431969552807,
      "loss": 0.1186,
      "step": 930
    },
    {
      "epoch": 1.3215046132008517,
      "grad_norm": 0.1952759474515915,
      "learning_rate": 0.00011236917221693626,
      "loss": 0.1249,
      "step": 931
    },
    {
      "epoch": 1.3229240596167495,
      "grad_norm": 0.18214453756809235,
      "learning_rate": 0.00011227402473834443,
      "loss": 0.1086,
      "step": 932
    },
    {
      "epoch": 1.3243435060326472,
      "grad_norm": 0.3730047643184662,
      "learning_rate": 0.00011217887725975263,
      "loss": 0.161,
      "step": 933
    },
    {
      "epoch": 1.325762952448545,
      "grad_norm": 0.18995772302150726,
      "learning_rate": 0.0001120837297811608,
      "loss": 0.1267,
      "step": 934
    },
    {
      "epoch": 1.327182398864443,
      "grad_norm": 0.16369737684726715,
      "learning_rate": 0.000111988582302569,
      "loss": 0.103,
      "step": 935
    },
    {
      "epoch": 1.3286018452803408,
      "grad_norm": 0.1697535663843155,
      "learning_rate": 0.00011189343482397717,
      "loss": 0.1065,
      "step": 936
    },
    {
      "epoch": 1.3300212916962384,
      "grad_norm": 0.17384809255599976,
      "learning_rate": 0.00011179828734538535,
      "loss": 0.0987,
      "step": 937
    },
    {
      "epoch": 1.3314407381121363,
      "grad_norm": 0.17038893699645996,
      "learning_rate": 0.00011170313986679353,
      "loss": 0.1065,
      "step": 938
    },
    {
      "epoch": 1.3328601845280341,
      "grad_norm": 0.16880187392234802,
      "learning_rate": 0.00011160799238820172,
      "loss": 0.0989,
      "step": 939
    },
    {
      "epoch": 1.3342796309439318,
      "grad_norm": 0.1523292064666748,
      "learning_rate": 0.0001115128449096099,
      "loss": 0.093,
      "step": 940
    },
    {
      "epoch": 1.3356990773598296,
      "grad_norm": 0.16526508331298828,
      "learning_rate": 0.00011141769743101808,
      "loss": 0.0995,
      "step": 941
    },
    {
      "epoch": 1.3371185237757275,
      "grad_norm": 0.14615917205810547,
      "learning_rate": 0.00011132254995242625,
      "loss": 0.084,
      "step": 942
    },
    {
      "epoch": 1.3385379701916253,
      "grad_norm": 0.16288377344608307,
      "learning_rate": 0.00011122740247383445,
      "loss": 0.1018,
      "step": 943
    },
    {
      "epoch": 1.339957416607523,
      "grad_norm": 0.11590070277452469,
      "learning_rate": 0.00011113225499524262,
      "loss": 0.0675,
      "step": 944
    },
    {
      "epoch": 1.3413768630234209,
      "grad_norm": 0.14819401502609253,
      "learning_rate": 0.00011103710751665082,
      "loss": 0.0888,
      "step": 945
    },
    {
      "epoch": 1.3427963094393187,
      "grad_norm": 0.15838517248630524,
      "learning_rate": 0.00011094196003805899,
      "loss": 0.0911,
      "step": 946
    },
    {
      "epoch": 1.3442157558552164,
      "grad_norm": 0.12764696776866913,
      "learning_rate": 0.00011084681255946718,
      "loss": 0.0687,
      "step": 947
    },
    {
      "epoch": 1.3456352022711142,
      "grad_norm": 0.14489686489105225,
      "learning_rate": 0.00011075166508087535,
      "loss": 0.0789,
      "step": 948
    },
    {
      "epoch": 1.347054648687012,
      "grad_norm": 0.13333258032798767,
      "learning_rate": 0.00011065651760228355,
      "loss": 0.0777,
      "step": 949
    },
    {
      "epoch": 1.34847409510291,
      "grad_norm": 0.11465080082416534,
      "learning_rate": 0.00011056137012369172,
      "loss": 0.0659,
      "step": 950
    },
    {
      "epoch": 1.3498935415188076,
      "grad_norm": 0.16852301359176636,
      "learning_rate": 0.00011046622264509992,
      "loss": 0.073,
      "step": 951
    },
    {
      "epoch": 1.3513129879347054,
      "grad_norm": 0.1426130086183548,
      "learning_rate": 0.00011037107516650809,
      "loss": 0.0715,
      "step": 952
    },
    {
      "epoch": 1.3527324343506033,
      "grad_norm": 0.1167796328663826,
      "learning_rate": 0.00011027592768791628,
      "loss": 0.0661,
      "step": 953
    },
    {
      "epoch": 1.354151880766501,
      "grad_norm": 0.09697416424751282,
      "learning_rate": 0.00011018078020932445,
      "loss": 0.0565,
      "step": 954
    },
    {
      "epoch": 1.3555713271823988,
      "grad_norm": 0.3888132870197296,
      "learning_rate": 0.00011008563273073265,
      "loss": 0.2377,
      "step": 955
    },
    {
      "epoch": 1.3569907735982967,
      "grad_norm": 0.5223394632339478,
      "learning_rate": 0.00010999048525214082,
      "loss": 0.3662,
      "step": 956
    },
    {
      "epoch": 1.3584102200141945,
      "grad_norm": 0.4344301223754883,
      "learning_rate": 0.00010989533777354902,
      "loss": 0.3116,
      "step": 957
    },
    {
      "epoch": 1.3598296664300924,
      "grad_norm": 0.4458158612251282,
      "learning_rate": 0.00010980019029495719,
      "loss": 0.2812,
      "step": 958
    },
    {
      "epoch": 1.36124911284599,
      "grad_norm": 0.43694454431533813,
      "learning_rate": 0.00010970504281636538,
      "loss": 0.2976,
      "step": 959
    },
    {
      "epoch": 1.362668559261888,
      "grad_norm": 0.36991703510284424,
      "learning_rate": 0.00010960989533777355,
      "loss": 0.3088,
      "step": 960
    },
    {
      "epoch": 1.3640880056777855,
      "grad_norm": 0.28801313042640686,
      "learning_rate": 0.00010951474785918175,
      "loss": 0.2514,
      "step": 961
    },
    {
      "epoch": 1.3655074520936834,
      "grad_norm": 0.3378835916519165,
      "learning_rate": 0.00010941960038058992,
      "loss": 0.277,
      "step": 962
    },
    {
      "epoch": 1.3669268985095813,
      "grad_norm": 0.3637514114379883,
      "learning_rate": 0.0001093244529019981,
      "loss": 0.2407,
      "step": 963
    },
    {
      "epoch": 1.3683463449254791,
      "grad_norm": 0.29766184091567993,
      "learning_rate": 0.00010922930542340629,
      "loss": 0.2391,
      "step": 964
    },
    {
      "epoch": 1.369765791341377,
      "grad_norm": 0.2889477610588074,
      "learning_rate": 0.00010913415794481447,
      "loss": 0.2171,
      "step": 965
    },
    {
      "epoch": 1.3711852377572746,
      "grad_norm": 0.3491814136505127,
      "learning_rate": 0.00010903901046622264,
      "loss": 0.2042,
      "step": 966
    },
    {
      "epoch": 1.3726046841731725,
      "grad_norm": 0.26398664712905884,
      "learning_rate": 0.00010894386298763084,
      "loss": 0.1969,
      "step": 967
    },
    {
      "epoch": 1.3740241305890701,
      "grad_norm": 0.2262667566537857,
      "learning_rate": 0.000108848715509039,
      "loss": 0.1871,
      "step": 968
    },
    {
      "epoch": 1.375443577004968,
      "grad_norm": 0.26554760336875916,
      "learning_rate": 0.0001087535680304472,
      "loss": 0.199,
      "step": 969
    },
    {
      "epoch": 1.3768630234208659,
      "grad_norm": 0.28888365626335144,
      "learning_rate": 0.00010865842055185537,
      "loss": 0.1879,
      "step": 970
    },
    {
      "epoch": 1.3782824698367637,
      "grad_norm": 0.25305262207984924,
      "learning_rate": 0.00010856327307326357,
      "loss": 0.1964,
      "step": 971
    },
    {
      "epoch": 1.3797019162526616,
      "grad_norm": 0.21910499036312103,
      "learning_rate": 0.00010846812559467174,
      "loss": 0.1632,
      "step": 972
    },
    {
      "epoch": 1.3811213626685592,
      "grad_norm": 0.27938950061798096,
      "learning_rate": 0.00010837297811607994,
      "loss": 0.1908,
      "step": 973
    },
    {
      "epoch": 1.382540809084457,
      "grad_norm": 0.2013658583164215,
      "learning_rate": 0.0001082778306374881,
      "loss": 0.1546,
      "step": 974
    },
    {
      "epoch": 1.3839602555003547,
      "grad_norm": 0.2206653654575348,
      "learning_rate": 0.0001081826831588963,
      "loss": 0.1439,
      "step": 975
    },
    {
      "epoch": 1.3853797019162526,
      "grad_norm": 0.20507606863975525,
      "learning_rate": 0.00010808753568030447,
      "loss": 0.1412,
      "step": 976
    },
    {
      "epoch": 1.3867991483321505,
      "grad_norm": 0.18784716725349426,
      "learning_rate": 0.00010799238820171267,
      "loss": 0.1515,
      "step": 977
    },
    {
      "epoch": 1.3882185947480483,
      "grad_norm": 0.22592055797576904,
      "learning_rate": 0.00010789724072312084,
      "loss": 0.1531,
      "step": 978
    },
    {
      "epoch": 1.3896380411639462,
      "grad_norm": 0.2258600890636444,
      "learning_rate": 0.00010780209324452904,
      "loss": 0.1415,
      "step": 979
    },
    {
      "epoch": 1.3910574875798438,
      "grad_norm": 0.17608104646205902,
      "learning_rate": 0.0001077069457659372,
      "loss": 0.1362,
      "step": 980
    },
    {
      "epoch": 1.3924769339957417,
      "grad_norm": 0.19677606225013733,
      "learning_rate": 0.0001076117982873454,
      "loss": 0.1097,
      "step": 981
    },
    {
      "epoch": 1.3938963804116393,
      "grad_norm": 0.19371064007282257,
      "learning_rate": 0.00010751665080875357,
      "loss": 0.1152,
      "step": 982
    },
    {
      "epoch": 1.3953158268275372,
      "grad_norm": 0.2072717845439911,
      "learning_rate": 0.00010742150333016177,
      "loss": 0.151,
      "step": 983
    },
    {
      "epoch": 1.396735273243435,
      "grad_norm": 0.16814540326595306,
      "learning_rate": 0.00010732635585156994,
      "loss": 0.1152,
      "step": 984
    },
    {
      "epoch": 1.398154719659333,
      "grad_norm": 0.1665930449962616,
      "learning_rate": 0.00010723120837297814,
      "loss": 0.102,
      "step": 985
    },
    {
      "epoch": 1.3995741660752308,
      "grad_norm": 0.18988312780857086,
      "learning_rate": 0.00010713606089438631,
      "loss": 0.1158,
      "step": 986
    },
    {
      "epoch": 1.4009936124911284,
      "grad_norm": 0.17646914720535278,
      "learning_rate": 0.0001070409134157945,
      "loss": 0.1093,
      "step": 987
    },
    {
      "epoch": 1.4024130589070263,
      "grad_norm": 0.19804489612579346,
      "learning_rate": 0.00010694576593720267,
      "loss": 0.1181,
      "step": 988
    },
    {
      "epoch": 1.4038325053229241,
      "grad_norm": 0.20103871822357178,
      "learning_rate": 0.00010685061845861086,
      "loss": 0.1283,
      "step": 989
    },
    {
      "epoch": 1.4052519517388218,
      "grad_norm": 0.15522681176662445,
      "learning_rate": 0.00010675547098001904,
      "loss": 0.0956,
      "step": 990
    },
    {
      "epoch": 1.4066713981547196,
      "grad_norm": 0.17475688457489014,
      "learning_rate": 0.00010666032350142721,
      "loss": 0.0893,
      "step": 991
    },
    {
      "epoch": 1.4080908445706175,
      "grad_norm": 0.17609241604804993,
      "learning_rate": 0.0001065651760228354,
      "loss": 0.1003,
      "step": 992
    },
    {
      "epoch": 1.4095102909865154,
      "grad_norm": 0.14097237586975098,
      "learning_rate": 0.00010647002854424358,
      "loss": 0.0727,
      "step": 993
    },
    {
      "epoch": 1.410929737402413,
      "grad_norm": 0.16441726684570312,
      "learning_rate": 0.00010637488106565176,
      "loss": 0.0683,
      "step": 994
    },
    {
      "epoch": 1.4123491838183109,
      "grad_norm": 0.1344594657421112,
      "learning_rate": 0.00010627973358705994,
      "loss": 0.077,
      "step": 995
    },
    {
      "epoch": 1.4137686302342087,
      "grad_norm": 0.12010911107063293,
      "learning_rate": 0.00010618458610846813,
      "loss": 0.0771,
      "step": 996
    },
    {
      "epoch": 1.4151880766501064,
      "grad_norm": 0.12441962212324142,
      "learning_rate": 0.0001060894386298763,
      "loss": 0.0749,
      "step": 997
    },
    {
      "epoch": 1.4166075230660042,
      "grad_norm": 0.13136529922485352,
      "learning_rate": 0.0001059942911512845,
      "loss": 0.0762,
      "step": 998
    },
    {
      "epoch": 1.418026969481902,
      "grad_norm": 0.1276669204235077,
      "learning_rate": 0.00010589914367269266,
      "loss": 0.0713,
      "step": 999
    },
    {
      "epoch": 1.4194464158978,
      "grad_norm": 0.1353890597820282,
      "learning_rate": 0.00010580399619410086,
      "loss": 0.0798,
      "step": 1000
    }
  ],
  "logging_steps": 1,
  "max_steps": 2112,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 6.909018480719462e+16,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
