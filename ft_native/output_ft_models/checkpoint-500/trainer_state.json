{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.7097232079489,
  "eval_steps": 423,
  "global_step": 500,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.0014194464158978,
      "grad_norm": 3.8753199577331543,
      "learning_rate": 2e-05,
      "loss": 2.6848,
      "step": 1
    },
    {
      "epoch": 0.0028388928317956,
      "grad_norm": 4.06846809387207,
      "learning_rate": 4e-05,
      "loss": 2.6312,
      "step": 2
    },
    {
      "epoch": 0.0042583392476933995,
      "grad_norm": 3.860471248626709,
      "learning_rate": 6e-05,
      "loss": 2.6378,
      "step": 3
    },
    {
      "epoch": 0.0056777856635912,
      "grad_norm": 2.591989517211914,
      "learning_rate": 8e-05,
      "loss": 2.4154,
      "step": 4
    },
    {
      "epoch": 0.007097232079488999,
      "grad_norm": 2.2637760639190674,
      "learning_rate": 0.0001,
      "loss": 2.2543,
      "step": 5
    },
    {
      "epoch": 0.008516678495386799,
      "grad_norm": 1.7446980476379395,
      "learning_rate": 0.00012,
      "loss": 2.0235,
      "step": 6
    },
    {
      "epoch": 0.0099361249112846,
      "grad_norm": 1.61598801612854,
      "learning_rate": 0.00014,
      "loss": 1.8347,
      "step": 7
    },
    {
      "epoch": 0.0113555713271824,
      "grad_norm": 1.692824363708496,
      "learning_rate": 0.00016,
      "loss": 1.5201,
      "step": 8
    },
    {
      "epoch": 0.0127750177430802,
      "grad_norm": 1.7318449020385742,
      "learning_rate": 0.00018,
      "loss": 1.1865,
      "step": 9
    },
    {
      "epoch": 0.014194464158977998,
      "grad_norm": 2.3941969871520996,
      "learning_rate": 0.0002,
      "loss": 0.8478,
      "step": 10
    },
    {
      "epoch": 0.015613910574875798,
      "grad_norm": 2.488072395324707,
      "learning_rate": 0.0001999048525214082,
      "loss": 0.5818,
      "step": 11
    },
    {
      "epoch": 0.017033356990773598,
      "grad_norm": 5.200738906860352,
      "learning_rate": 0.00019980970504281638,
      "loss": 0.4402,
      "step": 12
    },
    {
      "epoch": 0.018452803406671398,
      "grad_norm": 1.2015495300292969,
      "learning_rate": 0.00019971455756422456,
      "loss": 0.363,
      "step": 13
    },
    {
      "epoch": 0.0198722498225692,
      "grad_norm": 0.5281029939651489,
      "learning_rate": 0.00019961941008563274,
      "loss": 0.329,
      "step": 14
    },
    {
      "epoch": 0.021291696238467,
      "grad_norm": 0.5371572375297546,
      "learning_rate": 0.0001995242626070409,
      "loss": 0.2941,
      "step": 15
    },
    {
      "epoch": 0.0227111426543648,
      "grad_norm": 0.6878210306167603,
      "learning_rate": 0.0001994291151284491,
      "loss": 0.3151,
      "step": 16
    },
    {
      "epoch": 0.0241305890702626,
      "grad_norm": 0.7888450026512146,
      "learning_rate": 0.00019933396764985727,
      "loss": 0.2811,
      "step": 17
    },
    {
      "epoch": 0.0255500354861604,
      "grad_norm": 0.9834377765655518,
      "learning_rate": 0.00019923882017126548,
      "loss": 0.2909,
      "step": 18
    },
    {
      "epoch": 0.0269694819020582,
      "grad_norm": 1.111398696899414,
      "learning_rate": 0.00019914367269267363,
      "loss": 0.2877,
      "step": 19
    },
    {
      "epoch": 0.028388928317955996,
      "grad_norm": 1.3554736375808716,
      "learning_rate": 0.00019904852521408184,
      "loss": 0.2168,
      "step": 20
    },
    {
      "epoch": 0.029808374733853796,
      "grad_norm": 2.863246202468872,
      "learning_rate": 0.00019895337773549,
      "loss": 0.2247,
      "step": 21
    },
    {
      "epoch": 0.031227821149751596,
      "grad_norm": 0.4769139587879181,
      "learning_rate": 0.0001988582302568982,
      "loss": 0.2451,
      "step": 22
    },
    {
      "epoch": 0.032647267565649396,
      "grad_norm": 0.3291550576686859,
      "learning_rate": 0.00019876308277830637,
      "loss": 0.1798,
      "step": 23
    },
    {
      "epoch": 0.034066713981547196,
      "grad_norm": 0.3495163321495056,
      "learning_rate": 0.00019866793529971458,
      "loss": 0.1882,
      "step": 24
    },
    {
      "epoch": 0.035486160397444996,
      "grad_norm": 0.4018864035606384,
      "learning_rate": 0.00019857278782112273,
      "loss": 0.1911,
      "step": 25
    },
    {
      "epoch": 0.036905606813342796,
      "grad_norm": 0.5376371145248413,
      "learning_rate": 0.00019847764034253094,
      "loss": 0.156,
      "step": 26
    },
    {
      "epoch": 0.0383250532292406,
      "grad_norm": 0.8540717959403992,
      "learning_rate": 0.0001983824928639391,
      "loss": 0.1568,
      "step": 27
    },
    {
      "epoch": 0.0397444996451384,
      "grad_norm": 0.9595521092414856,
      "learning_rate": 0.0001982873453853473,
      "loss": 0.1599,
      "step": 28
    },
    {
      "epoch": 0.0411639460610362,
      "grad_norm": 0.7133545875549316,
      "learning_rate": 0.00019819219790675547,
      "loss": 0.1678,
      "step": 29
    },
    {
      "epoch": 0.042583392476934,
      "grad_norm": 0.43954452872276306,
      "learning_rate": 0.00019809705042816368,
      "loss": 0.1581,
      "step": 30
    },
    {
      "epoch": 0.0440028388928318,
      "grad_norm": 0.51581871509552,
      "learning_rate": 0.00019800190294957183,
      "loss": 0.1458,
      "step": 31
    },
    {
      "epoch": 0.0454222853087296,
      "grad_norm": 0.5623205900192261,
      "learning_rate": 0.00019790675547098004,
      "loss": 0.1473,
      "step": 32
    },
    {
      "epoch": 0.0468417317246274,
      "grad_norm": 0.6040207147598267,
      "learning_rate": 0.0001978116079923882,
      "loss": 0.1368,
      "step": 33
    },
    {
      "epoch": 0.0482611781405252,
      "grad_norm": 0.556030809879303,
      "learning_rate": 0.0001977164605137964,
      "loss": 0.1352,
      "step": 34
    },
    {
      "epoch": 0.049680624556423,
      "grad_norm": 0.49861204624176025,
      "learning_rate": 0.00019762131303520457,
      "loss": 0.1348,
      "step": 35
    },
    {
      "epoch": 0.0511000709723208,
      "grad_norm": 0.27800312638282776,
      "learning_rate": 0.00019752616555661275,
      "loss": 0.109,
      "step": 36
    },
    {
      "epoch": 0.0525195173882186,
      "grad_norm": 0.2570524215698242,
      "learning_rate": 0.00019743101807802093,
      "loss": 0.1141,
      "step": 37
    },
    {
      "epoch": 0.0539389638041164,
      "grad_norm": 0.3502003848552704,
      "learning_rate": 0.00019733587059942912,
      "loss": 0.1413,
      "step": 38
    },
    {
      "epoch": 0.05535841022001419,
      "grad_norm": 0.3913830518722534,
      "learning_rate": 0.0001972407231208373,
      "loss": 0.0961,
      "step": 39
    },
    {
      "epoch": 0.05677785663591199,
      "grad_norm": 0.4328113794326782,
      "learning_rate": 0.00019714557564224548,
      "loss": 0.097,
      "step": 40
    },
    {
      "epoch": 0.05819730305180979,
      "grad_norm": 0.38531455397605896,
      "learning_rate": 0.00019705042816365367,
      "loss": 0.1106,
      "step": 41
    },
    {
      "epoch": 0.05961674946770759,
      "grad_norm": 0.35614219307899475,
      "learning_rate": 0.00019695528068506185,
      "loss": 0.0956,
      "step": 42
    },
    {
      "epoch": 0.06103619588360539,
      "grad_norm": 0.26061660051345825,
      "learning_rate": 0.00019686013320647003,
      "loss": 0.0981,
      "step": 43
    },
    {
      "epoch": 0.06245564229950319,
      "grad_norm": 0.16854628920555115,
      "learning_rate": 0.00019676498572787822,
      "loss": 0.0877,
      "step": 44
    },
    {
      "epoch": 0.063875088715401,
      "grad_norm": 0.359539657831192,
      "learning_rate": 0.0001966698382492864,
      "loss": 0.0816,
      "step": 45
    },
    {
      "epoch": 0.06529453513129879,
      "grad_norm": 0.2330690622329712,
      "learning_rate": 0.00019657469077069458,
      "loss": 0.101,
      "step": 46
    },
    {
      "epoch": 0.0667139815471966,
      "grad_norm": 0.25437015295028687,
      "learning_rate": 0.00019647954329210277,
      "loss": 0.1031,
      "step": 47
    },
    {
      "epoch": 0.06813342796309439,
      "grad_norm": 0.2970932722091675,
      "learning_rate": 0.00019638439581351095,
      "loss": 0.0731,
      "step": 48
    },
    {
      "epoch": 0.0695528743789922,
      "grad_norm": 0.2961924374103546,
      "learning_rate": 0.00019628924833491913,
      "loss": 0.0669,
      "step": 49
    },
    {
      "epoch": 0.07097232079488999,
      "grad_norm": 0.288361519575119,
      "learning_rate": 0.00019619410085632732,
      "loss": 0.0664,
      "step": 50
    },
    {
      "epoch": 0.0723917672107878,
      "grad_norm": 0.8922231197357178,
      "learning_rate": 0.0001960989533777355,
      "loss": 0.5873,
      "step": 51
    },
    {
      "epoch": 0.07381121362668559,
      "grad_norm": 0.5401262044906616,
      "learning_rate": 0.00019600380589914368,
      "loss": 0.447,
      "step": 52
    },
    {
      "epoch": 0.07523066004258339,
      "grad_norm": 1.9218655824661255,
      "learning_rate": 0.00019590865842055187,
      "loss": 0.3904,
      "step": 53
    },
    {
      "epoch": 0.0766501064584812,
      "grad_norm": 0.5285252928733826,
      "learning_rate": 0.00019581351094196005,
      "loss": 0.3745,
      "step": 54
    },
    {
      "epoch": 0.07806955287437899,
      "grad_norm": 0.31439849734306335,
      "learning_rate": 0.00019571836346336823,
      "loss": 0.3307,
      "step": 55
    },
    {
      "epoch": 0.0794889992902768,
      "grad_norm": 0.4290097951889038,
      "learning_rate": 0.00019562321598477642,
      "loss": 0.3252,
      "step": 56
    },
    {
      "epoch": 0.08090844570617459,
      "grad_norm": 0.3310398459434509,
      "learning_rate": 0.0001955280685061846,
      "loss": 0.3135,
      "step": 57
    },
    {
      "epoch": 0.0823278921220724,
      "grad_norm": 0.32844847440719604,
      "learning_rate": 0.00019543292102759278,
      "loss": 0.2843,
      "step": 58
    },
    {
      "epoch": 0.08374733853797019,
      "grad_norm": 0.3329128324985504,
      "learning_rate": 0.00019533777354900097,
      "loss": 0.271,
      "step": 59
    },
    {
      "epoch": 0.085166784953868,
      "grad_norm": 0.2432798445224762,
      "learning_rate": 0.00019524262607040915,
      "loss": 0.2519,
      "step": 60
    },
    {
      "epoch": 0.08658623136976579,
      "grad_norm": 0.2554329037666321,
      "learning_rate": 0.00019514747859181733,
      "loss": 0.2459,
      "step": 61
    },
    {
      "epoch": 0.0880056777856636,
      "grad_norm": 0.2721186876296997,
      "learning_rate": 0.00019505233111322552,
      "loss": 0.2464,
      "step": 62
    },
    {
      "epoch": 0.08942512420156139,
      "grad_norm": 0.22954344749450684,
      "learning_rate": 0.0001949571836346337,
      "loss": 0.2196,
      "step": 63
    },
    {
      "epoch": 0.0908445706174592,
      "grad_norm": 0.26764383912086487,
      "learning_rate": 0.00019486203615604188,
      "loss": 0.228,
      "step": 64
    },
    {
      "epoch": 0.09226401703335699,
      "grad_norm": 0.22909002006053925,
      "learning_rate": 0.00019476688867745007,
      "loss": 0.2039,
      "step": 65
    },
    {
      "epoch": 0.0936834634492548,
      "grad_norm": 0.2602919340133667,
      "learning_rate": 0.00019467174119885825,
      "loss": 0.2007,
      "step": 66
    },
    {
      "epoch": 0.09510290986515259,
      "grad_norm": 0.22124329209327698,
      "learning_rate": 0.0001945765937202664,
      "loss": 0.1886,
      "step": 67
    },
    {
      "epoch": 0.0965223562810504,
      "grad_norm": 0.21159298717975616,
      "learning_rate": 0.00019448144624167462,
      "loss": 0.2003,
      "step": 68
    },
    {
      "epoch": 0.09794180269694819,
      "grad_norm": 0.2761073112487793,
      "learning_rate": 0.00019438629876308277,
      "loss": 0.2093,
      "step": 69
    },
    {
      "epoch": 0.099361249112846,
      "grad_norm": 0.20292288064956665,
      "learning_rate": 0.00019429115128449098,
      "loss": 0.1712,
      "step": 70
    },
    {
      "epoch": 0.10078069552874379,
      "grad_norm": 0.19448138773441315,
      "learning_rate": 0.00019419600380589914,
      "loss": 0.1434,
      "step": 71
    },
    {
      "epoch": 0.1022001419446416,
      "grad_norm": 0.20379109680652618,
      "learning_rate": 0.00019410085632730735,
      "loss": 0.1644,
      "step": 72
    },
    {
      "epoch": 0.10361958836053939,
      "grad_norm": 0.223442941904068,
      "learning_rate": 0.0001940057088487155,
      "loss": 0.1879,
      "step": 73
    },
    {
      "epoch": 0.1050390347764372,
      "grad_norm": 0.1932620406150818,
      "learning_rate": 0.00019391056137012372,
      "loss": 0.1413,
      "step": 74
    },
    {
      "epoch": 0.10645848119233499,
      "grad_norm": 0.2164372056722641,
      "learning_rate": 0.00019381541389153187,
      "loss": 0.1703,
      "step": 75
    },
    {
      "epoch": 0.1078779276082328,
      "grad_norm": 0.1846214234828949,
      "learning_rate": 0.00019372026641294008,
      "loss": 0.1742,
      "step": 76
    },
    {
      "epoch": 0.10929737402413059,
      "grad_norm": 0.16839642822742462,
      "learning_rate": 0.00019362511893434824,
      "loss": 0.1471,
      "step": 77
    },
    {
      "epoch": 0.11071682044002838,
      "grad_norm": 0.2075975090265274,
      "learning_rate": 0.00019352997145575645,
      "loss": 0.1939,
      "step": 78
    },
    {
      "epoch": 0.11213626685592619,
      "grad_norm": 0.21337558329105377,
      "learning_rate": 0.0001934348239771646,
      "loss": 0.1677,
      "step": 79
    },
    {
      "epoch": 0.11355571327182398,
      "grad_norm": 0.1997154951095581,
      "learning_rate": 0.00019333967649857282,
      "loss": 0.1525,
      "step": 80
    },
    {
      "epoch": 0.11497515968772179,
      "grad_norm": 0.20924781262874603,
      "learning_rate": 0.00019324452901998097,
      "loss": 0.1234,
      "step": 81
    },
    {
      "epoch": 0.11639460610361958,
      "grad_norm": 0.20264357328414917,
      "learning_rate": 0.00019314938154138916,
      "loss": 0.1459,
      "step": 82
    },
    {
      "epoch": 0.11781405251951739,
      "grad_norm": 0.15568575263023376,
      "learning_rate": 0.00019305423406279734,
      "loss": 0.1262,
      "step": 83
    },
    {
      "epoch": 0.11923349893541518,
      "grad_norm": 0.18003098666667938,
      "learning_rate": 0.00019295908658420552,
      "loss": 0.1073,
      "step": 84
    },
    {
      "epoch": 0.12065294535131299,
      "grad_norm": 0.18673446774482727,
      "learning_rate": 0.0001928639391056137,
      "loss": 0.1248,
      "step": 85
    },
    {
      "epoch": 0.12207239176721078,
      "grad_norm": 0.16824555397033691,
      "learning_rate": 0.0001927687916270219,
      "loss": 0.1181,
      "step": 86
    },
    {
      "epoch": 0.12349183818310859,
      "grad_norm": 0.15063299238681793,
      "learning_rate": 0.00019267364414843007,
      "loss": 0.1117,
      "step": 87
    },
    {
      "epoch": 0.12491128459900638,
      "grad_norm": 0.16303810477256775,
      "learning_rate": 0.00019257849666983826,
      "loss": 0.105,
      "step": 88
    },
    {
      "epoch": 0.1263307310149042,
      "grad_norm": 0.16362158954143524,
      "learning_rate": 0.00019248334919124644,
      "loss": 0.0931,
      "step": 89
    },
    {
      "epoch": 0.127750177430802,
      "grad_norm": 0.15042024850845337,
      "learning_rate": 0.00019238820171265462,
      "loss": 0.0981,
      "step": 90
    },
    {
      "epoch": 0.12916962384669978,
      "grad_norm": 0.14741793274879456,
      "learning_rate": 0.0001922930542340628,
      "loss": 0.0986,
      "step": 91
    },
    {
      "epoch": 0.13058907026259758,
      "grad_norm": 0.158548966050148,
      "learning_rate": 0.000192197906755471,
      "loss": 0.0884,
      "step": 92
    },
    {
      "epoch": 0.1320085166784954,
      "grad_norm": 0.14750920236110687,
      "learning_rate": 0.00019210275927687917,
      "loss": 0.0886,
      "step": 93
    },
    {
      "epoch": 0.1334279630943932,
      "grad_norm": 0.12116905301809311,
      "learning_rate": 0.00019200761179828736,
      "loss": 0.0841,
      "step": 94
    },
    {
      "epoch": 0.13484740951029098,
      "grad_norm": 0.12684527039527893,
      "learning_rate": 0.00019191246431969554,
      "loss": 0.0836,
      "step": 95
    },
    {
      "epoch": 0.13626685592618878,
      "grad_norm": 0.13355328142642975,
      "learning_rate": 0.00019181731684110372,
      "loss": 0.0852,
      "step": 96
    },
    {
      "epoch": 0.1376863023420866,
      "grad_norm": 0.11786916851997375,
      "learning_rate": 0.0001917221693625119,
      "loss": 0.066,
      "step": 97
    },
    {
      "epoch": 0.1391057487579844,
      "grad_norm": 0.1535901427268982,
      "learning_rate": 0.00019162702188392006,
      "loss": 0.0697,
      "step": 98
    },
    {
      "epoch": 0.14052519517388218,
      "grad_norm": 0.10121750086545944,
      "learning_rate": 0.00019153187440532827,
      "loss": 0.0604,
      "step": 99
    },
    {
      "epoch": 0.14194464158977999,
      "grad_norm": 0.09791870415210724,
      "learning_rate": 0.00019143672692673643,
      "loss": 0.0422,
      "step": 100
    },
    {
      "epoch": 0.1433640880056778,
      "grad_norm": 0.8991482853889465,
      "learning_rate": 0.00019134157944814464,
      "loss": 0.6245,
      "step": 101
    },
    {
      "epoch": 0.1447835344215756,
      "grad_norm": 0.536517858505249,
      "learning_rate": 0.0001912464319695528,
      "loss": 0.4928,
      "step": 102
    },
    {
      "epoch": 0.14620298083747338,
      "grad_norm": 0.3712337911128998,
      "learning_rate": 0.000191151284490961,
      "loss": 0.4136,
      "step": 103
    },
    {
      "epoch": 0.14762242725337119,
      "grad_norm": 0.29865872859954834,
      "learning_rate": 0.00019105613701236916,
      "loss": 0.3265,
      "step": 104
    },
    {
      "epoch": 0.149041873669269,
      "grad_norm": 0.3236014246940613,
      "learning_rate": 0.00019096098953377737,
      "loss": 0.3386,
      "step": 105
    },
    {
      "epoch": 0.15046132008516677,
      "grad_norm": 0.31175604462623596,
      "learning_rate": 0.00019086584205518553,
      "loss": 0.3567,
      "step": 106
    },
    {
      "epoch": 0.15188076650106458,
      "grad_norm": 0.3100660741329193,
      "learning_rate": 0.00019077069457659374,
      "loss": 0.3257,
      "step": 107
    },
    {
      "epoch": 0.1533002129169624,
      "grad_norm": 0.30780166387557983,
      "learning_rate": 0.0001906755470980019,
      "loss": 0.3294,
      "step": 108
    },
    {
      "epoch": 0.1547196593328602,
      "grad_norm": 0.2866743505001068,
      "learning_rate": 0.0001905803996194101,
      "loss": 0.3086,
      "step": 109
    },
    {
      "epoch": 0.15613910574875797,
      "grad_norm": 0.25725001096725464,
      "learning_rate": 0.00019048525214081826,
      "loss": 0.2569,
      "step": 110
    },
    {
      "epoch": 0.15755855216465578,
      "grad_norm": 0.2841548025608063,
      "learning_rate": 0.00019039010466222647,
      "loss": 0.2833,
      "step": 111
    },
    {
      "epoch": 0.1589779985805536,
      "grad_norm": 0.21969172358512878,
      "learning_rate": 0.00019029495718363463,
      "loss": 0.213,
      "step": 112
    },
    {
      "epoch": 0.1603974449964514,
      "grad_norm": 0.2883078455924988,
      "learning_rate": 0.00019019980970504284,
      "loss": 0.2762,
      "step": 113
    },
    {
      "epoch": 0.16181689141234917,
      "grad_norm": 0.21333856880664825,
      "learning_rate": 0.000190104662226451,
      "loss": 0.2362,
      "step": 114
    },
    {
      "epoch": 0.16323633782824698,
      "grad_norm": 0.2552758753299713,
      "learning_rate": 0.0001900095147478592,
      "loss": 0.2223,
      "step": 115
    },
    {
      "epoch": 0.1646557842441448,
      "grad_norm": 0.23104964196681976,
      "learning_rate": 0.00018991436726926736,
      "loss": 0.1887,
      "step": 116
    },
    {
      "epoch": 0.1660752306600426,
      "grad_norm": 0.27122604846954346,
      "learning_rate": 0.00018981921979067558,
      "loss": 0.2177,
      "step": 117
    },
    {
      "epoch": 0.16749467707594037,
      "grad_norm": 0.2582598626613617,
      "learning_rate": 0.00018972407231208373,
      "loss": 0.2375,
      "step": 118
    },
    {
      "epoch": 0.16891412349183818,
      "grad_norm": 0.2079077512025833,
      "learning_rate": 0.00018962892483349191,
      "loss": 0.1653,
      "step": 119
    },
    {
      "epoch": 0.170333569907736,
      "grad_norm": 0.2133801132440567,
      "learning_rate": 0.0001895337773549001,
      "loss": 0.1939,
      "step": 120
    },
    {
      "epoch": 0.1717530163236338,
      "grad_norm": 0.25759196281433105,
      "learning_rate": 0.00018943862987630828,
      "loss": 0.2134,
      "step": 121
    },
    {
      "epoch": 0.17317246273953157,
      "grad_norm": 0.1881779581308365,
      "learning_rate": 0.00018934348239771646,
      "loss": 0.1756,
      "step": 122
    },
    {
      "epoch": 0.17459190915542938,
      "grad_norm": 0.227374866604805,
      "learning_rate": 0.00018924833491912465,
      "loss": 0.1788,
      "step": 123
    },
    {
      "epoch": 0.1760113555713272,
      "grad_norm": 0.21692273020744324,
      "learning_rate": 0.00018915318744053283,
      "loss": 0.1817,
      "step": 124
    },
    {
      "epoch": 0.177430801987225,
      "grad_norm": 0.22101150453090668,
      "learning_rate": 0.00018905803996194101,
      "loss": 0.176,
      "step": 125
    },
    {
      "epoch": 0.17885024840312277,
      "grad_norm": 0.1902722418308258,
      "learning_rate": 0.0001889628924833492,
      "loss": 0.1228,
      "step": 126
    },
    {
      "epoch": 0.18026969481902058,
      "grad_norm": 0.20391739904880524,
      "learning_rate": 0.00018886774500475738,
      "loss": 0.1454,
      "step": 127
    },
    {
      "epoch": 0.1816891412349184,
      "grad_norm": 0.16759422421455383,
      "learning_rate": 0.00018877259752616556,
      "loss": 0.115,
      "step": 128
    },
    {
      "epoch": 0.18310858765081617,
      "grad_norm": 0.17888155579566956,
      "learning_rate": 0.00018867745004757375,
      "loss": 0.1211,
      "step": 129
    },
    {
      "epoch": 0.18452803406671398,
      "grad_norm": 0.1745491474866867,
      "learning_rate": 0.00018858230256898193,
      "loss": 0.133,
      "step": 130
    },
    {
      "epoch": 0.18594748048261178,
      "grad_norm": 0.168021097779274,
      "learning_rate": 0.00018848715509039012,
      "loss": 0.137,
      "step": 131
    },
    {
      "epoch": 0.1873669268985096,
      "grad_norm": 0.2074587345123291,
      "learning_rate": 0.0001883920076117983,
      "loss": 0.1185,
      "step": 132
    },
    {
      "epoch": 0.18878637331440737,
      "grad_norm": 0.18290935456752777,
      "learning_rate": 0.00018829686013320648,
      "loss": 0.1491,
      "step": 133
    },
    {
      "epoch": 0.19020581973030518,
      "grad_norm": 0.20074568688869476,
      "learning_rate": 0.00018820171265461467,
      "loss": 0.1425,
      "step": 134
    },
    {
      "epoch": 0.19162526614620298,
      "grad_norm": 0.16316284239292145,
      "learning_rate": 0.00018810656517602285,
      "loss": 0.1285,
      "step": 135
    },
    {
      "epoch": 0.1930447125621008,
      "grad_norm": 0.13204751908779144,
      "learning_rate": 0.00018801141769743103,
      "loss": 0.1088,
      "step": 136
    },
    {
      "epoch": 0.19446415897799857,
      "grad_norm": 0.17812924087047577,
      "learning_rate": 0.00018791627021883922,
      "loss": 0.1283,
      "step": 137
    },
    {
      "epoch": 0.19588360539389638,
      "grad_norm": 0.17065832018852234,
      "learning_rate": 0.0001878211227402474,
      "loss": 0.1153,
      "step": 138
    },
    {
      "epoch": 0.19730305180979418,
      "grad_norm": 0.14613445103168488,
      "learning_rate": 0.00018772597526165558,
      "loss": 0.1048,
      "step": 139
    },
    {
      "epoch": 0.198722498225692,
      "grad_norm": 0.1457911729812622,
      "learning_rate": 0.00018763082778306377,
      "loss": 0.0918,
      "step": 140
    },
    {
      "epoch": 0.20014194464158977,
      "grad_norm": 0.13844063878059387,
      "learning_rate": 0.00018753568030447195,
      "loss": 0.0909,
      "step": 141
    },
    {
      "epoch": 0.20156139105748758,
      "grad_norm": 0.14067940413951874,
      "learning_rate": 0.00018744053282588013,
      "loss": 0.1034,
      "step": 142
    },
    {
      "epoch": 0.20298083747338538,
      "grad_norm": 0.12681680917739868,
      "learning_rate": 0.00018734538534728832,
      "loss": 0.0947,
      "step": 143
    },
    {
      "epoch": 0.2044002838892832,
      "grad_norm": 0.14871253073215485,
      "learning_rate": 0.0001872502378686965,
      "loss": 0.108,
      "step": 144
    },
    {
      "epoch": 0.20581973030518097,
      "grad_norm": 0.20518921315670013,
      "learning_rate": 0.00018715509039010468,
      "loss": 0.1038,
      "step": 145
    },
    {
      "epoch": 0.20723917672107878,
      "grad_norm": 0.17616136372089386,
      "learning_rate": 0.00018705994291151287,
      "loss": 0.0715,
      "step": 146
    },
    {
      "epoch": 0.20865862313697658,
      "grad_norm": 0.11235202103853226,
      "learning_rate": 0.00018696479543292105,
      "loss": 0.0786,
      "step": 147
    },
    {
      "epoch": 0.2100780695528744,
      "grad_norm": 0.12890516221523285,
      "learning_rate": 0.00018686964795432923,
      "loss": 0.0701,
      "step": 148
    },
    {
      "epoch": 0.21149751596877217,
      "grad_norm": 0.09201624244451523,
      "learning_rate": 0.00018677450047573742,
      "loss": 0.0552,
      "step": 149
    },
    {
      "epoch": 0.21291696238466998,
      "grad_norm": 0.1286080777645111,
      "learning_rate": 0.00018667935299714557,
      "loss": 0.0494,
      "step": 150
    },
    {
      "epoch": 0.21433640880056778,
      "grad_norm": 0.6585749387741089,
      "learning_rate": 0.00018658420551855376,
      "loss": 0.7252,
      "step": 151
    },
    {
      "epoch": 0.2157558552164656,
      "grad_norm": 0.5025274157524109,
      "learning_rate": 0.00018648905803996194,
      "loss": 0.5642,
      "step": 152
    },
    {
      "epoch": 0.21717530163236337,
      "grad_norm": 0.3372633755207062,
      "learning_rate": 0.00018639391056137012,
      "loss": 0.4086,
      "step": 153
    },
    {
      "epoch": 0.21859474804826118,
      "grad_norm": 0.298308789730072,
      "learning_rate": 0.0001862987630827783,
      "loss": 0.3602,
      "step": 154
    },
    {
      "epoch": 0.22001419446415899,
      "grad_norm": 0.254146933555603,
      "learning_rate": 0.0001862036156041865,
      "loss": 0.3183,
      "step": 155
    },
    {
      "epoch": 0.22143364088005676,
      "grad_norm": 0.2620442509651184,
      "learning_rate": 0.00018610846812559467,
      "loss": 0.3252,
      "step": 156
    },
    {
      "epoch": 0.22285308729595457,
      "grad_norm": 0.2677079737186432,
      "learning_rate": 0.00018601332064700286,
      "loss": 0.3069,
      "step": 157
    },
    {
      "epoch": 0.22427253371185238,
      "grad_norm": 0.26175227761268616,
      "learning_rate": 0.00018591817316841104,
      "loss": 0.3178,
      "step": 158
    },
    {
      "epoch": 0.22569198012775019,
      "grad_norm": 0.2651664614677429,
      "learning_rate": 0.00018582302568981922,
      "loss": 0.3102,
      "step": 159
    },
    {
      "epoch": 0.22711142654364797,
      "grad_norm": 0.24599607288837433,
      "learning_rate": 0.0001857278782112274,
      "loss": 0.2539,
      "step": 160
    },
    {
      "epoch": 0.22853087295954577,
      "grad_norm": 0.28982409834861755,
      "learning_rate": 0.0001856327307326356,
      "loss": 0.236,
      "step": 161
    },
    {
      "epoch": 0.22995031937544358,
      "grad_norm": 0.24877092242240906,
      "learning_rate": 0.00018553758325404377,
      "loss": 0.2827,
      "step": 162
    },
    {
      "epoch": 0.2313697657913414,
      "grad_norm": 0.31241002678871155,
      "learning_rate": 0.00018544243577545196,
      "loss": 0.2639,
      "step": 163
    },
    {
      "epoch": 0.23278921220723917,
      "grad_norm": 0.23131708800792694,
      "learning_rate": 0.00018534728829686014,
      "loss": 0.2099,
      "step": 164
    },
    {
      "epoch": 0.23420865862313697,
      "grad_norm": 0.24522876739501953,
      "learning_rate": 0.00018525214081826832,
      "loss": 0.2242,
      "step": 165
    },
    {
      "epoch": 0.23562810503903478,
      "grad_norm": 0.23746757209300995,
      "learning_rate": 0.0001851569933396765,
      "loss": 0.2173,
      "step": 166
    },
    {
      "epoch": 0.2370475514549326,
      "grad_norm": 0.19376133382320404,
      "learning_rate": 0.0001850618458610847,
      "loss": 0.1677,
      "step": 167
    },
    {
      "epoch": 0.23846699787083037,
      "grad_norm": 0.24670350551605225,
      "learning_rate": 0.00018496669838249287,
      "loss": 0.1954,
      "step": 168
    },
    {
      "epoch": 0.23988644428672817,
      "grad_norm": 0.2263229489326477,
      "learning_rate": 0.00018487155090390106,
      "loss": 0.1983,
      "step": 169
    },
    {
      "epoch": 0.24130589070262598,
      "grad_norm": 0.2091400921344757,
      "learning_rate": 0.00018477640342530924,
      "loss": 0.186,
      "step": 170
    },
    {
      "epoch": 0.2427253371185238,
      "grad_norm": 0.20422160625457764,
      "learning_rate": 0.00018468125594671742,
      "loss": 0.1713,
      "step": 171
    },
    {
      "epoch": 0.24414478353442157,
      "grad_norm": 0.21125274896621704,
      "learning_rate": 0.0001845861084681256,
      "loss": 0.1888,
      "step": 172
    },
    {
      "epoch": 0.24556422995031937,
      "grad_norm": 0.2694367468357086,
      "learning_rate": 0.0001844909609895338,
      "loss": 0.2226,
      "step": 173
    },
    {
      "epoch": 0.24698367636621718,
      "grad_norm": 0.18443642556667328,
      "learning_rate": 0.00018439581351094197,
      "loss": 0.1524,
      "step": 174
    },
    {
      "epoch": 0.248403122782115,
      "grad_norm": 0.20904751121997833,
      "learning_rate": 0.00018430066603235016,
      "loss": 0.1829,
      "step": 175
    },
    {
      "epoch": 0.24982256919801277,
      "grad_norm": 0.19625532627105713,
      "learning_rate": 0.00018420551855375834,
      "loss": 0.2097,
      "step": 176
    },
    {
      "epoch": 0.2512420156139106,
      "grad_norm": 0.19554728269577026,
      "learning_rate": 0.00018411037107516652,
      "loss": 0.1498,
      "step": 177
    },
    {
      "epoch": 0.2526614620298084,
      "grad_norm": 0.19032320380210876,
      "learning_rate": 0.0001840152235965747,
      "loss": 0.1684,
      "step": 178
    },
    {
      "epoch": 0.2540809084457062,
      "grad_norm": 0.1574752926826477,
      "learning_rate": 0.00018392007611798286,
      "loss": 0.1511,
      "step": 179
    },
    {
      "epoch": 0.255500354861604,
      "grad_norm": 0.15841525793075562,
      "learning_rate": 0.00018382492863939107,
      "loss": 0.1521,
      "step": 180
    },
    {
      "epoch": 0.25691980127750175,
      "grad_norm": 0.22403736412525177,
      "learning_rate": 0.00018372978116079923,
      "loss": 0.1683,
      "step": 181
    },
    {
      "epoch": 0.25833924769339955,
      "grad_norm": 0.1463969349861145,
      "learning_rate": 0.00018363463368220744,
      "loss": 0.1201,
      "step": 182
    },
    {
      "epoch": 0.25975869410929736,
      "grad_norm": 0.182897686958313,
      "learning_rate": 0.0001835394862036156,
      "loss": 0.1254,
      "step": 183
    },
    {
      "epoch": 0.26117814052519517,
      "grad_norm": 0.16555581986904144,
      "learning_rate": 0.0001834443387250238,
      "loss": 0.1435,
      "step": 184
    },
    {
      "epoch": 0.262597586941093,
      "grad_norm": 0.16409210860729218,
      "learning_rate": 0.00018334919124643196,
      "loss": 0.1144,
      "step": 185
    },
    {
      "epoch": 0.2640170333569908,
      "grad_norm": 0.15717332065105438,
      "learning_rate": 0.00018325404376784017,
      "loss": 0.1227,
      "step": 186
    },
    {
      "epoch": 0.2654364797728886,
      "grad_norm": 0.14730651676654816,
      "learning_rate": 0.00018315889628924833,
      "loss": 0.1139,
      "step": 187
    },
    {
      "epoch": 0.2668559261887864,
      "grad_norm": 0.15004900097846985,
      "learning_rate": 0.00018306374881065654,
      "loss": 0.1141,
      "step": 188
    },
    {
      "epoch": 0.26827537260468415,
      "grad_norm": 0.14527259767055511,
      "learning_rate": 0.0001829686013320647,
      "loss": 0.1227,
      "step": 189
    },
    {
      "epoch": 0.26969481902058196,
      "grad_norm": 0.12058791518211365,
      "learning_rate": 0.0001828734538534729,
      "loss": 0.0967,
      "step": 190
    },
    {
      "epoch": 0.27111426543647976,
      "grad_norm": 0.15104159712791443,
      "learning_rate": 0.00018277830637488106,
      "loss": 0.1133,
      "step": 191
    },
    {
      "epoch": 0.27253371185237757,
      "grad_norm": 0.1208389550447464,
      "learning_rate": 0.00018268315889628927,
      "loss": 0.0909,
      "step": 192
    },
    {
      "epoch": 0.2739531582682754,
      "grad_norm": 0.13636882603168488,
      "learning_rate": 0.00018258801141769743,
      "loss": 0.1006,
      "step": 193
    },
    {
      "epoch": 0.2753726046841732,
      "grad_norm": 0.14653749763965607,
      "learning_rate": 0.00018249286393910564,
      "loss": 0.1028,
      "step": 194
    },
    {
      "epoch": 0.276792051100071,
      "grad_norm": 0.13390621542930603,
      "learning_rate": 0.0001823977164605138,
      "loss": 0.1013,
      "step": 195
    },
    {
      "epoch": 0.2782114975159688,
      "grad_norm": 0.17225858569145203,
      "learning_rate": 0.000182302568981922,
      "loss": 0.0982,
      "step": 196
    },
    {
      "epoch": 0.27963094393186655,
      "grad_norm": 0.1274978071451187,
      "learning_rate": 0.00018220742150333016,
      "loss": 0.0923,
      "step": 197
    },
    {
      "epoch": 0.28105039034776436,
      "grad_norm": 0.1318695843219757,
      "learning_rate": 0.00018211227402473837,
      "loss": 0.0639,
      "step": 198
    },
    {
      "epoch": 0.28246983676366216,
      "grad_norm": 0.13581371307373047,
      "learning_rate": 0.00018201712654614653,
      "loss": 0.0774,
      "step": 199
    },
    {
      "epoch": 0.28388928317955997,
      "grad_norm": 0.10221122205257416,
      "learning_rate": 0.00018192197906755474,
      "loss": 0.0662,
      "step": 200
    },
    {
      "epoch": 0.2853087295954578,
      "grad_norm": 0.5923755764961243,
      "learning_rate": 0.0001818268315889629,
      "loss": 0.5019,
      "step": 201
    },
    {
      "epoch": 0.2867281760113556,
      "grad_norm": 0.47801506519317627,
      "learning_rate": 0.00018173168411037108,
      "loss": 0.4882,
      "step": 202
    },
    {
      "epoch": 0.2881476224272534,
      "grad_norm": 0.416513055562973,
      "learning_rate": 0.00018163653663177926,
      "loss": 0.4185,
      "step": 203
    },
    {
      "epoch": 0.2895670688431512,
      "grad_norm": 0.30151981115341187,
      "learning_rate": 0.00018154138915318745,
      "loss": 0.3634,
      "step": 204
    },
    {
      "epoch": 0.29098651525904895,
      "grad_norm": 0.3366330564022064,
      "learning_rate": 0.00018144624167459563,
      "loss": 0.3766,
      "step": 205
    },
    {
      "epoch": 0.29240596167494676,
      "grad_norm": 0.3388964831829071,
      "learning_rate": 0.0001813510941960038,
      "loss": 0.3425,
      "step": 206
    },
    {
      "epoch": 0.29382540809084456,
      "grad_norm": 0.28328460454940796,
      "learning_rate": 0.000181255946717412,
      "loss": 0.3147,
      "step": 207
    },
    {
      "epoch": 0.29524485450674237,
      "grad_norm": 0.27872422337532043,
      "learning_rate": 0.00018116079923882018,
      "loss": 0.3018,
      "step": 208
    },
    {
      "epoch": 0.2966643009226402,
      "grad_norm": 0.26885318756103516,
      "learning_rate": 0.00018106565176022836,
      "loss": 0.3222,
      "step": 209
    },
    {
      "epoch": 0.298083747338538,
      "grad_norm": 0.26485034823417664,
      "learning_rate": 0.00018097050428163655,
      "loss": 0.3397,
      "step": 210
    },
    {
      "epoch": 0.2995031937544358,
      "grad_norm": 0.2604244351387024,
      "learning_rate": 0.00018087535680304473,
      "loss": 0.2642,
      "step": 211
    },
    {
      "epoch": 0.30092264017033354,
      "grad_norm": 0.22035960853099823,
      "learning_rate": 0.0001807802093244529,
      "loss": 0.2307,
      "step": 212
    },
    {
      "epoch": 0.30234208658623135,
      "grad_norm": 0.21684938669204712,
      "learning_rate": 0.0001806850618458611,
      "loss": 0.2531,
      "step": 213
    },
    {
      "epoch": 0.30376153300212916,
      "grad_norm": 0.24349994957447052,
      "learning_rate": 0.00018058991436726928,
      "loss": 0.2496,
      "step": 214
    },
    {
      "epoch": 0.30518097941802697,
      "grad_norm": 0.23300093412399292,
      "learning_rate": 0.00018049476688867746,
      "loss": 0.2167,
      "step": 215
    },
    {
      "epoch": 0.3066004258339248,
      "grad_norm": 0.20153824985027313,
      "learning_rate": 0.00018039961941008565,
      "loss": 0.1941,
      "step": 216
    },
    {
      "epoch": 0.3080198722498226,
      "grad_norm": 0.22291982173919678,
      "learning_rate": 0.00018030447193149383,
      "loss": 0.2212,
      "step": 217
    },
    {
      "epoch": 0.3094393186657204,
      "grad_norm": 0.2710703909397125,
      "learning_rate": 0.000180209324452902,
      "loss": 0.2324,
      "step": 218
    },
    {
      "epoch": 0.3108587650816182,
      "grad_norm": 0.22228169441223145,
      "learning_rate": 0.0001801141769743102,
      "loss": 0.2219,
      "step": 219
    },
    {
      "epoch": 0.31227821149751595,
      "grad_norm": 0.19645242393016815,
      "learning_rate": 0.00018001902949571838,
      "loss": 0.1829,
      "step": 220
    },
    {
      "epoch": 0.31369765791341375,
      "grad_norm": 0.2148899883031845,
      "learning_rate": 0.00017992388201712656,
      "loss": 0.1846,
      "step": 221
    },
    {
      "epoch": 0.31511710432931156,
      "grad_norm": 0.21384695172309875,
      "learning_rate": 0.00017982873453853472,
      "loss": 0.1629,
      "step": 222
    },
    {
      "epoch": 0.31653655074520937,
      "grad_norm": 0.20600435137748718,
      "learning_rate": 0.00017973358705994293,
      "loss": 0.1472,
      "step": 223
    },
    {
      "epoch": 0.3179559971611072,
      "grad_norm": 0.15907453000545502,
      "learning_rate": 0.00017963843958135109,
      "loss": 0.1376,
      "step": 224
    },
    {
      "epoch": 0.319375443577005,
      "grad_norm": 0.2737395763397217,
      "learning_rate": 0.0001795432921027593,
      "loss": 0.1338,
      "step": 225
    },
    {
      "epoch": 0.3207948899929028,
      "grad_norm": 0.18210864067077637,
      "learning_rate": 0.00017944814462416745,
      "loss": 0.1591,
      "step": 226
    },
    {
      "epoch": 0.3222143364088006,
      "grad_norm": 0.18131813406944275,
      "learning_rate": 0.00017935299714557566,
      "loss": 0.1454,
      "step": 227
    },
    {
      "epoch": 0.32363378282469835,
      "grad_norm": 0.1633884608745575,
      "learning_rate": 0.00017925784966698382,
      "loss": 0.1188,
      "step": 228
    },
    {
      "epoch": 0.32505322924059615,
      "grad_norm": 0.1833418607711792,
      "learning_rate": 0.00017916270218839203,
      "loss": 0.1387,
      "step": 229
    },
    {
      "epoch": 0.32647267565649396,
      "grad_norm": 0.20457161962985992,
      "learning_rate": 0.0001790675547098002,
      "loss": 0.1397,
      "step": 230
    },
    {
      "epoch": 0.32789212207239177,
      "grad_norm": 0.16873478889465332,
      "learning_rate": 0.00017897240723120837,
      "loss": 0.1374,
      "step": 231
    },
    {
      "epoch": 0.3293115684882896,
      "grad_norm": 0.16101014614105225,
      "learning_rate": 0.00017887725975261655,
      "loss": 0.1483,
      "step": 232
    },
    {
      "epoch": 0.3307310149041874,
      "grad_norm": 0.18449218571186066,
      "learning_rate": 0.00017878211227402474,
      "loss": 0.1686,
      "step": 233
    },
    {
      "epoch": 0.3321504613200852,
      "grad_norm": 0.154401496052742,
      "learning_rate": 0.00017868696479543292,
      "loss": 0.1242,
      "step": 234
    },
    {
      "epoch": 0.33356990773598294,
      "grad_norm": 0.1769985854625702,
      "learning_rate": 0.0001785918173168411,
      "loss": 0.1254,
      "step": 235
    },
    {
      "epoch": 0.33498935415188075,
      "grad_norm": 0.1648237556219101,
      "learning_rate": 0.0001784966698382493,
      "loss": 0.1232,
      "step": 236
    },
    {
      "epoch": 0.33640880056777855,
      "grad_norm": 0.1461450308561325,
      "learning_rate": 0.00017840152235965747,
      "loss": 0.1055,
      "step": 237
    },
    {
      "epoch": 0.33782824698367636,
      "grad_norm": 0.15033933520317078,
      "learning_rate": 0.00017830637488106565,
      "loss": 0.1054,
      "step": 238
    },
    {
      "epoch": 0.33924769339957417,
      "grad_norm": 0.11923829466104507,
      "learning_rate": 0.00017821122740247384,
      "loss": 0.1106,
      "step": 239
    },
    {
      "epoch": 0.340667139815472,
      "grad_norm": 0.13053034245967865,
      "learning_rate": 0.00017811607992388202,
      "loss": 0.1153,
      "step": 240
    },
    {
      "epoch": 0.3420865862313698,
      "grad_norm": 0.12194915115833282,
      "learning_rate": 0.0001780209324452902,
      "loss": 0.0878,
      "step": 241
    },
    {
      "epoch": 0.3435060326472676,
      "grad_norm": 0.12302149087190628,
      "learning_rate": 0.0001779257849666984,
      "loss": 0.1003,
      "step": 242
    },
    {
      "epoch": 0.34492547906316534,
      "grad_norm": 0.10759289562702179,
      "learning_rate": 0.00017783063748810657,
      "loss": 0.0948,
      "step": 243
    },
    {
      "epoch": 0.34634492547906315,
      "grad_norm": 0.11200319230556488,
      "learning_rate": 0.00017773549000951475,
      "loss": 0.0835,
      "step": 244
    },
    {
      "epoch": 0.34776437189496096,
      "grad_norm": 0.12429293245077133,
      "learning_rate": 0.00017764034253092294,
      "loss": 0.0768,
      "step": 245
    },
    {
      "epoch": 0.34918381831085876,
      "grad_norm": 0.08911320567131042,
      "learning_rate": 0.00017754519505233112,
      "loss": 0.0817,
      "step": 246
    },
    {
      "epoch": 0.35060326472675657,
      "grad_norm": 0.12302954494953156,
      "learning_rate": 0.0001774500475737393,
      "loss": 0.0935,
      "step": 247
    },
    {
      "epoch": 0.3520227111426544,
      "grad_norm": 0.09873057901859283,
      "learning_rate": 0.0001773549000951475,
      "loss": 0.0701,
      "step": 248
    },
    {
      "epoch": 0.3534421575585522,
      "grad_norm": 0.0786275565624237,
      "learning_rate": 0.00017725975261655567,
      "loss": 0.0564,
      "step": 249
    },
    {
      "epoch": 0.35486160397445,
      "grad_norm": 0.09582041203975677,
      "learning_rate": 0.00017716460513796385,
      "loss": 0.0691,
      "step": 250
    },
    {
      "epoch": 0.35628105039034774,
      "grad_norm": 0.5196399688720703,
      "learning_rate": 0.00017706945765937204,
      "loss": 0.5333,
      "step": 251
    },
    {
      "epoch": 0.35770049680624555,
      "grad_norm": 0.444915771484375,
      "learning_rate": 0.00017697431018078022,
      "loss": 0.4228,
      "step": 252
    },
    {
      "epoch": 0.35911994322214336,
      "grad_norm": 0.3279583752155304,
      "learning_rate": 0.0001768791627021884,
      "loss": 0.3702,
      "step": 253
    },
    {
      "epoch": 0.36053938963804116,
      "grad_norm": 0.295723021030426,
      "learning_rate": 0.0001767840152235966,
      "loss": 0.3464,
      "step": 254
    },
    {
      "epoch": 0.36195883605393897,
      "grad_norm": 0.32640308141708374,
      "learning_rate": 0.00017668886774500477,
      "loss": 0.3714,
      "step": 255
    },
    {
      "epoch": 0.3633782824698368,
      "grad_norm": 0.23948761820793152,
      "learning_rate": 0.00017659372026641295,
      "loss": 0.3377,
      "step": 256
    },
    {
      "epoch": 0.3647977288857346,
      "grad_norm": 0.2686794102191925,
      "learning_rate": 0.00017649857278782114,
      "loss": 0.3323,
      "step": 257
    },
    {
      "epoch": 0.36621717530163234,
      "grad_norm": 0.2624031603336334,
      "learning_rate": 0.00017640342530922932,
      "loss": 0.2537,
      "step": 258
    },
    {
      "epoch": 0.36763662171753014,
      "grad_norm": 0.2933129668235779,
      "learning_rate": 0.0001763082778306375,
      "loss": 0.3,
      "step": 259
    },
    {
      "epoch": 0.36905606813342795,
      "grad_norm": 0.23483724892139435,
      "learning_rate": 0.0001762131303520457,
      "loss": 0.2649,
      "step": 260
    },
    {
      "epoch": 0.37047551454932576,
      "grad_norm": 0.23871733248233795,
      "learning_rate": 0.00017611798287345387,
      "loss": 0.2487,
      "step": 261
    },
    {
      "epoch": 0.37189496096522356,
      "grad_norm": 0.2093488723039627,
      "learning_rate": 0.00017602283539486203,
      "loss": 0.2416,
      "step": 262
    },
    {
      "epoch": 0.37331440738112137,
      "grad_norm": 0.33524712920188904,
      "learning_rate": 0.00017592768791627024,
      "loss": 0.2572,
      "step": 263
    },
    {
      "epoch": 0.3747338537970192,
      "grad_norm": 0.2671527564525604,
      "learning_rate": 0.0001758325404376784,
      "loss": 0.2473,
      "step": 264
    },
    {
      "epoch": 0.376153300212917,
      "grad_norm": 0.21593506634235382,
      "learning_rate": 0.0001757373929590866,
      "loss": 0.2438,
      "step": 265
    },
    {
      "epoch": 0.37757274662881474,
      "grad_norm": 0.245122030377388,
      "learning_rate": 0.00017564224548049476,
      "loss": 0.2661,
      "step": 266
    },
    {
      "epoch": 0.37899219304471254,
      "grad_norm": 0.2078303098678589,
      "learning_rate": 0.00017554709800190297,
      "loss": 0.2199,
      "step": 267
    },
    {
      "epoch": 0.38041163946061035,
      "grad_norm": 0.22816424071788788,
      "learning_rate": 0.00017545195052331113,
      "loss": 0.2178,
      "step": 268
    },
    {
      "epoch": 0.38183108587650816,
      "grad_norm": 0.19368530809879303,
      "learning_rate": 0.00017535680304471934,
      "loss": 0.2057,
      "step": 269
    },
    {
      "epoch": 0.38325053229240597,
      "grad_norm": 0.25312456488609314,
      "learning_rate": 0.0001752616555661275,
      "loss": 0.1906,
      "step": 270
    },
    {
      "epoch": 0.3846699787083038,
      "grad_norm": 0.20021282136440277,
      "learning_rate": 0.0001751665080875357,
      "loss": 0.1888,
      "step": 271
    },
    {
      "epoch": 0.3860894251242016,
      "grad_norm": 0.19169561564922333,
      "learning_rate": 0.00017507136060894386,
      "loss": 0.2014,
      "step": 272
    },
    {
      "epoch": 0.3875088715400994,
      "grad_norm": 0.23282867670059204,
      "learning_rate": 0.00017497621313035207,
      "loss": 0.2081,
      "step": 273
    },
    {
      "epoch": 0.38892831795599714,
      "grad_norm": 0.15645594894886017,
      "learning_rate": 0.00017488106565176023,
      "loss": 0.14,
      "step": 274
    },
    {
      "epoch": 0.39034776437189495,
      "grad_norm": 0.20146551728248596,
      "learning_rate": 0.00017478591817316844,
      "loss": 0.1751,
      "step": 275
    },
    {
      "epoch": 0.39176721078779275,
      "grad_norm": 0.15790389478206635,
      "learning_rate": 0.0001746907706945766,
      "loss": 0.1428,
      "step": 276
    },
    {
      "epoch": 0.39318665720369056,
      "grad_norm": 0.13402463495731354,
      "learning_rate": 0.0001745956232159848,
      "loss": 0.1508,
      "step": 277
    },
    {
      "epoch": 0.39460610361958837,
      "grad_norm": 0.19556523859500885,
      "learning_rate": 0.00017450047573739296,
      "loss": 0.1774,
      "step": 278
    },
    {
      "epoch": 0.3960255500354862,
      "grad_norm": 0.26072943210601807,
      "learning_rate": 0.00017440532825880117,
      "loss": 0.1569,
      "step": 279
    },
    {
      "epoch": 0.397444996451384,
      "grad_norm": 0.14136475324630737,
      "learning_rate": 0.00017431018078020933,
      "loss": 0.1252,
      "step": 280
    },
    {
      "epoch": 0.3988644428672818,
      "grad_norm": 0.17014621198177338,
      "learning_rate": 0.00017421503330161754,
      "loss": 0.1561,
      "step": 281
    },
    {
      "epoch": 0.40028388928317954,
      "grad_norm": 0.16529017686843872,
      "learning_rate": 0.0001741198858230257,
      "loss": 0.1521,
      "step": 282
    },
    {
      "epoch": 0.40170333569907735,
      "grad_norm": 0.17142519354820251,
      "learning_rate": 0.00017402473834443388,
      "loss": 0.1061,
      "step": 283
    },
    {
      "epoch": 0.40312278211497515,
      "grad_norm": 0.1425708681344986,
      "learning_rate": 0.00017392959086584206,
      "loss": 0.1285,
      "step": 284
    },
    {
      "epoch": 0.40454222853087296,
      "grad_norm": 0.18765318393707275,
      "learning_rate": 0.00017383444338725024,
      "loss": 0.1233,
      "step": 285
    },
    {
      "epoch": 0.40596167494677077,
      "grad_norm": 0.14780974388122559,
      "learning_rate": 0.00017373929590865843,
      "loss": 0.1043,
      "step": 286
    },
    {
      "epoch": 0.4073811213626686,
      "grad_norm": 0.14468996226787567,
      "learning_rate": 0.0001736441484300666,
      "loss": 0.112,
      "step": 287
    },
    {
      "epoch": 0.4088005677785664,
      "grad_norm": 0.12419953942298889,
      "learning_rate": 0.0001735490009514748,
      "loss": 0.0896,
      "step": 288
    },
    {
      "epoch": 0.41022001419446413,
      "grad_norm": 0.1314602941274643,
      "learning_rate": 0.00017345385347288298,
      "loss": 0.1008,
      "step": 289
    },
    {
      "epoch": 0.41163946061036194,
      "grad_norm": 0.15486060082912445,
      "learning_rate": 0.00017335870599429116,
      "loss": 0.0982,
      "step": 290
    },
    {
      "epoch": 0.41305890702625975,
      "grad_norm": 0.12233155220746994,
      "learning_rate": 0.00017326355851569934,
      "loss": 0.0964,
      "step": 291
    },
    {
      "epoch": 0.41447835344215755,
      "grad_norm": 0.1104344055056572,
      "learning_rate": 0.00017316841103710753,
      "loss": 0.0955,
      "step": 292
    },
    {
      "epoch": 0.41589779985805536,
      "grad_norm": 0.0978473499417305,
      "learning_rate": 0.00017307326355851568,
      "loss": 0.0746,
      "step": 293
    },
    {
      "epoch": 0.41731724627395317,
      "grad_norm": 0.10647564381361008,
      "learning_rate": 0.0001729781160799239,
      "loss": 0.0688,
      "step": 294
    },
    {
      "epoch": 0.418736692689851,
      "grad_norm": 0.1035827174782753,
      "learning_rate": 0.00017288296860133205,
      "loss": 0.0785,
      "step": 295
    },
    {
      "epoch": 0.4201561391057488,
      "grad_norm": 0.12469320744276047,
      "learning_rate": 0.00017278782112274026,
      "loss": 0.0889,
      "step": 296
    },
    {
      "epoch": 0.42157558552164653,
      "grad_norm": 0.11159732937812805,
      "learning_rate": 0.00017269267364414842,
      "loss": 0.0786,
      "step": 297
    },
    {
      "epoch": 0.42299503193754434,
      "grad_norm": 0.09567723423242569,
      "learning_rate": 0.00017259752616555663,
      "loss": 0.0618,
      "step": 298
    },
    {
      "epoch": 0.42441447835344215,
      "grad_norm": 0.11323487013578415,
      "learning_rate": 0.00017250237868696478,
      "loss": 0.0735,
      "step": 299
    },
    {
      "epoch": 0.42583392476933996,
      "grad_norm": 0.36350539326667786,
      "learning_rate": 0.000172407231208373,
      "loss": 0.0587,
      "step": 300
    },
    {
      "epoch": 0.42725337118523776,
      "grad_norm": 0.4785170257091522,
      "learning_rate": 0.00017231208372978115,
      "loss": 0.518,
      "step": 301
    },
    {
      "epoch": 0.42867281760113557,
      "grad_norm": 0.4595721960067749,
      "learning_rate": 0.00017221693625118936,
      "loss": 0.4631,
      "step": 302
    },
    {
      "epoch": 0.4300922640170334,
      "grad_norm": 0.3478117287158966,
      "learning_rate": 0.00017212178877259752,
      "loss": 0.4587,
      "step": 303
    },
    {
      "epoch": 0.4315117104329312,
      "grad_norm": 0.33338478207588196,
      "learning_rate": 0.00017202664129400573,
      "loss": 0.3858,
      "step": 304
    },
    {
      "epoch": 0.43293115684882894,
      "grad_norm": 0.27139970660209656,
      "learning_rate": 0.00017193149381541388,
      "loss": 0.3406,
      "step": 305
    },
    {
      "epoch": 0.43435060326472674,
      "grad_norm": 0.24543721973896027,
      "learning_rate": 0.0001718363463368221,
      "loss": 0.3331,
      "step": 306
    },
    {
      "epoch": 0.43577004968062455,
      "grad_norm": 0.2112877368927002,
      "learning_rate": 0.00017174119885823025,
      "loss": 0.2871,
      "step": 307
    },
    {
      "epoch": 0.43718949609652236,
      "grad_norm": 0.304798424243927,
      "learning_rate": 0.00017164605137963846,
      "loss": 0.3329,
      "step": 308
    },
    {
      "epoch": 0.43860894251242016,
      "grad_norm": 0.21619813144207,
      "learning_rate": 0.00017155090390104662,
      "loss": 0.2867,
      "step": 309
    },
    {
      "epoch": 0.44002838892831797,
      "grad_norm": 0.24975015223026276,
      "learning_rate": 0.00017145575642245483,
      "loss": 0.3055,
      "step": 310
    },
    {
      "epoch": 0.4414478353442158,
      "grad_norm": 0.21540270745754242,
      "learning_rate": 0.00017136060894386298,
      "loss": 0.2333,
      "step": 311
    },
    {
      "epoch": 0.44286728176011353,
      "grad_norm": 0.18148262798786163,
      "learning_rate": 0.0001712654614652712,
      "loss": 0.2232,
      "step": 312
    },
    {
      "epoch": 0.44428672817601134,
      "grad_norm": 0.20767313241958618,
      "learning_rate": 0.00017117031398667935,
      "loss": 0.204,
      "step": 313
    },
    {
      "epoch": 0.44570617459190914,
      "grad_norm": 0.22825096547603607,
      "learning_rate": 0.00017107516650808753,
      "loss": 0.2421,
      "step": 314
    },
    {
      "epoch": 0.44712562100780695,
      "grad_norm": 0.23413901031017303,
      "learning_rate": 0.00017098001902949572,
      "loss": 0.2224,
      "step": 315
    },
    {
      "epoch": 0.44854506742370476,
      "grad_norm": 0.20314759016036987,
      "learning_rate": 0.0001708848715509039,
      "loss": 0.1989,
      "step": 316
    },
    {
      "epoch": 0.44996451383960256,
      "grad_norm": 0.19911476969718933,
      "learning_rate": 0.00017078972407231208,
      "loss": 0.2239,
      "step": 317
    },
    {
      "epoch": 0.45138396025550037,
      "grad_norm": 0.20485487580299377,
      "learning_rate": 0.00017069457659372027,
      "loss": 0.21,
      "step": 318
    },
    {
      "epoch": 0.4528034066713982,
      "grad_norm": 0.16163139045238495,
      "learning_rate": 0.00017059942911512845,
      "loss": 0.154,
      "step": 319
    },
    {
      "epoch": 0.45422285308729593,
      "grad_norm": 0.17442378401756287,
      "learning_rate": 0.00017050428163653663,
      "loss": 0.1623,
      "step": 320
    },
    {
      "epoch": 0.45564229950319374,
      "grad_norm": 0.1901973932981491,
      "learning_rate": 0.00017040913415794482,
      "loss": 0.1975,
      "step": 321
    },
    {
      "epoch": 0.45706174591909154,
      "grad_norm": 0.17201147973537445,
      "learning_rate": 0.000170313986679353,
      "loss": 0.1577,
      "step": 322
    },
    {
      "epoch": 0.45848119233498935,
      "grad_norm": 0.24799390137195587,
      "learning_rate": 0.00017021883920076119,
      "loss": 0.1743,
      "step": 323
    },
    {
      "epoch": 0.45990063875088716,
      "grad_norm": 0.1515909880399704,
      "learning_rate": 0.00017012369172216937,
      "loss": 0.1395,
      "step": 324
    },
    {
      "epoch": 0.46132008516678497,
      "grad_norm": 0.3801301419734955,
      "learning_rate": 0.00017002854424357755,
      "loss": 0.2097,
      "step": 325
    },
    {
      "epoch": 0.4627395315826828,
      "grad_norm": 0.2507898807525635,
      "learning_rate": 0.00016993339676498574,
      "loss": 0.1767,
      "step": 326
    },
    {
      "epoch": 0.4641589779985806,
      "grad_norm": 0.1987512707710266,
      "learning_rate": 0.00016983824928639392,
      "loss": 0.1428,
      "step": 327
    },
    {
      "epoch": 0.46557842441447833,
      "grad_norm": 0.1773303896188736,
      "learning_rate": 0.0001697431018078021,
      "loss": 0.167,
      "step": 328
    },
    {
      "epoch": 0.46699787083037614,
      "grad_norm": 0.15946342051029205,
      "learning_rate": 0.00016964795432921029,
      "loss": 0.1494,
      "step": 329
    },
    {
      "epoch": 0.46841731724627395,
      "grad_norm": 0.1415938287973404,
      "learning_rate": 0.00016955280685061847,
      "loss": 0.1197,
      "step": 330
    },
    {
      "epoch": 0.46983676366217175,
      "grad_norm": 0.1696440577507019,
      "learning_rate": 0.00016945765937202665,
      "loss": 0.1348,
      "step": 331
    },
    {
      "epoch": 0.47125621007806956,
      "grad_norm": 0.17328503727912903,
      "learning_rate": 0.00016936251189343484,
      "loss": 0.1301,
      "step": 332
    },
    {
      "epoch": 0.47267565649396737,
      "grad_norm": 0.14782625436782837,
      "learning_rate": 0.00016926736441484302,
      "loss": 0.1398,
      "step": 333
    },
    {
      "epoch": 0.4740951029098652,
      "grad_norm": 0.1644575446844101,
      "learning_rate": 0.0001691722169362512,
      "loss": 0.1488,
      "step": 334
    },
    {
      "epoch": 0.4755145493257629,
      "grad_norm": 0.13526898622512817,
      "learning_rate": 0.00016907706945765939,
      "loss": 0.1213,
      "step": 335
    },
    {
      "epoch": 0.47693399574166073,
      "grad_norm": 0.14072123169898987,
      "learning_rate": 0.00016898192197906757,
      "loss": 0.1249,
      "step": 336
    },
    {
      "epoch": 0.47835344215755854,
      "grad_norm": 0.12543220818042755,
      "learning_rate": 0.00016888677450047575,
      "loss": 0.1191,
      "step": 337
    },
    {
      "epoch": 0.47977288857345635,
      "grad_norm": 0.13075068593025208,
      "learning_rate": 0.00016879162702188394,
      "loss": 0.113,
      "step": 338
    },
    {
      "epoch": 0.48119233498935415,
      "grad_norm": 0.1628408282995224,
      "learning_rate": 0.00016869647954329212,
      "loss": 0.1083,
      "step": 339
    },
    {
      "epoch": 0.48261178140525196,
      "grad_norm": 0.12735773622989655,
      "learning_rate": 0.0001686013320647003,
      "loss": 0.0759,
      "step": 340
    },
    {
      "epoch": 0.48403122782114977,
      "grad_norm": 0.13596266508102417,
      "learning_rate": 0.00016850618458610849,
      "loss": 0.1073,
      "step": 341
    },
    {
      "epoch": 0.4854506742370476,
      "grad_norm": 0.10148149728775024,
      "learning_rate": 0.00016841103710751667,
      "loss": 0.0714,
      "step": 342
    },
    {
      "epoch": 0.4868701206529453,
      "grad_norm": 0.10629739612340927,
      "learning_rate": 0.00016831588962892485,
      "loss": 0.083,
      "step": 343
    },
    {
      "epoch": 0.48828956706884313,
      "grad_norm": 0.16768154501914978,
      "learning_rate": 0.00016822074215033304,
      "loss": 0.1239,
      "step": 344
    },
    {
      "epoch": 0.48970901348474094,
      "grad_norm": 0.15854743123054504,
      "learning_rate": 0.0001681255946717412,
      "loss": 0.0868,
      "step": 345
    },
    {
      "epoch": 0.49112845990063875,
      "grad_norm": 0.126465305685997,
      "learning_rate": 0.0001680304471931494,
      "loss": 0.0918,
      "step": 346
    },
    {
      "epoch": 0.49254790631653655,
      "grad_norm": 0.14185777306556702,
      "learning_rate": 0.00016793529971455756,
      "loss": 0.0713,
      "step": 347
    },
    {
      "epoch": 0.49396735273243436,
      "grad_norm": 0.10624835640192032,
      "learning_rate": 0.00016784015223596577,
      "loss": 0.0697,
      "step": 348
    },
    {
      "epoch": 0.49538679914833217,
      "grad_norm": 0.11770059168338776,
      "learning_rate": 0.00016774500475737393,
      "loss": 0.0699,
      "step": 349
    },
    {
      "epoch": 0.49680624556423,
      "grad_norm": 0.0840187519788742,
      "learning_rate": 0.00016764985727878214,
      "loss": 0.0478,
      "step": 350
    },
    {
      "epoch": 0.4982256919801277,
      "grad_norm": 0.6331724524497986,
      "learning_rate": 0.0001675547098001903,
      "loss": 0.5741,
      "step": 351
    },
    {
      "epoch": 0.49964513839602553,
      "grad_norm": 0.488374799489975,
      "learning_rate": 0.0001674595623215985,
      "loss": 0.446,
      "step": 352
    },
    {
      "epoch": 0.5010645848119234,
      "grad_norm": 0.38088110089302063,
      "learning_rate": 0.00016736441484300666,
      "loss": 0.4208,
      "step": 353
    },
    {
      "epoch": 0.5024840312278211,
      "grad_norm": 0.3756221830844879,
      "learning_rate": 0.00016726926736441487,
      "loss": 0.3902,
      "step": 354
    },
    {
      "epoch": 0.5039034776437189,
      "grad_norm": 0.2471628040075302,
      "learning_rate": 0.00016717411988582303,
      "loss": 0.3258,
      "step": 355
    },
    {
      "epoch": 0.5053229240596168,
      "grad_norm": 0.23875264823436737,
      "learning_rate": 0.00016707897240723124,
      "loss": 0.32,
      "step": 356
    },
    {
      "epoch": 0.5067423704755145,
      "grad_norm": 0.2846216559410095,
      "learning_rate": 0.0001669838249286394,
      "loss": 0.3069,
      "step": 357
    },
    {
      "epoch": 0.5081618168914124,
      "grad_norm": 0.2903178036212921,
      "learning_rate": 0.0001668886774500476,
      "loss": 0.2584,
      "step": 358
    },
    {
      "epoch": 0.5095812633073101,
      "grad_norm": 0.21420243382453918,
      "learning_rate": 0.00016679352997145576,
      "loss": 0.2632,
      "step": 359
    },
    {
      "epoch": 0.511000709723208,
      "grad_norm": 0.2651553452014923,
      "learning_rate": 0.00016669838249286397,
      "loss": 0.3097,
      "step": 360
    },
    {
      "epoch": 0.5124201561391057,
      "grad_norm": 0.22691239416599274,
      "learning_rate": 0.00016660323501427213,
      "loss": 0.258,
      "step": 361
    },
    {
      "epoch": 0.5138396025550035,
      "grad_norm": 0.2461789846420288,
      "learning_rate": 0.0001665080875356803,
      "loss": 0.2406,
      "step": 362
    },
    {
      "epoch": 0.5152590489709014,
      "grad_norm": 0.21322080492973328,
      "learning_rate": 0.0001664129400570885,
      "loss": 0.2157,
      "step": 363
    },
    {
      "epoch": 0.5166784953867991,
      "grad_norm": 0.22189216315746307,
      "learning_rate": 0.00016631779257849668,
      "loss": 0.245,
      "step": 364
    },
    {
      "epoch": 0.518097941802697,
      "grad_norm": 0.21649399399757385,
      "learning_rate": 0.00016622264509990486,
      "loss": 0.2343,
      "step": 365
    },
    {
      "epoch": 0.5195173882185947,
      "grad_norm": 0.2525191307067871,
      "learning_rate": 0.00016612749762131304,
      "loss": 0.212,
      "step": 366
    },
    {
      "epoch": 0.5209368346344926,
      "grad_norm": 0.2230282723903656,
      "learning_rate": 0.00016603235014272123,
      "loss": 0.2404,
      "step": 367
    },
    {
      "epoch": 0.5223562810503903,
      "grad_norm": 0.19831006228923798,
      "learning_rate": 0.0001659372026641294,
      "loss": 0.183,
      "step": 368
    },
    {
      "epoch": 0.5237757274662882,
      "grad_norm": 0.2026294767856598,
      "learning_rate": 0.0001658420551855376,
      "loss": 0.1909,
      "step": 369
    },
    {
      "epoch": 0.525195173882186,
      "grad_norm": 0.18979088962078094,
      "learning_rate": 0.00016574690770694578,
      "loss": 0.1757,
      "step": 370
    },
    {
      "epoch": 0.5266146202980837,
      "grad_norm": 0.21828339993953705,
      "learning_rate": 0.00016565176022835396,
      "loss": 0.1836,
      "step": 371
    },
    {
      "epoch": 0.5280340667139816,
      "grad_norm": 0.17623811960220337,
      "learning_rate": 0.00016555661274976214,
      "loss": 0.1601,
      "step": 372
    },
    {
      "epoch": 0.5294535131298793,
      "grad_norm": 0.14808376133441925,
      "learning_rate": 0.00016546146527117033,
      "loss": 0.1293,
      "step": 373
    },
    {
      "epoch": 0.5308729595457772,
      "grad_norm": 0.17691995203495026,
      "learning_rate": 0.00016536631779257848,
      "loss": 0.1526,
      "step": 374
    },
    {
      "epoch": 0.5322924059616749,
      "grad_norm": 0.17905530333518982,
      "learning_rate": 0.0001652711703139867,
      "loss": 0.1266,
      "step": 375
    },
    {
      "epoch": 0.5337118523775728,
      "grad_norm": 0.16115914285182953,
      "learning_rate": 0.00016517602283539485,
      "loss": 0.1249,
      "step": 376
    },
    {
      "epoch": 0.5351312987934705,
      "grad_norm": 0.1475640833377838,
      "learning_rate": 0.00016508087535680306,
      "loss": 0.1325,
      "step": 377
    },
    {
      "epoch": 0.5365507452093683,
      "grad_norm": 0.1882176697254181,
      "learning_rate": 0.00016498572787821122,
      "loss": 0.158,
      "step": 378
    },
    {
      "epoch": 0.5379701916252662,
      "grad_norm": 0.16310134530067444,
      "learning_rate": 0.00016489058039961943,
      "loss": 0.1441,
      "step": 379
    },
    {
      "epoch": 0.5393896380411639,
      "grad_norm": 0.1749742180109024,
      "learning_rate": 0.00016479543292102758,
      "loss": 0.1537,
      "step": 380
    },
    {
      "epoch": 0.5408090844570618,
      "grad_norm": 0.18974344432353973,
      "learning_rate": 0.0001647002854424358,
      "loss": 0.1483,
      "step": 381
    },
    {
      "epoch": 0.5422285308729595,
      "grad_norm": 0.15348966419696808,
      "learning_rate": 0.00016460513796384395,
      "loss": 0.1261,
      "step": 382
    },
    {
      "epoch": 0.5436479772888574,
      "grad_norm": 0.13731154799461365,
      "learning_rate": 0.00016450999048525216,
      "loss": 0.0929,
      "step": 383
    },
    {
      "epoch": 0.5450674237047551,
      "grad_norm": 0.1556616723537445,
      "learning_rate": 0.00016441484300666032,
      "loss": 0.1203,
      "step": 384
    },
    {
      "epoch": 0.5464868701206529,
      "grad_norm": 0.13559553027153015,
      "learning_rate": 0.00016431969552806853,
      "loss": 0.1004,
      "step": 385
    },
    {
      "epoch": 0.5479063165365508,
      "grad_norm": 0.13521873950958252,
      "learning_rate": 0.00016422454804947668,
      "loss": 0.122,
      "step": 386
    },
    {
      "epoch": 0.5493257629524485,
      "grad_norm": 0.13034307956695557,
      "learning_rate": 0.0001641294005708849,
      "loss": 0.1071,
      "step": 387
    },
    {
      "epoch": 0.5507452093683464,
      "grad_norm": 0.1721305400133133,
      "learning_rate": 0.00016403425309229305,
      "loss": 0.1119,
      "step": 388
    },
    {
      "epoch": 0.5521646557842441,
      "grad_norm": 0.15545590221881866,
      "learning_rate": 0.00016393910561370126,
      "loss": 0.0976,
      "step": 389
    },
    {
      "epoch": 0.553584102200142,
      "grad_norm": 0.1454247236251831,
      "learning_rate": 0.00016384395813510942,
      "loss": 0.0913,
      "step": 390
    },
    {
      "epoch": 0.5550035486160397,
      "grad_norm": 0.13429966568946838,
      "learning_rate": 0.00016374881065651763,
      "loss": 0.0924,
      "step": 391
    },
    {
      "epoch": 0.5564229950319376,
      "grad_norm": 0.1830882728099823,
      "learning_rate": 0.00016365366317792578,
      "loss": 0.0892,
      "step": 392
    },
    {
      "epoch": 0.5578424414478353,
      "grad_norm": 0.12371549755334854,
      "learning_rate": 0.000163558515699334,
      "loss": 0.0632,
      "step": 393
    },
    {
      "epoch": 0.5592618878637331,
      "grad_norm": 0.11389799416065216,
      "learning_rate": 0.00016346336822074215,
      "loss": 0.0821,
      "step": 394
    },
    {
      "epoch": 0.560681334279631,
      "grad_norm": 0.1552814394235611,
      "learning_rate": 0.00016336822074215033,
      "loss": 0.0961,
      "step": 395
    },
    {
      "epoch": 0.5621007806955287,
      "grad_norm": 0.1489168405532837,
      "learning_rate": 0.00016327307326355852,
      "loss": 0.0927,
      "step": 396
    },
    {
      "epoch": 0.5635202271114266,
      "grad_norm": 0.147613987326622,
      "learning_rate": 0.0001631779257849667,
      "loss": 0.091,
      "step": 397
    },
    {
      "epoch": 0.5649396735273243,
      "grad_norm": 0.10550914704799652,
      "learning_rate": 0.00016308277830637488,
      "loss": 0.0645,
      "step": 398
    },
    {
      "epoch": 0.5663591199432222,
      "grad_norm": 0.11544385552406311,
      "learning_rate": 0.00016298763082778307,
      "loss": 0.0705,
      "step": 399
    },
    {
      "epoch": 0.5677785663591199,
      "grad_norm": 0.09835031628608704,
      "learning_rate": 0.00016289248334919125,
      "loss": 0.0551,
      "step": 400
    },
    {
      "epoch": 0.5691980127750177,
      "grad_norm": 0.5734143257141113,
      "learning_rate": 0.00016279733587059943,
      "loss": 0.5311,
      "step": 401
    },
    {
      "epoch": 0.5706174591909156,
      "grad_norm": 0.3967488706111908,
      "learning_rate": 0.00016270218839200762,
      "loss": 0.42,
      "step": 402
    },
    {
      "epoch": 0.5720369056068133,
      "grad_norm": 0.3979634940624237,
      "learning_rate": 0.0001626070409134158,
      "loss": 0.4227,
      "step": 403
    },
    {
      "epoch": 0.5734563520227112,
      "grad_norm": 0.3215809464454651,
      "learning_rate": 0.00016251189343482398,
      "loss": 0.3438,
      "step": 404
    },
    {
      "epoch": 0.5748757984386089,
      "grad_norm": 0.28150179982185364,
      "learning_rate": 0.00016241674595623217,
      "loss": 0.3409,
      "step": 405
    },
    {
      "epoch": 0.5762952448545068,
      "grad_norm": 0.23088285326957703,
      "learning_rate": 0.00016232159847764035,
      "loss": 0.2856,
      "step": 406
    },
    {
      "epoch": 0.5777146912704045,
      "grad_norm": 0.24941927194595337,
      "learning_rate": 0.00016222645099904853,
      "loss": 0.3215,
      "step": 407
    },
    {
      "epoch": 0.5791341376863024,
      "grad_norm": 0.2608039975166321,
      "learning_rate": 0.00016213130352045672,
      "loss": 0.2984,
      "step": 408
    },
    {
      "epoch": 0.5805535841022001,
      "grad_norm": 0.2782004177570343,
      "learning_rate": 0.0001620361560418649,
      "loss": 0.3123,
      "step": 409
    },
    {
      "epoch": 0.5819730305180979,
      "grad_norm": 0.29299309849739075,
      "learning_rate": 0.00016194100856327308,
      "loss": 0.3224,
      "step": 410
    },
    {
      "epoch": 0.5833924769339958,
      "grad_norm": 0.2324187159538269,
      "learning_rate": 0.00016184586108468127,
      "loss": 0.2626,
      "step": 411
    },
    {
      "epoch": 0.5848119233498935,
      "grad_norm": 0.31871795654296875,
      "learning_rate": 0.00016175071360608945,
      "loss": 0.3211,
      "step": 412
    },
    {
      "epoch": 0.5862313697657914,
      "grad_norm": 0.2733614146709442,
      "learning_rate": 0.00016165556612749763,
      "loss": 0.288,
      "step": 413
    },
    {
      "epoch": 0.5876508161816891,
      "grad_norm": 0.2656559348106384,
      "learning_rate": 0.00016156041864890582,
      "loss": 0.2593,
      "step": 414
    },
    {
      "epoch": 0.589070262597587,
      "grad_norm": 0.2410850077867508,
      "learning_rate": 0.000161465271170314,
      "loss": 0.2606,
      "step": 415
    },
    {
      "epoch": 0.5904897090134847,
      "grad_norm": 0.2182936817407608,
      "learning_rate": 0.00016137012369172218,
      "loss": 0.2177,
      "step": 416
    },
    {
      "epoch": 0.5919091554293825,
      "grad_norm": 0.2415715456008911,
      "learning_rate": 0.00016127497621313037,
      "loss": 0.2419,
      "step": 417
    },
    {
      "epoch": 0.5933286018452804,
      "grad_norm": 0.18864968419075012,
      "learning_rate": 0.00016117982873453855,
      "loss": 0.1927,
      "step": 418
    },
    {
      "epoch": 0.5947480482611781,
      "grad_norm": 0.19137367606163025,
      "learning_rate": 0.00016108468125594673,
      "loss": 0.1817,
      "step": 419
    },
    {
      "epoch": 0.596167494677076,
      "grad_norm": 0.222140371799469,
      "learning_rate": 0.00016098953377735492,
      "loss": 0.2109,
      "step": 420
    },
    {
      "epoch": 0.5975869410929737,
      "grad_norm": 0.23214884102344513,
      "learning_rate": 0.0001608943862987631,
      "loss": 0.2105,
      "step": 421
    },
    {
      "epoch": 0.5990063875088716,
      "grad_norm": 0.19018523395061493,
      "learning_rate": 0.00016079923882017128,
      "loss": 0.1821,
      "step": 422
    },
    {
      "epoch": 0.6004258339247693,
      "grad_norm": 0.19725508987903595,
      "learning_rate": 0.00016070409134157947,
      "loss": 0.1668,
      "step": 423
    },
    {
      "epoch": 0.6004258339247693,
      "eval_loss": 0.18132758140563965,
      "eval_runtime": 350.2051,
      "eval_samples_per_second": 3.018,
      "eval_steps_per_second": 1.008,
      "step": 423
    },
    {
      "epoch": 0.6018452803406671,
      "grad_norm": 0.20987409353256226,
      "learning_rate": 0.00016060894386298765,
      "loss": 0.1645,
      "step": 424
    },
    {
      "epoch": 0.603264726756565,
      "grad_norm": 0.19481807947158813,
      "learning_rate": 0.00016051379638439583,
      "loss": 0.1913,
      "step": 425
    },
    {
      "epoch": 0.6046841731724627,
      "grad_norm": 0.2456100434064865,
      "learning_rate": 0.000160418648905804,
      "loss": 0.2019,
      "step": 426
    },
    {
      "epoch": 0.6061036195883606,
      "grad_norm": 0.21857067942619324,
      "learning_rate": 0.0001603235014272122,
      "loss": 0.198,
      "step": 427
    },
    {
      "epoch": 0.6075230660042583,
      "grad_norm": 0.1517402082681656,
      "learning_rate": 0.00016022835394862036,
      "loss": 0.1194,
      "step": 428
    },
    {
      "epoch": 0.6089425124201562,
      "grad_norm": 0.1431589126586914,
      "learning_rate": 0.00016013320647002857,
      "loss": 0.1407,
      "step": 429
    },
    {
      "epoch": 0.6103619588360539,
      "grad_norm": 0.1423029750585556,
      "learning_rate": 0.00016003805899143672,
      "loss": 0.1311,
      "step": 430
    },
    {
      "epoch": 0.6117814052519518,
      "grad_norm": 0.1388556808233261,
      "learning_rate": 0.0001599429115128449,
      "loss": 0.127,
      "step": 431
    },
    {
      "epoch": 0.6132008516678495,
      "grad_norm": 0.15656495094299316,
      "learning_rate": 0.0001598477640342531,
      "loss": 0.1219,
      "step": 432
    },
    {
      "epoch": 0.6146202980837473,
      "grad_norm": 0.1460929661989212,
      "learning_rate": 0.00015975261655566127,
      "loss": 0.1175,
      "step": 433
    },
    {
      "epoch": 0.6160397444996452,
      "grad_norm": 0.14177191257476807,
      "learning_rate": 0.00015965746907706946,
      "loss": 0.122,
      "step": 434
    },
    {
      "epoch": 0.6174591909155429,
      "grad_norm": 0.163153275847435,
      "learning_rate": 0.00015956232159847764,
      "loss": 0.1422,
      "step": 435
    },
    {
      "epoch": 0.6188786373314408,
      "grad_norm": 0.14655143022537231,
      "learning_rate": 0.00015946717411988582,
      "loss": 0.1119,
      "step": 436
    },
    {
      "epoch": 0.6202980837473385,
      "grad_norm": 0.1259157657623291,
      "learning_rate": 0.000159372026641294,
      "loss": 0.0865,
      "step": 437
    },
    {
      "epoch": 0.6217175301632364,
      "grad_norm": 0.14728648960590363,
      "learning_rate": 0.0001592768791627022,
      "loss": 0.1048,
      "step": 438
    },
    {
      "epoch": 0.6231369765791341,
      "grad_norm": 0.1266816407442093,
      "learning_rate": 0.00015918173168411037,
      "loss": 0.1025,
      "step": 439
    },
    {
      "epoch": 0.6245564229950319,
      "grad_norm": 0.12114408612251282,
      "learning_rate": 0.00015908658420551856,
      "loss": 0.1071,
      "step": 440
    },
    {
      "epoch": 0.6259758694109298,
      "grad_norm": 0.14592917263507843,
      "learning_rate": 0.00015899143672692674,
      "loss": 0.1268,
      "step": 441
    },
    {
      "epoch": 0.6273953158268275,
      "grad_norm": 0.12582072615623474,
      "learning_rate": 0.00015889628924833492,
      "loss": 0.0941,
      "step": 442
    },
    {
      "epoch": 0.6288147622427254,
      "grad_norm": 0.12897424399852753,
      "learning_rate": 0.0001588011417697431,
      "loss": 0.086,
      "step": 443
    },
    {
      "epoch": 0.6302342086586231,
      "grad_norm": 0.11238119751214981,
      "learning_rate": 0.0001587059942911513,
      "loss": 0.0952,
      "step": 444
    },
    {
      "epoch": 0.631653655074521,
      "grad_norm": 0.1291157603263855,
      "learning_rate": 0.00015861084681255947,
      "loss": 0.1033,
      "step": 445
    },
    {
      "epoch": 0.6330731014904187,
      "grad_norm": 0.0873965173959732,
      "learning_rate": 0.00015851569933396766,
      "loss": 0.0675,
      "step": 446
    },
    {
      "epoch": 0.6344925479063165,
      "grad_norm": 0.13512372970581055,
      "learning_rate": 0.00015842055185537584,
      "loss": 0.0967,
      "step": 447
    },
    {
      "epoch": 0.6359119943222143,
      "grad_norm": 0.14364920556545258,
      "learning_rate": 0.00015832540437678402,
      "loss": 0.0781,
      "step": 448
    },
    {
      "epoch": 0.6373314407381121,
      "grad_norm": 0.0919160544872284,
      "learning_rate": 0.0001582302568981922,
      "loss": 0.0646,
      "step": 449
    },
    {
      "epoch": 0.63875088715401,
      "grad_norm": 0.09288521856069565,
      "learning_rate": 0.0001581351094196004,
      "loss": 0.0578,
      "step": 450
    },
    {
      "epoch": 0.6401703335699077,
      "grad_norm": 0.5773417949676514,
      "learning_rate": 0.00015803996194100857,
      "loss": 0.6209,
      "step": 451
    },
    {
      "epoch": 0.6415897799858056,
      "grad_norm": 0.39017078280448914,
      "learning_rate": 0.00015794481446241676,
      "loss": 0.393,
      "step": 452
    },
    {
      "epoch": 0.6430092264017033,
      "grad_norm": 0.29224568605422974,
      "learning_rate": 0.00015784966698382494,
      "loss": 0.3267,
      "step": 453
    },
    {
      "epoch": 0.6444286728176012,
      "grad_norm": 0.3001405596733093,
      "learning_rate": 0.00015775451950523312,
      "loss": 0.3313,
      "step": 454
    },
    {
      "epoch": 0.6458481192334989,
      "grad_norm": 0.2834407389163971,
      "learning_rate": 0.0001576593720266413,
      "loss": 0.342,
      "step": 455
    },
    {
      "epoch": 0.6472675656493967,
      "grad_norm": 0.23928727209568024,
      "learning_rate": 0.0001575642245480495,
      "loss": 0.2847,
      "step": 456
    },
    {
      "epoch": 0.6486870120652946,
      "grad_norm": 0.2410030961036682,
      "learning_rate": 0.00015746907706945765,
      "loss": 0.3095,
      "step": 457
    },
    {
      "epoch": 0.6501064584811923,
      "grad_norm": 0.22346051037311554,
      "learning_rate": 0.00015737392959086586,
      "loss": 0.288,
      "step": 458
    },
    {
      "epoch": 0.6515259048970902,
      "grad_norm": 0.23195426166057587,
      "learning_rate": 0.00015727878211227401,
      "loss": 0.2825,
      "step": 459
    },
    {
      "epoch": 0.6529453513129879,
      "grad_norm": 0.20546920597553253,
      "learning_rate": 0.00015718363463368222,
      "loss": 0.251,
      "step": 460
    },
    {
      "epoch": 0.6543647977288858,
      "grad_norm": 0.24309487640857697,
      "learning_rate": 0.00015708848715509038,
      "loss": 0.2642,
      "step": 461
    },
    {
      "epoch": 0.6557842441447835,
      "grad_norm": 0.2406681925058365,
      "learning_rate": 0.0001569933396764986,
      "loss": 0.2356,
      "step": 462
    },
    {
      "epoch": 0.6572036905606813,
      "grad_norm": 0.21421076357364655,
      "learning_rate": 0.00015689819219790675,
      "loss": 0.2481,
      "step": 463
    },
    {
      "epoch": 0.6586231369765791,
      "grad_norm": 0.24608848989009857,
      "learning_rate": 0.00015680304471931496,
      "loss": 0.2426,
      "step": 464
    },
    {
      "epoch": 0.6600425833924769,
      "grad_norm": 0.22647297382354736,
      "learning_rate": 0.00015670789724072311,
      "loss": 0.2414,
      "step": 465
    },
    {
      "epoch": 0.6614620298083748,
      "grad_norm": 0.20979394018650055,
      "learning_rate": 0.00015661274976213132,
      "loss": 0.2067,
      "step": 466
    },
    {
      "epoch": 0.6628814762242725,
      "grad_norm": 0.1916702538728714,
      "learning_rate": 0.00015651760228353948,
      "loss": 0.1637,
      "step": 467
    },
    {
      "epoch": 0.6643009226401704,
      "grad_norm": 0.21306970715522766,
      "learning_rate": 0.0001564224548049477,
      "loss": 0.2083,
      "step": 468
    },
    {
      "epoch": 0.6657203690560681,
      "grad_norm": 0.18934592604637146,
      "learning_rate": 0.00015632730732635585,
      "loss": 0.1861,
      "step": 469
    },
    {
      "epoch": 0.6671398154719659,
      "grad_norm": 0.16308657824993134,
      "learning_rate": 0.00015623215984776406,
      "loss": 0.146,
      "step": 470
    },
    {
      "epoch": 0.6685592618878637,
      "grad_norm": 0.18044470250606537,
      "learning_rate": 0.00015613701236917221,
      "loss": 0.176,
      "step": 471
    },
    {
      "epoch": 0.6699787083037615,
      "grad_norm": 0.1645916849374771,
      "learning_rate": 0.00015604186489058042,
      "loss": 0.1327,
      "step": 472
    },
    {
      "epoch": 0.6713981547196594,
      "grad_norm": 0.15306499600410461,
      "learning_rate": 0.00015594671741198858,
      "loss": 0.1419,
      "step": 473
    },
    {
      "epoch": 0.6728176011355571,
      "grad_norm": 0.17582587897777557,
      "learning_rate": 0.0001558515699333968,
      "loss": 0.1426,
      "step": 474
    },
    {
      "epoch": 0.674237047551455,
      "grad_norm": 0.16724251210689545,
      "learning_rate": 0.00015575642245480495,
      "loss": 0.1479,
      "step": 475
    },
    {
      "epoch": 0.6756564939673527,
      "grad_norm": 0.14700044691562653,
      "learning_rate": 0.00015566127497621316,
      "loss": 0.136,
      "step": 476
    },
    {
      "epoch": 0.6770759403832506,
      "grad_norm": 0.1487903892993927,
      "learning_rate": 0.00015556612749762131,
      "loss": 0.1351,
      "step": 477
    },
    {
      "epoch": 0.6784953867991483,
      "grad_norm": 0.20699326694011688,
      "learning_rate": 0.0001554709800190295,
      "loss": 0.1403,
      "step": 478
    },
    {
      "epoch": 0.6799148332150461,
      "grad_norm": 0.15087048709392548,
      "learning_rate": 0.00015537583254043768,
      "loss": 0.1224,
      "step": 479
    },
    {
      "epoch": 0.681334279630944,
      "grad_norm": 0.1315482258796692,
      "learning_rate": 0.00015528068506184586,
      "loss": 0.1058,
      "step": 480
    },
    {
      "epoch": 0.6827537260468417,
      "grad_norm": 0.18645484745502472,
      "learning_rate": 0.00015518553758325405,
      "loss": 0.1342,
      "step": 481
    },
    {
      "epoch": 0.6841731724627396,
      "grad_norm": 0.19662021100521088,
      "learning_rate": 0.00015509039010466223,
      "loss": 0.1463,
      "step": 482
    },
    {
      "epoch": 0.6855926188786373,
      "grad_norm": 0.12735840678215027,
      "learning_rate": 0.00015499524262607041,
      "loss": 0.0963,
      "step": 483
    },
    {
      "epoch": 0.6870120652945352,
      "grad_norm": 0.1387183964252472,
      "learning_rate": 0.0001549000951474786,
      "loss": 0.0957,
      "step": 484
    },
    {
      "epoch": 0.6884315117104329,
      "grad_norm": 0.13882651925086975,
      "learning_rate": 0.00015480494766888678,
      "loss": 0.1095,
      "step": 485
    },
    {
      "epoch": 0.6898509581263307,
      "grad_norm": 0.17697463929653168,
      "learning_rate": 0.00015470980019029496,
      "loss": 0.0947,
      "step": 486
    },
    {
      "epoch": 0.6912704045422285,
      "grad_norm": 0.12020111083984375,
      "learning_rate": 0.00015461465271170315,
      "loss": 0.0928,
      "step": 487
    },
    {
      "epoch": 0.6926898509581263,
      "grad_norm": 0.11944457143545151,
      "learning_rate": 0.00015451950523311133,
      "loss": 0.0909,
      "step": 488
    },
    {
      "epoch": 0.6941092973740242,
      "grad_norm": 0.12041766196489334,
      "learning_rate": 0.00015442435775451951,
      "loss": 0.0907,
      "step": 489
    },
    {
      "epoch": 0.6955287437899219,
      "grad_norm": 0.11162368953227997,
      "learning_rate": 0.0001543292102759277,
      "loss": 0.084,
      "step": 490
    },
    {
      "epoch": 0.6969481902058198,
      "grad_norm": 0.11690417677164078,
      "learning_rate": 0.00015423406279733588,
      "loss": 0.0778,
      "step": 491
    },
    {
      "epoch": 0.6983676366217175,
      "grad_norm": 0.13275593519210815,
      "learning_rate": 0.00015413891531874406,
      "loss": 0.1012,
      "step": 492
    },
    {
      "epoch": 0.6997870830376153,
      "grad_norm": 0.12870202958583832,
      "learning_rate": 0.00015404376784015225,
      "loss": 0.0807,
      "step": 493
    },
    {
      "epoch": 0.7012065294535131,
      "grad_norm": 0.13579553365707397,
      "learning_rate": 0.00015394862036156043,
      "loss": 0.0824,
      "step": 494
    },
    {
      "epoch": 0.7026259758694109,
      "grad_norm": 0.1251567006111145,
      "learning_rate": 0.00015385347288296861,
      "loss": 0.0808,
      "step": 495
    },
    {
      "epoch": 0.7040454222853088,
      "grad_norm": 0.10862187296152115,
      "learning_rate": 0.0001537583254043768,
      "loss": 0.0766,
      "step": 496
    },
    {
      "epoch": 0.7054648687012065,
      "grad_norm": 0.10223617404699326,
      "learning_rate": 0.00015366317792578498,
      "loss": 0.0595,
      "step": 497
    },
    {
      "epoch": 0.7068843151171044,
      "grad_norm": 0.125504732131958,
      "learning_rate": 0.00015356803044719316,
      "loss": 0.0711,
      "step": 498
    },
    {
      "epoch": 0.7083037615330021,
      "grad_norm": 0.09378299862146378,
      "learning_rate": 0.00015347288296860135,
      "loss": 0.0569,
      "step": 499
    },
    {
      "epoch": 0.7097232079489,
      "grad_norm": 0.09513435512781143,
      "learning_rate": 0.00015337773549000953,
      "loss": 0.0519,
      "step": 500
    }
  ],
  "logging_steps": 1,
  "max_steps": 2112,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 3,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 3.452618747945779e+16,
  "train_batch_size": 3,
  "trial_name": null,
  "trial_params": null
}
