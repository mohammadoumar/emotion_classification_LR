{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "#import numpy as np\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "\n",
    "#from itertools import chain\n",
    "#from collections import Counter\n",
    "#from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Utilisateurs/umushtaq/emotion_analysis_comics/ddialogue/saved_models/dd_utt_Llama-3.2-1B-Instruct-bnb-4bit/ddialogue_results_2.pickle\", 'rb') as f:\n",
    "    \n",
    "    results = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = results['grounds']\n",
    "predictions = results['predictions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grounds), len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = [json.loads(x)[\"emotions\"] for x in grounds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [json.loads(x[\"content\"]) for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = [x['emotions'] for x in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 1000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(grounds), len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n",
      "369\n",
      "659\n"
     ]
    }
   ],
   "source": [
    "bad_idx = []\n",
    "\n",
    "for i, (l_g, l_p) in enumerate(zip(grounds, predictions)):\n",
    "    if len(l_g) != len(l_p):\n",
    "        print(i)\n",
    "        bad_idx.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = [item for i, item in enumerate(grounds) if i not in bad_idx]\n",
    "predictions = [item for i, item in enumerate(predictions) if i not in bad_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = [item for sublist in grounds for item in sublist]\n",
    "predictions = [item for sublist in predictions for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger      0.629     0.333     0.436       117\n",
      "     disgust      0.750     0.273     0.400        44\n",
      "        fear      0.500     0.294     0.370        17\n",
      "   happiness      0.678     0.551     0.608      1015\n",
      "     neutral      0.898     0.946     0.921      6269\n",
      "     sadness      0.472     0.245     0.323       102\n",
      "    surprise      0.509     0.483     0.496       116\n",
      "\n",
      "    accuracy                          0.863      7680\n",
      "   macro avg      0.634     0.446     0.508      7680\n",
      "weighted avg      0.851     0.863     0.854      7680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(grounds, predictions, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered = [(g, p) for g, p in zip(grounds, predictions) if g != \"neutral\"]\n",
    "\n",
    "# Separate the filtered ground truths and predictions\n",
    "filtered_grounds, filtered_predictions = zip(*filtered) if filtered else ([], [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1-score: 0.49326718639262934\n"
     ]
    }
   ],
   "source": [
    "if filtered_grounds:\n",
    "    micro_f1 = f1_score(filtered_grounds, filtered_predictions, average=\"micro\")\n",
    "    print(\"Micro F1-score:\", micro_f1)\n",
    "else:\n",
    "    print(\"No non-neutral classes to evaluate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_map = {\n",
    "    0: \"neutral\",\n",
    "    1: \"anger\",\n",
    "    2: \"disgust\",\n",
    "    3: \"fear\",\n",
    "    4: \"happiness\",\n",
    "    5: \"sadness\",\n",
    "    6: \"surprise\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_emotion_map = {v: k for k, v in emotion_map.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "grounds = [reverse_emotion_map[emotion] for emotion in grounds]\n",
    "predictions = [reverse_emotion_map[emotion] for emotion in predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_idxs = [idx for idx, elem in enumerate(grounds) if elem == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_y_true = [x for idx, x in enumerate(grounds) if idx not in neutral_idxs]\n",
    "filtered_y_pred = [x for idx, x in enumerate(predictions) if idx not in neutral_idxs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro F1-score (excluding class ): 0.4933\n",
      "Weighted F1-score (excluding class ): 0.6420\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "# Example ground truth (y_true) and predicted labels (y_pred)\n",
    "#y_true = np.array([0, 1, 1, 2, 2, 2, 0, 1, 2, 0])  # True labels\n",
    "#y_pred = np.array([0, 1, 0, 2, 2, 1, 0, 0, 2, 2])  # Predicted labels\n",
    "\n",
    "# Class to exclude\n",
    "#excluded_class = 0\n",
    "\n",
    "# Mask to filter out the excluded class\n",
    "#mask = grounds != excluded_class\n",
    "\n",
    "# Filtered labels\n",
    "#filtered_y_true = grounds[mask]\n",
    "#filtered_y_pred = predictions[mask]\n",
    "\n",
    "#filtered_y_true = [x for x in grounds if x != excluded_class]\n",
    "#filtered_y_pred = [x for x in predictions if x != excluded_class]\n",
    "\n",
    "# Compute F1-scores without the excluded class\n",
    "micro_f1 = f1_score(filtered_y_true, filtered_y_pred, average='micro')\n",
    "weighted_f1 = f1_score(filtered_y_true, filtered_y_pred, average='weighted')\n",
    "\n",
    "# Print results\n",
    "print(f\"Micro F1-score (excluding class ): {micro_f1:.4f}\")\n",
    "print(f\"Weighted F1-score (excluding class ): {weighted_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (triton_env)",
   "language": "python",
   "name": "ft_llama"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
